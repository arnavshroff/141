id,index,timestamp,eventType,contentId,authorPersonId,authorSessionId,authorUserAgent,authorRegion,authorCountry,contentType,url,title,text,lang,total_events
0,3096,1487246811,CONTENT SHARED,-4029704725707465084,6013226412048763966,-6569695881431984742,"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36",SP,BR,HTML,http://www.cnbc.com/2016/12/21/former-google-career-coach-shares-a-useful-visual-trick.html,former google career coach shares a visual trick for figuring out what to do with your life,"If you want 2017 to be an exciting year, design it that way. That's the advice of former Google career coach and job strategist Jenny Blake , who has helped more than a thousand people improve their work lives. She recommends creating a ""mind map,"" a visual diagram of your interests and goals. Drawing one doesn't take long and could help you figure out the next project, hobby or career change that will make the new year happier and more successful, Blake says. ""My favorite way to brainstorm creatively, whether it's about values or setting goals for the new year, is through mind maps,"" Blake tells CNBC. To make one, write down the year at the center of a piece of paper, and then draw spokes with different themes that are important to you. For example, your spokes could be business, personal life, health and fitness, fun, or skill building. From each of those themes, draw additional spokes to connect the themes to ways you want to improve or experiment in that area. Blake recommends you ask yourself: ""What's important to me about that? And what does success look like?"" For example, in ""personal life,"" you could write, ""Meet up with a friend at least once a month"" or ""Take a music class."" The visual trick works because it helps you see the different ways you can improve, Blake writes in her book "" Pivot: The Only Move That Matters Is Your Next One ."" ""With mind mapping you are taking pen to paper, going old-school,"" Blake says. And if you're like many professionals who sit too much and stare too long at a computer, drawing might actually be fun. ""The goal is to break out of linear thinking,"" Blake says. ""Go broad. Go big. Go sideways, and then experiment to see which of your ideas is most likely to lead to a resonant next step."" Video by Andrea Kramar.",en,433
1,1671,1467813728,CONTENT SHARED,-133139342397538859,4918484843075254252,-5701227433817087697,,,,HTML,http://gq.globo.com/Prazeres/Poder/Carreira/noticia/2016/06/novo-workaholic-trabalha-pratica-esportes-e-tem-tempo-para-familia-conheca.html,"novo workaholic trabalha, pratica esportes e tem tempo para a família. conheça","Novo workaholic não abre mão do esporte e da família (Foto: iStock) Se alguém, um dia, precisar reunir os personagens desta reportagem para uma rodada de negócios, fica aqui uma sugestão: marque o encontro na Vila Olímpica. Maratona, triatlo, judô, taekwondo e windsurfe são algumas das modalidades praticadas - em alguns casos com dedicação e resultados semiprofissionais - por este pequeno conjunto de executivos. Mas boa sorte com as agendas. Todos presidem empresas, obviamente com rotinas intensas, e dificilmente permitem que compromissos extracurriculares roubem o tempo sagrado para suas famílias ou, no caso dos solteiros, para os hobbies que fazem brilhar seus olhos. Representantes de uma geração que conviveu com chefes e colegas viciados em trabalho (e em certos momentos se confundiram com eles), esses profissionais fizeram do esporte a pedra angular para a construção de rotinas criativas. Sua dedicação a empresas tão diferentes entre si como P&G, Leo Burnett e Unisys não é avaliada pelo número de horas passadas no escritório, uma obsessão típica dos velhos baby boomers. ""Talvez porque eles não tivessem outros interesses na vida"", afirma Alberto Ogata, diretor técnico da Associação Brasileira de Qualidade de Vida (ABQV). Tampouco é medida pelos símbolos de status corporativo que conspiravam contra a saúde dos executivos, a começar pela vaga de estacionamento próxima ao elevador. Pelas novas regras com as quais eles estão subvertendo o jogo corporativo, o que vale são os resultados entregues e o modo como se dá essa entrega: com equilíbrio e foco invejáveis. É o que permite a altos executivos, em posições de destaque nas companhias que lideram, terem vidas saudáveis, interessantes e, quase sempre, divertidas. É o que faz deles a primeira geração dos chamados novos workaholics. O ponto de partida para essa mudança de paradigma encontra-se no Vale do Silício. O engravatado durão que demitia 10% de seus funcionários todo ano deu lugar ao ex-hippie. E as empresas passaram a trocar horários a cumprir por tarefas a executar Com idades entre 40 e 50 anos, eles estão no auge de suas carreiras, lideram multinacionais e têm vasta experiência internacional. São todos apaixonados pelos esportes que praticam, mas têm uma visão utilitária da atividade física. Em troca da disciplina férrea que tira a maioria deles da cama antes de o sol nascer, esperam benefícios (aplicáveis ao trabalho) como energia, serenidade e capacidade de executar impecavelmente sob pressão. Pragmaticamente, eles estão trocando quantidade por qualidade no trabalho porque entenderam que mais tempo no escritório não significa mais trabalho feito. Um estudo recente da Business Roundtable, uma associação de CEOs de grandes empresas nos EUA, concluiu que embora um pico concentrado de muitas horas de trabalho possa aumentar o rendimento em prazos curtos (fechamentos de trimestre, lançamento de produtos, etc.), longas jornadas sistemáticas tendem a diminuir a produtividade. Os pesquisadores constataram que pessoas trabalhando 60 horas por semana - ou 12 horas por dia - durante dois meses não produziram mais do que num regime tradicional de 40 horas semanais - ou oito horas diárias. Já uma semana de 80 horas provoca burnout em menos de um mês. Há raízes históricas para a transformação em curso. ""No início dos anos 80 tivemos as primeiras manifestações de executivos que se saturaram da busca pelo sucesso, justamente por terem percebido que o custo de alcançá-lo é imenso"", afirma o professor Esdras Vasconcellos, do Departamento de Psicologia Social e do Trabalho da Universidade de São Paulo. ""Naquela época, reduzir o ritmo era mais difícil. Então, alguns abdicaram da carreira executiva."" Ouviam-se casos anedóticos de executivos que largavam tudo para abrir uma pousada no Nordeste. No auge da onda workaholic, eles tiveram o insight que originou o que Vasconcellos chama de ecologia da vida. ""Esse novo pensamento não traz uma abdicação da vida civilizada, moderna, até certo ponto consumista"", pondera. ""Traz, sim, uma leveza simbolizada pela troca do tênis (longas partidas, alta competitividade) pela ioga (meditação, retiros espirituais)."" Geograficamente, o ponto de partida para essa mudança de paradigma encontra-se no Vale do Silício - não por acaso, na Califórnia. Volte aos anos 60. Ali floresceu todo o movimento hippie, toda a contracultura que influenciou São Francisco e região. ""Esses anos revolucionários romperam totalmente com valores antigos"", afirma Vasconcellos. Duas décadas depois, a Califórnia estava no epicentro do surgimento da informática, do computador pessoal como facilitador de processos de trabalho. Placas tectônicas do mundo corporativo se moveram durante o período de ascensão e estrelato de Steve Jobs. Da sua explosão como empreendedor, nos anos 80, à sua assimilação pelo establishment empresarial na primeira década do século 21 - que coincide com o caso de Jack Welch, até então o grande ícone do executivo vencedor e o protótipo do workaholic. O engravatado durão que demitia 10% de seus funcionários todo ano deu lugar ao ex-hippie com grande apetite por LSD, meditação e comida vegana. Era o início de uma revolução cultural. Pragmática. As empresas, ao perceber que a permissão para que as pessoas trabalhem com um pouco mais de leveza acaba trazendo resultados, decidiram relaxar. Trocaram horários a cumprir por tarefas a executar. Os ambientes de trabalho ganharam quadras de esporte, áreas de lazer, restaurantes gourmet. Se workaholic é o profissional que não consegue ou tem grande dificuldade de se desconectar do trabalho, o que importa não é o número de hobbies e atividades esportivas que ele encaixa na rotina. O desafio para esse executivo é o poder de se desligar Esse movimento ganhou impulso com uma mudança na cultura das empresas na década de 90, quando começa a implantação dos programas de qualidade de vida. A geração que estava, então, em início de carreira já recebeu esse novo modelo mental de autogerenciamento nos seus anos de formação. ""Esses executivos têm consciência elevada e tratam a saúde como elemento de sustentabilidade pessoal"", diz Carlos Legal, sócio da consultoria Legalas, especializada em educação corporativa. Parte deles abre mão voluntariamente de posições no topo das organizações. Todavia, há aqueles que querem tudo: chegar ao topo de suas empresas, porém sem sacrificar vida pessoal e vida familiar. Na visão de Vicky Bloch, uma das mais renomadas coachs de CEOs do país, o perfil predominante no topo das companhias brasileiras ainda é o do workaholic clássico. Profissionais atléticos são pontos fora da curva e não um retrato fiel da realidade do Brasil, onde 51% das mulheres e 47% dos homens hoje são sedentários. A obesidade é uma epidemia, sobretudo na base da pirâmide organizacional das empresas. Mas isso está mudando, e rapidamente. ""Estamos em transição"", nota Vicky. A mudança maior está se dando com a entrada no primeiro escalão das empresas de executivos na faixa de 35 a 45 anos. Pessoas que já são associadas ao que ela chama de geração flexível, no que diz respeito à sua relação com o tempo e o espaço em que o trabalho é executado. Gente mais saudável, sem dúvida, mas não necessariamente menos viciada em trabalho, pondera Vicky. Afinal, se workaholic é o sujeito que não consegue se desconectar do trabalho, o que importa não é o número de hobbies e atividades esportivas que ele encaixa na rotina. O desafio é o poder de se desligar. Alguns sintomas típicos do antigo vício em trabalho, é bem verdade, tornam-se mais raros na geração flexível - como os casamentos desfeitos. De acordo com as pesquisas de Bryan Robinson, psicoterapeuta, professor emérito da Universidade da Carolina do Norte e autor do livro Chained to the Desk (""Acorrentado à Mesa"", sem edição em português), casamentos com ao menos um cônjuge workaholic têm 40% mais riscos de acabar em divórcio. Contudo, se a cachaça do workaholic é o trabalho, a dependência, em muitos casos, continua presente. ""Só que hoje ele não trabalha apenas no escritório"", diz Vicky. ""Trabalha em casa, no carro, correndo."" O próximo desafio, então, talvez seja completar a transição desses novos workaholics em autênticos pós-workaholics. A troca da guarda geracional, sem dúvida, está fazendo sua parte. Basta conferir o depoimento de Ricardo Sangion, principal executivo da rede social Pinterest no Brasil. Mais jovem do time aqui reunido, ele se orgulha de dormir muito, trabalhar pouco, entregar resultados e ainda ter tempo para administrar um site, um bar e uma pousada na Bahia. Nem tudo, porém, se resume à idade. Valores antes reprimidos estão sendo integrados à vida executiva. ""A espiritualidade passa a ser importante"", exemplifica o professor Vasconcellos. E há o poderoso efeito demonstração. ""As empresas começam a perceber que até profissionais na posição de CEO já não estão mais disponíveis para alcançar resultados a qualquer preço"", observa o consultor Carlos Legal. Ao dizer que não estão dispostos a fazer qualquer tipo de sacrifício para chegar ao topo, esses profissionais aceleram o processo de mudança no próprio modo de funcionar das organizações.",pt,315
2,1814,1468867647,CONTENT SHARED,-6783772548752091658,4918484843075254252,-8995217520473210153,,,,HTML,http://www.caroli.org/livro-retrospectivas-divertidas/,livro: retrospectivas divertidas,"Neste livro, nós fornecemos um conjunto de ferramentas de atividades para transformar um grupo de pessoas em um time eficaz. Manter a diversão para os participantes e proporcionar um ambiente onde todos poderão refletir e discutir sem perder a diversão é fundamental para a melhoria contínua. Apresentaremos aqui um catálogo de atividades diversas, apropriadas, uma a uma, a diferentes contextos e equipes. Todas equipes devem fazer uma retrospectiva por semana, a menos que estejam sem tempo. Daí devem fazer duas!",pt,294
3,1317,1465484901,CONTENT SHARED,8657408509986329668,-8020832670974472349,838596071610016700,,,,HTML,https://medium.com/practical-blend/pull-request-first-f6bb667a9b6,pull request first - practical blend,"Pull request first After two years of working with pull requests a bit differently from the norm, I now take for granted that most developers submit pull requests only once their work is ""ready"" for review. Obviously... right? Or... not? There's a different approach I'd argue provides more transparency to your team, makes sure everyone is aligned and helps to keep a historical record of what decisions were made when developing a feature. Simply... Open a pull request before you write code.* Wait, what? Rather than treating pull requests solely as means for reviewing code, use the pull request as the master of record for the work related to a feature. This isn't a new idea - this is an approach our product team at bitHound practiced for nearly 2 years, inspired by GitHub itself ( Zach Holman's talk explains it). * And yes, technically GitHub requires at least one commit to open a pull request, so you have to write something first. There are several benefits that we saw from adopting this approach: Increased transparency Your Open Pull Requests view on GitHub becomes an instant snapshot of the current state of dev work in your company. Isn't that what the scrum board is for? Oh, sure it is - and once everyone races to move/write their stickies 30 seconds before standup, you'll get a similar view (for about 5 minutes). Specs as you go By using checkboxes in the pull request description , you can build out the current list of todos, what issues the pull request addresses, and update that as you uncover hidden gotchas during development. Those checkbox counts even bubble up to your pull request list for easy at-a-glance status updates. This allows other collaborators to know what the progress is on the feature, and let's them jump in at the appropriate spot. We'd often use these checkboxes to track issues we discovered in development rather than opening new tickets. Or, reference existing issues that will get closed when the pull request gets merged . Continuous review Why review when someone has ""finished"" only to realize they might have taken a wrong turn somewhere early in development? With open pull requests as you work, reviews can be done as the work is done. Sure this isn't always practical when everyone is super busy, but it works great when you're bringing a new team member on board, or just working on a doozy of a feature. It's helped catch problems before they become disasters. Great for asynchronous/remote work While our work style was very much centred around in-person discussion, we did have enough occasions where someone was working remotely (home, different country, whatever). I found that this is where the pull request was critical in understanding the state of the project. As long as you kept the pull request current, it didn't matter if you missed an earlier Slack or office discussion that changed its direction. What's the catch? Is everyone on board? Like any approach to organizing your work, it's only as good as the effort the team puts into it. So you need to keep each other honest to make sure pull requests are updated in a timely fashion. Does it scale? I don't know. Like any methodology, make sure it makes sense for you. When it stops working, tweak something or look for alternatives. What's the priority? Unlike a scrum board, there is no clear indication of progress as you can't reorder open pull requests (without the use of some extensions at least), but you can probably get away with a good labeling/milestone system that helps alleviates this.",en,294
4,588,1461629452,CONTENT SHARED,-6843047699859121724,7527226129639571966,-1297230017812472163,,,,HTML,https://medium.com/@jeffersoncn/ganhe-6-meses-de-acesso-ao-pluralsight-maior-plataforma-de-treinamento-online-dd2b1c9a22b9,"ganhe 6 meses de acesso ao pluralsight, maior plataforma de treinamento online","Ganhe 6 meses de acesso ao Pluralsight, maior plataforma de treinamento online Muitos não sabem, mas o Pluralsight é uma das melhores, senão a melhor, plataforma de treinamentos online. Com quase 5000 cursos nas áreas de desenvolvimento, infraestrutura, dados, entre outras e que são ministrados por mais de 800 especialistas, é fácil reconhecer o valor de se ter acesso a uma biblioteca tão rica de conhecimento como esta. Abaixo temos exemplos de alguns cursos dentre a lista dos mais populares da plataforma: Building a Web App with ASP.NET Core RC1, MVC 6, EF7 & AngularJS Angular 2: Getting Started ASP.NET Core 1.0 Fundamentals C# Fundamentals with Visual Studio 2015 Building Applications with ASP.NET MVC 4 JavaScript Best Practices ASP.NET MVC 5 Fundamentals TypeScript Fundamentals Docker for Web Developers Applying Functional Principles in C# ASP.NET in Multi-tenant App, Examples in MVC, ExtJS, and Angular Python Fundamentals Java Fundamentals: The Java Language Rapid JavaScript Training C# Best Practices: Improving on the Basics Design Patterns Library Getting Started with Entity Framework 6 Rebuilding Web Forms Applications in MVC Advanced JavaScript Building Your First Xamarin.Android App from Start to Store Building Web Applications with Node.js and Express 4.0 WCF End-to-End WPF and MVVM: Test Driven Development of ViewModels Se interessou por algum deles? E que tal ganhar uma assinatura gratuita de 6 meses com acesso completo à toda essa coleção de cursos do Pluralsight e começar agora a aprender aquelas habilidades que você vem buscado desenvolver? A Microsoft em parceria com o Pluralsight nos oferecem essa oportunidade! Quer saber como? Então vamos lá! Primeiramente precisamos nos inscrever no programa Visual Studio Dev Essentials utilizando uma conta Microsoft ( hotmail , live ou outlook ). O cadastro é simples e gratuito e você pode fazê-lo nesse link : 2. Agora que já temos acesso ao Visual Studio Dev Essentials , podemos então clicar em "" Get Code "" que fica próximo ao logo do Pluralsight . 3. Ao clicar em "" Get Code "" aparecerá em seu lugar "" Activate "", que ao ser clicado, vai nos redirecionar para a página de cadastro do Pluralsight . 4. No formulário de cadastro, o campo do código já estará preenchido, bastando entrar com as informações restantes antes de clicar em "" Activate Benefit "". 5. Pronto! Você pode agora aproveitar os seus 6 meses de estudos no Pluralsight gratuitamente. Dica extra pra quem tem condição de aluno verificada no Dreamspark Também é possível obter uma assinatura de 3 meses gratuitamente no Pluralsight através do Dreamspark . No catálogo de software do Dreamspark , ao rolar a página até o final e chegar na sessão "" Treinamento e Certificação "", clique no logo do Pluralsight ou acesse esse link . Selecione o idioma: English e clique em "" Obter chave "". No final da página há um link para o resgate do código que irá redirecionar para o cadastro no Pluralsight . Preencha os dados, ative o benefício e pronto! Agora você tem um acesso de 3 meses a todos os cursos do Pluralsight ! Espero que aproveitem bem os estudos! Até a próxima!",pt,281
5,2308,1473593226,CONTENT SHARED,-2358756719610361882,-9120685872592674274,5016857076790212194,,,,HTML,http://www.attps.com.br/cinco-motivos-para-investir-em-automacao-de-testes-e-reduzir-o-custo-do-erro/,custo do erro - cinco motivos para investir em automação de testes,"Atualmente, o custo de manutenção de software representa cerca de 50% do custo de uma empresa desenvolvedora de software. O Instituto de Pesquisa Triângulo (RTI), sediado no Estados Unidos, realizou um estudo para o Instituto Nacional de Padrões e Tecnologia (NIST) dos EUA, em 2002, para estimar o impacto do teste de software inadequado sobre a economia dos Estados Unidos. As suas conclusões foram que esses custos ou perdas financeiras ocasionadas por defeitos de software ficaram na ordem de US$ 59,5 bilhões, ou 0,06% do PIB dos EUA. Cerca de 37%, ou US$ 22 bilhões, deste custo poderia ser evitado caso fossem aplicadas técnicas de automação de testes de forma correta, com infrastrutura adequada e realizados de forma sistemática. Um estudo recente da Universidade de Cambridge mostra que o custo global com defeitos de software, nas atividades de encontro e remoção dos defeitos, cresce anualmente a uma taxa de US$ 312 bilhões de dólares. Este custo representa em média a metade do tempo de desenvolvimento de projetos de software. Os desenvolvedores, em média, gastam 50,1% do seu tempo em atividades que não são de desenvolvimento. Metade do tempo útil do programador é gasto em atividades de correção de defeitos. Diante destes custos elevadíssimos, destacamos CINCO MOTIVOS PARA INVESTIR EM AUTOMAÇÃO DE TESTES DE SOFTWARE PARA REDUZIR ERROS EM PRODUÇÃO. 1- Teste manual sozinho não consegue ter a mesma abrangência de execução dos testes automatizados Cada grupo de desenvolvimento de software testa seus próprios produtos, contudo software entregues sempre tem defeitos. Os analistas de teste se esforçam para pegá-los antes do produto ser lançado, mas os defeitos insistem em reaparecer, mesmo com os melhores processos de testes manuais. Automação de testes de software é a melhor maneira de aumentar a eficácia, a eficiência e a cobertura de execução do seu teste de software, aumentando assim a confiabilidade e a estabilidade dos sistemas em ambiente de produção. 2- Teste manual é executado por seres humanos. E seres humanos invariavelmente falham. Teste de software manual é uma atividade realizada por um ser humano sentado em frente ao computador, que de forma cuidadosa, passa por telas de aplicativos, tentando várias combinações de uso e de entrada, comparando os resultados com o comportamento esperado e evidenciando as suas observações. Testes manuais são repetidos por várias vezes durante os ciclos de desenvolvimento de alterações no código fonte e outras situações, como vários ambientes operacionais e configurações de hardware. Estas repetições de teste manuais podem ser falhas, cada pessoa pode interpretar uma ação ou resultado de forma diferente no momento da execução do teste ou ainda, se ausentar por diversos motivos, gerando atrasos ao projeto. 3- Teste automatizado é executado por robôs, 24 horas por dia, 7 dias por semana. Uma ferramenta de testes automatizados é capaz de reproduzir ações pré-gravadas e pré-definidas, comparar os resultados com o comportamento esperado e relatar o sucesso ou fracasso desses testes manuais para um analista de teste. Uma vez que os testes automatizados são criados, eles podem facilmente ser repetidos e estendidos, uma tarefa impossível com os testes manuais. Devido a isso, os gerentes experientes descobriram que o teste de software automatizado é um componente essencial dos projetos de desenvolvimento bem-sucedidos. 4- A cobertura de teste aumenta de forma significativa nos testes automatizados Teste de software automatizado pode aumentar a profundidade e o alcance de testes para ajudar a melhorar a qualidade do software. Testes longos e que muitas vezes são evitados durante o teste manual podem ser executados sem supervisão. Eles podem até mesmo ser executados em vários computadores com diferentes configurações. Teste de software automatizado pode olhar para dentro de um aplicativo e ver o conteúdo da memória, tabelas de dados, conteúdo de arquivos, comparar imagem de telas, e os estados internos do programa para determinar se o produto está se comportando conforme o esperado. Automação de teste pode facilmente executar milhares de diferentes casos de teste complexos durante cada teste, fornecendo uma cobertura que é impossível com testes manuais. 5- Teste automatizado de software economiza tempo e dinheiro Testes de software tem que ser repetido muitas vezes durante os ciclos de desenvolvimento, para garantir que a qualidade seja mantida em níveis aceitáveis. Cada código fonte alterado precisará ter testes de software repetido. Para cada versão do software liberada, pode ser necessário que seja testado em todos os sistemas operacionais e configurações de hardware. Fazer isto manualmente é uma tarefa muito dispendiosa e demorada. Uma vez criado, os testes automatizados podem ser executados uma e outra vez sem nenhum custo adicional, e eles são muito mais rápidos do que os testes manuais. Teste de software automatizado pode reduzir o tempo para executar testes repetitivos de dias para horas, com um nível de acertividade muito maior do que os seres humanos podem conseguir realizar. A economia de tempo é traduzida diretamente em redução de custos, aumento de qualidade e satisfação do cliente. Veja neste comparativo entre teste manual e teste automatizado. O custo inicial do teste automatizado é de fato maior, mas é um custo que se paga já a partir do quinto sprint. E o custo do teste manual continua a crescer de forma linear até não poder ser mais viável, onde passa a deixar de ser feito e irá gerar custos de manutenção corretiva, além de desgastes na relação com o cliente e perda de mercado do seu produto. O ROI (return of investment) dos testes automatizados sobre teste manual pode ser da ordem de 63% ao final de um ano. Algo espetacular, concorda? Por Rodrigo Almeida, Gerente de Qualidade de Software",pt,280
6,1902,1469678235,CONTENT SHARED,-8208801367848627943,-3390049372067052505,2045534933671019150,,,,HTML,http://www.geekwire.com/2016/ray-kurzweil-world-isnt-getting-worse-information-getting-better/,ray kurzweil: the world isn't getting worse - our information is getting better,"Ray Kurzweil, the author, inventor, computer scientist, futurist and Google employee, was the featured keynote speaker Thursday afternoon at Postback , the annual conference presented by Seattle mobile marketing company Tune. His topic was the future of mobile technology. In Kurzweil's world, however, that doesn't just mean the future of smartphones - it means the future of humanity. Continue reading for a few highlights from his talk. On the effect of the modern information era: People think the world's getting worse, and we see that on the left and the right, and we see that in other countries. People think the world is getting worse. ... That's the perception. What's actually happening is our information about what's wrong in the world is getting better. A century ago, there would be a battle that wiped out the next village, you'd never even hear about it. Now there's an incident halfway around the globe and we not only hear about it, we experience it. On the potential of human genomics: It's not just collecting what is basically the object code of life that is expanding exponentially. Our ability to understand it, to reverse-engineer it, to simulate it, and most importantly to reprogram this outdated software is also expanding exponentially. Genes are software programs. It's not a metaphor. They are sequences of data. But they evolved many years ago, many tens of thousands of years ago, when conditions were different. How technology will change humanity's geographic needs: We're only crowded because we've crowded ourselves into cities. Try taking a train trip across the United States, or Europe or Asia or anywhere in the world. Ninety-nine percent of the land is not used. Now, we don't want to use it because you don't want to be out in the boondocks if you don't have people to work and play with. That's already changing now that we have some level of virtual communication. We can have workgroups that are spread out. ... But ultimately, we'll have full-immersion virtual reality from within the nervous system, augmented reality. On connecting the brain directly to the cloud: We don't yet have brain extenders directly from our brain. We do have brain extenders indirectly. I mean this (holds up his smartphone) is a brain extender. ... Ultimately we'll put them directly in our brains. But not just to do search and language translation and other types of things we do now with mobile apps, but to actually extend the very scope of our brain. Why machines won't displace humans: We're going to merge with them, we're going to make ourselves smarter. We're already doing that. These mobile devices make us smarter. We're routinely doing things we couldn't possibly do without these brain extenders.",en,266
7,1942,1469992520,CONTENT SHARED,2581138407738454418,6756039155228175109,611377856408166257,,,,HTML,https://medium.com/@rdsubhas/10-modern-software-engineering-mistakes-bc67fbef4fc8,10 modern software over-engineering mistakes,"10 Modern Software Over-Engineering Mistakes Few things are guaranteed to increase all the time: Distance between stars, Entropy in the visible universe, and Fucking business requirements . Many articles say Dont over-engineer but don't say why or how. Here are 10 clear examples. Important Note: Some points below like ""Don't abuse generics"" are being misunderstood as ""Don't use generics at all"", ""Don't create unnecessary wrappers"" as ""Don't create wrappers at all"", etc. I'm only discussing over-engineering and not advocating cowboy coding. 1. Engineering is more clever than Business Engineers think we're the smartest people around because we build stuff. This first mistake often makes us over-engineer. But if we plan for 100 things, Business will always come up with the 101st thing we never thought of. If we solve 1,000 problems, they will come back with 10,000 problems. We think we have everything under control - but we have no clue what's headed our way. In my 15 year involvement with coding, I have never seen a single business ""converge"" on requirements. They only diverge. Its simply the nature of business and its not the business people's fault. TL;DR - The House (Business) Always Wins Tip: If you don't have time to go through the entire post, then this one point is enough. 2. Reusable Business Functionality When Business throws more and more functionality (as expected), we sometimes react like this: We try to group and generalize logic as much as possible. This is why most MVC systems end up in either Fat Models or Fat Controllers. But as we saw already, business requirements only diverge, they never converge. Instead, how should we have reacted: Shared logic and abstractions tend to stabilise over time in natural systems. They either stay flat or relatively go down as functionality gets broader. When the opposite happens, it creates systems that are Too big to fail (leading closer to the dreaded rewrite). Example: We created a User profile system for a previous client. Started with a CRUD controller with shared functionality because we assumed everything is going to be similar. But ended up with 13 different signup flows - initial social connection, a long signup form upon first entry, smaller and edit page sections, completely different looking profile page and so on and on - that it made very little sense to share stuff in the end. Similarly, an Order View and Order Edit flow ends up so inherently different from the actual Ordering flow. Try to vertically split business functionality first before splitting horizontally. This works in all cases - isolated services, trunk-based services, language-specific modules, etc. Also helps to switch from one form to another with ease. Otherwise it becomes increasingly complicated to change parts of the system. TL;DR - Prefer Isolating Actions than Combining Tip: Pick one external-facing action (Endpoint/Page/Job/etc) in your codebase. How many context switches does someone need to understand what's going on? 3. Everything is Generic (Sometimes goes together with previous point, but also seen applied individually in separate projects) Want to connect to a database? Lets write a Generic Adapter Query that database? Generic Query Pass it some params? Generic Params Build those params? Generic Builder Map the response? Generic Data Mapper Handle the user request? Generic Request Execute the whole thing? Generic Executor and so on and so on Sometimes Engineers get carried away. Instead of trying to solve the business problem, we waste our time trying to find the perfect abstractions. The answer is so simple. Designs are always playing catch up to changing real world requirements. So even if we found a perfect abstraction by miracle, it comes tagged with an expiry date because #1 - The House wins in the end. The best quality of a Design today is how well it can be undesigned. There is an amazing article on write code that is easy to delete, not easy to extend . TL;DR - Duplication is better than the wrong abstraction Conversely, Duplication is sometimes essential for the right abstraction. Because only when we see many parts of the system share ""similar"" code, a better shared abstraction emerges. The Quality of Abstraction is in the weakest link. Duplication exposes many use cases and makes boundaries clearer. Tip: Shared abstractions across services sometimes leads to Microservices ending up as a Distributed Monolith . 4. Shallow Wrappers The practice of wrapping every external library before using it. Unfortunately most wrappers we write are Shallow. We are juggling between delivering functionality and writing a good wrapper. So our wrappers are mostly tightly bound to the underlying library (in some cases being a 1:1 mirror, or doing 1/10th of what the original library does with 10x effort). If we change the underlying library later, usages of this wrapper everywhere usually end up having to be changed as well. Sometimes we also mix up business logic inside wrappers, making it neither a good wrapper nor a good business solution, but some kind of gluey layer in between. This is 2016. External libraries and clients have improved leaps and bounds. OSS Libraries are fantastic. They have high quality and well tested codebases written by awesome people, who have had dedicated, focused time writing it. Most have clear, testable, instrumentable APIs, allowing us to follow the standard pattern of Initialize - Instrument - Implement. TL;DR - Wrappers are an exception, not the norm. Don't wrap good libraries for the sake of wrapping Tip: Creating an ""agnostic"" wrapper is no laughing matter. ""Swap out library"" comes from a mindset of ""Configurability"" which is covered in detail in the ""<X>-ity"" section later. 5. Applying Quality like a Tool Blindly applying Quality concepts (like changing all variables to ""private final"", writing an interface for all classes, etc) is NOT going to make code magically better. Check Enterprise FizzBuzz (or Hello World ). It has a gazillion code. In the micro-level each class follows SOLID principles, uses all sorts of great Design patterns (factory, builder, strategy, etc) and coding techniques (generics, enums, etc). It gets high Code quality ratings from CQM tools. But if we take a step back, this prints Fizz Buzz . TL;DR - Always take a step back and look at the macro picture Conversely, automated CQM tools are good at tracking Test coverage, but can't tell whether we are testing the right thing. A benchmark tool can track performance, but can't tell whether stuff runs parallel or sequential. Only a Human has to look at the big picture. Which takes us to... 5.1. Sandwich Layers Lets take a concise, closely bound action and split it into 10 or 20 sandwiched layers, where none of the individual layers make any sense without the whole. Because we want to apply the concept of ""Testable code"", or ""Single Responsibility Principle"", or something. In the Past  - this was done by a chain of Inheritance. A extends B extends C extends D and so on. Now  - People do the exact same thing, except they make each class have an interface/implementation and inject it into the next layer, because duh SOLID. Concepts like SOLID came up in response to abuse of Inheritance and other OOP concepts. Most engineers are unaware of where/why these concepts came from, but just end up following the memo. TL;DR - Concepts need shift in Mindset. Cannot be applied blindly like tools. Learn a different language and try the other mindset of doing things. That makes a fundamentally better developer. Pouring old wine in a new labelled bottle doesn't work for concepts. We never have to tear apart a clear design in the name of applying a concept. 6. Overzealous Adopter Syndrome Discovered Generics. Now even a simple ""HelloWorldPrinter"" becomes ""HelloWorldPrinter<String,Writer>"". Don't use Generics when its obvious that a problem handles only specific data types, or when normal type signatures are enough. Discovered Strategy Pattern. Every ""if"" condition is now a strategy. Why? Discovered how to write a DSL. Gonna use DSLs everywhere. I don't know... Used Mocks. Gonna mock every single object I'm testing. how to even... Metaprogramming is awesome, let me use it everywhere describe why... Enums/Extension Methods/Traits/whatever are awesome, let me use it everywhere this is wrong. TL;DR - TL;DRs should not be used everywhere 7. <X>-ity Configurability Security Scalability Maintainability Extensibility ... Vague. Unchallenged. Hard to argue against. FUD. Example 1: Lets build a CMS for our forms for ""Extensibility"". Business people can add new fields easily. Result: Business people never used it. When they had to, they would have a developer sit right beside them and do it. Maybe all we needed was a simple Developer guide to add a new field in few hours, instead of a point-and-click interface? Example 2: Lets design a big Database layer for easy ""Configurability"". We should be able to switch database in a single Magic file. Result: In 10 years, I've seen only one business make serious effort to completely swap a fully vested database. And when it happened, the ""Magic file"" did not help. There was so much operational work. Incompatibilities and gaps in functionality. And the client asked us to switch ""one half"" of our models to the new NoSQL database. We tore our hair apart - our Magic toggle was a single point of change, but this was cross-cutting. In today's world, we're well past a point where there is no way to design a single configurable layer for modern document/KV stores (e.g. Redis/ CouchDB/ DynamoDB/ etc). Not even SQL Databases like Postgres/ HSQLDB/ SQLite are compatible for that matter. Either you completely dumb down your data layer (and struggle with delivering functionality), or acknowledge the database as part of your solution (e.g. postgres geo/json features) and throw away configurability guilt. Your stack is as much part of your solution as your code. When you let go of this vague X-ity, better solutions start to emerge. e.g. You can now break data access vertically (small DAOs for each action) instead of horizontally (magic configurable layer), or even pick and choose different data stores for different functionality [micro] services style. Example 3: We built an OAuth system for enterprise clients. For the Internal administrators - we were asked to use a secondary Google OAuth system. Because Security. If someone hacks our OAuth, business didn't want them to get access to admin credentials. Google OAuth is more secure, and who can argue against ""more security"" at any point? Result: If someone really wanted to hack into our system, they don't have to go through the OAuth layer. We had many vulnerabilities lying around. e.g. they could have just done privilege escalation. So all that effort of supporting two different OAuth user profiles and systems everywhere had little to no returns in securing our system, compared to properly securing our base first. TL;DR - Don't let <X>-ities go unchallenged. Clearly define and evaluate the Scenario/Story/Need/Usage. Tip: Ask a simple question - ""What's an example story/scenario?"" - and then dig deep on that scenario. This exposes flaws in most <X>-ities. 8. In House ""Inventions"" It feels cool in the beginning. But these are most common sources of Legacy in few years. Some examples: In-house libraries (HTTP, mini ORM/ODM, Caching, Config, etc) In-house frameworks (CMS, Event Streaming, Concurrency, Background Jobs, etc) In-house tools (Buildchains, Deployment tools, etc) Things that are missed: It takes a lot of skills and deep understanding of the problem domain. A ""Service runner"" library needs expertise of how daemons work, process management, I/O redirection, PID files and so on. A CMS is not just about rendering fields with a datatype - it has inter-field dependencies, validations, wizards, generic renderers and so on. Even a simple ""retry"" library is not so simple. There is a constant effort required to keeping this going. Even a tiny open source library takes a lot of time to maintain. If you open source it, nobody cares. Except the original starters and people paid to work on it. The original starters will eventually move away with the ""Inventor of X"" tag in their résumé. Contributing to existing frameworks takes up more time NOW. But creating an ""invention"" takes up considerably more time going forward. TL;DR - Reuse. Fork. Contribute. Reconsider. Finally, if really pushed to go ahead, do it only with an Internal OSS mindset. Fight with existing competition. Work to convince even internal people to use this. Don't take it for granted since you are an insider. 9. Following the Status Quo Once something is implemented in a certain way, everyone implicitly starts building on top of it. Nobody questions the status quo. Working code is considered ""the right way"". Even in cases where it was never intended, people go all the way around to even slightly fit into what's existing. A Healthy System churns. An Unhealthy system is additive-only. Areas of code that don't see commits for a long time are smells. We are expected to keep every part of the system churning. Here is a wonderful article explaining this in detail . How teams iterate vs How they should, Every single day: TL;DR - Refactoring is part of each and every story. No code is untouchable 10. Bad Estimation Frequently we see really good teams/coders end up producing shit. We see their codebase and wonder ""WTF, was this really developed by that team/person I thought was awesome?"" Quality needs time and not just skill. And smart developers frequently overestimate their capability. Finally they end up taking ugly hacks to finish stuff on a self-committed suicide timeline. TL;DR - Bad Estimation destroys Quality even before a single line of code is written If you made it this far, thanks! Reminder that I'm only discussing over-engineering and not advocating cowboy coding. Further links:",en,255
8,1724,1468247818,CONTENT SHARED,-1297580205670251233,534764222466712491,-9077035980881675856,,,,HTML,https://viagensdealline.com/2013/08/20/a-minha-viagem-a-maternidade/,a minha viagem à maternidade #tetodomundo,"Já fazia uma semana, desde o dia 26 de dezembro, que eu não parava de sentir uma fraqueza horrível pelo corpo, um embrulho constante nas entranhas e um mau humor insuportável. O mundo todo tinha perdido suas cores, eu estava com uma sensação fortíssima de morte iminente. Era primeiro de janeiro de 1999, toda a minha família se reunia num resort no interior de São Paulo para comemorar a passagem do ano. Mas aquele réveillon eu não comemorei com eles, eu estava de cama. Havia dormido desde o dia 30 de dezembro e não conseguia levantar para nada. Quando minha mãe passou pelo meu quarto naquela manhã de ano novo, eu implorei a ela: - Mãe, eu estou falando muito sério com você. Eu preciso ir para um hospital urgente! Eu tenho certeza que algo muito grave está acontecendo comigo. Por favor, mãe, me ajuda! Mesmo diante de minha urgência hospitalar e de meu claro desespero, minha mãe, que é uma médica pediatra, respondeu: - Fica calma, minha filha... Olha, não tem nenhum hospital ou clínica aqui perto. Amanhã bem cedinho a gente já vai embora para casa e eu prometo que assim que chegar em Brasília a gente te leva direto para o hospital, está bem? Descansa só mais esse dia de hoje que amanhã tudo vai se esclarecer. ... E que você tenha o ano mais feliz da sua vida, minha filha! Não adiantava continuar insistindo. Eu ali no meu leito de morte e minha mãe sem tirar aquele sorriso insuportável do rosto. Mas eu estava tão fraca que não conseguia nem sentir vontade de esconjurá-la. Fiz a viagem de volta para Brasília como pude, aos trancos, buracos e barrancos. Sonhando com o prometido de que alguém me levaria para o hospital assim que chegasse e que algum médico me daria a cura para aquela sensação terrível que consumia meu corpo por completo. Triste ilusão. Assim que chegamos, ao invés de hospital, minha mãe recomendou a Leonardo, o meu namorado que estava viajando comigo, que passássemos direto numa farmácia e comprássemos um teste de gravidez. TESTE DE QUÊ??? Eu estava fisica e psicologicamente abalada demais para rir de uma palhaçada tão sem graça como aquela. Eu vinha de uma depressão crônica de vários anos, havia ficado um bom tempo sem nem menstruar por causa da doença. Eu comia bem, mas há muito que só emagrecia. Já estava pele e osso, sempre me considerando uma pessoa muito debilitada. Até nem tomava mais anticoncepcional porque as pílulas só pioravam o meu estado de saúde, tanto físico quanto mental. ""Eu nem me considero uma mulher completamente adulta ainda"" - pensava eu. ""Sou magra demais e não tenho nenhuma maturidade para ser mãe. Que óbvio que eu não estou grávida!"". Aceitei fazer a porcaria do teste só para que minha mãe tirasse aquele sorrisinho impertinente do rosto e me levasse finalmente direto para uma emergência. Fui até o banheiro igual a um zumbi, levando na mão o pacotinho com o tal do teste. Leonardo ficou esperando do lado de fora calado, com aquela cara de quem há dias não tinha a menor idéia do que estava acontecendo comigo. Fiz o meu xixi e o papelzinho indicador ficou completamente rosa. Dei uma olhada nas instruções para saber o que aquilo significava e a bula era bastante clara. Era simples demais. Azul, teste negativo. Rosa, teste positivo. Como assim positivo? O quê diabos isso significa?? Mostrei para o Leonardo e eu nunca vou esquecer o tamanho do sorriso que ele deu. Mas eu mesma, na minha cabeça não tinha nenhum pensamento muito claro passando por ali. Talvez desespero seja isso mesmo... Eu estava em choque, catatônica. Então de repente, não tinha mais nada embaixo dos meus pés. As coisas, parece que começavam a rodar na minha frente. Me deu uma vontade imensa de voltar atrás, de descobrir a fórmula para viajar no tempo e reverter todo aquele pesadelo absurdo. Como assim positivo?? Eu estou no meio de uma porção de planos para o futuro, para a minha carreira, para o meu crescimento! Inclusive, a minha lista de coisas para fazer esse ano está gigante! Não existe a menor possibilidade de aparecer uma criança a essa altura do campeonato. Eu tenho ZERO de estabilidade, sem condição nenhuma de sustentar um filho ainda por cima! A primeira imagem que veio à minha cabeça era do paredão de concreto de um viaduto escuro e mal cuidado. Numa noite de lua clara, onde eu estava sentada embaixo de uma tenda de papelão com aquela criança no meu colo, coberta só com um pano sujo qualquer. Lembro-me de ter visto a fome claramente naquela imagem. Um cansaço extremo. Um abandono absoluto da vida. Vou acabar de baixo da ponte... Pensei em quem seria o primeiro a morrer, se seria eu ou o bebê... Era óbvio que eu não iria conseguir sustentar aquela criança. Esse era o fim. ....... Lição número um da maternidade, e talvez a maior de todas as lições: A natureza é perfeita e implacável. E a nossa arrogância é uma total estupidez. Gente, eu já tinha 24 anos de idade! Como é que eu podia ser uma pessoa tão despreparada e imatura assim? Mas eu era. Os jovens de hoje são totalmente superprotegidos. Tanto fisica quanto psicologicamente, aos 24 anos qualquer um já está para lá de pronto para ter um filho. Como é que eu não sabia disso?? Quanta arrogância a minha, de achar que eu não estava pronta... Levaram-se pouquíssimos dias para eu começar a compreender o tamanho da sabedoria da natureza. Existe sim vida após o resultado positivo de um teste de farmácia. Dias se passaram e eu ainda não havia morrido! Na verdade, exatamente nada tinha mudado na minha rotina até então. ""Interessante..."" - pensei eu. Tudo ainda era muito confuso, eu ainda estava muito zonza. Mas assim que a ""catatonisse"" passou, lá por volta da primeira semana depois de tomar ciência dessa condição chamada gravidez, eu começava a perceber que um amor muito forte já brotava dentro de mim. Uma coisa muito diferente de tudo aquilo que eu havia sentido antes. Uma coisa bem mais profunda, parece que mais pura... Alguns outros poucos dias mais e então tudo na minha cabeça já mudava completamente. Assim que a maior de todas as responsabilidades bate a sua porta, você amadurece na marra. Ou talvez, só pára de segurar uma maturidade que na verdade sempre esteve ali, mas que nunca tinha sido preciso utilizá-la. ...... As lições da maternidade são diárias e diversas. Infelizmente, no caso da minha gravidez não foram só os três primeiros meses de mal estar e enjôo não. Eu passei todos os noves meses me sentindo um lixo. E minha mãe falou que com ela aconteceu o mesmo, tanto quando estava grávida de mim, quando de meu irmão. Eu me sentia tão mal, que nem tenho muitas lembranças do dia que fui fazer a ecografia para descobrir o sexo do bebê. Lembro só de o Léo ter ficado muito contente com o resultado. Mas para mim mesma, nada daquilo tinha a menor importância. Já por outro lado, também por causa da gravidez, Léo e eu decidimos nos casar. Um evento para mais de duzentos familiares e amigos. Foi o dia que eu me senti mais linda em toda a minha vida. Maquiagem, cabelo, unhas, sapato, hormônios a flor da pele... Tudo! Uma produção impecável. Lembro-me perfeitamente do exato momento da abertura do grande portal da igreja, lotada. Eu vinha com um véu de cinco metros de comprimento todo bordado, saindo gloriosamente de um Chevy Camaro esporte, vermelho cor de rubi. O meu vestido branco era deslumbrante, do jeito que toda a noiva deve se achar. Eu escolhi o estilo ""Julieta de Shakespeare"", porque no fundo eu sou uma romântica e também para aproveitar e já disfarçar a barriguinha que àquela altura estava para lá de saliente. Eu achei que meu coração fosse parar quando a música alta e majestal começou a tocar só por causa da minha presença. Havia uma multidão de olhos arregalados em minha direção. Eu vinha de braços dados com o meu pai, que naquele dia sentia um orgulho muito grande de mim. Podia ver meu irmão lá na frente, no altar, com seu lindo sorriso no rosto. E minha mãe com minha única avó ainda viva, magníficas! E já chorando, lógico. A festa foi muito simples, mas grande e extremamente alegre, lotada de pessoas que nos desejavam uma felicidade absoluta. Senti a presença muito forte de um primo amado que eu tinha perdido há pouco tempo, eu sabia que ele estava ali compartilhando aquele momento comigo. Mas voltando à história da gravidez, bem no dia do meu casamento os meus peitos estavam gigantes de tanto leite. Lindos, redondos e naturais. Para quem nunca teve peito nenhum, essa com certeza é uma das boas lembranças de quando se está grávida. Lembro-me de ter comprado umas coisinhas aqui e ali, de ter montado o quartinho do bebê com móveis doados pela família e reformados por mim com a ajuda da minha mãe. Por sinal, apoio sempre total na minha vida e diário durante aquela fase tão difícil. Fiz eu mesma a pátina do bercinho, da cômoda e do armário. Lembro-me de ter bordado quadrinhos infantis para pendurar na parede; do dia que estava em Goiânia comprando todo o enxoval; e lembro-me também de um churrasco na beira do lago que fizemos para barganhar fraldas descartáveis dos amigos (foto abaixo). Enchemos um armário inteiro só de fraldas. Mas lembro também de infecção urinária, azia, estresse, gases doloridos, estrias, aftas, inchaço, de no último mês estar com um barrigão tão grande que não conseguia mais ver nem meus pés nem minhas pernas, de estar sentido uma dificuldade enorme para respirar, comer, dormir, trabalhar, tomar banho, andar, mexer, sentar, levantar, conversar, pensar, sorrir, existir, ... O tamanho que estava ficando a minha barriga nas últimas semanas daquela contagem regressiva começou a me dar pânico. Só não surtei de vez porque na verdade quando não se respira direito, o raciocínio vai lá para baixo, e quando não se dá conta de pensar, isso inclui também não pensar besteira. Simplesmente vegetei, apavorada. ........ No dia 20 de agosto daquele mesmo ano, no exato dia do aniversário de 20 anos do meu irmão mais novo, eu acordei as 6 da manhã com uma dor intestinal meio chatinha, fui ao banheiro algumas vezes mas nada acontecia... Mimada como sempre fui, já liguei logo para a minha mamãezinha para pedir ajuda. - Mãe, que remédio eu posso tomar para dor de barriga? Resolvi perguntar, porque uma das lições sobre gravidez que a gente já aprende desde o começo, é que não se pode mais botar qualquer coisa para dentro de você. Tudo tem regra. - Quando isso começou, minha filha? - ""respondeu"" minha mãe com outra pergunta. - Tem menos de uma hora - disse eu. - Me deixa falar com o Léo. - Oi, Dona Cleusa... - disse Leonardo pegando o telefone. - Léo, vai começando a preparar uma malinha com as coisas dela e do bebê. Nós vamos aqui ligar para o obstetra para saber como ele quer proceder. Peguei o telefone da mão do Léo e disse já sem paciência: - Mãe, que parte da frase ""Eu estou só com uma dor de barriga"" você não está entendendo? - Filha... Contração é igual à dor de barriga. Essa foi a primeira lição daquele dia que seria longo... O recado do meu obstetra, que estava em meio a sua caminhada matinal, era para que eu aguardasse ainda mais algumas horas e entrasse em contato com ele lá pelo meio da manhã para reportar a evolução do caso. Eu acho que em menos de 15 minutos depois daquela conversa apavorante, eu já tinha feito a tal da malinha e já estava dentro do carro obrigando o Léo a me dirigir até o hospital no grito. Eu estava em um completo estado de pânico. Já no hospital foi quando eu comecei a sentir as primeiras contrações para valer. Muito curtas ainda, mas já dilacerantes. O meu obstetra chegou logo em seguida e me examinou de prontidão. Falou que estava tudo indo muito bem e que eu tinha que esperar dilatar mais não sei o quê, do não sei o quê lá. Foi quando então que eu implorei, por tudo o que era mais sagrado, confesso que num tom muito mais alto do que ele merecia, para que ele tirasse imediatamente aquela criança de dentro de mim. A minha barriga estava gigante e eu tinha muita certeza na minha cabeça que nunca que eu iria conseguir colocar tudo aquilo ali para fora sozinha. O Doutor Sebastião é um santo! Ele aguentou todos os meus pitis daquele dia com toda a serenidade de um grande amigo e de um profissional de extrema competência. Ele sabia que o meu bebê não tinha virado de cabeça para baixo, que estava com três voltas do cordão umbilical em seu pescoço e que o meu caso teria que ser Cesária de qualquer forma. Ele não me deixou sofrer absolutamente mais nada. Já deitada numa maca em direção ao centro cirúrgico, num movimento desconcertante, encarando aquele teto pálido e frio, cheio de luzes passando rapidamente pelos meus olhos, sem ter a mínima noção do que me aguardava lá na frente, foi então que eu senti um pânico ainda maior. O maior de todos na minha vida até então. Eu me lembro de estar chorando como uma louca desesperada. Só me acalmei quando o meu anjo da guarda veio, segurou na minha mão e falou bem baixinho ao meu ouvido: ""Pode se acalmar agora, eu estou aqui com você"". Era o meu primo Evandro, de apelido Deco, assistente do meu parto aquele dia. A sala que me colocaram estava vazia, só eu e ele ali. Ele me sentou na maca com o seu sorriso doce, sua voz mansa e seu tato de médico sábio. Depois chegou outro cara de branco, também com um sorriso, passou a mão pelas minhas costas através daquela camisola de doente, com a abertura atrás, em que eu estava vestida. Eu senti uma picadinha bem de leve na região da coluna. Deco ainda segurava a minha mão calmamente. O outro doutor fazia mais algumas coisas ali por trás que eu não tinha a menor idéia do que era porque eu não conseguia ver nada. Muito depois que eu fui saber que ele estava enfiando aquele agulhão gigante da anestesia peridural na minha espinha. Mas eu não sentia nada por causa da anestesia prévia que ele tinha me dado. Obrigada doutor. Eu implorava muito para o Deco me colocar para dormir urgentemente. Ele me perguntou se eu não ia querer ver o meu bebê assim que nascesse. Eu então surtei de vez. Só de cogitar a possibilidade de ver sangue, o meu coração já disparava para a boca. ""Deco, me coloca para dormir agora! Pelo amor de Deus!!!"" Ele então consentiu. Me deitou de volta na maca, embaixo de uma carrossel de luzes fortíssimas e começou a preparar umas coisinhas por ali, até que alguém botou um grande pano branco na minha frente, entre minha cabeça e minha barriga. Deco então falou: - Já vou te colocar para dormir em um minuto. - Eu não quero ver sangue! - eu dizia com terror. - Você não vai ver nada. Confie em mim. Eu senti um pequeno estalinho na barriga e bem rapidamente depois um jovem doutor saiu de trás do pano branco, levantou a minha mão, fez alguma coisa por ali e bem simpaticamente me pediu que contasse até dez. - Um... Eu não me lembro do dois. Também muito depois, fiquei sabendo que naquele momento em que eu estava crente que eles nem tinham começado a cirurgia ainda, na verdade já haviam aberto a minha barriga toda. Na minha cabeça, cirurgia era uma coisa que demorava horas e horas, mas a realidade é que a minha Cesária não durou nem trinta minutos. Por várias razões médicas, durante o parto a mãe só pode ficar inconsciente após a criança sair do útero. Aquele estalinho que eu senti já era o bebê se descolando de mim através do corte apertado. Tudo foi muito rápido. Eu dormi no segundo antes de ouvir o meu filho chorar. ........ Acordei e meus olhos se relutaram a abrir. Estava encostada na parede de um corredor do hospital, ainda deitada numa maca. Fiquei inconsciente por mais ou menos uma hora depois do parto. Quando minha visão desembaçou, a primeira coisa que apareceu na minha frente foi o sorriso enorme do meu pai. - Ooooooooooi! - ele falou. - Como você está se sentindo? - Eu sei lá... - respondi. Estava ainda sonolenta. Uma enfermeira (eu acho) me transportou até o quarto em que eu iria passar a noite. Só sei que em menos de uma hora, o efeito da anestesia começou a diminuir, e foi então que eu comecei a sentir a maior dor de todo esse mundo. A parte de baixo da minha barriga começou a latejar um pouquinho, depois foi piorando, e em questão de minutos era como se estivesse uma faca alucinada serrando a minha barriga de um lado para o outro sem parar, bem no lugar dos pontos e da faixa branca do curativo. Lá estava eu aos berros de novo. Já mandaram chamar logo o meu ""personal"" anjo da guarda. Deco, como todo anjo que se preze, aplicou na minha veia alguma droga sobrenatural, que teve o poder de acabar completamente com toda aquela dor que eu estava sentindo. Ele botou a mão no vidrinho do soro que escorria em mim, e em questão de milésimos de segundos uma grande onda percorreu todo o meu corpo, do primeiro fiozinho de cabelo até a pontinha da unha do meu pé. Uma grande onda de paz. Eu sorri de verdade pela primeira vez depois de longos nove meses. Na verdade, eu gargalhei! Foi bom demais. Logo então, meio que de repente, a porta do quarto se abriu. E nesse momento a visão mais maravilhosa do mundo surgiu magicamente à minha frente. Era um príncipe todo de branco. O príncipe mais encantado de todos os príncipes. ""O cavaleiro errante enviado pelo Universo para resgatar a minha vida das trevas da imaturidade. O salvador abençoado que veio inundar o vazio da minha existência com o beijo de seu amor eterno."" Ele tinha os olhinhos fechados, estavam vermelhos. A mãozinha era cheia de dedinhos. Que coisa MAIS LINDA! Mastercard que me perdoe, mas ver o rosto de seu filho pela primeira vez não tem preço. É o cataclisma de meses e meses de ansiedade pensando em como seria aquela imagem. O coração vai a mil. Mesmo quando injustamente ele é a cara do pai e não puxou nada de você, nada disso tem importância. ""O MEU filho é MUITO LINDO!"" - repetia eu sem conseguir parar. Acho que era uma mistura do efeito das drogas anestésicas com a visão do paraíso, tudo ao mesmo tempo. Veja a foto abaixo dos meus dois anjos juntinhos! Como que um dia passou pela minha cabeça que eu pudesse não estar pronta para esse momento? Quanta arrogância... Muito obrigada por essa grande lição, minha mãe natureza. ......... Hoje o meu filho faz quatorze anos. Eu ainda tento entender o que foi que eu fiz para merecer um filho tão perfeito. O bebê mais lindo, a criança mais doce, o menino mais inteligente, o pré-adolescente mais meigo, o companheiro mais amoroso, o futuro homem mais sensível que existe. Eu nunca pensei muito em ter filhos, mas sempre que imaginava como eles seriam, nem de longe passava pela minha cabeça que seria algo tão maravilhoso como o meu Luiz. O maior amor de toda a minha vida. Feliz aniversário, meu filho amado. Escrevi esse post de presente para você.",pt,253
9,2095,1471520397,CONTENT SHARED,-1633984990770981161,2195040187466632600,1000828687449358272,,,,HTML,https://medium.com/@roxrogge/ux-ou-ui-4c0a1bcb4b83,ux ou ui?,"UX ou UI? Tenho escutado essa pergunta com frequência e sempre me dói muito respondê-la. Há um bom tempo vemos vários comentários sobre UX Designer e UI Designer sem que as pessoas saibam, de fato, o que faz cada um desses profissionais. Mas antes de falarmos sobre as diferenças vamos rapidamente refletir e voltar uns 15 anos. Parece muito tempo, não? Mas você parou para pensar como passou rápido e como foi nossa evolução tecnológica? Em 2002, quando minha filha tinha um ano, eu e meu marido já estávamos preocupados com a segurança dela na adolescência. Um dia estávamos conversando e ele olhou para o seu incrível celular mega blaster, um motorola V120 do Guga, com rádio FM e gravador, e disse: - Não se preocupe, quando nossa filha tiver 15 anos o celular já vai ter câmera e vamos conseguir vê-la em real-time onde ela estiver! E não é que ele estava certo? Evoluiu muito rápido. E como você acha que aconteceu essa evolução? Foi apenas pela busca da beleza do produto ou pela necessidade do usuário? Nessa época as siglas UX e UI não eram tão populares para definir cargos: éramos todos Designers e fomos trabalhando mesmo sem uma denominação específica e muitos sem perceber o que já estavam fazendo. A necessidade do usuário e os avanços tecnológicos criaram uma demanda especializada. Mesmo que a experiência do usuário sempre tenha existido, ela começou a ganhar uma definição nos anos 90, quando Don Norman disse em uma entrevista: ""Eu inventei o termo experiência do usuário porque achava que interface do usuário e usabilidade eram muito restritos, eu queria cobrir todos os aspectos da experiência de uma pessoa com o sistema, incluindo design industrial, gráficos, a interface, a interação física e o manual. Desde então o termo tem se espalhado amplamente..."" [Don Norman / meados de 1990] Chegamos em um tempo em que existem milhares de apps no celular, smart watch, tablet, smart tv, IoT [internet das coisas] e a todo momento surgem novos produtos que usam interfaces e precisam de uma boa experiência para o produto ter vida no mercado. Nesta última década a área de design se desenvolveu com muita rapidez, e por isso evoluiu também o número de especialidades na profissão e as formas de denominá-las: Design, Web Design, Web-Dev, Design visual, Design de interface, IxD, Design de Interação, Design de Produto, UI, UI Design, UI/UX Design, Usabilidade, UX, UX Design, Design da Experiência do usuário, UX Strategy, UCD, Design Centrado no Usuário, Design de Aplicação, Design de Navegação, User Experience Researcher, Design Industrial, Design da Informação, AI, Information Architect, Arquiteto da Informação, Data-Informed Design, Data-Driven Design, Data-""whatever"" Design, Service Design, só para mencionar algumas. Tem design para tudo hoje em dia! As funções se tornam disciplinas isoladas entre suas especializações. E com isso há dúvidas na área e muitas pessoas não sabem que caminho seguir. Então afinal, qual a diferença entre UX e UI? De forma simplificada , please? UX, User Experience, Experiência do Usuário UX é toda a pesquisa para uma melhor interação e comportamento do site/app/produto/marca. Pense em pesquisa, testes, experiência, emoções, conteúdo, hierarquia, fluxos... UI, User Interface, Interface de Usuário UI é o visual do site/app/produto/marca. Pense em interação, harmonia visual, fonte, respiro, tipografia, cores, formas... Podemos dizer que em um determinado projeto, o UX pesquisa e testa como o produto vai dar prazer ao usuário, e o UI cuida da interface, a parte visual do produto. Mas e aí? UX é parte de UI? Ou UI é parte de UX? Eles andam de mãos dadas desde sempre. É impossível pensar um projeto sem pesquisa, sem saber quem vai usar o produto, sem pensar nas frustrações e motivações dos usuários. É o UX que direciona a escolha da UI. E a UI também determina a experiência. Usando uma metáfora simples: você não pode ir à partida de futebol no estádio de um time usando uma camiseta da mesma cor do time rival. Tem que verificar e pesquisar! Da mesma forma, um aplicativo não pode ter as cores do concorrente. ... Enfim, experiência e interface são multidisciplinares, envolvendo os aspectos da psicologia, neurociência, antropologia, ciência da computação, design gráfico, design industrial e ciência cognitiva. É preciso pesquisar, desenvolver empatia, e usar métricas. Sem pesquisa não há uma boa experiência do usuário e sem design não existirá uma boa interface, e sem os dois não existirá um produto inovador. As disciplinas não podem ser tratadas isoladamente. Seria tipo avião sem asa, fogueira sem brasa, futebol sem bola ou Piu-Piu sem Frajola! Como comentei no post anterior, sou apaixonada pela experiência do usuário e amo design. Sinceramente não consigo separar UX & UI e acredito que todo profissional de design já tenha os dois incorporados em seu trabalho, mesmo que não perceba. E minha filha tem um belo celular com câmera, com UI e UX dignos de 2016!",pt,249
10,2351,1473971457,CONTENT SHARED,3367026768872537336,-3535274684588209118,-6495023147849651556,,,,HTML,http://br.blog.trello.com/melhore-a-comunicacao-na-empresa/,seja esperto no trabalho: melhore a comunicação na empresa com 12 robôs,"Seja Esperto no Trabalho: Melhore a Comunicação na Empresa Com 12 Bots ""Eu odeio ferramentas que me ajudam a trabalhar com mais eficiência e auxiliam a melhorar a comunicação na empresa"", disse absolutamente ninguém, em momento algum. Entram em cena seus próprios minions. Robôs (ou bots, em inglês) são incríveis para a produtividade. Eles podem automatizar tarefas que você teria que fazer por sua conta, deixando tempo livre para focar nas coisas que realmente precisam do seu toque de humanidade. Robôs também te livram daquela sobrecarga de novos apps. Em vez de baixar mais um aplicativo para fazer uma tarefa específica - e acabar com um monte de novas notificações para verificar o tempo todo - você pode turbinar a utilidade dos aplicativos que já tem. É a mesma coisa que personalizar seu uso do Trello com uma miscelânea de aplicativos de terceiros e Power-Ups . Robôs baseados em textos podem ajudar a melhorar a comunicação na empresa e com sua equipe, ou engajar os usuários nos aplicativos que você (e eles) já usam - aplicativos de chat como o Slack, redes sociais como Twitter e WhatsApp, e até mesmo com seu e-mail ou com o ecossistema do seu smartphone. Você também pode usar os robôs para se manter nos apps e plataformas que mais usa, diminuindo as chances de seu cérebro ""dar uma viajada"" com essa constante mudança de contexto . Tem robôs para quase tudo no trabalho e para todas as plataformas que você usa no dia a dia. Se a repercussão sobre isso já chegou até você, mas não teve uma chance de experimentar, aqui tem alguns robôs para você se cadastrar e começar a melhorar a comunicação na empresa, poupar tempo e trabalho imediatamente: Robôs de Comunicação na Empresa 1. Slackbot Evidentemente esta lista começa com o robô do Slack, o Salckbot , um dos melhores robôs da nossa seleção. O Slackbot poderia ser facilmente classificado como um ""assistente pessoal robótico"", mas uma vez que ele vive na plataforma de mensagens do Slack, sua função principal é ajudar você e sua equipe a melhorar a comunicação na empresa e trabalhar com mais produtividade. Tem um monte de maneiras de programar o Slackbot, e o Slack até fornece um API para criar ""Slack bots"" (não confunda com o próprio Slackbot) para rodar dentro do Slack. Você pode começar criando respostas automáticas personalizadas para aquelas perguntas mais comuns que te tomam tempo, tais como ""Qual a senha do wi-fi?"" ou ""Que horas é a nossa reunião semanal de equipe, mesmo?"". 2. Robô Standup Muitas empresas e equipes usam reuniões diárias de atualização para alinhar o time sobre o andamento dos projetos atuais e superar possíveis obstáculos juntos. Se você trabalha com sua equipe a distância , ou tem membros que usam horários diferentes, ter um robô que automaticamente inicia, toca e resume reuniões diárias de alinhamento pode soar com um sonho. O Standup gera atualizações automáticas no Slack que não exigem reuniões cara a cara em tempo real, todo dia ou toda semana. Sem mais desculpas para não melhorar a comunicação na empresa! 3. Drift 2.0 O novo Driftbot permite que se respondam consultas no site usando inteligência artificial para ajudar a direcionar as perguntas certas para as pessoas certas, melhorando a comunicação na empresa para que ninguém desperdice tempo nem esforços. A integração Drift + Slack permite falar com usuários do site sem ter que sair do Slack, poupando o tempo que se perde mudando de uma plataforma para outra. Robôs ajudando humanos a interagir com outros seres humanos é mais do que suficiente para aquecer as válvulas de um coração artificial. Assistentes Pessoais ""Robóticos"" 4. x.ai Conheça a x.ai Amy, a toda poderosa assistente pessoal robótica que agenda seus compromissos quando você manda um e-mail para ela. Quando você recebe um pedido de reunião, você pode simplesmente mandar um e-mail para amy@x.ai e a robô automaticamente faz o vai e volta de e-mails ""Eu estou disponível entre as 2:00 e as 4:00, nos dias ímpares, e nunca antes das 9:00 na segunda terça-feira"" para agendar as reuniões. Municiada com sua agenda de compromissos e preferência de horários, Amy faz toda programação de um jeito realmente muito legal. 5. Jarvis Esta corujinha adorável é seu novo companheiro, um robô para o Facebook Messenger que vai te mandar lembretes de tarefas baseado nos pedidos que você faz naturalmente, em linguagem comum. Se você prefere não falar em voz alta ao celular para dar instruções ao Siri, Google ou Cortana para lembrá-lo de atualizar prazos de vencimento em seus cartões Trello toda quinta às 10:00 - ou se você trabalha com Mídias Sociais, ou mesmo é alguém que passa muito tempo no Messenger - assistentes pessoais baseados em texto podem fazer uma tonelada de sentido. 6. WorkLife Para muita gente, principalmente quem trabalha a distância e a galera de tecnologia, abrir o Slack é uma das primeiras coisas que você faz quando vai começar a trabalhar. O Robô WorkLife Slack garante que você não tenha que abrir seu aplicativo de agenda ou seus e-mails para saber onde você vai e com quem você tem reuniões naquele dia. O robô te manda toda manhã um resumo de suas próximas reuniões, inclusive tudo que precisa saber sobre qual linha usar para um ""conference call"" e os assuntos que serão tratados. Esse é o tipo de coisa que eu gosto de ver em um robô! Robôs de Dados e Desenvolvimento 7. Hubot O Hubot começou a vida como um robô interno do GitHub, mas agora está disponível em código aberto. Anunciado como ""o robô da sua empresa"", ele vem com alguns scripts pré-programados para ajudar em coisas como tradução e postagem de imagens, mas também pode ser personalizada com uma tonelada de scripts criados colaborativamente ou que você mesmo codificou. 8. Baremetrics bot Se você ama métricas e construir empresas SaaS , você provavelmente ama Baremetrics - e você provavelmente também vai amar o robô Métricas Baremetrics e Slack ! Esta integração envia informações cruciais sobre os clientes diretamente para o Slack, assim você nunca vai ter que abrir o Baremetrics em separado para verificar novas inscrições, LTV (Life Time Value), MRR (Receita Recorrente Mensal) ou taxas de churn (cancelamento) novamente. 9. Statsbot Como as melhores invenções, o Statsbot surgiu do desejo de seus criadores de solucionar um problema próprio. Eles se viam constantemente indo aos seus painéis de controle do Mixpanel , tirando uma foto de um gráfico ou tabela e depois anexando no Slack em resposta a perguntas ou para explicar uma opinião para a equipe. Então eles criaram o Statsbot, um robô de métricas que mantém você e sua equipe atualizados sobre as métricas do Google Analytics, Mixpanel e mais, direto no Slack, melhorando muito a comunicação na empresa. Robôs Para Produtividade Pessoal e da Equipe 10. Tomatobot O método de Pomodoro é um jeito bem legal de dividir seu dia em períodos mais produtivos e focados. Com tantas pessoas adotando essa ferramenta de trabalho baseada no tempo, não é de se admirar que haja um robô para isso. Apropriadamente chamado de Robô de Produtividade Tomatobot , ele envia seus avisos de tempo diretamente para o seu canal do Slack. Você também pode identificar mensagens específicas como distrações ou digitar o que terminou - no final de cada Pomodoro, o Tomatobot vai te mostrar as mensagens marcadas como distrações para você poder vê-las e também tudo que conseguiu fazer. 11. Ace Quantos galhos um castor poderia quebrar para você se ele fosse um robô que gerencia inteligentemente sua produtividade no Slack, como o Robô de Produtividade Ace pode fazer? Este inteligente robozinho (e seu avatar de castor) acompanha despesas, tarefas, votações e muito mais com facilidade, tudo no Slack. Pauleira esse negócio, hein? 12. Trello para Slack Se você é um usuário ativo tanto do Slack quanto do Trello, você provavelmente não conseguiria sequer contar quantas vezes por dia você transfere informação de um para o outro. Trello para Slack permite que você crie instantaneamente cartões Trello de dentro dos canais Slack (até com GIFs de gatinhos no meio do post!), atualizar cartões, anexar cartões a conversas, incluir pessoas em cartões e muito mais. Mais do que apenas comandos, este robô fornece vários botões para tarefas comuns do Trello, assim você pode ir fazendo o que precisa direto, sem mudar de ferramenta. Deu para entender agora? Com uma legião de robozinhos (seus minions!), só esperando para melhorar a comunicação na empresa, você vai ser muito mais produtivo, eficiente e cada vez mais admirado por todos! Postagens Relacionadas",pt,247
11,1162,1464708954,CONTENT SHARED,2857117417189640073,22763587941636338,8000922562339244508,,,,HTML,https://sprintstories.com/running-gv-sprints-inside-corporates-learn-from-my-mistakes-526f67c1960f?gi=e381d0ad2b1,running gv sprints inside corporates - learn from my mistakes - sprint stories,"Running GV sprints inside corporates - learn from my mistakes GV (Google Ventures) has just released Sprint , a book about their design sprint process. This seems like a good time to share what I've learned from facilitating more than 30 sprints with corporates here in New Zealand. I hope my experience will help you avoid some of the pitfalls. Where I've learned what I've learned I'm a graphic, UI, UX, interaction, product designer/manager ... I really don't know what to call myself anymore. After several years as a lead designer, I worked as a UX consultant from 2012 until the end of 2015. Through this work, I quickly realized that consulting was broken. We could deliver great work that the client loved, but when we walked out the door everything went back to normal. Our ideals of lean thinking, rapid iteration and user feedback walked out with us. We started experimenting with working on-site with the client running, Lean UX and GV sprints. Finally, we started making a difference ... but there were problems... Running a sprint on an obstacle course Running the sprint process was very challenging in corporate environments - like running a sprint on an obstacle course. Many of the problems we came across are interrelated and point to issues that are common, if not universal, to 'corporates'. I also made my own mistakes and, learning from it all, I've found better ways. The issues I faced, the mistakes I made, and what I learned... 1. The war surrounding the War Room The drama that occurs when you take over a space in a corporate office for more than an hour, let alone a week, can increase to stunning levels. Faced with this loss of precious meeting-room space, I've seen other teams turn hostile, bookings sabotaged and angry people standing over a poor PA's desk, demanding to know why the room was fully booked. I've seen someone literally stomping mad, pointing at us through the glass wall and ranting furiously to the colleague beside them. (We just kept our heads down and kept drawing.) Solutions Here's some of things I did to reduce the drama: Straight to the top . I asked the CEO's PA to book the meeting room in the CEO's name. No one complained. Hide it. We removed the room from the booking system and covered up the glass wall with large foam boards. Move it. Teams have favorite meeting rooms. If you book it for a week you'll annoy them for that week. If it stretches into two, then drama is imminent. Instead, move to a different meeting room each week. Avoid the drama . Don't use a bookable meeting room. Find a space with at least one wall you can plant yourself beside. Get creative ... you're likely in a big building with all sorts of space. 2. Attendance chaos You've got a War Room, now you need a team. The Decider can make cameos, but the rest need to be available close to full-time for five days. In some companies you'll work with a passionate team who have been given permission to drop everything else. Freed of their obligations, they walk in with a clear mind and tackle the sprint with enthusiasm. But in other companies the team has only been encouraged to take part. Without that real permission, their to-do list, inbox and regular meetings are forever on their mind. Despite working on critical business problems, and even with the CEO's blessing, they will ask to pop in and out and attend other meetings, which is disruptive to both the team and the process. And then there are the ones who attend, but are forever focused on their laptop (that they refuse to give up)... Despite briefings and emphasizing this commitment, I found people likened it to a long meeting - 'Surely they don't need me the whole time?' Solutions If you want people full-time for five days straight you might have to compromise. Break it up . Instead of five days in a row, try for five days over a few weeks. Don't run a sprint for less than seven hours a day though, as if it's shorter people tend to not be entirely present as their mind is busy preparing for the rest of their day. Compress it. For one client I compressed the first three days of the sprint into two consecutive days. I then prepared the prototypes by myself and got the team together for an hour to provide feedback. After making the changes, I got the team to observe the user studies. By showing the clients how the process worked, they became more interested and invested, with the result that they actually started backfilling people so the team could truly spend five days straight on a sprint. Both of these are not ideal, but if it's the difference between running most of the process or none of it ... compromise this little bit. 3. Saboteurs in plain sight Even Daniel Burka , a design partner at GV, admits he was a little skeptical about the process when he did his first sprint - but after just one it all made sense. Each group that walks into the War Room will have a mix of people: those who understand the principles and are thrilled to be trying something different; those who are on the fence, like Daniel; and those who actually don't want to be there. It is simply counterproductive having people who don't want to be there, no matter how useful their subject-matter expertise. They will clash with you on every activity you lead the group in, because they don't want to do it. They will question the process, and will re-litigate and debate every point. Solutions 1. Ask them to opt in . Insist you meet all the attendees one-on-one in the weeks before the sprint and have a quick chat. Cover the process, the way the team works, and why the company is trying it out. Ask them to choose whether to take part or not, but also to agree to the way they'll work. Most people I've warmed up like this have entered the sprint and at least given it a go. 2. Let them be experts. Instead of booking them in for the whole week, instead get them to pop in on Monday so the team can interview them. Tease out their knowledge and then release them back into the wild. 4. Responsibility avoidance In the first sprints we ran, we didn't realize we were making a fundamental mistake: when you're responsible for facilitating a process, and coaching/training people in that process, you should not also be responsible for a deliverable. One key concept of the sprint process is to set time constraints to propel the team forward. However, negative behaviors such as re-litigation, too much talking and not following agreed principles will limit progress. This usually happens because the team members don't feel that they're actually responsible for delivering a result. They instead feel that they're there to be subject-matter experts, or simply take part. At the end of one of these early sprints, due to this slow progress the resulting prototype that was tested was not ready, and therefore the tests were not conclusive. Because I was technically responsible for delivering a result, I was then open to criticism, and so was the process. Solutions Do not agree to deliver anything. You are facilitating a process, but the team as a whole must be responsible for the outcome . There are some ways you can encourage this shared responsibility: Principles . Print off key principles that the team should agree to at the start of the sprint and stick them to the wall. Remind the team of these principles whenever they waiver. Everyone's in until the end . It's easy for prototyping to fall onto the people who can build a prototype. This often feels like a good time for others to run off and tackle their inboxes, leaving the deadline responsibility to one or two people. But instead, everyone should stay in the room. The rest of the team can work on other things if they have to, but if the prototypers ask for anything - to source images, write copy, even get coffee - the others should help them. Manage saboteurs in hiding . Some people may say all the right things and start off taking part, but later show signs that they are not compelled by the looming deadline. This often occurs when someone has felt that the agreed direction doesn't match their preference. Their behavior can become disruptive, usually through debate and re-litigation. At its worst, others join their cause and consensus is lost. As the facilitator, it's your job to remind them of the agreed principles and ask the Decider to reiterate the decisions. They then need to agree to the principles again and continue, or leave the room. 5. The corporate immune system and ambiguity The corporate immune system has been well-documented . Simply put, the 'body' of the corporate doesn't like change and the individuals responsible for it - especially if they're outsiders - and fights against it. The issues above are symptoms of this, but I found the immune system to be visible in many other ways. For example: The Decider was influenced mid-sprint by politics outside the room and wanted to re-litigate everything. We were blocked mid-sprint from talking to five customers for our user studies because a team wanted to control 'the message' the customers were hearing. A team decided the way we were working was not the [CompanyName] way. A team felt that the sprint team were making them look bad by uncovering customer problems that they were responsible for. They became very hostile. Then there is ambiguity . This is totally what you learn as you get older as a designer. I've been doing this for almost 20 years. And the older you get and the more you do this, the more you get confident that you just don't know. And then you look for a process where you can learn what's working and what's not working more quickly. That's why I like sprints so much. It takes off that weight of needing to be right. It's okay, you don't know, just admit you don't know. That's fine. - Daniel Burka Ambiguity is fine. That's what the process is for. Teams should embrace ambiguity, ask good questions, examine the problem, and ensure they're testing and learning the right things to remove ambiguity as fast as possible. Each sprint builds confidence and removes risk. However, in most corporates ambiguity is seen as weakness. These companies train people that they should always have the answer. People are even rewarded for coming up with plausible-sounding solutions quickly, even when ignoring potentially catastrophic assumptions. (Watch Claudia Batten's great short talk on Solutions Allies in high places . As one person, you can't begin to tackle an entire organization's culture. Ask the CEO (or the most senior person you have access to) to tackle these issues head on, and do your best to keep the sprint team focused on their deadlines. Show your progress . Invite teams in to see how you're working and present what has been learned so far. Results, progress, data and transparency are your best defense against the immune system. Get an internal spokesperson. If you're an outsider that has come in to do the facilitating, do not become the voice of the process and progress. Get the team members from inside the company to present to the rest - this will have more impact with the skeptics. Get out. When you try to move a project quickly inside one of these companies you'll discover these obstacles and behaviors. Changing individual human behaviors is hard, changing a group culture is insurmountable by a sprint facilitator. If the above options aren't working and the culture is too toxic to let the team deliver any results, walk away. embracing the squiggly line .) Even during show and tell at the end of the sprint, I've heard someone comment that it was 'five days wasted' as they already had the solution. At least when they're brash enough to say it out loud the team could provide evidence to defend themselves; however, in most cases this opinion is only shared away from the team. Like nothing else, ambiguity encourages the immune system to poison attempts at change. The sprint team is the key As a facilitator, you have to focus on your sprint team and can't get involved in any drama occurring outside of the room - even if there's foot-stomping and fury. It's the people you let inside the room that will make or break the process. But, because the process represents change, this can lead to uncertainty, even within the sprint team. Again, results, progress, data and transparency are your best defense. But to comfort the team from day one so you get those results, they need to be prepared. Sprint warm-ups This is the process I used to warm up the team for my last series of sprints to guarantee the best results: I interviewed several stakeholders early on and got a sense of some of the internal issues the company was struggling with. I sat down with each sprint participant and talked about a range of issues and got them to identify the ones that they had encountered. I talked them through the sprint process, going into detail in places where it combated the issues they had identified. I asked them whether they felt it was worth experimenting with the process. They always said yes. I repeated it with the group at the start of the sprint, focusing on the issues specific to that company, and how the process can help. Throughout the sprint I asked for permission to move onto each next step to avoid any tension between me as facilitator and the team. I created an atmosphere where the team corrected each other when people broke agreed principles. I focused on coaching individuals rather than the group as a whole. If someone seemed to be struggling I'd ensure I talked to them one on one. The corporate experiment As you will have gathered, I had some frustrating experiences while running sprints with corporate clients. In some cases, despite learning so much and the prototypes resulting in great customer feedback, projects were cancelled and everyone moved on. The sprint process was not designed with the corporate obstacle course in mind, and for a while I felt it could never work. However, I've come out the other side and I truly believe this process and its underlying principles are the key, even in these organizations. If I was a CEO of a company struggling to innovate or solve critical problems, I would encourage a group to run this process as soon as possible. Not to deliver a result, but to highlight all the obstacles that the leadership had become blind to. If the leaders are determined and don't hesitate, they may be able to clear the obstacles in time for the sprint team to achieve something big ... in just five days .",en,241
12,1875,1469487944,CONTENT SHARED,8224860111193157980,6013226412048763966,5651952480198335454,,,,HTML,http://www.jornaldoempreendedor.com.br/destaques/inspiracao/psicologa-de-harvard-diz-que-as-pessoas-julgam-voce-em-segundos-por-esses-criterios/,psicóloga de harvard diz que as pessoas julgam você em segundos por esses critérios | jornal do empreendedor,"As pessoas avaliam você em segundos, mas o que exatamente eles estão avaliando? A professora de Harvard Business School, Amy Cuddy vem estudando as primeiras impressões ao lado dos colegas psicólogos Susan Fiske e Peter Glick por mais de 15 anos, e descobriu padrões nessas interações. Em seu novo livro, ""Presença"", Cuddy diz que as pessoas respondem rapidamente duas perguntas quando eles te encontram pela primeira vez: Posso confiar nesta pessoa? Eu posso respeitar esta pessoa? Os psicólogos referem-se a estas dimensões como cordialidade e competência, respectivamente, e, idealmente, você quer ser percebido tendo ambos. Curiosamente, Cuddy diz que a maioria das pessoas, especialmente em um contexto profissional, acreditam que a competência é o fator mais importante . Afinal, eles querem provar que eles são inteligentes e talentosos o suficiente para lidar com o seu negócio. Mas, na verdade a cordialidade, ou confiabilidade, é o fator mais importante na forma como as pessoas avaliam você. ""De uma perspectiva evolucionária"", diz Cuddy, ""é mais crucial para a nossa sobrevivência saber se uma pessoa merece a nossa confiança."" Faz sentido quando você considera que para os homens das cavernas era mais importante descobrir se seu companheiro estava lá para matá-lo e roubar todos os seus bens ou se ele era competente o suficiente para construir um bom fogo com você. O novo livro de Cuddy explora formas para nos sentirmos mais confiantes. O livro já é best-seller na Amazon dos EUA e está aqui . Enquanto a competência é altamente valorizada, Cuddy diz que ela é avaliada apenas depois que a confiança é estabelecida. E, que se concentrar demais em exibir a sua força pode sair pela culatra. A competência é altamente valorizada mas é avaliada apenas depois que a confiança é... Click To Tweet Cuddy diz que estudantes de MBA estão muitas vezes tão preocupados em parecer inteligentes e competentes que isso pode levá-los a ignorar eventos sociais, não pedir ajuda, e geralmente parecer inacessível. ""Uma pessoa calorosa, confiável que também é forte provoca admiração, mas só depois que você estabelece a confiança é que sua força se torna um dom e não uma ameaça."" Estes overachievers podem se frustrar ao não receberem a oferta de emprego porque ninguém os conheceu melhor para confiar neles como pessoas. ""Se alguém que você está tentando influenciar não confiar em você, você não vai chegar muito longe. Na verdade, você pode até provocar suspeitas porque você parecer apenas um grande manipulador"", diz Cuddy. Amy fez um TED Talk muito interessante e está legendado abaixo: Artigo da Business Insider .",pt,236
13,2265,1473105755,CONTENT SHARED,7507067965574797372,-4028919343899978105,-3666565354944070559,,,,HTML,http://gizmodo.uol.com.br/disputa-tabs-vs-espacos/,um bilhão de arquivos mostram quem vence a disputa tabs vs. espaços entre programadores,"Esta é uma das maiores batalhas já travadas entre os programadores: você deveria usar a tecla tab ou cinco espaços ao recuar linhas em seu código-fonte? * Como aprender programação? * Aprenda mais sobre programação usando este baralho com trechos de código O debate, em última análise, se resume à forma como o código-fonte é exibido ao ser editado. A razão pela qual desenvolvedores se irritam é que o código fica desajeitado se o mesmo método não for usado em todo o arquivo. Isto pode se tornar especialmente complicado quando várias pessoas trabalham no mesmo projeto. O debate já se arrasta há tanto tempo que os programadores se referem a outros como ""gente de tab"" e ""gente de espaço"". Isso foi até mencionado em um episódio recente da série Silicon Valley , da HBO: Richard prefere tabs e fica irritado em ver Winnie - que trabalha para o Facebook - usando espaços. O breve relacionamento acaba por causa disso! O chileno Felipe Hoffa, que trabalha para o Google, decidiu encontrar um vencedor para este debate. Assim, ele analisou 1 bilhão de arquivos entre 14 diferentes linguagens de programação para descobrir qual método é realmente mais popular e, em seguida, publicou os resultados em um post no Medium . Os dados vieram de arquivos do GitHub armazenados no BigQuery . Ele removeu duplicatas e arquivos com menos de 10 linhas de código. O desenvolvedor também deu um voto por arquivo: assim, se o desenvolvedor usou abas e espaços, o voto foi para o método usado mais frequentemente. Por fim, os 400.000 repositórios principais foram classificados pelo número de estrelas que receberam no GitHub entre janeiro e maio de 2016. Eis os resultados: Como você pode notar a partir dos dados, o vencedor aqui são os espaços. Em toda grande linguagem de programação (fora C), os espaços foram mais utilizados nos arquivos mais populares no GitHub. Esta evidência é a mais definitiva em revelar se tabs ou espaços são mais populares no código-fonte. Mas por quê? Alguns dirão que é simples: os espaços serão exibidos da mesma forma em qualquer hardware ou software de visualização de texto, e são mais visíveis do que tabs. Enquanto isso, tabs são mais organizados e geram código-fonte mais enxuto. Sinto que o debate espaços vs. tabs ainda está longe de acabar. Imagens por HBO e Medium/@hoffa",pt,233
14,2000,1470418766,CONTENT SHARED,-6156751702010469220,3302556033962996625,769925043682591828,,,,HTML,https://blog.codinghorror.com/the-broken-window-theory/,the broken window theory,"In a previous entry , I touched on the broken window theory. You might be familiar with the Pragmatic Progammers' take on this : Don't leave ""broken windows"" (bad designs, wrong decisions, or poor code) unrepaired. Fix each one as soon as it is discovered. If there is insufficient time to fix it properly, then board it up. Perhaps you can comment out the offending code, or display a ""Not Implemented"" message, or substitute dummy data instead. Take some action to prevent further damage and to show that you're on top of the situation. We've seen clean, functional systems deteriorate pretty quickly once windows start breaking. There are other factors that can contribute to software rot, and we'll touch on some of them elsewhere, but neglect accelerates the rot faster than any other factor. That's excellent advice for programmers, but it's not the complete story. The broken window theory is based on an Atlantic Monthly article published in 1982. It's worth reading the article to get a deeper understanding of the human factors driving the theory: Second, at the community level, disorder and crime are usually inextricably linked, in a kind of developmental sequence. Social psychologists and police officers tend to agree that if a window in a building is broken and is left unrepaired, all the rest of the windows will soon be broken. This is as true in nice neighborhoods as in rundown ones. Window-breaking does not necessarily occur on a large scale because some areas are inhabited by determined window-breakers whereas others are populated by window-lovers; rather, one unrepaired broken window is a signal that no one cares, and so breaking more windows costs nothing. (It has always been fun.) Philip Zimbardo, a Stanford psychologist, reported in 1969 on some experiments testing the broken-window theory. He arranged to have an automobile without license plates parked with its hood up on a street in the Bronx and a comparable automobile on a street in Palo Alto, California. The car in the Bronx was attacked by ""vandals"" within ten minutes of its ""abandonment."" The first to arrive were a family--father, mother, and young son--who removed the radiator and battery. Within twenty-four hours, virtually everything of value had been removed. Then random destruction began--windows were smashed, parts torn off, upholstery ripped. Children began to use the car as a playground. Most of the adult ""vandals"" were well-dressed, apparently clean-cut whites. The car in Palo Alto sat untouched for more than a week. Then Zimbardo smashed part of it with a sledgehammer. Soon, passersby were joining in. Within a few hours, the car had been turned upside down and utterly destroyed. Again, the ""vandals"" appeared to be primarily respectable whites. Untended property becomes fair game for people out for fun or plunder and even for people who ordinarily would not dream of doing such things and who probably consider themselves law-abiding. Because of the nature of community life in the Bronx--its anonymity, the frequency with which cars are abandoned and things are stolen or broken, the past experience of ""no one caring""--vandalism begins much more quickly than it does in staid Palo Alto, where people have come to believe that private possessions are cared for, and that mischievous behavior is costly. But vandalism can occur anywhere once communal barriers--the sense of mutual regard and the obligations of civility--are lowered by actions that seem to signal that ""no one cares."" There's even an entire book on this subject . What's fascinating to me is that the mere perception of disorder-- even with seemingly irrelevant petty crimes like graffiti or minor vandalism -- precipitates a negative feedback loop that can result in total disorder: We suggest that ""untended"" behavior also leads to the breakdown of community controls. A stable neighborhood of families who care for their homes, mind each other's children, and confidently frown on unwanted intruders can change, in a few years or even a few months, to an inhospitable and frightening jungle. A piece of property is abandoned, weeds grow up, a window is smashed. Adults stop scolding rowdy children; the children, emboldened, become more rowdy. Families move out, unattached adults move in. Teenagers gather in front of the corner store. The merchant asks them to move; they refuse. Fights occur. Litter accumulates. People start drinking in front of the grocery; in time, an inebriate slumps to the sidewalk and is allowed to sleep it off. Pedestrians are approached by panhandlers. At this point it is not inevitable that serious crime will flourish or violent attacks on strangers will occur. But many residents will think that crime, especially violent crime, is on the rise, and they will modify their behavior accordingly. They will use the streets less often, and when on the streets will stay apart from their fellows, moving with averted eyes, silent lips, and hurried steps. ""Don't get involved."" For some residents, this growing atomization will matter little, because the neighborhood is not their ""home"" but ""the place where they live."" Their interests are elsewhere; they are cosmopolitans. But it will matter greatly to other people, whose lives derive meaning and satisfaction from local attachments rather than worldly involvement; for them, the neighborhood will cease to exist except for a few reliable friends whom they arrange to meet. Programming is insanely detail oriented, and perhaps this is why: if you're not on top of the details, the perception is that things are out of control, and it's only a matter of time before your project spins out of control. Maybe we should be sweating the small stuff .",en,221
15,3091,1487127440,CONTENT SHARED,1469580151036142903,-4465926797008424436,1603046601233039757,"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.37 Safari/537.36",SP,BR,HTML,https://dev.to/raddikx/dont-document-your-code-code-your-documentation,don't document your code. code your documentation.,"This is one of the great discussions among developers: document or not document your code? Is it worth writing documentation in your code? I thought this topic was completely overcome and it was clear that, except some few occasions (implementing a public API), documentation was not necessary. Until I saw a code review where the reviewer pointed out the lack of documentation as an issue. Really? I was one of those who used to document my code... or at least I tried. I was so convinced that code had to be documented. As a backup or reminder for my future myself or any other developer luck enough to end up in my code. Although I always realized it was always out of date. And by then, I already wondered: what is the purpose of documenting the code if the documentation is always outdated? Until several years ago I read the book Clean Code . I saw it ""crystal clear"", there is no need to document your code if you code your documentation. With this I mean to use meaningful variable and method names. If the name of the member already tells you the information that is keeping and the name of the method tells you what the method is doing you can end up reading the code without the need to figure out or document what your code is doing. Extract as much code as you can to methods. Even if you end up having a method with only 3 or 4 lines. Each method should do one thing and only one thing. And the name must explain what it does. Each member of a class must have a name that only reading it you know which information you can find there. Same for variables and input parameters. Following this simple steps you can have a code you can read, having the documentation right in the same code. Yes, I know, there are those times you have to implement code with a complex algorithm or you are copying a code you found on Internet which might be complex, you might not understand and you might not extract in simple and meaningful methods. Yes, there are always exception. What do you think? Do you document or write the documentation in your code? This post was originally published in Medium",en,209
16,2629,1477329558,CONTENT SHARED,-4333957157636611418,-7496361692498935601,-4955259521885105809,"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.143 Safari/537.36",SP,BR,HTML,https://business.stackoverflow.com/blog/why-programmers-want-private-offices,why programmers want private offices,"Ask any of your employees or coworkers what they think makes a workplace attractive and you're bound to get a variety of results. Some may refer to the benefits, while others are more concerned about the standard working hours. Developers, however, often will bring up the physical work environment when asked this question. But why? The type of work that developers do day in and day out requires a space that's noiseless with minimal interruptions. Distractions such as phone calls, chatty coworkers, or constant questions from colleagues completely interrupt the developer's flow of work and kill their productivity. For them to perform optimally, they need space and quiet. It's as simple as that. Let's start off with a few findings from various workplace productivity studies. In the book Peopleware , writers Tom DeMarco and Timothy Lister discuss their public productivity survey, which showed that ""The top quartile of participants, those who did the exercise most rapidly and effectively, worked in a space that was substantially different from that of the bottom quartile. The top performers' space was quieter, more private, better protected from interruption, and larger."" Additionally, IBM and architect Gerald McCue studied the work habits of developers in their current workspaces, as well as mock-ups of proposed (more private) workspaces. They found that the minimum accommodation for each developer would be ""100 square feet of dedicated space per worker, 30 square feet of work surface per person, and noise protection in the form of enclosed offices."" Why Do Private Offices Matter for Job Candidates? When a developer hears about a new job (whether they applied themselves or were passively recruited), they usually have the luxury of being picky. Since there's so much demand for them - multiple jobs for every unique developer - they want to ensure they are making the right choice by choosing a company that fits their values and needs. For many developers, one of those values is a top-notch private workspace. Our CEO Joel Spolsky said it best -- ""Put yourself in the job candidate's shoes. Company number 1 shows you a big crowded room, with a bunch of desks shoved in tightly, lots of marketing guys shouting on the phone next to the programmers and a bunch of sales jocks shouting tasteless jokes. Company number 2 shows you through a quiet hallway, a sunlit, plush office with a window and a door that closes. All else being equal, which job are you going to take?"" But What If We Don't Have Permission/Room/Budget for Private Offices? When companies do a cost/benefit analysis for improving the working space, they often focus too much on the cost. This is likely because the costs are easy to calculate, while the benefits are not. The potential benefits, which could be things like increased productivity, reduced turnover rate, increased revenue, often would outweigh the associated costs if calculated properly. How does your company calculate these benefits? That's the million-dollar question. According to Peopleware , ""the entire cost of workspace for one developer is a small percentage of the salary paid to the developer. In general, it varies in the range from 6 to 16 percent. For a programmer working in company-owned space, you should expect to pay $15 directly to the worker for every dollar you spend on space and amenities. If you add the cost for employee benefits, the total investment in the worker could easily be 20 times the cost of his or her workplace."" While not every company can afford to implement private offices for their developers (especially startups with a small budget), it's certainly not impossible. Lots of companies have invested in their developers and cut costs in other areas to make it work, giving them a competitive advantage in the technical hiring market.",en,192
17,1216,1464879718,CONTENT SHARED,880612740433495828,6013226412048763966,3831181614891772761,,,,HTML,http://vocerh.uol.com.br/noticias/entrevista/o-que-voce-deve-fazer-para-se-tornar-um-lider-melhor.phtml,o que você deve fazer para se tornar um líder melhor?,"Para ser um grande coach, você deve fazer mais perguntas em vez de tentar dar todas as respostas Marcelo Nóbrega* Bom líder: executivos em posição gerencial sentem-se compelidos a rapidamente orientar suas equipes no que e como fazer | Crédito: Pexels O segredo de todo bom líder é ter uma equipe de sucesso - é o que dizem nove entre dez gurus da administração. E como armar uma equipe de sucesso? Dando coaching. E como ser um grande coach e fazer coaching bem feito? A resposta é surpreendentemente simples. Você deve fazer mais perguntas em vez de tentar dar todas as respostas. É o que ensina Michael Bungay Stainer no livro The Coaching Habit (ainda sem tradução para o português). Michael é consultor de empresas, coach e autor de vários livros. Em suas próprias palavras, seu trabalho é ajudar executivos a causar maior impacto trabalhando menos. De forma Inteligente, sagaz, articulada e bem-humorada, Michael apresenta as sete perguntas que transformarão a maneira como você lidera sua equipe. Executivos em posição gerencial sentem-se compelidos a rapidamente orientar suas equipes no que e como fazer. Se você ocupa uma posição gerencial, tenho certeza de que sua reação quando um colaborador lhe apresenta um problema é tentar resolvê-lo, dar conselhos, agir e apagar o incêndio. Afinal, você é o chefe e sabe mais, certo? O que você não percebe é que, agindo desta maneira, se torna um gargalo para o desempenho e aprendizado da equipe. Coaching e feedback frequentes são mais poderosos do que as avaliações de desempenho tradicionais que são realizadas apenas uma ou duas vezes por ano. Diante de tantos cursos, textos, propagandas, certificações etc ... é fácil ficar confuso com o que é coaching. Que tal começar lendo este livro? Coaching não é dar conselhos ou resolver problemas, mas saber fazer as perguntas certas para ajudar o coachee a encontrar sua maneira de superar um obstáculo. Dedique mais tempo a ouvir para que encontre ele mesmo suas soluções. O resultado valerá a pena. E você trabalhará menos. Como? Comece perguntando: ""Porque você veio me procurar?"" - assim você define o foco da conversa. Siga com: ""O que mais?"" - explore um pouco pois raramente a primeira resposta é a verdadeira razão. Depois foque novamente: ""Qual seu verdadeiro desafio?"" - a real dificuldade ficará explícita e, se houver, o seu papel na solução também. Esse é apenas o início de uma fórmula simples para transformar qualquer conversa. Aprenda com The Coaching Habit: coaching é uma atitude. E você pode ser coach de sua equipe, pares, chefe e de você mesmo. * Este artigo é de autoria de Marcelo Nóbrega (diretor de recursos humanos da Arcos Dourados) e não representa necessariamente a opinião da revista",pt,191
18,1557,1467123962,CONTENT SHARED,-5002383425685129595,-3390049372067052505,2471338745689678750,,,,HTML,http://www.mckinsey.com/global-themes/leadership/changing-change-management,changing change management,"Research tells us that most change efforts fail. Yet change methodologies are stuck in a predigital era. It's high time to start catching up. Change management as it is traditionally applied is outdated. We know, for example, that 70 percent of change programs fail to achieve their goals, largely due to employee resistance and lack of management support. We also know that when people are truly invested in change it is 30 percent more likely to stick. While companies have been obsessing about how to use digital to improve their customer-facing businesses, the application of digital tools to promote and accelerate internal change has received far less scrutiny. However, applying new digital tools can make change more meaningful-and durable-both for the individuals who are experiencing it and for those who are implementing it. The advent of digital change tools comes at just the right time. Organizations today must simultaneously deliver rapid results and sustainable growth in an increasingly competitive environment. They are being forced to adapt and change to an unprecedented degree: leaders have to make decisions more quickly; managers have to react more rapidly to opportunities and threats; employees on the front line have to be more flexible and collaborative. Mastering the art of changing quickly is now a critical competitive advantage. For many organizations, a five-year strategic plan-or even a three-year one-is a thing of the past. Organizations that once enjoyed the luxury of time to test and roll out new initiatives must now do so in a compressed period while competing with tens or hundreds of existing (and often incomplete) initiatives. In this dynamic and fast-paced environment, competitive advantage will accrue to companies with the ability to set new priorities and implement new processes quicker than their rivals. The power of digital to drive change Large companies are increasingly engaged in multiple simultaneous change programs, often involving scores of people across numerous geographies. While traditional workshops and training courses have their place, they are not effective at scale and are slow moving. B2C companies have unlocked powerful digital tools to enhance the customer journey and shift consumer behavior. Wearable technology, adaptive interfaces, and integration into social platforms are all areas where B2C companies have innovated to make change more personal and responsive. Some of these same digital tools and techniques can be applied with great effectiveness to change-management techniques within an organization. Digital dashboards and personalized messages, for example, can build faster, more effective support for new behaviors or processes in environments where management capacity to engage deeply and frequently with every employee is constrained by time and geography. Digitizing five areas in particular can help make internal change efforts more effective and enduring. 1. Provide just-in-time feedback The best feedback processes are designed to offer the right information when the recipient can actually act on it. Just-in-time feedback gives recipients the opportunity to make adjustments to their behavior and to witness the effects of these adjustments on performance. Consider the experience of a beverage company experiencing sustained share losses and stagnant market growth in a highly competitive market in Africa. The challenge was to motivate 1,000-plus sales representatives to sell with greater urgency and effectiveness. A simple SMS message system was implemented to keep the widely distributed sales reps, often on the road for weeks at a time, plugged into the organization. Each rep received two to three daily SMS messages with personalized performance information, along with customer and market insights. For example, one message might offer feedback on which outlets had placed orders below target; another would alert the rep to a situation that indicated a need for increased orders, such as special events or popular brands that were trending in the area. Within days of implementing the system, cross-selling and upselling rates increased to more than 50 percent from 4 percent, and within the first year, the solution delivered a $25 million increase in gross margin, which helped to swing a 1.5 percent market-share loss into a 1 percent gain. 2. Personalize the experience Personalization is about filtering information in a way that is uniquely relevant to the user and showing each individual's role in and contribution to a greater group goal. An easy-to-use system can be an effective motivator and engender positive peer pressure. This worked brilliantly for a rail yard looking to reduce the idle time of its engines and cars by up to 10 percent. It implemented a system that presented only the most relevant information to each worker at that moment, such as details on the status of a train under that worker's supervision, the precise whereabouts of each of the trains in the yard, or alerts indicating which train to work on. Providing such specific and relevant information helped workers clarify priorities, increase accountability, and reduce delays. 3. Sidestep hierarchy Creating direct connections among people across the organization allows them to sidestep cumbersome hierarchal protocols and shorten the time it takes to get things done. It also fosters more direct and instant connections that allow employees to share important information, find answers quickly, and get help and advice from people they trust. In the rail-yard example, a new digital communications platform was introduced to connect relevant parties right away, bypassing middlemen and ensuring that issues get resolved quickly and efficiently. For example, if the person in charge of the rail yard has a question about the status of an incoming train, he or she need only log into the system and tap the train icon to pose the question directly to the individuals working on that train. Previously, all calls and queries had to be routed through a central source. This ability to bridge organizational divides is a core advantage in increasing agility, collaboration, and effectiveness. 4. Build empathy, community, and shared purpose In increasingly global organizations, communities involved in change efforts are often physically distant from one another. Providing an outlet for colleagues to share and see all the information related to a task, including progress updates and informal commentary, can create an important esprit de corps . Specific tools are necessary to achieve this level of connectivity and commitment. Those that we have seen work well include shared dashboards, visualizations of activity across the team, ""gamification"" to bolster competition, and online forums where people can easily speak to one another (for example, linking a Twitter-like feed to a work flow or creating forums tied to leaderboards so people can easily discuss how to move up in the rankings). This approach worked particularly well with a leading global bank aiming to reduce critical job vacancies. The sourcing team made the HR process a shared experience, showing all stakeholders the end-to-end view-dashboards identifying vacancies; hiring requisitions made and approved; candidates identified, tested, and interviewed; offers made and accepted; and hire letters issued. This transparency and openness built a shared commitment to getting results, a greater willingness to deliver on one's own step in the process, and a greater willingness to help one another beyond functional boundaries. 5. Demonstrate progress Organizational change is like turning a ship: the people at the front can see the change but the people at the back may not notice for a while. Digital change tools are helpful in this case to communicate progress so that people can see what is happening in real time. More sophisticated tools can also show individual contributions toward the common goal. We have seen how this type of communication makes the change feel more urgent and real, which in turn creates momentum that can help push an organization to a tipping point where a new way of doing things becomes the way things are done. Digital tools and platforms, if correctly applied, offer a powerful new way to accelerate and amplify the ability of an organization to change. However, let's be clear: the tool should not drive the solution. Each company should have a clear view of the new behavior it wants to reinforce and find a digital solution to support it. The best solutions are tightly focused on a specific task and are rolled out only after successful pilots are completed. The chances of success increase when management actively encourages feedback from users and incorporates it to give them a sense of ownership in the process. About the author(s) Boris Ewenstein is a principal in McKinsey's Johannesburg office, where Wesley Smith is a consultant and Ashvin Sologar is an associate principal.",en,190
19,600,1461698179,CONTENT SHARED,6044362651232258738,6013226412048763966,4613953821207157764,,,,HTML,http://vocesa.uol.com.br/noticias/acervo/cinco-competencias-comportamentais-para-voce-ser-um-bom-lider.phtml,cinco competências comportamentais para você ser um bom líder,"escritório | Crédito: pixabay Por que algumas pessoas se tornam bem-sucedidas e outras não? Essa era a pergunta que incomodava Marcelo Veras, professor de planejamento de carreira e presidente da Inova Business School, de Campinas. Para ele, livros, workshops e cursos não eram o suficiente para responder à questão, que ouvia constantemente dos alunos em sala de aula. ""Queria saber o que as pessoas que de fato chegaram ao topo tinham a dizer sobre isso"", afirma. Foi assim que, em julho de 2006, começou uma pesquisa sobre o assunto. Procurou pessoas em posições importantes de liderança, como diretores nacionais, vice-presidentes e presidentes para fazer uma pergunta simples: ""Quais competências o trouxeram até aqui e como você definiria cada uma delas?"". Foram mais de 170 entrevistados desde então. A partir das respostas, Marcelo reuniu uma lista das principais habilidades apontadas pelos bem-sucedidos. Elas são divididas em três categorias: comportamentais (como agimos em relação a nós mesmos e às pessoas); técnicas (domínio da área de atuação e de competências básicas de linguagem e leitura); e de gestão, que, claro, têm a ver com nossa atitude na condição de líderes de pessoas e de negócios. Marcelo compara essas competências a um macarrão à bolonhesa. As competências técnicas são o espaguete, as comportamentais, o molho e o resultado final, as competências de gestão, são o macarrão à bolonhesa. ""No curto prazo, ter apenas algumas competências funciona, mas, no fim, só uma combinação sólida é que mantém os líderes em seus cargos"", diz. Ou seja, as habilidades fazem mais sentido quando combinadas entre si e usadas de forma coerente. E, claro, dificilmente alguém terá todas elas superdesenvolvidas, mas criará um conjunto sólido delas - a sua própria receita. ""Querer desenvolver todas as competências no mesmo grau é utopia"", diz Adriana Prates, presidente da Dasein Executive Search, consultoria de recrutamento, de Belo Horizonte. ""As pessoas são diferentes e vão se destacar por diferentes motivos."" O segredo é identificar quais são as mais importantes para você. Ter essa clareza nem sempre é fácil, até porque envolve aceitar as limitações que temos, além de um conhecimento aprofundado de si mesmo. Esse entendimento serve, inclusive, para ver quando vale mais melhorar os pontos fortes e deixar os fracos de lado. Tudo isso demanda saber escutar os outros e receber bem os feedbacks, além de criar o hábito de pensar sobre si mesmo. Para fazer isso, Silvana Mello, diretora da Lee Hecht Harrison, consultoria de transição de carreira, de São Paulo, propõe um exercício de autoanálise baseado em três dimensões. A primeira é tentar definir o que se busca em termos de carreira e vida no longo prazo. A segunda é entender por que você busca esses objetivos e o que motiva suas atitudes. A terceira é pensar como você fará para alcançar esses objetivos e que valores usará para chegar lá. ""Gosto desse modelo de tripé porque ele serve para buscar uma coerência no dia a dia e se obrigar a questionar sempre para onde você está indo e como"", diz Silvana. De fato, um dos principais fatores que determinam o sucesso de uma empreitada é a clareza sobre por que se está fazendo aquilo. Mas, além disso, é preciso olhar para fora e notar como você está se comportando em relação ao meio em que atua. Isso significa prestar atenção ao que está acontecendo e identificar quais as demandas implícitas e explícitas das empresas e de seus colegas. Normalmente, o melhor sinal de que é preciso desenvolver uma competência é quando você percebe que não é (ou não foi) capaz de lidar tão bem quanto gostaria com uma situação. Expor-se a diferentes cenários - dentro ou fora do trabalho - facilita esse aprendizado. ""Saia da rotina de vez em quando para perceber coisas novas"", diz Paula Chimenti, professora do Coppead, escola de negócios da Universidade Federal do Rio de Janeiro. É a melhor forma de perceber o que você ainda precisa melhorar e o que já tem de bom. E ter essa percepção é o que ajuda na motivação. ""É difícil desenvolver algo se você não sentiu a necessidade"", diz. Por isso, é preciso ter um olhar constantemente voltado à melhoria e ao crescimento pessoal para dar conta de notar seus pontos fortes e fracos. O desenvolvimento de competências não é um processo isolado, mas combinado a diversos fatores: seus objetivos e personalidade, a necessidade dos outros e o meio em que você quer crescer. ""A relação entre competência e o contexto é inseparável"", diz Roberto Aylmer, professor da Fundação Dom Cabral, de Minas Gerais. O que vai diferenciar cada um são as atitudes, ou seja, as competências comportamentais. Já as competências técnicas são obtidas por meio de estudo e aprendizado contínuo. As de gestão são como você combina as anteriores de forma a ser um líder bem-sucedido. A seguir você encontra os cinco ingredientes relacionados com o comportamento que mais levaram as pessoas ao sucesso. ""Você não tem como controlar os problemas, mas pode melhorar a forma como reage a eles"", diz Adriana Prates, da Dasein. Ter autonomia em relação aos sentimentos para escolher como vai se comportar faz parte do equilíbrio emocional. Para chegar a esse ponto, é preciso ser capaz de entender suas próprias emoções - que não devem ser suprimidas ou ignoradas, mas geridas. Assistir a si mesmo no dia a dia e perceber como você se sente e quais tipos de situação trazem determinadas reações é uma forma de melhorar essa habilidade. Saber, por exemplo, que você tende a ficar alterado com um tipo de cenário pode ajudá-lo a resolver o que causa aquilo e a monitorar ocasiões futuras. A resposta pode ser desde encontrar válvulas de escape, como um hobby, até fazer terapia ou mudar o ponto de vista. Se for difícil perceber onde estão seus pontos frágeis, peça a opinião de pessoas em quem você confia. ""É como exercício físico, não dá para fazer um tempo e depois parar"", diz Adriana. ""Ser flexível é aceitar o desconhecido"", diz Silvana Mello, da Lee Hecht Harrison. Sair da zona de conforto é difícil, mas essencial para ter flexibilidade para encarar coisas novas e mudar de ideia. ""Tenha em mente que sempre podemos crescer mais, e que para isso precisamos conhecer o novo"", afirma. Fora do trabalho, vale desenvolver essa característica sempre que possível, se colocando em situações diferentes. A experiência diversificada, aliás, o ajudará a perceber quando você deve ser mais firme e quando deve mudar de ideia. Ser humilde em relação ao quanto você mesmo sabe sobre as coisas é importante. ""Hoje as pessoas estão muito mimadas e pouco flexíveis"", diz Adriana. ""Essa é com certeza uma competência que fará toda diferença nas empresas."" Mostre essa habilidade tendo abertura a opiniões diferentes das suas e mantendo o debate focado em ideias, e não em pessoas. O objetivo deve ser conseguir o melhor para todos - e não ter razão sempre. 3. Comprometimento e Execução Dificilmente uma pessoa que deixa tarefas para a última hora e não consegue pensar nos resultados chegará ao topo. Mas, muitas vezes, é difícil manter um desempenho consistente, porque isso exige clareza sobre seus objetivos. ""Quando você vê no trabalho um meio para atingir um fim, é mais fácil encontrar o comprometimento"", diz Adriana. Se você não cumpre seus prazos com a eficiência e rapidez que deveria, reavalie o que o impede de se envolver com aquela tarefa. Aplique o mesmo comprometimento na vida pessoal: não se atrase para encontros e cumpra sua palavra. Como qualquer hábito, o senso de urgência funciona melhor quando entra na rotina. 4. Etiqueta pessoal e profissional ""Etiqueta é saber reduzir a dissonância, a diferença entre você e o ambiente"", diz Roberto Aylmer, da Fundação Dom Cabral. ""Quando a pessoa lê o ambiente e adota uma postura e linguagem compatíveis, pode ter uma grande força de integração."" É verdade que as primeiras impressões também têm a ver com a forma como nos portamos. Uma pessoa mais gentil passará uma impressão de maior credibilidade, ao mesmo tempo em que uma pessoa supercompetente mas pouco respeitosa causará uma péssima impressão. O jeito é prestar atenção em quem é admirado, estudar regras de etiqueta e, na dúvida, perguntar aos outros sobre qual a maneira apropriada de se vestir e de se comportar em certos ambientes. 5. Relacionamento e Network Saber trabalhar com os outros para um objetivo em comum é requisito básico, não importa a sua posição. Se você não faz ideia de como o que você fala pode ser recebido pelos outros, é mau sinal. ""É necessário entender o impacto que causa nas pessoas e não querer sempre que seu desejo prevaleça"", afirma Márcia Portazio, coordenadora do ESPM Carreiras, de São Paulo. E quem tem boa capacidade de relacionamento não faz discriminação e trata do mesmo modo o estagiário e o presidente - com atenção e respeito. Se você não está convencido sobre isso, pense também que as posições hoje podem rapidamente se inverter. O network precisa ser pensado a partir do que você oferecerá para o outro, e não só do que o outro pode trazer a você. Esta matéria faz parte da reportagem ""20 competências essenciais para você ser um bom líder"" Você S/A | Edição 212 | Março de 2016",pt,184
20,580,1461608361,CONTENT SHARED,5238119115012015307,-3390049372067052505,6068971232842203123,,,,HTML,https://hbr.org/2016/05/embracing-agile,embracing agile,"Idea in Brief The Problem Agile methods such as scrum, kanban, and lean development are spreading beyond IT to other functions. Although some companies are scoring big improvements in productivity, speed to market, and customer and employee satisfaction, others are struggling. The Root Cause Leaders don't really understand agile. As a result, they unwittingly continue to employ conventional management practices that undermine agile projects. The Solution Learn the basics of agile. Understand the conditions in which it does or doesn't work. Start small and let it spread organically. Allow ""master"" teams to customize it. Employ agile at the top. Destroy the barriers to agile behaviors. Agile innovation methods have revolutionized information technology. Over the past 25 to 30 years they have greatly increased success rates in software development, improved quality and speed to market, and boosted the motivation and productivity of IT teams. Now agile methodologies-which involve new values, principles, practices, and benefits and are a radical alternative to command-and-control-style management-are spreading across a broad range of industries and functions and even into the C-suite. National Public Radio employs agile methods to create new programming. John Deere uses them to develop new machines, and Saab to produce new fighter jets. Intronis, a leader in cloud backup services, uses them in marketing. C.H. Robinson, a global third-party logistics provider, applies them in human resources. Mission Bell Winery uses them for everything from wine production to warehousing to running its senior leadership group. And GE relies on them to speed a much-publicized transition from 20th-century conglomerate to 21st-century ""digital industrial company."" By taking people out of their functional silos and putting them in self-managed and customer-focused multidisciplinary teams, the agile approach is not only accelerating profitable growth but also helping to create a new generation of skilled general managers. The spread of agile raises intriguing possibilities. What if a company could achieve positive returns with 50% more of its new-product introductions? What if marketing programs could generate 40% more customer inquiries? What if human resources could recruit 60% more of its highest-priority targets? What if twice as many workers were emotionally engaged in their jobs? Agile has brought these levels of improvement to IT. The opportunity in other parts of the company is substantial. But a serious impediment exists. When we ask executives what they know about agile, the response is usually an uneasy smile and a quip such as ""Just enough to be dangerous."" They may throw around agile-related terms (""sprints,"" ""time boxes"") and claim that their companies are becoming more and more nimble. But because they haven't gone through training, they don't really understand the approach. Consequently, they unwittingly continue to manage in ways that run counter to agile principles and practices, undermining the effectiveness of agile teams in units that report to them. These executives launch countless initiatives with urgent deadlines rather than assign the highest priority to two or three. They spread themselves and their best people across too many projects. They schedule frequent meetings with members of agile teams, forcing them to skip working sessions or send substitutes. Many of them become overly involved in the work of individual teams. They talk more than listen. They promote marginal ideas that a team has previously considered and back-burnered. They routinely overturn team decisions and add review layers and controls to ensure that mistakes aren't repeated. With the best of intentions, they erode the benefits that agile innovation can deliver. Further Reading Innovation Digital Article IT's most famous idea didn't start in IT. Innovation is what agile is all about. Although the method is less useful in routine operations and processes, these days most companies operate in highly dynamic environments. They need not just new products and services but also innovation in functional processes, particularly given the rapid spread of new software tools. Companies that create an environment in which agile flourishes find that teams can churn out innovations faster in both those categories. From our work advising and studying such companies, we have discerned six crucial practices that leaders should adopt if they want to capitalize on agile's potential. 1. Learn How Agile Really Works Some executives seem to associate agile with anarchy (everybody does what he or she wants to), whereas others take it to mean ""doing what I say, only faster."" But agile is neither. (See the sidebar ""Agile Values and Principles."") It comes in several varieties, which have much in common but emphasize slightly different things. They include scrum, which emphasizes creative and adaptive teamwork in solving complex problems; lean development, which focuses on the continual elimination of waste; and kanban, which concentrates on reducing lead times and the amount of work in process. One of us (Jeff Sutherland) helped develop the scrum methodology and was inspired to do so in part by ""The New New Product Development Game,"" a 1986 HBR article coauthored by another of us (Hirotaka Takeuchi). Because scrum and its derivatives are employed at least five times as often as the other techniques, we will use its methodologies to illustrate agile practices. There are at least a dozen agile innovation methodologies, which share values and principles but differ in their emphases. Experts often combine various approaches. Here are three of the most popular forms and the contexts in which each works best. Guiding Principles Empower creative, cross-functional teams Visualize workflows and limit work in process Eliminate waste from the system as a whole Favorable Conditions for Adoption Creative cultures with high levels of trust and collaboration, or Radical innovation teams that want to change their working environment Process-oriented cultures that prefer evolutionary improvements with few prescribed practices Process-oriented cultures that prefer evolutionary improvements with overarching values but no prescribed practices Prescribed Roles Initiative owners responsible for rank ordering team priorities and delivering value to customers and the business Process facilitators who guide the work process Small, cross-functional, innovation teams None None Prescribed Work Rules Five events: Sprint planning to prepare for the next round of work Fixed time sprints of consistent duration (1-4 weeks) to create a potentially releasable product increment Daily stand-ups of 15 minutes to review progress and surface impediments Sprint reviews that inspect the new working increment Sprint retrospectives for the team to inspect and improve itself Three deliverables (or ""artifacts""): Portfolio backlog, a fluid and rank-ordered list of potential innovation features Sprint backlog, the subset of portfolio backlog items selected for completion in the next sprint Releasable working increments Start with what you do now Visualize workflows and stages Limit the work in process at each development stage Measure and improve cycle times None Approach to Cultural Change Quickly adopt minimally prescribed practices, even if they differ substantially from those in the rest of the organization Master prescribed practices and then adapt them through experimentation Respect current structures and processes Increase visibility into workflows Encourage gradual, collaborative changes Respect current structures and processes Stress agile values throughout the organization while minimizing organizational resistance Advantages Facilitates radical breakthroughs while (unlike skunkworks) retaining the benefits of operating as part of the parent organization Delivers the most valuable innovations earliest Rapidly increases team happiness Builds general management skills Avoids clashes with the parent organization's culture Maximizes the contributions of team members through flexible team structures and work cycles Facilitates rapid responses to urgent issues through flexible work cycles Optimizes the system as a whole and engages the entire organization Provides the ultimate flexibility in customizing work practices Challenges Leaders may struggle to prioritize initiatives and relinquish control to self-managing teams New matrix-management skills are required to coordinate dozens or hundreds of multi-disciplinary teams Fixed iteration times may not be suitable for some problems (especially those that arise on a daily basis) Some team members may be underutilized in certain sprint cycles Practitioners must figure out how best to apply most agile values and principles Wide variation in practices can complicate the prioritization of initiatives and coordination among teams When initiatives don't succeed, it can be hard to determine whether teams selected the wrong tools or used the right tools in the wrong ways Novices trying to change behaviors may find the lack of prescriptive methodologies frustrating Evolutionary improvements can make radical breakthroughs less likely and major improvements less rapid Leaders need to make the grind of continuously eliminating waste feel inspirational and fun Source Darrell K. Rigby, Jeff Sutherland, and Hirotaka Takeuchi From ""Embracing Agile,"" April 2016 The fundamentals of scrum are relatively simple. To tackle an opportunity, the organization forms and empowers a small team, usually three to nine people, most of whom are assigned full-time. The team is cross-functional and includes all the skills necessary to complete its tasks. It manages itself and is strictly accountable for every aspect of the work. The team's ""initiative owner"" (also known as a product owner) is ultimately responsible for delivering value to customers (including internal customers and future users) and to the business. The person in this role usually comes from a business function and divides his or her time between working with the team and coordinating with key stakeholders: customers, senior executives, and business managers. The initiative owner may use a technique such as design thinking or crowdsourcing to build a comprehensive ""portfolio backlog"" of promising opportunities. Then he or she continually and ruthlessly rank-orders that list according to the latest estimates of value to internal or external customers and to the company. The initiative owner doesn't tell the team who should do what or how long tasks will take. Rather, the team creates a simple road map and plans in detail only those activities that won't change before execution. Its members break the highest-ranked tasks into small modules, decide how much work the team will take on and how to accomplish it, develop a clear definition of ""done,"" and then start building working versions of the product in short cycles (less than a month) known as sprints. A process facilitator (often a trained scrum master) guides the process. This person protects the team from distractions and helps it put its collective intelligence to work. The process is transparent to everyone. Team members hold brief daily ""stand-up"" meetings to review progress and identify roadblocks. They resolve disagreements through experimentation and feedback rather than endless debates or appeals to authority. They test small working prototypes of part or all of the offering with a few customers for short periods of time. If customers get excited, a prototype may be released immediately, even if some senior executive isn't a fan, or others think it needs more bells and whistles. The team then brainstorms ways to improve future cycles and prepares to attack the next top priority. In 2001, 17 rebellious software developers (including Jeff Sutherland) met in Snowbird, Utah, to share ideas for improving traditional ""waterfall"" development, in which detailed requirements and execution plans are created up front and then passed sequentially from function to function. This approach worked fine in stable environments, but not when software markets began to change rapidly and unpredictably. In that scenario, product specifications were outdated by the time the software was delivered to customers, and developers felt oppressed by bureaucratic procedures. The rebels proposed four new values for developing software, described principles to guide adherence to those values, and dubbed their call to arms ""The Agile Manifesto."" To this day, development frameworks that follow these values and principles are known as agile techniques. Here is an adapted version of the manifesto: People Over Processes and Tools Projects should be built around motivated individuals who are given the support they need and trusted to get the job done. Teams should abandon the assembly-line mentality in favor of a fun, creative environment for problem solving, and should maintain a sustainable pace. Employees should talk face-to-face and suggest ways to improve their work environment. Management should remove impediments to easier, more fruitful collaboration. Working Prototypes Over Excessive Documentation Innovators who can see their results in real market conditions will learn faster, be happier, stay longer, and do more-valuable work. Teams should experiment on small parts of the product with a few customers for short periods, and if customers like them, keep them. If customers don't like them, teams should figure out fixes or move on to the next thing. Team members should resolve arguments with experiments rather than endless debates or appeals to authority. Respond to Change Rather Than Follow a Plan Most detailed predictions and plans of conventional project management are a waste of time and money. Although teams should create a vision and plan, they should plan only those tasks that won't have changed by the time they get to them. And people should be happy to learn things that alter their direction, even late in the development process. That will put them closer to the customer and make for better results. Customer Collaboration Over Rigid Contracts Time to market and cost are paramount, and specifications should evolve throughout the project, because customers can seldom predict what they will actually want. Rapid prototyping, frequent market tests, and constant collaboration keep work focused on what they will ultimately value. Compared with traditional management approaches, agile offers a number of major benefits, all of which have been studied and documented. It increases team productivity and employee satisfaction. It minimizes the waste inherent in redundant meetings, repetitive planning, excessive documentation, quality defects, and low-value product features. By improving visibility and continually adapting to customers' changing priorities, agile improves customer engagement and satisfaction, brings the most valuable products and features to market faster and more predictably, and reduces risk. By engaging team members from multiple disciplines as collaborative peers, it broadens organizational experience and builds mutual trust and respect. Finally, by dramatically reducing the time squandered on micromanaging functional projects, it allows senior managers to devote themselves more fully to higher-value work that only they can do: creating and adjusting the corporate vision; prioritizing strategic initiatives; simplifying and focusing work; assigning the right people to tasks; increasing cross-functional collaboration; and removing impediments to progress. 2. Understand Where Agile Does or Does Not Work Agile is not a panacea. It is most effective and easiest to implement under conditions commonly found in software innovation: The problem to be solved is complex; solutions are initially unknown, and product requirements will most likely change; the work can be modularized; close collaboration with end users (and rapid feedback from them) is feasible; and creative teams will typically outperform command-and-control groups. In our experience, these conditions exist for many product development functions, marketing projects, strategic-planning activities, supply-chain challenges, and resource allocation decisions. They are less common in routine operations such as plant maintenance, purchasing, sales calls, and accounting. And because agile requires training, behaviorial change, and often new information technologies, executives must decide whether the anticipated payoffs will justify the effort and expense of a transition. Conditions Favorable Unfavorable Market Environment Customer preferences and solution options change frequently. Market conditions are stable and predictable. Customer Involvement Close collaboration and rapid feedback are feasible. Customers know better what they want as the process progresses. Requirements are clear at the outset and will remain stable. Customers are unavailable for constant collaboration. Innovation Type Problems are complex, solutions are unknown, and the scope isn't clearly defined. Product specifications may change. Creative breakthroughs and time to market are important. Cross-functional collaboration is vital. Similar work has been done before, and innovators believe the solutions are clear. Detailed specifications and work plans can be forecast with confidence and should be adhered to. Problems can be solved sequentially in functional silos. Modularity of Work Incremental developments have value, and customers can use them. Work can be broken into parts and conducted in rapid, iterative cycles. Late changes are manageable. Customers cannot start testing parts of the product until everything is complete. Late changes are expensive or impossible. Impact of Interim Mistakes They provide valuable learning. They may be catastrophic. Agile innovation also depends on having a cadre of eager participants. One of its core principles is ""Build projects around motivated individuals. Give them the environment and support they need, and trust them to get the job done."" When the majority of a company, a function, or a team chooses to adopt agile methodologies, leaders may need to press the holdouts to follow suit or even replace them. But it's better to enlist passionate volunteers than to coerce resisters. OpenView Venture Partners, a firm that has invested in about 30 companies, took this path. Having learned about agile from some of the companies in its portfolio, Scott Maxwell, the firm's founder, began using its methodologies at the firm itself. He found that they fit some activities more easily than others. Agile worked well for strategic planning and marketing, for instance, where complex problems can often be broken into modules and cracked by creative multidisciplinary teams. That wasn't the case for selling: Any sales call can change a representative's to-do list on the spot, and it would be too complicated and time-consuming to reassemble the sales team, change the portfolio backlog, and reassign accounts every hour. Maxwell provided the companies in OpenView's portfolio with training in agile principles and practices and let them decide whether to adopt the approach. Some of them immediately loved the idea of implementing it; others had different priorities and decided to hold off. Intronis was one fan. Its marketing unit at the time relied on an annual plan that focused primarily on trade shows. Its sales department complained that marketing was too conservative and not delivering results. So the company hired Richard Delahaye, a web developer turned marketer, to implement agile. Under his guidance the marketing team learned, for example, how to produce a topical webinar in a few days rather than several weeks. (A swiftly prepared session on CryptoLocker malware attracted 600 registrants-still a company record.) Team members today continue to create calendars and budgets for the digital marketing unit, but with far less line-item detail and greater flexibility for serendipitous developments. The sales team is much happier. 3. Start Small and Let the Word Spread Large companies typically launch change programs as massive efforts. But the most successful introductions of agile usually start small. They often begin in IT, where software developers are likely to be familiar with the principles. Then agile might spread to another function, with the original practitioners acting as coaches. Each success seems to create a group of passionate evangelists who can hardly wait to tell others in the organization how well agile works. The adoption and expansion of agile at John Deere, the farm equipment company, provides an example. George Tome, a software engineer who had become a project manager within Deere's corporate IT group, began applying agile principles in 2004 on a low-key basis. Gradually, over several years, software development units in other parts of Deere began using them as well. This growing interest made it easier to introduce the methodology to the company's business development and marketing organizations. In 2012 Tome was working as a manager in the Enterprise Advanced Marketing unit of the R&D group responsible for discovering technologies that could revolutionize Deere's offerings. Jason Brantley, the unit head, was concerned that traditional project management techniques were slowing innovation, and the two men decided to see whether agile could speed things up. Tome invited two other unit managers to agile training classes. But all the terminology and examples came from software, and to one of the managers, who had no software background, they sounded like gibberish. Tome realized that others would react the same way, so he tracked down an agile coach who knew how to work with people without a software background. In the past few years he and the coach have trained teams in all five of the R&D group's centers. Tome also began publishing weekly one-page articles about agile principles and practices, which were e-mailed to anyone interested and later posted on Deere's Yammer site. Hundreds of Deere employees joined the discussion group. ""I wanted to develop a knowledge base about agile that was specific to Deere so that anyone within the organization could understand it,"" Tome says. ""This would lay the foundation for moving agile into any part of the company."" Using agile techniques, Enterprise Advanced Marketing has significantly compressed innovation project cycle times-in some cases by more than 75%. One example is the development in about eight months of a working prototype of a new ""machine form"" that Deere has not yet disclosed. ""If everything went perfectly in a traditional process,"" Brantley says, ""it would be a year and a half at best, and it could be as much as two and a half or three years."" Agile generated other improvements as well. Team engagement and happiness in the unit quickly shot from the bottom third of companywide scores to the top third. Quality improved. Velocity (as measured by the amount of work accomplished in each sprint) increased, on average, by more than 200%; some teams achieved an increase of more than 400%, and one team soared 800%. Success like this attracts attention. Today, according to Tome, in almost every area at John Deere someone is either starting to use agile or thinking about how it could be used. 4. Allow ""Master"" Teams to Customize Their Practices Japanese martial arts students, especially those studying aikido, often learn a process called shu-ha-ri. In the shu state they study proven disciplines. Once they've mastered those, they enter the ha state, where they branch out and begin to modify traditional forms. Eventually they advance to ri, where they have so thoroughly absorbed the laws and principles that they are free to improvise as they choose. Mastering agile innovation is similar. Before beginning to modify or customize agile, a person or team will benefit from practicing the widely used methodologies that have delivered success in thousands of companies. For instance, it's wise to avoid beginning with part-time assignment to teams or with rotating membership. Empirical data shows that stable teams are 60% more productive and 60% more responsive to customer input than teams that rotate members. Further Reading Managing People Feature The Toyota principles can be applied in operations involving expertise. Over time, experienced practitioners should be permitted to customize agile practices. For example, one principle holds that teams should keep their progress and impediments constantly visible. Originally, the most popular way of doing this was by manually advancing colored sticky notes from the ""to-do"" column to ""doing"" to ""done"" on large whiteboards (known as kanban boards). Many teams are still devoted to this practice and enjoy having nonmembers visit their team rooms to view and discuss progress. But others are turning to software programs and computer screens to minimize input time and allow the information to be shared simultaneously in multiple locations. A key principle guides this type of improvisation: If a team wants to modify particular practices, it should experiment and track the results to make sure that the changes are improving rather than reducing customer satisfaction, work velocity, and team morale. Spotify, the music-streaming company, exemplifies an experienced adapter. Founded in 2006, the company was agile from birth, and its entire business model, from product development to marketing and general management, is geared to deliver better customer experiences through agile innovation. But senior leaders no longer dictate specific practices; on the contrary, they encourage experimentation and flexibility as long as changes are consistent with agile principles and can be shown to improve outcomes. As a result, practices vary across the company's 70 ""squads"" (Spotify's name for agile innovation teams) and its ""chapters"" (the company term for functional competencies such as user interface development and quality testing). Although nearly every squad consists of a small cross-functional team and uses some form of visual progress tracking, ranked priorities, adaptive planning, and brainstorming sessions on how to improve the work process, many teams omit the ""burndown"" charts (which show work performed and work remaining) that are a common feature of agile teams. Nor do they always measure velocity, keep progress reports, or employ the same techniques for estimating the time required for a given task. These squads have tested their modifications and found that they improve results. 5. Practice Agile at the Top Some C-suite activities are not suited to agile methodologies. (Routine and predictable tasks-such as performance assessments, press interviews, and visits to plants, customers, and suppliers-fall into this category.) But many, and arguably the most important, are. They include strategy development and resource allocation, cultivating breakthrough innovations, and improving organizational collaboration. Senior executives who come together as an agile team and learn to apply the discipline to these activities achieve far-reaching benefits. Their own productivity and morale improve. They speak the language of the teams they are empowering. They experience common challenges and learn how to overcome them. They recognize and stop behaviors that impede agile teams. They learn to simplify and focus work. Results improve, increasing confidence and engagement throughout the organization. A number of companies have reallocated 25% or more of selected leaders' time from functional silos to agile leadership teams. These teams rank-order enterprisewide portfolio backlogs, establish and coordinate agile teams elsewhere in the organization to address the highest priorities, and systematically eliminate barriers to their success. Here are three examples of C-suites that took up agile: 1. Catching up with the troops. Systematic, a 525-employee software company, began applying agile methodologies in 2005. As they spread to all its software development teams, Michael Holm, the company's CEO and cofounder, began to worry that his leadership team was hindering progress. ""I had this feeling that I was saying, 'Follow me-I'm just behind you,'"" he told us. ""The development teams were using scrum and were doing things differently, while the management team was stuck doing things the same old-fashioned way""-moving too slowly and relying on too many written reports that always seemed out-of-date. So in 2010 Holm decided to run his nine-member executive group as an agile team. The team reprioritized management activities, eliminating more than half of recurring reports and converting others to real-time systems while increasing attention to business-critical items such as sales proposals and customer satisfaction. The group started by meeting every Monday for an hour or two but found the pace of decision making too slow. So it began having daily 20-minute stand-ups at 8:40 am to discuss what members had done the day before, what they would do that day, and where they needed help. More recently the senior team began to use physical boards to track its own actions and the improvements coming from the business units. Other functions, including HR, legal, finance, and sales, now operate in much the same way. 2. Speeding a corporate transition. In 2015 General Electric rebranded itself as a ""digital industrial company,"" with a focus on digitally enabled products. Part of the transformation involved creating GE Digital, an organizational unit that includes all 20,000-plus of the company's software-related employees. Brad Surak, who began his career as a software engineer and is now GE Digital's COO, was intimately familiar with agile. He piloted scrum with the leadership team responsible for developing industrial internet applications and then, more recently, began applying it to the new unit's management processes, such as operating reviews. Surak is the initiative owner, and an engineering executive is the scrum master. Together they have prioritized backlog items for the executive team to address, including simplifying the administrative process that teams follow to acquire hardware and solving knotty pricing issues for products requiring input from multiple GE businesses. Agile Alliance: For guides to agile practices, links to ""The Agile Manifesto,"" and training videos Scrum Alliance: For a ""Scrum Guide,"" conference presentations and videos, and the ""State of Scrum"" research report ScrumLab Open: For training presentations, videos, webinars, and published papers Annual State of Agile Survey: For key statistics such as usage rates, customer benefits, barriers to adoption and success, and specific practices used The scrum team members run two-week sprints and conduct stand-up meetings three times a week. They chart their progress on a board in an open conference room where any employee can see it. Surak says, ""It takes the mystery out of what executives do every day. Our people want to know if we are in tune with what they care about as employees."" The team collects employee happiness surveys, conducts root cause analysis on the impediments to working more effectively, and reports back to people throughout the organization, saying (in effect), ""We heard you. Here is how we will improve things."" Surak believes that this shows the organization that ""executives work in the same ways as engineers,"" increasing employee motivation and commitment to agile practices. 3. Aligning departments and functions on a common vision. Erik Martella, the vice president and general manager of Mission Bell Winery, a production facility of Constellation Brands, introduced agile and helped it spread throughout the organization. Leaders of each department served as initiative owners on the various agile teams within their departments. Those individual teams achieved impressive results, but Martella worried that their time was being spread too thin and that department and enterprise priorities weren't always aligned. He decided to pull department leaders into an executive agile team focused on the enterprise initiatives that held the greatest value and the greatest opportunity for cross-functional collaboration, such as increasing process flows through the warehouse. The team is responsible for building and continually refining the backlog of enterprise priorities, ensuring that agile teams are working on the right problems and have sufficient resources. Team members also protect the organization from pet projects that don't deserve high priority. For instance, shortly after Martella started implementing agile, he received an e-mail from a superior in Constellation's corporate office suggesting that the winery explore a personal passion of the sender. Previously, Martella might have responded, ""OK, we'll jump right on it."" Instead, he replied that the winery was following agile principles: The idea would be added to the list of potential opportunities and prioritized. As it happened, the executive liked the approach-and when he was informed that his suggestion had been assigned a low priority, he readily accepted the decision. Working on agile teams can also help prepare functional managers-who rarely break out of their silos in today's overspecialized organizations-for general management roles. It exposes them to people in other disciplines, teaches collaborative practices, and underscores the importance of working closely with customers-all essential for future leaders. 6. Destroy the Barriers to Agile Behaviors Research by Scrum Alliance, an independent nonprofit with 400,000-plus members, has found that more than 70% of agile practitioners report tension between their teams and the rest of the organization. Little wonder: They are following different road maps and moving at different speeds. Here's a telling example: A large financial services company we examined launched a pilot to build its next mobile app using agile methodologies. Of course, the first step was to assemble a team. That required a budget request to authorize and fund the project. The request went into the batch of submissions vying for approval in the next annual planning process. After months of reviews, the company finally approved funding. The pilot produced an effective app that customers praised, and the team was proud of its work. But before the app was released, it had to pass vulnerability testing in a traditional ""waterfall"" process (a protracted sequence in which the computer code is tested for documentation, functionality, efficiency, and standardization), and the queue for the process was long. Then the app had to be integrated into core IT systems-which involved another waterfall process with a six-to-nine-month logjam. In the end, the total time to release improved very little. Further Reading Human resource management Magazine Article The Toyota story has been intensively researched and painstakingly documented, yet what really happens inside the company remains a mystery. Here's new insight into the unspoken rules that give Toyota its competitive edge. Here are some techniques for destroying such barriers to agile: Get everyone on the same page. Individual teams focusing on small parts of large, complex problems need to see, and work from, the same list of enterprise priorities-even if not all the teams responsible for those priorities are using agile processes. If a new mobile app is the top priority for software development, it must also be the top priority for budgeting, vulnerability testing, and software integration. Otherwise, agile innovations will struggle in implementation. This is a key responsibility of an executive team that itself practices agile. Don't change structures right away; change roles instead. Many executives assume that creating more cross-functional teams will necessitate major changes in organizational structure. That is rarely true. Highly empowered cross-functional teams do, by definition, need some form of matrix management, but that requires primarily that different disciplines learn how to work together simultaneously rather than separately and sequentially. Name only one boss for each decision. People can have multiple bosses, but decisions cannot. In an agile operating model it must be crystal clear who is responsible for commissioning a cross-functional team, selecting and replacing team members, appointing the team leader, and approving the team's decisions. An agile leadership team often authorizes a senior executive to identify the critical issues, design processes for addressing them, and appoint a single owner for each innovation initiative. Other senior leaders must avoid second-guessing or overturning the owner's decisions. It's fine to provide guidance and assistance, but if you don't like the results, change the initiative owner-don't incapacitate him or her. Focus on teams, not individuals. Studies by the MIT Center for Collective Intelligence and others show that although the intelligence of individuals affects team performance, the team's collective intelligence is even more important. It's also far easier to change. Agile teams use process facilitators to continually improve their collective intelligence-for example, by clarifying roles, teaching conflict resolution techniques, and ensuring that team members contribute equally. Shifting metrics from output and utilization rates (how busy people are) to business outcomes and team happiness (how valuable and engaged people are) also helps, as do recognition and reward systems that weight team results higher than individual efforts. Lead with questions, not orders. General George S. Patton Jr. famously advised leaders never to tell people how to do things: ""Tell them what to do, and they will surprise you with their ingenuity."" Rather than give orders, leaders in agile organizations learn to guide with questions, such as ""What do you recommend?"" and ""How could we test that?"" This management style helps functional experts grow into general managers, and it helps enterprise strategists and organizations evolve from silos battling for power and resources into collaborative cross-functional teams. Agile innovation has revolutionized the software industry, which has arguably undergone more rapid and profound change than any other area of business over the past 30 years. Now it is poised to transform nearly every other function in every industry. At this point, the greatest impediment is not the need for better methodologies, empirical evidence of significant benefits, or proof that agile can work outside IT. It is the behavior of executives. Those who learn to lead agile's extension into a broader range of business activities will accelerate profitable growth. A version of this article appeared in the May 2016 issue (pp.40-48, 50) of Harvard Business Review .",en,182
21,1578,1467219707,CONTENT SHARED,8901449108040307914,-6786856227257648356,4843271338129443641,,,,HTML,https://www.infoq.com/br/presentations/monitoramento-em-tempo-real-com-elasticsearch-e-kibana,monitoramento em tempo real com elasticsearch e kibana,"Resumo Descubra o que acontece com o seu código em produção, com Elasticsearch, Kibana e Codahale Metrics. Nessa apresentação explicarei como indexar em uma base nosql uma quantidade massiva de informações sobre a utilização do seu sistema, proporcionando uma forma de antecipar problemas e responder rapidamente sobre anomalias no seu código, em tempo real. Minibiografia Arquiteto de software a mais de 9 anos, projetando e conduzindo soluções complexas em grandes clientes. Formado na Universidade Federal de Uberlândia em 2002, iniciou sua carreira na Ci&T em 2004 e, desde 2014, atua como responsável pelo time de arquitetos na vertical de Resources & Logistics, garantindo que as práticas de arquitetura e tecnologias corretas sejam aplicada nestes projetos. A comunidade de desenvolvimento DevIsland organizou novamente o DEVDAY, um evento para desenvolvedores de software. Na sua sexta edição, o DEVDAY 2015 reuniu aproximadamente 500 desenvolvedores. Mais de 20 profissionais reconhecidos pelo mercado nacional que compartilharam seus conhecimentos nas 20 palestras do evento. Foram abordados diversos assuntos focados em desenvolvimento de software ou que de alguma forma afetam a vida de um desenvolvedor.",pt,179
22,1245,1464962305,CONTENT SHARED,2372438485070148864,3636910968448833585,-7707155044672153170,,,,HTML,https://www.infoq.com/articles/Continuous-Delivery-Maturity-Model,the continuous delivery maturity model,"The principles and methods of Continuous Delivery are rapidly gaining recognition as a successful strategy for true business agility. For many organizations the question is no longer ""why?"", but rather ""how?"" How do you start with Continuous Delivery, and how do you transform your organization to ensure sustainable results. This Maturity Model aims to give structure and understanding to some of the key aspects you need to consider when adopting Continuous Delivery in your organization. Why a maturity model? Continuous Delivery is all about seeing the big picture, to consider all aspects that affect the ability to develop and release your software. For any non-trivial business of reasonable size this will unfortunately include quite a lot of steps and activities. The end-to-end process of developing and releasing software is often long and cumbersome, it involves many people, departments and obstacles which can make the effort needed to implement Continuous Delivery seem overwhelming. Where do we start? Do we have to do everything, what parts can we leave out? Where do we get the biggest and quickest effect? These are questions that inevitably will come up when you start looking at implementing Continuous Delivery. This is why we created the Continuous Delivery Maturity Model, to give structure and understanding to the implementation of Continuous Delivery and its core components. Inspiration for this model comes mainly from our combined experiences in several continuous delivery implementation projects, and also from selected books, papers and blog-posts on the subject where Jez Humble & David Farley's groundbreaking book Continuous Delivery and Eric Minick & Jeffrey Fredricks terrific whitepaper Enterprise Continuous Delivery Model are two that deserve special recognition. With this model we aim to be broader, to extend the concept beyond automation and spotlight all the key aspects you need to consider for a successful Continuous Delivery implementation across the entire organization. A structured approach The journey that started with the Agile movement a decade ago is finally getting a strong foothold in the industry. Business leaders now have begun to embrace the fact that there is a new way of thinking about software development. A paradigm shift, if you like, that will inevitably segment the industry into those who struggle to keep up with competition, and those who have the ability to stay agile and ahead of competition by having an IT organization that responds quickly and efficiently, and serves as a true business enabler. IT can once again start pushing innovation instead of restraining it by expensive, slow, unpredictable and outdated processes. There are many ways to enter this new era and here we will describe a structured approach to attaining the best results. While agile methodologies often are described to best grow from inside the organization we have found that this approach also has limitations. Some parts of the organization are not mature enough to adapt and consequently inhibit development, creating organizational boundaries that can be very hard to break down. The best way to include the whole organization in the change is to establish a solid platform with some important prerequisites that will enable the organization to evolve in the right direction. This platform includes adopting specific tools, principles, methods and practices that we have organized into five key categories, Culture & Organization, Design & Architecture, Build & Deploy, Test & Versification and Information & Reporting. S tructuring Continuous Delivery implementation into these categories that follows a natural maturity progression will give you a solid base for a fast transformation with sustainable results. The purpose of the maturity model is to highlight these five essential categories, and to give you an understanding of how mature your company is. Your assessment will give you a good base when planning the implementation of Continuous Delivery and help you identify initial actions that will give you the best and quickest effect from your efforts. The model will indicate which practices are essential, which should be considered advanced or expert and what is required to move from one level to the next. Five levels The model defines five maturity levels: base, beginner, intermediate, advanced and expert. The model is calibrated with a typical industry average around the base level where we see most organizations are at today. Some organizations have beginner maturity in some categories and below base (outside the model) in others but on average base-level is the typical starting point for many organizations. The intermediate level is what we consider a mature level of continuous delivery practices where you benefit from the larger effects. The advanced level includes practices that will bring substantial value and effect to a higher effort. The last maturity level, expert, includes practices that will be very valuable for some organizations that want or need to specialize in certain practices. The levels are not strict and mandatory stages that needs to be passed in sequence, but rather should serve as a base for evaluation and planning. It is however important to try to keep the overall maturity level fairly even and to keep in mind that big changes may cause skepticism and reluctance in the organization, so an incremental approach to moving through the levels is recommended. Five categories The model also defines five categories that represent the key aspects to consider when implementing Continuous Delivery. Each category has it's own maturity progression but typically an organization will gradually mature over several categories rather than just one or two since they are connected and will affect each other to a certain extent. Culture & Organization The organization and it's culture are probably the most important aspects to consider when aiming to create a sustainable Continuous Delivery environment that takes advantage of all the resulting effects. Base A typical organization will have, at base level, started to prioritize work in backlogs, have some process defined which is rudimentarily documented and developers are practicing frequent commits into version control. Beginner Moving to beginner level, teams stabilize over projects and the organization has typically begun to remove boundaries by including test with development. Multiple backlogs are naturally consolidated into one per team and basic agile methods are adopted which gives stronger teams that share the pain when bad things happen. Intermediate At the intermediate level you will achieve more extended team collaboration when e.g. DBA, CM and Operations are beginning to be a part of the team or at least frequently consulted by the team. Multiple processes are consolidated and all changes, bugs, new features, emergency fixes, etc, follow the same path to production. Decisions are decentralized to the team and component ownership is defined which gives teams the ability to build in quality and to plan for sustainable product and process improvements. Advanced At the advanced level, the team will have the competence and confidence it needs to be responsible for changes all the way to production. Continuous improvement mechanisms are in place and e.g. a dedicated tools team is set up to serve other teams by improving tools and automation. At this level, releases of functionality can be disconnected from the actual deployment, which gives the projects a somewhat different role. A project can focus on producing requirements for one or multiple teams and when all or enough of those have been verified and deployed to production the project can plan and organize the actual release to users separately. This separation of concerns will optimize the projects flexibility of packaging and releasing functionality and at the same time give the development teams better flow and speed since they don't have to stack up changes and wait for coordinated project releases. Expert At expert level some organizations choose to make a bigger effort and form complete cross functional teams that can be completely autonomous. With extremely short cycle time and a mature delivery pipeline, such organizations have the confidence to adopt a strict roll-forward only strategy to production failures. Design & Architecture The design and architecture of your products and services will have an essential impact on your ability to adopt continuous delivery. If a system is built with continuous delivery principles and a rapid release mind set from the beginning, the journey will be much smoother. However, an upfront complete redesign of the entire system is not an attractive option for most organizations, which is why we have included this category in the maturity model. Base A typical organization will have one or more legacy systems of monolithic nature in terms of development, build and release. Many organizations at the base maturity level will have a diversified technology stack but have started to consolidate the choice of technology and platform, this is important to get best value from the effort spent on automation. Beginner At beginner level, the monolithic structure of the system is addressed by splitting the system into modules. Modules give a better structure for development, build and deployment but are typically not individually releasable like components. Doing this will also naturally drive an API managed approach to describe internal dependencies and also influence applying a structured approach to manage 3rd party libraries. At this level the importance of applying version control to database changes will also reveal itself. Intermediate Moving to intermediate level will result in a solid architectural base for continuous delivery when adopting branch by abstraction and other techniques for feature hiding for the purpose of minimizing repository branching to enable true continuous integration. At this level the work with modularization will evolve into identifying and breaking out modules into components that are self-contained and separately deployed. At this stage it will also be natural to start migrating scattered and ad-hoc managed application and runtime configuration into version control and treat it as part of the application just like any other code. Advanced At the advanced level you will have split the entire system into self contained components and adopted a strict api-based approach to inter-communication so that each component can be deployed and released individually. With a mature component based architecture, where every component is a self-contained releasable unit with business value, you can achieve small and frequent releases and extremely short release cycles. At this level it is easy for the business to experiment with rapid release of new features, monitor and verify expected business results; therefore techniques to push and graph business metrics from the application becomes important and natural part of the overall design and architecture. Expert At expert level, some organizations will evolve the component based architecture further and value the perfection of reducing as much shared infrastructure as possible by also treating infrastructure as code and tie it to application components. The result is a system that is totally reproducible from source control, from the O/S and all the way up to application. Doing this enables you to reduce a lot of complexity and cost in other tools and techniques for e.g. disaster recovery that serves to ensure that the production environment is reproducible. Instead of having a separate process, disaster recovery is simply done by pushing out the last release from the pipeline like any other release. This together with virtualization gives extreme flexibility in setting up test and production environments with minimum manual effort. Build & Deploy Build and deployment is of course core to Continuous Delivery and this is where a lot of tools and automation come into the pipeline; this is what is most is commonly perceived when Continuous Delivery is discussed. At first glance a typical mature delivery pipeline can be very overwhelming; depending on how mature the current build and deployment process is in the organization, the delivery pipeline can be more or less complex. In this category we will describe a logical maturity progression to give structure and understanding to the different parts and levels it includes. Base At a base level you will have a code base that is version controlled and scripted builds are run regularly on a dedicated build server. The deployment process is manual or semi-manual with some parts scripted and rudimentarily documented in some way. Beginner> Beginner level introduces frequent polling builds for faster feedback and build artifacts are archived for easier dependency management. Tagging and versioning of builds is structured but manual and the deployment process is gradually beginning to be more standardized with documentation, scripts and tools. Intermediate At intermediate level, builds are typically triggered from the source control system on each commit, tying a specific commit to a specific build. Tagging and versioning of builds is automated and the deployment process is standardized over all environments. Built artifacts or release packages are built only once and are designed to be able to be deployed in any environment. The standardized deployment process will also include a base for automated database deploys (migrations) of the bulk of database changes, and scripted runtime configuration changes. A basic delivery pipeline is in place covering all the stages from source control to production. Advanced At this stage it might also become necessary to scale out the build to multiple machines for parallel processing and for specific target environments. Techniques for zero downtime deploys can be important to include in the automated process to gain better flexibility and to reduce risk and cost when releasing. At this level you might also explore techniques to automate the trailing part of more complex database changes and database migrations to completely avoid manual routines for database updates. Expert Expert practices will include zero touch continuous deployment to production where every commit can potentially make it all the way to production automatically. Another expert practice is taking the infrastructure as code and virtualization even further and making the build process produce not only deployment artifacts but complete virtual machines which are promoted down the pipeline all the way to production replacing the existing ones. Test & Verification Testing is without doubt very important for any software development operation and is an absolutely crucial part of a successful implementation of Continuous Delivery. Similar to Build & Deploy, maturity in this category will involve tools and automation. However, it is also important to constantly increase the test-coverage of the application to build up the confidence in speed with frequent releases. Usually test involves verifying expected functionality according to requirements in different ways but we also want to emphasize the importance of verifying the expected business value of released features. Base At the base stage in the maturity model a development team or organization will typically practice unit-testing and have one or more dedicated test environments separate from local development machines. This system and integration level testing is typically done by a separate department that conducts long and cumbersome test periods after development ""code freeze"". Beginner When moving to beginner level you will naturally start to investigate ways of gradually automating the existing manual integration testing for faster feedback and more comprehensive regression tests. For accurate testing the component should be deployed and tested in a production like environment with all necessary dependencies. Intermediate Beginner level lets you broaden the scope of your automatic tests to functional testing which includes bigger parts of the component than unit tests but will have ""mocked"" implementations of it's external dependencies, e.g. database or other backend services. These tests are especially valuable when working in a highly component based architecture or when good complete integration tests are difficult to implement or too slow to run frequently. At this level you will most likely start to look at gradually automating parts of the acceptance testing. While integration tests are component specific, acceptance tests typically span over several components and across multiple systems. Advanced Advanced practices include fully automatic acceptance tests and maybe also generating structured acceptance criteria directly from requirements with e.g. specification by example and domains specific languages. This means no manual testing or verification is needed to pass acceptance but typically the process will still include some exploratory testing that feeds back into automated tests to constantly improve the test coverage and quality. If you correlate test coverage with change traceability you can start practicing risk based testing for better value of manual exploratory testing. At the advanced level some organizations might also start looking at automating performance tests and security scans. Expert It might seem strange to state that verifying expected business result is an expert practice but this is actually something that is very rarely done as a natural part of the development and release process today. Verifying expected business value of changes becomes more natural when the organization, culture and tooling has reached a certain maturity level and feedback of relevant business metrics is fast and accessible. As an example the implementation of a new feature must also include a way to verify the expected business result by making sure the relevant metrics can be pulled or pushed from the application. The definition of done must also be extended from release to sometime later when business has analyzed the effects of the released feature or change.. Information & Reporting Any business process includes a specific information set which can be reported on; the process of developing and releasing software is no exception, and the set of information that is handled in this particular area would typically include concepts like component, requirement, version, developer, release, environment etc. In this category we want to show the importance of handling this information correctly when adopting Continuous Delivery. Information must e.g. be concise, relevant and accessible at the right time to the right persons in order to obtain the full speed and flexibility possible with Continuous Delivery. Apart from information directly used to fulfill business requirements by developing and releasing features, it is also important to have access to information needed to measure the process itself and continuously improve it. Base At the base level in this category it is important to establish some baseline metric for the current process, so you can start to measure and track. At this level reporting is typically done manually and on-demand by individuals. Interesting metrics can e.g. be cycle-time, delivery time, number of releases, number of emergency fixes, number of incidents, number of features per release, bugs found during integration test etc. Beginner At beginner level, you start to measure the process and track the metrics for a better understanding of where improvement is needed and if the expected results from improvements are obtained. Reporting at this stage would typically include static analysis of code and quality reports which might be scheduled so that the latest reports are always accessible to facilitate decisions on quality and where improvements are needed. Intermediate Moving to intermediate the level of automation requires you to establish a common information model that standardizes the meaning of concepts and how they are connected. This model will typically give answers to questions like; what is a component?; how are requirements related to changes and components? When this model is established not only can you automate build, test and deployment even further but you can also build in traceability and information transparency to the pipeline and e.g. automatically generate information like release notes and test plans. Automatic reporting and feedback on events is implemented and at this level it will also become natural to store historical reports connected to e.g. builds or other events. This gives management crucial information to make good decisions on how to adjust the process and optimize for e.g. flow and capacity. Advanced At advanced level when you have started to practice frequent releases and the tools, products and services are mature enough to e.g. push business metrics, a natural progression is to set up a graphing service where people can get specific real time information that is relevant to their specific need. At this level real time graphs and other reports will typically also include trends over time. As a complement to static code analysis and unit tests coverage reports you might also at this level start looking at dynamic test coverage and profiling information from production like runtime environments when e.g. running automatic integrations tests. Expert Moving to expert level in this category typically includes improving the real time information service to provide dynamic self-service useful information and customized dashboards. As a result of this you can also start cross referencing and correlating reports and metrics across different organizational boundaries,. This information lets you broaden the perspective for continuous improvement and more easy verify expected business results from changes. Jump start the journey Every company is unique and has its own specific challenges when it comes to changing the way things work, like implementing Continuous Delivery. This maturity model will give you a starting point and a base for planning the transformation of the company towards Continuous Delivery. After evaluating your organization according to the model you need to set the goals and identify which practices will give your organization the best outcomes. If there are practices you do not want to adopt you need to analyse the consequences of excluding them. It is also important to decide on an implementation strategy, you can e.g. start small using slack in the existing process to improve one thing at a time. However, from our experience you will have a better chance of a successful implementation if you jump start the journey with a dedicated project with a clear mandate and aggressive goals on e.g. reducing cycle time. A typical Continuous Delivery implementation project will need skills and competence according to the categories and practices in the maturity model in order to implement a solid platform of tools, methods and practices that the organization can continue to grow and improve on. (Click on the image to enlarge it) About the authors Andreas Rehn is an Enterprise Architect and a strong advocate for Continuous Delivery, DevOps, Agile and Lean methods in systems development. With extensive experience in many disciplines of software development and a solid understanding of process, information and management theories and practises; he is dedicated to help customers implement Continuous Delivery and transform their business by adopting new methods for efficient and modern software development. Tobias Palmborg , Believes that Continuous Delivery describes the vision that scrum, XP and the agile manifesto once set out to be. Continuous Delivery is not just about automating the release pipeline but how to get your whole change flow, from grain to bread ,in a state of the art shape. Former Head of Development at one of europes largest online gaming company. Tobias is currently implementing Continuous Delivery projects at several customers. Patrik Boström is one of the founders of Diabol. Senior developer and architect with experience in operations of large system. Strong believer that Continuous Delivery and DevOps is the natural step in the evolution of Agile and Lean movement. Wants to change the way we look at systems development today, moving it to the next level where we focus more time on developing features than doing manually repetitive tasks. Where we visualize and understand the path from idea to where it is released and brings business value.",en,178
23,1333,1465557971,CONTENT SHARED,-5148591903395022444,-2979881261169775358,6661249173876371595,,,,HTML,http://code.joejag.com/2016/anti-if-the-missing-patterns.html,anti-if: the missing patterns,"Around 10 years ago I encountered the anti-if campaign and found it to be an absurd concept. How on earth would you make a useful program without using an if statement? Preposterous. But then it gets you thinking. Do you remember that heavily nested code you had to understand last week? That kinda sucked right? If only there was a way to make it simpler. The anti-if campaign site is sadly low on practical advice. This post intends to remedy that with a collection of patterns you can adopt when the need arises. But first let's look at the problem that if statements pose. The problems of if statements The first problem with if statements is that they often make it easy to modify code in bad ways. Let's start with the birth of a new if statement: This isn't too bad at this point, but we've already given us some problems. When I read this code I have to check how CodeBlockA and CodeBlockB are modifying the same SharedState. This can be easy to read at first but can become difficult as the CodeBlocks grow and the coupling becomes more complicated. You'll often see the above CodeBlocks abused with further nested if statements and local returns. Making it hard to see what the business logic is through the routing logic. The second problem with if statements is when they are duplicated. This means means a domain concept is missing. It's all too easy to increase coupling by bringing things together than don't need to be. Making code harder to read and change. The third problem with if statements is that you have to simulate execution in your own head. You must beome a mini-computer. That's taking away from your mental energy, energy that would be better spent thinking about solving the problem, rather than how the intracate code branches weave together. I want to get to the point of telling you patterns we can do instead, but first a word of warning. Moderation in all things, especially moderation If statements usually make your code more complicated. But we don't want to outright ban them. I've seen some pretty heinous code created with the goal of removing all traces of if statements. We want to avoid falling into that trap. For each pattern we'll read about I'm going to give you a tolerance value for when to use it. A single if statement which isn't duplicated anywhere else is probably fine. It's when you have duplicated if statements that you want your spider sense to be tingling. At the outside of your code base, where you talk to the dangerous outside world, you are going to want to validate incoming responses and change your beahaviour accordingly. But inside our own codebases, where we behind those trusted gatekeepers, I think we have a great opportunity to use simple, richer and more powerful alternatives. Pattern 1: Boolean Params Context: You have a method that takes a boolean which alters its behaviour Problem: Any time you see this you actually have two methods bundled into one. That boolean represents an opportunity to name a concept in your code. Tolerance: Usually when you see this context you can work out at compile time which path the code will take. If that is the case then always use this pattern. Solution: Split the method into two new methods. Voilà, the if is gone. Pattern 2: Switch to Polymorphism Context: You are switching based on type. Problem: When we add a new type we have to remember to update the switch statement. Additionally the cohesion is suffering in this Bird class as multiple concepts of different birds are being added. Tolerance: A single switch on type is fine. It's when their are multiple switches then bugs can be introduced as a person adding a new type can forget to update all the switches that exist on this hidden type. There is an excellent write up on the 8thlight blog on this context. Solution: Use Polymorphism. Anyone introducing a new type cannot forget to add the associated behaviour, note: This example only has one method being switched on for brevity, it's more compelling when there are multiple switches Patten 3: NullObject/Optional over null passing Context: A outsider asked to understand the primary purpose of your code base answers with ""to check if things equal null"". Problem: Your methods have to check if they are being passed non null values. Tolerance: It's necessary to be defensive at the outer parts of your codebase, but being defensive inside your codebase probably means the code that you are writing is offensive. Don't write offensive code. Solution: Use a NullObject or Optional type instead of ever passing a null. An empty collection is a great alternative. Patten 4: Inline statements into expressions Context: You have an if statement tree that calculates a boolean expression. Problem: This code forces you to use your brain to simulate how a computer will step through your method. Tolerance: Very little. Code like this is easier to read on one line. Or broken into different parts. Solution: Simplify the if statements into a single expression. Pattern 5: Give a coping strategy Context: You are calling some other code, but you aren't sure if the happy path will suceceed. Problem: These sort of if statements multiply each time you deal with the same object or data structure. They have a hidden coupling where 'null' means someting. Other objects may return other magic values that mean no result. Tolerance: It's better to push this if statement into one place, so it isn't duplicated and we can remove the coupling on the empty object magic value. Solution: Give the code being called a coping strategy. Ruby's Hash#fetch is a good example which Java has copied . This pattern can be taken even further to remove exceptions . Happy hunting Hopefully you can use some of these patterns on the code you are working on just now. I find them useful when refactoring code to better understand it. Remember if statements aren't all evil. But we have a rich set of features in modern languages to use instead which we should take advantage of.",en,176
24,1511,1466729247,CONTENT SHARED,-4503975842879662368,8250502084456448386,970831914672612417,,,,HTML,http://backlinko.com/google-ranking-factors,google ranking factors: the complete list,"You probably already know that Google uses about 200 ranking factors in their algorithm... But what the heck are they? Well today you're in for a treat because I've put together a complete list. Some are proven. Some are controversial. Others are SEO nerd speculation. But they're all here . Bonus: Download a free checklist that will show you how to tap into the 10 most important Google ranking factors listed here. Domain Factors 1. Domain Age: In this video , Matt Cutts states that: ""The difference between a domain that's six months old versus one year old is really not that big at all."". In other words, they do use domain age...but it's not very important. 2. Keyword Appears in Top Level Domain: Doesn't give the boost that it used to, but having your keyword in the domain still acts as a relevancy signal. After all, they still bold keywords that appear in a domain name. 3. Keyword As First Word in Domain: A domain that starts with their target keyword has an edge over sites that either don't have the keyword in their domain or have the keyword in the middle or end of their domain. 4. Domain registration length: A Google patent states: ""Valuable (legitimate) domains are often paid for several years in advance, while doorway (illegitimate) domains rarely are used for more than a year. Therefore, the date when a domain expires in the future can be used as a factor in predicting the legitimacy of a domain"". 5. Keyword in Subdomain Name: Moz's 2011 panel agreed that a keyword appearing in the subdomain can boost rankings: 6. Domain History: A site with volatile ownership (via whois) or several drops may tell Google to ""reset"" the site's history, negating links pointing to the domain. 7. Exact Match Domain: EMDs may still give you an edge...if it's a quality site. But if the EMD happens to be a low-quality site, it's vulnerable to the EMD update : 8. Public vs. Private WhoIs: Private WhoIs information may be a sign of ""something to hide"". Matt Cutts is quoted as stating at Pubcon 2006: ""...When I checked the whois on them, they all had ""whois privacy protection service"" on them. That's relatively unusual. ...Having whois privacy turned on isn't automatically bad, but once you get several of these factors all together, you're often talking about a very different type of webmaster than the fellow who just has a single site or so."" 9. Penalized WhoIs Owner: If Google identifies a particular person as a spammer it makes sense that they would scrutinize other sites owned by that person. 10. Country TLD extension: Having a Country Code Top Level Domain (.cn, .pt, .ca) helps the site rank for that particular country...but limits the site's ability to rank globally. Page-Level Factors 11. Keyword in Title Tag: The title tag is a webpage's second most important piece of content (besides the content of the page) and therefore sends a strong on-page SEO signal. 12. Title Tag Starts with Keyword : According to Moz data , title tags that starts with a keyword tend to perform better than title tags with the keyword towards the end of the tag: 13. Keyword in Description Tag: Another relevancy signal. Not especially important now, but still makes a difference. 14. Keyword Appears in H1 Tag: H1 tags are a ""second title tag"" that sends another relevancy signal to Google, according to results from this correlation study : 15. Keyword is Most Frequently Used Phrase in Document: Having a keyword appear more than any other likely acts as a relevancy signal. 16. Content Length: Content with more words can cover a wider breadth and are likely preferred to shorter superficial articles. SERPIQ found that content length correlated with SERP position : 17. Keyword Density : Although not as important as it once was, keyword density is still something Google uses to determine the topic of a webpage. But going overboard can hurt you. 18. Latent Semantic Indexing Keywords in Content (LSI): LSI keywords help search engines extract meaning from words with more than one meaning (Apple the computer company vs. the fruit). The presence/absence of LSI probably also acts as a content quality signal. 19. LSI Keywords in Title and Description Tags: As with webpage content, LSI keywords in page meta tags probably help Google discern between synonyms. May also act as a relevancy signal. 20. Page Loading Speed via HTML: Both Google and Bing use page loading speed as a ranking factor. Search engine spiders can estimate your site speed fairly accurately based on a page's code and filesize. 21. Duplicate Content: Identical content on the same site (even slightly modified) can negatively influence a site's search engine visibility. 22. Rel=Canonical: When used properly , use of this tag may prevent Google from considering pages duplicate content. 23. Page Loading Speed via Chrome : Google may also use Chrome user data to get a better handle on a page's loading time as this takes into account server speed, CDN usage and other non HTML-related site speed signals. 24. Image Optimization: Images on-page send search engines important relevancy signals through their file name, alt text, title, description and caption. 25. Recency of Content Updates: Google Caffeine update favors recently updated content, especially for time-sensitive searches. Highlighting this factor's importance, Google shows the date of a page's last update for certain pages: 26. Magnitude of Content Updates : The significance of edits and changes is also a freshness factor. Adding or removing entire sections is a more significant update than switching around the order of a few words. 27. Historical Updates Page Updates: How often has the page been updated over time? Daily, weekly, every 5-years? Frequency of page updates also play a role in freshness. 28. Keyword Prominence : Having a keyword appear in the first 100-words of a page's content appears to be a significant relevancy signal. 29. Keyword in H2, H3 Tags : Having your keyword appear as a subheading in H2 or H3 format may be another weak relevancy signal. Moz's panel agrees: 30. Keyword Word Order: An exact match of a searcher's keyword in a page's content will generally rank better than the same keyword phrase in a different order. For example: consider a search for: ""cat shaving techniques"". A page optimized for the phrase ""cat shaving techniques"" will rank better than a page optimized for ""techniques for shaving a cat"". This is a good illustration of why keyword research is really, really important. 31. Outbound Link Quality : Many SEOs think that linking out to authority sites helps send trust signals to Google. 32. Outbound Link Theme: According to Moz , search engines may use the content of the pages you link to as a relevancy signal. For example, if you have a page about cars that links to movie-related pages, this may tell Google that your page is about the movie Cars, not the automobile. 33. Grammar and Spelling: Proper grammar and spelling is a quality signal, although Cutts gave mixed messages in 2011 on whether or not this was important. 34. Syndicated Content: Is the content on the page original? If it's scraped or copied from an indexed page it won't rank as well as the original or end up in their Supplemental Index . 35. Helpful Supplementary Content: According to a now-public Google Rater Guidelines Document , helpful supplementary content is an indicator of a page's quality (and therefore, Google ranking). Examples include currency converters, loan interest calculators and interactive recipes. 36. Number of Outbound Links: Too many dofollow OBLs may ""leak"" PageRank, which can hurt that page's rankings. 37. Multimedia: Images, videos and other multimedia elements may act as a content quality signal. 38. Number of Internal Links Pointing to Page: The number of internal links to a page indicates its importance relative to other pages on the site. 39. Quality of Internal Links Pointing to Page : Internal links from authoritative pages on domain have a stronger effect than pages with no or low PR. 40. Broken Links: Having too many broken links on a page may be a sign of a neglected or abandoned site. The Google Rater Guidelines Document uses broken links as one was to assess a homepage's quality. 41. Reading Level: There's no doubt that Google estimates the reading level of webpages. In fact, Google used to give you reading level stats: But what they do with that information is up for debate. Some say that a basic reading level will help you rank better because it will appeal to the masses. But others associate a basic reading level with content mills like Ezine Articles. 42. Affiliate Links : Affiliate links themselves probably won't hurt your rankings. But if you have too many, Google's algorithm may pay closer attention to other quality signals to make sure you're not a ""thin affiliate site"". 43. HTML errors/W3C validation : Lots of HTML errors or sloppy coding may be a sign of a poor quality site. While controversial, many in SEO think that WC3 validation is a weak quality signal. 44. Page Host's Domain Authority : All things being equal, a page on an authoritative domain will rank higher than a page on a domain with less authority. 45. Page's PageRank: Not perfectly correlated. But in general higher PR pages tend to rank better than low PR pages. 46. URL Length: Search Engine Journal notes that excessively long URLs may hurt search visibility. 47. URL Path : A page closer to the homepage may get a slight authority boost. 48. Human Editors: Although never confirmed, Google has filed a patent for a system that allows human editors to influence the SERPs. 49. Page Category: The category the page appears on is a relevancy signal. A page that's part of a closely related category should get a relevancy boost compared to a page that's filed under an unrelated or less related category. 50. WordPress Tags: Tags are WordPress-specific relevancy signal. According to Yoast.com : ""The only way it improves your SEO is by relating one piece of content to another, and more specifically a group of posts to each other"" 51. Keyword in URL : Another important relevancy signal. 52. URL String: The categories in the URL string are read by Google and may provide a thematic signal to what a page is about: 53. References and Sources: Citing references and sources, like research papers do, may be a sign of quality. The Google Quality Guidelines states that reviewers should keep an eye out for sources when looking at certain pages: ""This is a topic where expertise and/or authoritative sources are important..."". However, Google has denied that they use external links as a ranking signal. 54. Bullets and Numbered Lists: Bullets and numbered lists help break up your content for readers, making them more user friendly. Google likely agrees and may prefer content with bullets and numbers. 55. Priority of Page in Sitemap: The priority a page is given via the sitemap.xml file may influence ranking. 56. Too Many Outbound Links: Straight from the aforementioned Quality rater document: ""Some pages have way, way too many links, obscuring the page and distracting from the Main Content"" 57. Quantity of Other Keywords Page Ranks For: If the page ranks for several other keywords it may give Google an internal sign of quality. 58. Page Age: Although Google prefers fresh content, an older page that's regularly updated may outperform a newer page. 59. User Friendly Layout: Citing the Google Quality Guidelines Document yet again: ""The page layout on highest quality pages makes the Main Content immediately visible"" 60. Parked Domains : A Google update in December of 2011 decreased search visibility of parked domains. 61. Useful Content: As pointed out by Backlinko reader Jared Carrizales , Google may distinguish between ""quality"" and ""useful"" content . Site-Level Factors 62. Content Provides Value and Unique Insights: Google has stated that they're on the hunt for sites that don't bring anything new or useful to the table, especially thin affiliate sites. 63. Contact Us Page: The aforementioned Google Quality Document states that they prefer sites with an ""appropriate amount of contact information"". Supposed bonus if your contact information matches your whois info. 64. Domain Trust/TrustRank: Site trust - measured by how many links away your site is from highly-trusted seed sites - is a massively important ranking factor. You can read more about TrustRank here . 65. Site Architecture: A well put-together site architecture (especially a silo structure) helps Google thematically organize your content. 66. Site Updates: How often a site is updated - and especially when new content is added to the site - is a site-wide freshness factor. 67. Number of Pages: The number of pages a site has is a weak sign of authority. At the very least a large site helps distinguish it from thin affiliate sites. 68. Presence of Sitemap: A sitemap helps search engines index your pages easier and more thoroughly, improving visibility. 69. Site Uptime : Lots of downtime from site maintenance or server issues may hurt your ranking (and can even result in deindexing if not corrected). 70. Server Location : Server location may influence where your site ranks in different geographical regions. Especially important for geo-specific searches. 71. SSL Certificate : Google has confirmed that they index SSL certificates and that they use HTTPS as a ranking signal. 72. Terms of Service and Privacy Pages : These two pages help tell Google that a site is a trustworthy member of the internet. 73. Duplicate Meta Information On-Site : Duplicate meta information across your site may bring down all of your page's visibility. 74. Breadcrumb Navigation: This is a style of user-friendly site-architecture that helps users (and search engines) know where they are on a site: Both SearchEngineJournal.com and Ethical SEO Consulting claim that this set-up may be a ranking factor. 75. Mobile Optimized: Google's official stance on mobile is to create a responsive site. It's likely that responsive sites get an edge in searches from a mobile device. In fact, they now add "" Mobile friendly"" tags to sites that display well on mobile devices. Google also started penalizing site s in Mobile search that aren't mobile friendly 76. YouTube: There's no doubt that YouTube videos are given preferential treatment in the SERPs (probably because Google owns it ): In fact, Search Engine Land found that YouTube.com traffic increased significantly after Google Panda . 77. Site Usability: A site that's difficult to use or to navigate can hurt ranking by reducing time on site, pages viewed and bounce rate. This may be an independent algorithmic factor gleaned from massive amounts of user data. 78. Use of Google Analytics and Google Webmaster Tools: Some think that having these two programs installed on your site can improve your page's indexing. They may also directly influence rank by giving Google more data to work with (ie. more accurate bounce rate, whether or not you get referral traffic from your backlinks etc.). 79. User reviews/Site reputation: A site's on review sites like Yelp.com and RipOffReport.com likely play an important role in the algorithm. Google even posted a rarely candid outline of their approach to user reviews after an eyeglass site was caught ripping off customers in an effort to get backlinks. Backlink Factors 80. Linking Domain Age: Backlinks from aged domains may be more powerful than new domains. 81. # of Linking Root Domains: The number of referring domains is one of the most important ranking factors in Google's algorithm, as you can see from this chart from Moz (bottom axis is SERP position): 82. # of Links from Separate C-Class IPs: Links from separate class-c IP addresses suggest a wider breadth of sites linking to you. 83. # of Linking Pages : The total number of linking pages - even if some are on the same domain - is a ranking factor. 84. Alt Tag (for Image Links) : Alt text is an image's version of anchor text. 85. Links from .edu or .gov Domains : Matt Cutts has stated that TLD doesn't factor into a site's importance. However, that doesn't stop SEOs from thinking that there's a special place in the algo for .gov and .edu TLDs. 86. Authority of Linking Page: The authority (PageRank) of the referring page is an extremely important ranking factor. 87. Authority of Linking Domain : The referring domain's authority may play an independent role in a link's importance (ie. a PR2 page link from a site with a homepage PR3 may be worth less than a PR2 page link from PR8 Yale.edu). 88. Links From Competitors: Links from other pages ranking in the same SERP may be more valuable for a page's rank for that particular keyword. 89. Social Shares of Referring Page : The amount of page-level social shares may influence the link's value. 90. Links from Bad Neighborhoods: Links from ""bad neighborhoods"" may hurt your site . 91. Guest Posts: Although guest posting can be part of a white hat SEO campaign , links coming from guest posts - especially in an author bio area - may not be as valuable as a contextual link on the same page. 92. Links to Homepage Domain that Page Sits On: Links to a referring page's homepage may play special importance in evaluating a site's - and therefore a link's - weight. 93. Nofollow Links: One of the most controversial topics in SEO. Google's official word on the matter is: ""In general, we don't follow them."" Which suggests that they do... at least in certain cases. Having a certain % of nofollow links may also indicate a natural vs. unnatural link profile. 94. Diversity of Link Types: Having an unnaturally large percentage of your links come from a single source (ie. forum profiles, blog comments) may be a sign of webspam. On the other hand, links from diverse sources is a sign of a natural link profile. 95. ""Sponsored Links"" Or Other Words Around Link: Words like ""sponsors"", ""link partners"" and ""sponsored links"" may decrease a link's value. 96. Contextual Links: Links embedded inside a page's content are considered more powerful than links on an empty page or found elsewhere on the page. A good example of contextual links are backlinks from guestographics . 97. Excessive 301 Redirects to Page: Links coming from 301 redirects dilute some (or even all) PR, according to a Webmaster Help Video . 98. Backlink Anchor Text : As noted in this description of Google's original algorithm: ""First, anchors often provide more accurate descriptions of web pages than the pages themselves."" Obviously, anchor text is less important than before (and likely a webspam signal). But it still sends a strong relevancy signal in small doses. 99. Internal Link Anchor Text : Internal link anchor text is another relevancy signal, although probably weighed differently than backlink anchor text. 100. Link Title Attribution : The link title (the text that appears when you hover over a link) is also used as a weak relevancy signals. 101. Country TLD of Referring Domain: Getting links from country-specific top level domain extensions (.de, .cn, .co.uk) may help you rank better in that country. 102. Link Location In Content: Links in the beginning of a piece of content carry slightly more weight than links placed at the end of the content. 103. Link Location on Page: Where a link appears on a page is important. Generally, links embedded in a page's content are more powerful than links in the footer or sidebar area. 104. Linking Domain Relevancy: A link from site in a similar niche is significantly more powerful than a link from a completely unrelated site. That's why any effective SEO strategy today focuses on obtaining relevant links. 105. Page Level Relevancy: The Hilltop Algorithm states that link from a page that's closely tied to page's content is more powerful than a link from an unrelated page. 106. Text Around Link Sentiment: Google has probably figured out whether or not a link to your site is a recommendation or part of a negative review. Links with positive sentiments around them likely carry more weight. 107. Keyword in Title: Google gives extra love to links on pages that contain your page's keyword in the title (""Experts linking to experts"".) 108. Positive Link Velocity: A site with positive link velocity usually gets a SERP boost. 109. Negative Link Velocity: Negative link velocity can significantly reduce rankings as it's a signal of decreasing popularity. 110. Links from ""Hub"" Pages: Aaron Wall claims that getting links from pages that are considered top resources (or hubs) on a certain topic are given special treatment. 111. Link from Authority Sites: A link from a site considered an ""authority site"" likely pass more juice than a link from a small, microniche site. 112. Linked to as Wikipedia Source: Although the links are nofollow, many think that getting a link from Wikipedia gives you a little added trust and authority in the eyes of search engines. 113. Co-Occurrences: The words that tend to appear around your backlinks helps tell Google what that page is about . 114. Backlink Age: According to a Google patent , older links have more ranking power than newly minted backlinks. 115. Links from Real Sites vs. Splogs: Due to the proliferation of blog networks, Google probably gives more weight to links coming from ""real sites"" than from fake blogs. They likely use brand and user-interaction signals to distinguish between the two. 116. Natural Link Profile: A site with a ""natural"" link profile is going to rank highly and be more durable to updates. 117. Reciprocal Links: Google's Link Schemes page lists ""Excessive link exchanging"" as a link scheme to avoid. 118. User Generated Content Links: Google is able to identify links generated from UGC vs. the actual site owner. For example, they know that a link from the official WordPress.com blog at en.blog.wordpress.com is very different than a link from besttoasterreviews.wordpress.com. 119. Links from 301: Links from 301 redirects may lose a little bit of juice compared to a direct link. However, Matt Cutts says that a 301 is similar to a direct link. 120. Schema.org Microformats: Pages that support microformats may rank above pages without it. This may be a direct boost or the fact that pages with microformatting have a higher SERP CTR: 121. DMOZ Listed : Many believe that Google gives DMOZ listed sites a little extra trust. 122. TrustRank of Linking Site: The trustworthiness of the site linking to you determines how much ""TrustRank"" gets passed onto you. 123. Number of Outbound Links on Page: PageRank is finite. A link on a page with hundreds of OBLs passes less PR than a page with only a few OBLs. 124. Forum Profile Links: Because of industrial-level spamming, Google may significantly devalue links from forum profiles. 125. Word Count of Linking Content: A link from a 1000-word post is more valuable than a link inside of a 25-word snippet. 126. Quality of Linking Content: Links from poorly written or spun content don't pass as much value as links from well-written, multimedia-enhanced content. 127. Sitewide Links: Matt Cutts has confirmed that sitewide links are ""compressed"" to count as a single link. User Interaction 128. Organic Click Through Rate for a Keyword : Pages that get clicked more in CTR may get a SERP boost for that particular keyword. 129. Organic CTR for All Keywords : A page's (or site's) organic CTR for all keywords is ranks for may be a human-based, user interaction signal. 130. Bounce Rate: Not everyone in SEO agrees bounce rate matters, but it may be a way of Google to use their users as quality testers (pages where people quickly bounce is probably not very good). 131. Direct Traffic: It's confirmed that Google uses data from Google Chrome to determine whether or not people visit a site (and how often). Sites with lots of direct traffic are likely higher quality than sites that get very little direct traffic. 132. Repeat Traffic : They may also look at whether or not users go back to a page or site after visiting. Sites with repeat visitors may get a Google ranking boost. 133. Blocked Sites : Google has discontinued this feature in Chrome. However, Panda used this feature as a quality signal. 134. Chrome Bookmarks: We know that Google collects Chrome browser usage data . Pages that get bookmarked in Chrome might get a boost. 135. Google Toolbar Data: Search Engine Watch's Danny Goodwin reports that Google uses toolbar data as a ranking signal. However, besides page loading speed and malware, it's not known what kind of data they glean from the toolbar. 136. Number of Comments: Pages with lots of comments may be a signal of user-interaction and quality. 137. Dwell Time: Google pays very close attention to ""dwell time"": how long people spend on your page when coming from a Google search. This is also sometimes referred to as ""long clicks vs short clicks"". If people spend a lot of time on your site, that may be used as a quality signal. Special Algorithm Rules 138. Query Deserves Freshness: Google gives newer pages a boost for certain searches . 139. Query Deserves Diversity: Google may add diversity to a SERP for ambiguous keywords, such as ""Ted"", ""WWF"" or ""ruby"". 140. User Browsing History : Sites that you frequently visit while signed into Google get a SERP bump for your searches. 141. User Search History: Search chain influence search results for later searches . For example, if you search for ""reviews"" then search for ""toasters"", Google is more likely to show toaster review sites higher in the SERPs. 142. Geo Targeting: Google gives preference to sites with a local server IP and country-specific domain name extension. 143. Safe Search: Search results with curse words or adult content won't appear for people with Safe Search turned on. 144. Google+ Circles: Google shows higher results for authors and sites that you've added to your Google Plus Circles 145. DMCA Complaints: Google ""downranks"" pages with DMCA complaints . 146. Domain Diversity : The so-called "" Bigfoot Update "" supposedly added more domains to each SERP page. 147. Transactional Searches : Google sometimes displays different results for shopping-related keywords, like flight searches. 148. Local Searches: Google often places Google+ Local results above the ""normal"" organic SERPs. 149. Google News Box: Certain keywords trigger a Google News box: 150. Big Brand Preference: After the Vince Update , Google began giving big brands a boost for certain short-tail searches. 151. Shopping Results: Google sometimes displays Google Shopping results in organic SERPs: 152. Image Results: Google elbows our organic listings for image results for searches commonly used on Google Image Search. 153. Easter Egg Results: Google has a dozen or so Easter Egg results . For example, when you search for ""Atari Breakout"" in Google image search, the search results turn into a playable game (!). Shout out to Victor Pan for this one. 154. Single Site Results for Brands: Domain or brand-oriented keywords bring up several results from the same site . Social Signals 155. Number of Tweets: Like links, the tweets a page has may influence its rank in Google. 156. Authority of Twitter Users Accounts : It's likely that Tweets coming from aged, authority Twitter profiles with a ton of followers (like Justin Bieber) have more of an effect than tweets from new, low-influence accounts. 157. Number of Facebook Likes : Although Google can't see most Facebook accounts , it's likely they consider the number of Facebook likes a page receives as a weak ranking signal. 158. Facebook Shares: Facebook shares - because they're more similar to a backlink - may have a stronger influence than Facebook likes . 159. Authority of Facebook User Accounts: As with Twitter, Facebook shares and likes coming from popular Facebook pages may pass more weight. 160. Pinterest Pins: Pinterest is an insanely popular social media account with lots of public data. It's probably that Google considers Pinterest Pins a social signal. 161. Votes on Social Sharing Sites: It's possible that Google uses shares at sites like Reddit, Stumbleupon and Digg as another type of social signal. 162. Number of Google+1's: Although Matt Cutts gone on the record as saying Google+ has "" no direct effect "" on rankings, it's hard to believe that they'd ignore their own social network. 163. Authority of Google+ User Accounts: It's logical that Google would weigh +1's coming from authoritative accounts more than from accounts without many followers. 164. Known Authorship : In February 2013, Google CEO Eric Schmidt famously claimed: ""Within search results, information tied to verified online profiles will be ranked higher than content without such verification, which will result in most users naturally clicking on the top (verified) results."" Although the Google+ authorship program has been shut down , it's likely Google uses some form of authorship to determine influential content producers online (and give them a boost in rankings). 165. Social Signal Relevancy: Google probably uses relevancy information from the account sharing the content and the text surrounding the link. 166. Site Level Social Signals: Site-wide social signals may increase a site's overall authority, which will increase search visibility for all of its pages. Brand Signals 167. Brand Name Anchor Text: Branded anchor text is a simple - but strong - brand signal. 168. Branded Searches: It's simple: people search for brands. If people search for your site in Google (ie. ""Backlinko twitter"", Backlinko + ""ranking factors""), Google likely takes this into consideration when determining a brand. 169. Site Has Facebook Page and Likes: Brands tend to have Facebook pages with lots of likes. 170. Site has Twitter Profile with Followers: Twitter profiles with a lot of followers signals a popular brand. 171. Official Linkedin Company Page: Most real businesses have company Linkedin pages. 172. Employees Listed at Linkedin: Rand Fishkin thinks that having Linkedin profiles that say they work for your company is a brand signal. 173. Legitimacy of Social Media Accounts: A social media account with 10,000 followers and 2 posts is probably interpreted a lot differently than another 10,000-follower strong account with lots of interaction. 174. Brand Mentions on News Sites : Really big brands get mentioned on Google News sites all the time. In fact, some brands even have their own Google News feed on the first page: 175. Co-Citations : Brands get mentioned without getting linked to. Google likely looks at non-hyperlinked brand mentions as a brand signal. 176. Number of RSS Subscribers: Considering that Google owns the popular Feedburner RSS service , it makes sense that they would look at RSS Subscriber data as a popularity/brand signal. 177. Brick and Mortar Location With Google+ Local Listing: Real businesses have offices. It's possible that Google fishes for location-data to determine whether or not a site is a big brand. 178. Website is Tax Paying Business: Moz reports that Google may look at whether or not a site is associated with a tax-paying business. On-Site WebSpam Factors 179. Panda Penalty : Sites with low-quality content (particularly content farms ) are less visible in search after getting hit by a Panda penalty . 180. Links to Bad Neighborhoods: Linking out to ""bad neighborhoods"" - like pharmacy or payday loan sites - may hurt your search visibility. 181. Redirects: Sneaky redirects is a big no-no . If caught, it can get a site not just penalized, but de-indexed. 182. Popups or Distracting Ads: The official Google Rater Guidelines Document says that popups and distracting ads is a sign of a low-quality site. 183. Site Over-Optimization: Includes on-page factors like keyword stuffing, header tag stuffing, excessive keyword decoration. 184. Page Over-Optimization: Many people report that - unlike Panda - Penguin targets individual page (and even then just for certain keywords). 185. Ads Above the Fold : The "" Page Layout Algorithm "" penalizes sites with lots of ads (and not much content) above the fold. 186. Hiding Affiliate Links: Going too far when trying to hide affiliate links (especially with cloaking) can bring on a penalty. 187. Affiliate Sites: It's no secret that Google isn't the biggest fan of affiliates . And many think that sites that monetize with affiliate links are put under extra scrutiny. 188. Autogenerated Content: Google isn't a big fan of autogenerated content. If they suspect that your site's pumping out computer-generated content, it could result in a penalty or de-indexing. 189. Excess PageRank Sculpting: Going too far with PageRank sculpting - by nofollowing all outbound links or most internal links - may be a sign of gaming the system. 190. IP Address Flagged as Spam: If your server's IP address is flagged for spam, it may hurt all of the sites on that server . 191. Meta Tag Spamming: Keyword stuffing can also happen in meta tags. If Google thinks you're adding keywords to your meta tags to game the algo, they may hit your site with a penalty. Off Page Webspam Factors 192. Unnatural Influx of Links : A sudden (and unnatural) influx of links is a sure-fire sign of phony links. 193. Penguin Penalty: Sites that were hit by Google Penguin are significantly less visible in search. 194. Link Profile with High % of Low Quality Links: Lots of links from sources commonly used by black hat SEOs (like blog comments and forum profiles) may be a sign of gaming the system. 195. Linking Domain Relevancy: The famous analysis by MicroSiteMasters.com found that sites with an unnaturally high amount of links from unrelated sites were more susceptible to Penguin. 196. Unnatural Links Warning: Google sent out thousands of ""Google Webmaster Tools notice of detected unnatural links"" messages. This usually precedes a ranking drop, although not 100% of the time . 197. Links from the Same Class C IP : Getting an unnatural amount of links from sites on the same server IP may be a sign of blog network link building. 198. ""Poison"" Anchor Text: Having ""poison"" anchor text (especially pharmacy keywords) pointed to your site may be a sign of spam or a hacked site. Either way, it can hurt your site's ranking. 199. Manual Penalty: Google has been known to hand out manual penalties, like in the well-publicized Interflora fiasco . 200. Selling Links: Selling links can definitely impact toolbar PageRank and may hurt your search visibility. 201. Google Sandbox: New sites that get a sudden influx of links are sometimes put in the Google Sandbox , which temporarily limits search visibility. 202. Google Dance: The Google Dance can temporarily shake up rankings. According to a Google Patent , this may be a way for them to determine whether or not a site is trying to game the algorithm. 203. Disavow Tool: Use of the Disavow Tool may remove a manual or algorithmic penalty for sites that were the victims of negative SEO. 204. Reconsideration Request : A successful reconsideration request can lift a penalty. 205. Temporary Link Schemes: Google has (apparently) caught onto people that create - and quickly remove - spammy links. Also know as a temporary link scheme . ""How Can I Use This Information For My Site?"" I created a free step-by-step checklist that you can use to quickly apply the most important information from this post to your site. The checklist contains the 10 most important ranking factors on this list... ...and super-actionable strategies that you can use to get higher rankings and more traffic. Click below to download the free checklist:",en,176
25,1076,1464034990,CONTENT SHARED,-8742648016180281673,6644119361202586331,595221735013488469,,,,HTML,"http://revistaepoca.globo.com/Revista/Epoca/0,,EDG78613-8056-483,00.html",aposta na inovação,"RITUAL Bruno Guiçardi, sócio da Ci&T, e o pote de rolhas de champanhe. A cada cliente novo, uma garrafa aberta para comemorar Eles ainda não completaram 40 anos, não sabem o que é ter uma carteira de trabalho assinada e saboreiam o gostinho de faturar dezenas de milhões de reais. Instalados num dos pólos de tecnologia mais avançados do país, o Pólis, da Universidade de Campinas (Unicamp), no interior de São Paulo, os empresários César Gon, de 35 anos, Bruno Guiçardi, de 35, e Fernando Matt, de 33, são donos da Ci&T, especializada em desenvolvimento e terceirização de aplicações de softwares. A empresa é a única brasileira na lista das estrelas emergentes na área de tecnologia da informação, segundo a pesquisa The Global Outsourcing 100, publicada pela revista americana Fortune. A presença no ranking deverá ajudar a abrir mais portas no exterior. Quando deixaram a Unicamp, onde se formaram em Ciência da Computação, em meados dos anos 90, os jovens decidiram tocar o próprio negócio. O que não lhes faltava era autoconfiança. ""Éramos ousados e não escondíamos"", afirma Guiçardi. Os três acreditavam tanto na própria competência que disputaram um projeto para o gigante IBM logo após a formatura. Venceram a concorrência e foram obrigados a correr para abrir uma empresa. A sede funcionava em um quarto alugado, numa modesta casa num bairro afastado de Campinas, e tinha apenas três computadores usados. Hoje, a Ci&T funciona numa área de 1.200 metros quadrados, tem 500 funcionários e faturou R$ 40 milhões no ano passado, 25% dos quais com negócios no exterior. A empresa mantém seis escritórios no Brasil, uma filial nos Estados Unidos e uma representação na Inglaterra. Nos últimos dois anos, as exportações dos serviços cresceram a uma média de 200% ao ano, segundo os sócios. Com uma carteira de clientes como IBM, HP, Johnson & Johnson, Petrobras, Vale do Rio Doce, Embraer e Yahoo!, a Ci&T desenvolve uma média de 150 projetos por ano. É dela, por exemplo, o portal que integra os distribuidores da HP na América Latina e o site da Natura, pelo qual a empresa se relaciona com suas mais de 566 mil vendedoras autônomas. ""Trabalhamos para antecipar o futuro, apostando em processos e sistemas que só mais tarde se tornarão padrão no mercado"", diz Guiçardi. Para comemorar cada projeto entregue, a equipe cumpre sempre o mesmo ritual: abre um champanhe e guarda a rolha, em um grande pote de vidro, devidamente identificada com a data e o nome do cliente. ""Para nossa alegria, já renovamos o pote muitas vezes."" Antecipar tendência, no entanto, nem sempre termina em brindes. Há riscos nessa opção de trabalho mais arrojada. O maior deles é lidar com resultados imprevisíveis. ""Muitas vezes, investimos em tecnologias que não emplacam"", diz Guiçardi. Outro problema é conseguir monitorar o crescimento acelerado. ""Quando atingimos 300 funcionários, não havíamos feito nenhuma preparação ou treinamento de pessoal."" O resultado foi uma confusão generalizada que afetou o atendimento aos clientes. Houve atrasos na entrega dos projetos e queda na qualidade dos serviços. Os sócios levaram um ano para pôr ordem na casa. A Ci&T caminha lado a lado com a academia. Há três anos a empresa instalou dentro da Unicamp um Laboratório de Inovação. Lá mantém 12 pesquisadores dedicados a desenvolver novos processos de tecnologia da informação. A proposta chamou a atenção da Fundação de Amparo à Pesquisa do Estado de São Paulo (Fapesp), que está investindo R$ 1 milhão em novos estudos com a Ci&T. No início do ano, os sócios montaram uma empresa chamada Digital Assets, especializada em softwares de governança e reutilização de programas. O empreendimento nasceu dentro da Ci&T, e quando tinha uma carteira razoável de clientes mudou-se para a incubadora tecnológica da Unicamp. ""Incubada, ela teria maiores chances de atrair capital de investidores."" Foi exatamente o que aconteceu. A Digital Asset recebeu dinheiro do fundo mineiro Novarum de capital de risco e está quase deixando a incubadora. ""Nosso sonho era faturar R$ 1 milhão. Hoje estamos de olho nos R$ 100 milhões"", diz Guiçardi. Da revista Pequenas Empresas & Grandes Negócios Foto: Ricardo Correa/PEGN",pt,175
26,2442,1474982976,CONTENT SHARED,6031953227014493100,-35428957105270993,2954312914497351516,"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.116 Safari/537.36",MG,BR,HTML,http://epoca.globo.com/vida/noticia/2016/09/o-chefe-e-gay-e-dai.html,o chefe é gay. e daí?,"O técnico de laboratório de cinema Lucca Najar e o estagiário de comunicação Gael Benitez são transexuais masculinos. Ambos nasceram e foram registrados como pertencentes ao sexo feminino, mas se sentiam no corpo errado. Para evitar discriminação, comportaram-se como era esperado tradicionalmente de mulheres, até o início da vida adulta - Najar tem 25 anos, e Benitez 21. A história dos dois começou a mudar quando foram trabalhar e estudar no Centro Universitário Una, de Belo Horizonte, empresa premiada no ranking ÉPOCA GPTW neste ano. Os dois encontraram um ambiente favorável para assumir sua real identidade de gênero. Najar foi contratado ainda como mulher em 2012 e hoje sente orgulho ao ver seu nome social masculino no crachá, nas folhas de prova e na lista de chamada. ""Fui bem recebido por colegas, professores e funcionários"", afirma. O mesmo aconteceu com Benitez. ""Percebi que o Una era um lugar tranquilo para iniciar minha transição"", afirma. >> Elektro, Google e Sama lideram entre as melhores empresas para trabalhar em 2016 EXEMPLAR Lucca Najar, Roberto Reis e Gael Benitez (da esq. para a dir.) , do Una. O programa de combate ao preconceito na empresa já desperta interesse acadêmico (Foto: Alexandre mota/Nitro/ÉPOCA) Gente educada ajuda muito, mas o ambiente acolhedor do Una resulta não apenas da boa vontade de professores e funcionários. Trata-se de uma decisão da empresa e de efeitos do projeto Una-se Contra a LGBTfobia, concebido em 2011 e coordenado por Roberto Reis, professor do Instituto de Comunicação e Arte (LGBT é a sigla para lésbicas, gays, bissexuais, travestis e transexuais). Reis é gay, mas em empregos anteriores nunca havia falado disso, por receio. No Una, o medo desapareceu. ""É importante e libertador poder me posicionar"", afirma. O projeto inclui ações educativas para todos que circulam pelos 12 campus da instituição. Os professores participam de oficinas sobre diversidade de identidade e orientação sexual. Em 2015 e 2016, o Una recebeu o Prêmio Direitos Humanos e Cidadania LGBT, promovido pelo CellosMG, ONG responsável pela organização da Parada do Orgulho LGBT na capital mineira. Carolina Marra, reitora do Una, reconhece a dificuldade de combater o preconceito numa instituição com mais de 30 mil alunos. ""O primeiro passo é admitir que há preconceito, depois assumir que isso não é legal e reagir como instituição"", diz. Esse tipo de ambiente ainda é raro no mundo corporativo, mesmo entre as empresas dispostas a passar pelo crivo do GPTW. Mas há uma transformação visível em curso. Neste ano, no grupo das empresas premiadas, nove profissionais, oito do Google e um da farmacêutica AstraZeneca, optaram por não se identificar como homens nem como mulheres. Na ficha demográfica de gênero dos funcionários nessas duas organizações, pela primeira vez ÉPOCA GPTW usa a opção ""outros"". >> ""Quero diversidade social, de gênero, raça e idade"", diz a presidente da Microsoft Mesmo entre as empresas interessadas em lidar com o tema, há dúvidas sobre como proceder. ""Ainda há polêmica em questões simples - qual banheiro usar? Colocar ou não na mesa de trabalho um porta-retratos com a foto do companheiro ou companheira?"", afirma Reinaldo Bulgarelli, secretário executivo do Fórum de Empresas e Direitos LGBT e sócio-diretor da consultoria Txai, que atua em responsabilidade social. Bulgarelli criou o fórum em 2013 e lançou uma carta pública com dez compromissos para a promoção dos direitos humanos de LGBTs. Apenas 33 empresas a assinaram, até o momento. Organizações interessadas em enfrentar o tema podem buscar referências onde já se acumula alguma experiência. Ben Boyd, executivo-chefe para América Latina e Canadá da agência de comunicação e consultoria Edelman, sugere um roteiro. Primeiro, conseguir o apoio de líderes-chave da organização. Depois, definir um executivo que assuma a responsabilidade sobre o programa. ""Vale a pena conversar com outras empresas que tenham políticas de inclusão e grupos de funcionários LGBTs, para aprender as melhores práticas e evitar erros"", afirma. Boyd é gay e criou em sua empresa o programa Edelman Equal. PORTAS ABERTAS Camila Crispim, desenvolvedora de software na ThoughtWorks. Ela cansou de ouvir ""papinho preconceituoso"" - e achou uma organização que leva a questão a sério (Foto: Leo Caldas/ÉPOCA) Entre as premiadas no GPTW, há diferentes iniciativas e experiências. A consultora sênior Paula Miyuke Tomiyoshi, de 36 anos, ainda não tinha feito sua transformação física quando ingressou na desenvolvedora de software SAP, há sete anos. Vestia-se como homem. Soube da política da SAP para funcionários LGBTs quando conversou com seu gerente sobre a vontade de mudar de gênero. ""Ele me tranquilizou e ofereceu a opção de eu trabalhar em casa durante a transição"", diz. Ela aceitou. Por nove meses, dividiu-se entre as tarefas do escritório, o tratamento com hormônios e cirurgias de feminilização. No período em que Paula esteve fora, a SAP tomou providências para tornar seu retorno o mais natural possível. Houve conversas na equipe e o psicólogo que cuidava dela falou com o pessoal e tirou dúvidas sobre transexualidade. A empresa enviou uma carta a clientes enfatizando seu apoio à funcionária. Apesar dos cuidados, Paula se sentiu discriminada por alguns colegas na volta ao escritório, já como mulher. ""Mesmo que a empresa tenha uma política, o preconceito é uma questão individual"", afirma. O programa de diversidade da SAP, chamado de Pride@SAP, deverá ser ampliado, e a empresa faz uma pesquisa com os fornecedores. ""A SAP tem uma posição clara sobre o assunto. Queremos trabalhar com companhias comprometidas com a diversidade e a inclusão"", diz Marcelo Carvalho, diretor de RH. Outra empresa de tecnologia da lista ÉPOCA GPTW, o Google tem vários comitês de diversidade criados pelos funcionários. Um deles é o Gayglers, liderado por Zen Junior, de 30 anos, analista de suporte. O grupo se reúne quinzenalmente e promove palestras internas e externas, eventos e apresentação de vídeos sobre LGBTs. Zen escolheu o Google para trabalhar há quatro anos quando soube do Gayglers. ""Sou realizado no Google, faço o que gosto e ainda posso militar pela causa LGBT"", afirma. ""O Google defende que as pessoas sejam elas mesmas. Não aceitamos preconceito"", diz Monica Santos, diretora de RH. >> GPTW 2016: As empresas que mostram o jeito certo de enfrentar a crise Companhias que toleram discriminação no ambiente de trabalho arriscam-se, no mínimo, a perder bons profissionais. Gabriel Brasileiro, de 30 anos, supervisor de comércio exterior, é gay e começou a trabalhar aos 18 numa multinacional sem abertura para esse tipo de diálogo. ""É muito difícil representar o tempo todo, ouvir piadas e ficar calado"", afirma. Por isso, procurou emprego na Monsanto, pois sabia que a empresa tinha uma política de promoção da diversidade. Foi contratado em dezembro de 2013 e assumiu a orientação sexual com tranquilidade. Chefia uma equipe de 12 pessoas e se sente respeitado por todos. Carlos Brito, diretor de RH para a América do Sul da Monsanto, acha que as empresas têm papel crucial em ajudar a sociedade a se tornar mais tolerante. ""Elas têm de ouvir, entender os desafios e se comprometer com o tema"", diz. A falta de empenho da maioria das companhias em lidar com o tema prejudica as equipes e o indivíduo. A analista de recrutamento e seleção Nathalia Carneiro tem 26 anos e é bissexual. Em dois empregos anteriores, teve de esconder seu namoro com outra mulher. Diz que, quando descobriram, não conseguiu mais avançar na carreira. Decidiu trabalhar na empresa de tecnologia Movile porque soube da receptividade da empresa e da política de oferecer chances de desenvolvimento independentemente da orientação sexual de seus profissionais. ""Fui acolhida e sou respeitada"", afirma. Profissionais dos grupos LGBTs que trabalham em ambientes preconceituosos gastam energia preciosa tentando não ser julgados nem mal interpretados. Camila Crispim, de 28 anos, gay, é desenvolvedora de software na ThoughtWorks. Em empregos anteriores, evitava participar de happy hours com os colegas e oferecer carona para mulheres. Não queria que pensassem que estava assediando alguém. Uma das empresas parecia ter postura mais aberta, mas tolerava que funcionários manifestassem os preconceitos. ""Cansei de ouvir papinhos do tipo 'não sou preconceituoso, mas não quero um filho gay'"", diz. ""Vivia uma vida com a família e os amigos e outra no emprego."" No início de 2013, Camila foi contratada pela ThoughtWorks e se sentiu acolhida. ""Não aceitamos que ninguém seja chamado de 'veadinho'"", diz Taise Assis, diretora de Justiça Econômica e Social da empresa. ""Esse tipo de violência não é aceito aqui. Na primeira vez perdoamos, mas na segunda não."" Todo novo funcionário assina um contrato antiassédio e participa de uma palestra sobre diversidade. A empresa fez uma parceria com o projeto Transcidadania, da prefeitura de São Paulo, que visa à reintegração social de travestis e transexuais em situação de vulnerabilidade social. A ThoughtWorks recebeu de fevereiro a abril deste ano nove transexuais na empresa e ofereceu um curso gratuito de informática. A empresa também apoiou o desenvolvimento do site Transerviços por uma ex-funcionária transexual, que lista profissionais e empresas que não são transfóbicas. As corporações começam a descobrir as vantagens de olhar a diversidade com mais atenção. Monica Santos, do Google, acredita que a diversidade na empresa torne os funcionários mais satisfeitos, engajados e produtivos. É fato que organizações com maior diversidade na equipe tendem a lidar melhor com mecanismos de controladoria externa e chegam a soluções mais variadas para enfrentar problemas. Mesmo assim, a parcela de trabalhadores gays que não se sentem à vontade para assumir a homossexualidade diante de qualquer colega permanece grande - varia de 34% na Holanda a 92% na Índia, segundo pesquisas anuais feitas pela consultoria australiana Out Now. No Brasil, essa parcela é de 65%. A discriminação é uma violência contra direitos humanos e trabalhistas - além de provocar um monumental desperdício de energia, atenção e dinheiro.",pt,173
27,1737,1468337707,CONTENT SHARED,1356221992133852808,-1032019229384696495,1370975477017368957,,,,HTML,https://techcrunch.com/2016/07/11/the-brilliant-mechanics-of-pokemon-go/,the brilliant mechanics of pokémon go,"A court ruled that it could be a federal crime to share your Netflix password If you haven't seen it already, you will soon walking down the street. Every person you pass who is fervently looking at their phone is likely playing the number one game in the country right now: Pokémon Go. You might think it's popular because of the brand. Nintendo, which refused to make a Pokémon game for the longest time on a smartphone, has finally caved and brought its beloved franchise to the small screen. But what may be overlooked amid all that is that the game, on its own, is phenomenally well designed on its own, despite myriad bugs and endless server outages. If you look at all aspects of the game loop - the engagement, retention, virality and monetization - it nails pretty much everything on the head. Niantic managed to hit a very rare, exceptional home run on every textbook point of the game's development. That's not an easy feat. Only a few games in the history of the iPhone have managed similar success. The closest analogues are probably Minecraft and Candy Crush Saga, which also rocketed to not only the top of the App Store download charts, but also the top grossing charts. Pokémon - much like Minecraft before it - launched immediately at the top of the charts. So its immediate success, based on the App Store rankings, isn't necessarily unprecedented. So, what makes this game so engaging and, from what we've seen so far, potentially very addictive? Let's break it down into its core pieces. Engagement Some of the best popular games have bite-time playing sessions. But the session time in Pokémon go can essentially be as long as the player wants, because there is a constant way to increase the length of the session time by walking to more pit-stops. That's a really hard thing to do in a game. Most session times are restricted to levels or gated with lives or energy. For Pokémon Go, there's just enough friction to inspire players to potentially pay to extend the length of their play session with less work, but also offer them the ability to go out of their way to extend that session time without having to pay. When I think about the structure of the play sessions in Pokémon Go, I often think of a role-playing game called Persona 4 Golden for the PlayStation Vita. The overall unit of time in the game is a day in the life of your character. The sessions are built into bite-size chunks based on the time of the day - morning, afternoon, and evening - and save points are littered through most parts of the game. And the combat sections of the game are also segmented into levels, with the option of leaving a dungeon at any point to save your game and end the playing session. In the same way as Pokémon Go, the game's session time can basically be extended to as long as the player wants while still keeping the basic mechanics of the game intact. In the case of Persona 4 Golden, that friction isn't necessary because the player has already bought the game, but for Pokémon Go it's incredibly well executed. Pokémon Go, like other well-designed popular mobile games, offers a quick ramp up that teases a lot of front-loaded rewards to get the player to come through the door and shut it behind them. That's important to grab their attention, but there's also multiple layers of rewards that keep players wanting to stay in the game. You can collect items in order to power up your Pokémon and evolve them, but it's also important to level up your own character. There are different layers of currency built into the game that progress along different time curves, giving each layer of progression its own speed and flavor. In that way, players can hit rewards at different increments of the game without feeling trapped in a grind for everything to level up at the same time. Amid the entire play session, the game has to stay open. That keeps you from getting distracted and flipping out to other apps. I find myself walking with my phone in my pocket, but with the game open often enough while wearing headphones. Whenever there's a chime, I take the phone out of my pocket and start playing - whether that's collecting Pokéballs or trying to capture something new (or some crappy junk Pokémon for the sake of experience). The game world is vibrant and beautiful, making it something easy and fun to see. It's filled with flair and flashes that are visually stimulating and signal new elements of the game. All this makes the player want to keep their eyes - or ears - glued to their phone, ready to engage with it the moment something new happens. All of this is great design, and doesn't even mention the brand equity Pokémon has built up. Nintendo has sold nearly 60 million 3DS units . Pokémon X & Y alone have totaled nearly 15 million in sales . That's an incredible nearly 25% penetration rate for all Pokémon enabled devices. If Nintendo were to barely scratch that with the nearly 2.5 billion smartphones in the world ( according to Statista ) that alone represents a staggering install base. Pokémon already is a worldwide phenomenon, and that alone is probably enough to get the player in the door beyond simply encountering other players and hearing about it organically. Pokémon Go currently only supports the original 150 Pokémon as well, tapping into the an untapped nostalgia that players have been waiting nearly a decade for. Retention An array of user-generated gameplay experiences is critical to building strong retention, and all the pieces are already built into the Pokémon Go experience. Each capture session is unique - the angle of the Pokéball is different, the placement of the Pokémon is different, and there's also an opportunity to have a unique experience also tied with the real world. You have probably seen on your Facebook feed screenshots of Pokémon sitting on other peoples' heads or in their laps. Each capture moment offers a unique player session, and while many will be similar, there's the tantalizing opportunity to have something truly unique that's really exciting. There's also an incredibly sticky part of user-generated content that exists alongside the game: the actual walk. Each walk a player goes for, in theory, is unique. The environment in the real world is different. the people who you run into may be different, the weather may be different, or the time of day. The environment in the game is also different, with Gyms constantly in flux and new Pokémon appearing at different intervals and in different places. Each walk also actively engages the player physically - and exercise naturally triggers a positive feel for your body, adding an additional layer of delight to the gameplay experience. This is such a new mechanic for a popular game that's unprecedented. For most games, the user-generated gameplay is restricted to an imaginary universe. It's a level on Candy Crush Saga that you get that lucky explosive cookie. It's a player-versus-player round in World of Warcraft where you get that lucky critical hit. It's a round of Destiny where you are just on fire and keep getting headshots. But all of these take place inside a screen, interfaced by a controller - whether that's a real controller or a touchscreen. The mastery curve is also smooth - over time you build a strong array of Pokémon that help you advance further in the game. The end of the game, like the regular Pokémon games, is a moving target, and there's basically always someone out there that's slightly better than you. That gives players a constant incentive to continue progressing along the mastery curve. Virality What's also unprecedented is Niantic's spin on the game's viral loop. In Pokémon Go, there's no feature that allows you to extend the life of your playing session by inviting or reaching out to friends. In fact, the social graph is almost non-existent in Pokémon Go. Instead, your in-game social graph is an extension of a supplemented version of your real-world social graph. A smartphone owner sees someone playing the game, becomes curious, downloads the game, and plays it - both interacting with other players and inspiring curiosity in other potential new players. And the rest of the time you're looking at screenshots of what's happening in the game in your Facebook feed, or texting friends when you managed to catch that rare Pokémon. You can read stories in many places on the internet of people randomly interacting with each other related to Pokémon Go. I experienced this already when walking around San Francisco, only to have a car drive by with one of the passengers yelling that there was a rare-ish Pokémon down the street (it was an Ivysaur, for those keeping tabs). This is table stakes for the Pokémon Go experience, and it's what gets new players in the door. This kind of virality is especially powerful because it isn't limited to an existing social graph. The whole viral loop is augmented in such a way that a non-connected interaction in the real world can lead to a new player, a download, and then monetization of that player. That's why I think this interpretation of the viral loop mechanic is so fascinating and is going to be so successful. Never before has a game immediately achieved such popularity in such a way that it regularly intersects with the real world. A lot of people consider it to be an augmented-reality experience, and in many ways you could consider it to be that. But it's not just an experience that uses your camera to play - it's an experience that crosses the boundary between an imaginary universe and the real world. I think the proper term that should be applied to this would be mixed-reality. The phrase augmented reality just doesn't give the game enough credit for being able to break that fourth wall and constantly move the player between an imaginary world and the real world. And it also represents an enormous opportunity for the game if Niantic decides to implement other important aspects of Pokémon like trading. Without an embedded social graph the game has already grown to immense popularity. Just imagine if it began to add an additional layer of player interaction - even if, again, it only takes place in the real world. Monetization So it's no wonder that the game has already hit the top of the App Store's top grossing charts. There's a lot going on in the monetization component of Pokémon Go, but again, it nails nearly every angle of attack to get players to make a payment. You can extend the life of your play session with more Pokéballs. You can speed up your growth curve by getting Egg Incubators, further increasing your array of potential Pokémon to further progress in the game. You can increase the rate of the engaging capture sessions by buying Incense. The same can be said for Lure Modules, which not only represent accelerated progression in the game but yet another way to tap into Pokémon Go's viral loop. Players congregate around areas - whether for catching Pokémon or building up their Pokéball stock - and that increases the probability that new, curious players will come by and discover the game independent of the App Store or other methods like Facebook App Install ads. Users paying for this contribute to the entire community of players given the benefit it offers everyone else. The most important aspect of this is that the gameplay, unlike most of the most-popular mobile and social games, is not gated. Paying Niantic and Nintendo money simply allows players to progress more quickly, but it doesn't impede their progress overall. Players have an opportunity to progress through the game at their own rate. As the saying goes, you still need the slow boat to China if you're going to be successful. Niantic here does such a good job of creating just enough friction that, at the exact moment, it can capture an opportunity for monetization. Players don't feel compelled to spend money, and instead they're offered a delightful experience when they elect to spend money. Those eye-popping visuals continue, they keep throwing Pokéballs, and they don't have to wait to see some of the most powerful Pokémon game. Final thoughts All this together creates a very powerful, sticky, and accelerating game loop that is helping the game grow at such an incredible rate. But there's another underlying thread amid all this: It bodes very well for holdout franchises to expand into mobile devices amid fear of cannibalizing devices or other parts of the market. Even Final Fantasy, in a way, has found its way onto mobile devices with Final Fantasy Record Keeper. Nintendo, amid the runaway early success of the game, added $9 billion to its market cap . This is such a strong, powerful signal to holdout franchises that haven't quite entered the smartphone ecosystem. And there's a good reason to do so: if that 2.5 billion device number from Statista is accurate, it offers such an enormous opportunity that it may be well worth eating up some potential hardware sales. This is a transition that the advertising market faced in the not too distant past. Google is constantly hounded by the need to shift its advertising revenue to mobile devices, in the hope that less valuable ads can be traded for a larger volume of ads on mobile devices. Facebook has built a business worth hundreds of billions of dollars off its mobile advertising products. Video game stalwarts will face the same dilemma: do you trade hardware and console sales in favor of the incredible volume of smartphone users? Is it worth the risk to assume people will still buy your consoles when Mario is available on your phone? Can a company like Nintendo offer an array of experiences that span multiple devices? In this week's blowout success of Pokémon Go, the answer for now appears to be rounded up to a Yes. Alas, there's no way to mash down+A+B. Get on that, Niantic. Featured Image: Eduardo Woo / Flickr UNDER A CC BY-SA 2.0 LICENSE",en,171
28,949,1463146938,CONTENT SHARED,-5784991738549272379,2279740393166882579,1579934520221634711,,,,HTML,http://irving.com.br/esp8266/nodemcu-esp8266-o-modulo-que-desbanca-o-arduino-e-facilitara-a-internet-das-coisas/,nodemcu (esp8266) o módulo que desbanca o arduino e facilitará a internet das coisas...,"A partir de 2005 o Arduino se tornou a principal ferramenta de makers e hobbistas para a o prototipação de projetos eletrônicos. Pois bem, eis que em meados de 2014 surgiu um concorrente à altura (eu diria muito superior): O ESP8266, um módulo eletrônico muito menor, mais veloz, potente e muito mais barato do que o Arduino, além de já possuir conexão Wi-Fi. Com isso, esse módulo possibilita os mais diversos projetos, conectados à Internet e é possível programá-lo usando o mesmo software e linguagem do Arduino. O potencial do ESP é tão grande que ele venceu o prêmio de hardware do ano (2015/16) pelo IoT Awards . Algumas informações sobre ele: Confusão de nomes, quem é quem? ESP8266 (na verdade ESP8266EX) é um chip de arquitetura 32 bits com Wi-Fi integrado, medindo apenas 5mm x 5mm E produzido pela companhia chinesa Espressif . O seu tamanho tão pequeno dificulta a utilização, mesmo soldá-lo é uma tarefa complicada, então uma outra empresa chinesa, a AI-Thinker , passou a produzir módulos utilizando o chip e colocando alguns componentes extras (Memória adicional, memória EEPROM, antena...). Esses módulos sofrem rápidas atualizações e começaram a ser chamados por números, os mais conhecidos são: ESP-01 - que contava apenas com 8 conectores, e servia mais para ser utilizado como um módulo para o Arduino; ESP-07 - que foi bastante vendido, contava com 16 pinos, antena de cerâmica e conector para antena externa; E o ESP-12E - Uma atualização do ESP-12, que conta com 22 pinos, entre esses pinos extras estão os da interface SPI, que possibilita ligarmos vários módulos ao ESP (displays, cartões SD, e diversos 'shields'...) Agora vamos ao NodeMCU A gravação dos módulos ESP não é das mais simples, é necessário um conversor USB/Serial (FTDI), para que os dados possam ser passados do computador para o módulo. Outro ponto dos módulos ESP é que eles utilizam 3,3V (inclusive alguns conversores só possuem saída de 5V), o que não é uma tensão fácil de obter. O NodeMCU é constituído por um módulo ESP-12E, um conversor FTDI, e um regulador de tensão de 5 para 3,3V e possui o espaçamento entre os pinos padrão (2,54mm) o que nos permite conectá-los à placas de prototipação (protoboard), e sua programação é bem mais simples, sendo necessário apenas ligar um cabo USB que também serve como alimentação para o circuito. O projeto original foi pensado para se utilizar a linguagem de programação Lua (Brasileira inclusive) mas é possível fazer upload dos códigos em outra linguagem normalmente. Várias empresas lançaram diferentes versões, a ""original"" NodeMCU , as mais acessíveis WeMos , Amica e DOIT , e as mais confiáveis SparkFun Thing e Adafruit HUZZAH . Tamanho Uma imagem vale mais que mil palavras, nesta imagem podemos ver o Arduino Uno, um NodeMCU e um shield Wi-Fi. O NodeMCU faz o papel dos outros dois componentes juntos, ou seja, é capaz de receber programação e já tem uma interface Wi-Fi, possibilitando uma imensa gama de projetos. Vamos aos números: O Arduino Uno mede aproximadamente 5,3cm x 7,9cm e o shield Wi-Fi tem praticamente o mesmo tamanho (é usado em cima do Arduino Uno, não há aumento no espaço utilizado), já o NodeMCU mede 2,5m x 5,1cm, um terço o tamanho do Arduino, isso sem contar na possibilidade de usar apenas o chip principal, que mede apenas 2,4cm x 1,6cm (quase 11 vezes menor). Hardware Neste quesito a vitória do NodeMCU é mais expressiva, o Arduino UNO possui um microcontrolador (ATMega 328P) de 16MHz (velocidade de processamento, um celular atual é aproximadamente 100 vezes mais veloz), possui uma memória RAM de 2KB (Memória onde são armazenadas as variáveis, os valores que você salva no meio do código) e uma memória flash de 32KB (espaço para armazenamento do programa em si). Já o NodeMCU possui um processador (Tensilica LX106) que pode atingir até 160MHz (10 vezes mais rápido que o Arduino), uma memória RAM de 20KB e uma memória flash de 4MB! (Tão grande que é possível fazer download de uma atualização do próprio código!). Os shields Wi-Fi mais comuns possuem conexões 802.11 b e g (11MBps e 56MBps de limite de velocidade, respectivamente). Já o ESP possui as mesmas conexões além da 802.11n que possui um limite 300MBps de velocidade e tem um alcance até duas vezes maior. Preço Aqui a situação fica melhor ainda! Nos sites chineses é possível encontrar um ESP8266 por até US$1,70 e um NodeMCU por menos de US$2,30 (menos de 12 reais), um Arduino UNO original não sai por menos de 20 dólares. No mercado brasileiro, usando apenas um site como comparação um NodeMCU sai por aproximadamente R$70, e um ESP8266-12E por R$45. Um Arduino UNO cópia está saindo por R$60 e um shield Wi-Fi não sai por menos de R$250... Ou seja, o custo-benefício do ESP/NodeMCU é altíssimo e é possível encontrá-lo a preços mais acessíveis no MercadoLivre, por exemplo. Facilidade Como foi dito, a programação do ESP8266 pode ser feita de várias maneiras: Em C, através da SDK do fabricante (Espressif) Em Lua, que é a linguagem oficial do NodeMCU E em C++*, a mesma linguagem no Arduino A mais simples de ser utilizada é a linguagem do Arduino, as bibliotecas são muito completas, sendo necessário pouca programação para já conectar o ESP no seu roteador Wi-Fi e fazer as mais diferentes e úteis aplicações. O código de exemplo abaixo, procura por todas as redes nas redondezas e as imprime na tela juntamente com a potência do sinal e o tipo de encriptação. Contras Claro que nem tudo são flores, um dos pontos negativos do ESP é a presença de somente uma entrada analógica, o que dificulta a utilização de muitos sensores simples, além disso, o valor máximo aceito nessa entrada é de 1 Volt, sendo necessário algumas conversões para obter o valor desejado. Há também um número menor de portas digitais, mas nada que comprometa a maioria dos projetos. E aí? É fácil evidenciar a enorme vantagem que o ESP/NodeMCU leva sobre outros dispositivos, ele mescla o hardware potente e de baixo custo das manufaturas chinesas com a facilidade de programação do Arduino, que nasceu na Europa mas é difundido pelo mundo todo. Este pequeno chip, como a mídia especializada sempre diz, é claramente um IoT enabler , tem tudo para estar presente nos mais diversos gadgets daqui pra frente. E a Espressif já anunciou a pré-produção do ESP32 , um chip ainda melhor que o 8266 com processador dual core, 500KB de RAM e Bluetooh 4.0",pt,166
29,1409,1466013280,CONTENT SHARED,1854874463930846880,534764222466712491,7914515688450890484,,,,HTML,https://mulheresnacomputacao.com/2016/06/14/o-dia-em-que-tive-mais-medo-de-estar-errada/,o dia em que tive mais medo de estar errada.,"Passei meu sábado em Belém do Pará, mais especificamente na Ilha de Combú na Floresta Amazônica. E eu tive um dos dias mais incríveis da minha vida. Mas foi também o dia em que eu tive muito muito medo. Medo de estar errada. Muito errada. Vou contar a história inteira. E vou voltar lá pra minha primeira infância. Desde pequena eu sou bem curiosa e acho que não sabia, mas isso era sede de mundo. Sede de saber o que tem em cada canto desse planeta. Entender como tudo funciona e isso me fez estar onde eu estou. Arrumei uma aliada muito boa lá pela metade da minha jornada, que sem ela não seria possível: a tecnologia. Achei respostas. Decidi que faria isso da vida. Mostrar essa aliada pra todo mundo. Ser a representante da tecnologia que eu conhecia, que me libertou e não que ameaçava. Ela não precisava ser só minha e no último sábado, lá pelas seis da manhã quando abri os olhos eu estava bastante confiante nisso apesar do sono, estava vindo de uma avalanche de novas informações, passado por cinco capitais em seis dias....enfim estava tudo bem bem, até que... Entrei em um barco, e partimos pra uma ilha, a Ilha de Combu... Tinham várias pessoas no barco , voluntários, minha equipe, pessoal da Intel, time do Barco Hacker, enfim...só gente do bem! Tomo mundo tagarelando a viagem toda. As palafitas, a navegação, a falta de conexão estável, a falta de tomadas era esperado até certo ponto, mas quando vimos 17 crianças de 3 a 12 anos que conheciam a energia elétrica apenas à 4 anos todos nos calamos. Não tinha a menor ideia do que fazer! Tive um minuto de branco! Todas as minhas crenças passaram em alguns segundos na minha frente! Será que é tecnologia a resposta pra elas? Será que um dia elas iam conseguir aproveitar aquele conhecimento? Será que elas vão entender? Será que é isso mesmo? Será que é pra todo mundo mesmo? Será? Será? Questionei tudo e por pouco não desisti...quase que fiquei só ali conversando e disfarçando...coloquei todo mundo em roda pra ganhar tempo e pensar no que eu faria. Olhei na carinha de cada um e todos estavam vidrados em mim, no meu conhecimento, no que eu estava falando...tudo isso enquanto duas menininhas se empurravam pra ver quem sentava do meu lado. Crianças! Como todas as outras...ABERTAS! CURIOSAS! SEM MEDO! Aquela cena que era trivial, serviu de faísca e começou a transformação! Minha cabeça estava sempre um pensamento na frente tentando traduzir pra eles o que era um microcontrolador, uma protoboard, um resistor...de um jeito que a lição final fosse: EU SOU CAPAZ! FUI EU QUE FIZ ISSO! ISSO É TECNOLOGIA E EU NÃO PRECISO TER MEDO. Sentamos no chão, perdemos o medo da placa, montamos circuito, programamos, todos, juntos, iguais! Conforme iam conseguindo acender as luzinhas (LEDs) e escolher o intervalo que quisessem todos vibravam, riam, gritavam, comemoravam...fiquei maravilhada com a rapidez com que eles se empoderaram com aquele conhecimento! Eu já estava querendo chorar e gritar pro mundo que estava dando certo, que tecnologia empoderava qualquer um, independento do lugar do mundo que estivéssemos! Mas ainda tinha a prova final...eles tinham 15 minutos pra pensar e desenhar soluções usando aquilo que eles tinham visto para os problemas da comunidade, para quê eles usariam aquele aprendizado? Será que eles iam conseguir aplicar? Eu precisava me segurar mais um pouco! Me controlar. Eles ficaram maravilhados com as canetinhas e réguas, muitos não tinham uma. Deixamos várias de presente. Começaram as apresentações, eles estavam com vergonha, não sabiam se estavam certos, se eram bons...ai se eles soubessem! Não se envergonhariam nem por um segundo...muito pelo contrário! As ideias eram incríveis...resolviam problemas reais, problemas deles com uma simplicidade admirável. Vou contar a que mais me chamou atenção: um braço extensível com uma plaquinha que coleta informações como cor do açaí e informa se ele está maduro ou não, evitando que alguém tenha que subir no pé, num primeiro momento parece meio superficial, até que eles foram me mostrar como é o processo...uma pessoa com uma folha amarrando os pés tem que subir uns 15 metros, correndo o risco de cair ou a árvore quebrar só pra ver se já está na hora de colher....pra alguém conseguir fazer isso tem que ""calejar"" o pé e a mão na raça...um problema que não está sendo resolvido! Isso é problema de verdade! Isso é solução de impacto! Quando os pitches acabaram e eles foram comer o lanche, coloquei meu óculos escuros e chorei! Chorei! Chorei porque eles estão apartados, eles estão isolados, excluídos, eles não estão tendo os problemas resolvidos... Chorei porque não tem preço ver a expansão dos sonhos de crianças. Os horizontes mudam quando um LED acende! E basta um! Ensinar tecnologia para eles é muito mais que passar conhecimento técnico, é mostrar que eles podem! Depois dessa eu nunca mais vou sentir medo! E se sentir...vou com medo mesmo! Para quem quiser saber mais sobre tudo que a gente vem fazendo dá uma olhada no Mastertech , na Ponte21 e na Maratona Maker ❤ Beijos! PS.: Comentem aí como tecnologia mudou a vida de vocês!",pt,165
30,1492,1466620390,CONTENT SHARED,6997620589258672675,-48161796606086482,-4406729997686684554,,,,HTML,https://pagamento.me/abrimos-uma-conta-no-original/,abrimos uma conta no original.,"Você já deve ter percebido que esse portal fala bastante de bancos digitais, certo? Fizemos um post "" Nubank - a experiência"" ( ), em 31 de Janeiro de 2015, onde contamos como foi ter se cadastrado, validado os dados e ter digitalizado os documentos pelo app. Foi uma verdadeira aula de ""usabilidade"" e de praticidade. Recebemos o cartão em 7 dias após cadastro. Fizemos no mês passado, o processo completo de abertura de conta no Original. Acompanhe como foi. Banco digital: primeiras impressões A conta foi aberta em 19 dias exatos. Iniciamos o processo de cadastro via app dia 03/04 e recebemos a confirmação de abertura com o número da conta e agência, no dia 22/04. Já o cartão, foi enviado somente no dia 05 de maio. Mais de um mês pós cadastro. Bancos de varejo como Itau, Santander e Bradesco, com seu processo normal de abertura, teriam 5 dias para abrir a conta, mais 5 dias para enviar o cartão. O que totalizaria o processo completo em 10 dias. Para nós o grande atrito está na coleta da assinatura digital presencial do Original, que ainda está lenta entre cadastro e coleta. Do cadastro (03/04) até o dia da coleta e conferência presencial dos documentos (20 de abril), foram 17 dias. Apesar do desafio do prazo, foi uma boa experiência. Na prática foram 4 passos básicos: 03/04 - Cadastro no app; 20/04 - Coleta da assinatura digital; 22/04 - Confirmação da conta aberta; 05/05 - Recebimento do cartão Vamos às ilustrações. 1. Cadastro no app do Original O ponto de melhoria visto pela nossa equipe foi justamente a questão do cadastro dos dados no app. Por se tratar de uma abertura de conta, acreditamos que algumas questões de regulamentação e KYC (know your customer) devam ser seguidas à risca, ainda. Porém, o tempo para finalizar o cadastro total, digitalização de documentos e fotos, durou cerca de 20 minutos. O ideal era reduzir ainda mais isso. Mesmo assim, foi um belo trabalho do banco, criar um mecanismo para abrir contas digitais de uma forma realmente prática. Bem mais simples e mais rápido do que abrir contas tradicionais. Adeus fila! 2. Coleta da assinatura digital Recebemos um agente de coleta, para confirmar o documento e coletar novamente a digital dos dedos, só que dessa vez presencialmente. O double check , segundo o próprio representante, é norma. A burocracia trouxe um mimo: o banco enviou um café especial para dar as boas vindas. 3. Confirmação da conta aberta Com a confirmação da conta aberta, fizemos o acesso via web e já fizemos os testes funcionais: depositando dinheiro, simulando investimento e testando a plataforma. A agência Try mandou muito bem na concepção / design. Ficamos bem surpresos com o resultado das cores e da facilidade de navegação da estrutura. Bem diferente dos bancos tradicionais. 4. Recebimento do cartão Ufa! Tá na mão. E ainda veio com outro brinde �� O Original abriu uma porta bem importante em se posicionar como um banco digital de fato. Não ter agência física é de realmente uma grande mudança, mas também um possível realidade nos próximos anos. Sugestão: leia o texto "" 3 sinais que fazem das fintechs, uma bomba explosiva para os bancos brasileiros"" e entenda como isso de abrir contas em agências bancárias tradicionais, vai morrer em breve. Belo trabalho Original �� Fotos: reprodução Original.com.br",pt,161
31,1366,1465827586,CONTENT SHARED,-8518096793350810174,1895326251577378793,-1287126046112431871,,,,HTML,http://computerworld.com.br/microsoft-adquire-linkedin-por-us-262-bilhoes,"microsoft adquire linkedin por us$ 26,2 bilhões","O mercado de tecnologia começou a semana em ritmo intenso. A Microsoft anunciou no começo dessa segunda-feira (13/06) que desembolsará nada menos que US$ 26,2 bilhões, em dinheiro, pelo LinkedIn. Em comunicado ao mercado, a fabricante do Windows assegurou que a rede social corporativa manterá sua marca, cultura e independência. Jeff Weiner seguirá no comando do projeto, reportando-se a Satya Nadella. A compra representa o maior valor já desembolsado pela Microsoft em sua história. A quantia é superior ao preço pago pela Nokia e Skype, combinados, e corresponde a US$ 196 por ação. O LinkedIn é a maior rede social corporativa do mundo, contabilizando um total a 433 milhões de usuários ao redor do mundo e uma oferta de serviços que vai desde ferramentas de recrutamento até publicidade. Apesar de manter independência, a companhia passará a integrar o segmento de Produtividade e Processos de Negócio da gigante de software. ""Juntos, vamos acelerar o crescimento do LinkedIn, bem como do Office 365 e Dynamics na busca por dar mais poder a cada pessoa e organização"", pontou Nadella. Weiner afirma que a combinação das ferramentas da rede social com as ferramentas em nuvem da nova controladora criarão condições de ""mudar a forma como o mundo trabalha"". A expectativa é que a transação esteja completa ao final de 2016. A conclusão do negócio ainda depende de aprovações de órgãos reguladores de mercado. As empresas disponibilizaram um vídeo no YouTube , no qual Nadella e Weiner falam sobre os motivos e estratégias por trás da aquisição. Mais compras Também nessa segunda-feira, a Symantec revelou que desembolsaria US$ 4,65 bilhões pela Blue Coat . Combinadas as operações, a companhia deve faturar US$ 4,4 bilhões em 2016. Do total, 62% das receitas originam-se de contratos com clientes corporativos.",pt,156
32,785,1462381699,CONTENT SHARED,1738052593226421681,6013226412048763966,5796685774598185282,,,,HTML,http://www.pnl.com.br/programacao-neurolinguistica/publicacoes/como-resolver-conflitos-no-ambiente-corporativo-usando-a-pnl-,como resolver conflitos no ambiente corporativo us,"por Gilberto Craidy Cury em 25 de abril de 2016 Muitas pessoas confundem divergência com conflito, com briga, mas na verdade, o conflito, principalmente dentro do ambiente corporativo, nada mais é do que a diferença de opiniões somada à má comunicação. As diferentes opiniões no trabalho são comuns e quando bem gerenciadas levam ao fortalecimento das equipes e ao crescimento profissional. A maneira sadia de lidar com elas é também aquela que mais facilita o desenvolvimento de indivíduos e da organização: a boa comunicação. Segundo uma pesquisa realizada pela Universidade de Harvard, cerca de 70% dos problemas nas organizações tem relação direta ou indireta com problemas de comunicação. Sendo assim, ensinar as pessoas e equipes a se comunicar melhor é uma excelente maneira de gerenciar os problemas. No entanto, não basta colocar um de frente pro outro e pedir que falem, é preciso desenvolver nas pessoas a capacidade de entender umas às outras e respeitar opiniões divergentes. Nesse contexto, a Programação Neurolinguística - PNL é a ferramenta mais eficiente de ser aplicada aos treinamentos. A PNL é, entre outros, um conjunto de ferramentas e técnicas de comunicação que trabalha individualmente nos colaboradores sua sensibilidade, capacidade de percepção, intuição, flexibilidade, empatia e inteligência emocional. Ela permite que possamos aprender e modificar modelos de comunicação interpessoal e intrapessoal, em pouco tempo e de maneira muito eficaz. Promove, também, o autoconhecimento, alicerce do auto desenvolvimento. Outra qualidade trabalhada pela ferramenta é a capacidade de gerenciar pessoas, uma vez que a liderança qualificada exige a capacidade de entender e respeitar os indivíduos. A qualidade do sucesso na liderança depende da qualidade das habilidades pessoais de se comunicar e do bom relacionamento entre interlocutores. É importante expressar nossos posicionamentos e objetivos com clareza, gerando uma atmosfera de confiança, com habilidade para influenciar nosso interlocutor. Saber reconhecer sinais verbais e não-verbais, distinguir qualidades de voz e entonação, conhecer estratégias e modelos de negociação, utilizar a criatividade para a solução de problemas, são alguns dos caminhos que a PNL apresenta. Dentro do universo das empresas, trabalhar as pessoas é a melhor forma de desenvolver equipes. Afinal de contas, as equipes são um conjunto de indivíduos que trabalham juntos para um resultado ideal. Os gestores que decidem investir em PNL podem esperar uma mudança imediata no clima organizacional, diminuição de conflitos, melhor entrosamento, maior qualidade de vida, entre tantos outros benefícios. Existe um senso comum entre os gestores de RH que diz que as pessoas são admitidas pelas qualidades técnicas e demitidas pelas incompetências comportamentais. O técnico você aprende em muitos lugares. A prática, você conquista com o tempo. Já a habilidade comportamental, também reconhecida como atitude, a PNL é capaz desenvolver e transformar tanto no âmbito pessoal quanto no profissional. O grande diferencial de se trabalhar a comunicação entre as equipes com a PNL é que os treinamentos feitos são capazes de promover mudanças profundas, não precisando aplicar novos treinamentos à equipe. Isso significa muito mais eficácia e assertividade, já que as transformações são duradouras, perenes. Bom para o profissional, para o líder e principalmente para a empresa. Gilberto Cury é presidente da SBPNL - Sociedade Brasileira de Programação Neurolinguística.",pt,152
33,2505,1475597263,CONTENT SHARED,-5920475612630001479,2754566407772265068,8294541181816066509,"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.116 Safari/537.36",MG,BR,HTML,https://hackernoon.com/how-it-feels-to-learn-javascript-in-2016-d3a717dd577f?gi=9a4d3c4ae4de,how it feels to learn javascript in 2016,"Edit: Thanks for pointing typos and mistakes, I'll update the article as noted. Discussion in The following is inspired by the article ""It's the future"" from Circle CI. You can read the original . This piece is just an opinion, and like any JavaScript framework, it shouldn't be taken too seriously. No JavaScript frameworks were created during the writing of this article. -The actual term is Front End engineer, but yeah, I'm the right guy. I do web in 2016. Visualisations, music players, flying drones that play football, you name it. I just came back from JsConf and ReactConf, so I know the latest technologies to create web apps. HackerNews -Oh my god no, no one uses jQuery anymore. You should try learning React, it's 2016. and -It's a super cool library made by some guys at Facebook, it really brings control and performance to your application, by allowing you to handle any view changes very easily. . -Yeah, but first you need to add React and React DOM as a library in your webpage. Hey, I got this new web project, but to be honest I haven't coded much web in a few years and I've heard the landscape changed a bit. You are the most up-to date web dev around here right? -So one is the actual library and the second one is for manipulating the DOM, which now you can describe in JSX. Cool. I need to create a page that displays the latest activity from the users, so I just need to get the data from the REST endpoint and display it in some sort of filterable table, and update it if anything changes in the server. I was thinking maybe using jQuery to fetch and display the data? -JSX is just a JavaScript syntax extension that looks pretty much like XML. It's kind of another way to describe the DOM, think of it as a better HTML. Oh, OK. What's React? -It's 2016. No one codes HTML directly anymore. That sounds neat. Can I use React to display data from the server? -Not quite. You need to add Babel, and then you are able to use React. Wait, why two libraries? -Oh, Babel is a transpiler that allows you to target specific versions of JavaScript, while you code in any version of JavaScript. You don't HAVE to include Babel to use ReactJS, but unless you do, you are stuck with using ES5, and let's be real, it's 2016, you should be coding in ES2016 like the rest of the cool kids do. JSX? What is JSX? -ES5 stands for ECMAScript 5. It's the edition that has most people target since it has been implemented by most browsers nowadays. What's wrong with HTML? -Yes, you know, the scripting standard JavaScript was based on in 1999 after its initial release in 1995, back then when JavaScript was named Livescript and only ran in the Netscape Navigator. That was very messy back then, but thankfully now things are very clear and we have, like, 7 editions of this implementation. Right. Anyway, if I add these two libraries then I can use React? -The fifth and seventh edition respectively. Another library? What's Babel? -You mean ES6? Yeah, I mean, each edition is a superset of the previous one, so if you are using ES2016, you are using all the features of the previous versions. ES5? ES2016? I'm getting lost over here. What's ES5 and ES2016? -Well, you COULD use ES6, but to use cool features like async and await, you need to use ES2016. Otherwise you are stuck with ES6 generators with coroutines to block asynchronous calls for proper control flow. ECMAScript? -It's 2016 man, no one uses jQuery anymore, it ends up in a bunch of spaghetti code. Everyone knows that. 7 editions. For real. And ES5 and ES2016 are? -Well, you include those three libraries but bundle them up with a module manager to load only one file. Wait, what happened with the sixth? -The definition depends on the environment, but in the web we usually mean anything that supports AMD or CommonJS modules. Right. And why use ES2016 over ES6 then? -Definitions. There are ways to describe how multiple JavaScript libraries and classes should interact. You know, exports and requires? You can write multiple JavaScript files defining the AMD or CommonJS API and you can use something like Browserify to bundle them up. I have no idea what you just said, and all these names are confusing. Look, I'm just loading a bunch of data from a server, I used to be able to just include jQuery from a CDN and just get the data with AJAX calls, why can't I just do that? -It's a tool that allows you to bundle CommonJS described dependencies to files that can be run in the browser. It was created because most people publish those dependencies in the npm registry. Right. So my alternative is to load three libraries to fetch data and display a HTML table. -It's a very big public repository where smart people put code and dependencies as modules. I see. And what's a module manager? -Not really. It's more like a centralised database where anyone can publish and download libraries, so you can use them locally for development and then upload them to a CDN if you want to. Riiight. And AMD and CommonJS are...? -Yes, but it's 2016 now, no one uses Bower anymore. OK, that makes sense... I think. What is Browserify? -Yes. So for instance, if you want to use React , you download the React module and import it in your code. You can do that for almost every popular JavaScript library. npm registry? -Angular is so 2015. But yes. Angular would be there, alongside VueJS or RxJS and other cool 2016 libraries. Want to learn about those? Like a CDN? Oh, like Bower! -It is, that's why you use a task manager like Grunt or Gulp or Broccoli to automate running Browserify. Heck, you can even use Mimosa. Oh, I see... so I need to download the libraries from npm then? -Task managers. But they are not cool anymore. We used them in like, 2015, then we used Makefiles, but now we wrap everything with Webpack. Oh, like Angular! -Yeah, but apparently in the web we love making things complicated and then going back to the basics. We do that every year or so, just wait for it, we are going to do assembly in the web in a year or two. Let's stick with React, I'm already learning too many things now. So, if I need to use React I fetch it from this npm and then use this Browserify thing? -It's another module manager for the browser while being kind of a task runner as well. It's like a better version of Browserify. That seems overly complicated to just grab a bunch of dependencies and tie them together. -Well, maybe not better, it's just more opinionated on how your dependencies should be tied. Webpack allows you to use target different module managers, and not only CommonJS ones, so for instance native ES6 supported modules. Grunt? Gulp? Broccoli? Mimosa? The heck are we talking about now? -Everyone is, but you shouldn't care anymore with SystemJS. Makefiles? I thought that was mostly used on C or C++ projects. -Well, unlike Browserify and Webpack 1.x, System is dynamic module loader that allows you to tie multiple modules in multiple files instead of bundling them in one big file. Sigh. You mentioned something called Webpack? -Yes, but because HTTP/2 is coming now multiple HTTP requests are actually better. Oh, Ok. Why is it better? -Not really. I mean, you could add them as external scripts from a CDN, but you would still need to include Babel then. I'm extremely confused by this whole CommonJS/ES6 thing. Jesus christ, another noun-js. Ok, and what is this SystemJS? Wait, but I thought we wanted to build our libraries in one big file and load that! -I would transpile it from Typescript using a Webpack + SystemJS + Babel combo. Wait, so can't we just add the three original libraries for React?? -Typescript IS JavaScript, or better put, a superset of JavaScript, more specifically JavaScript on version ES6. You know, that sixth version we talked about before? Sigh. And that is bad right? -Oh, because it allows us to use JavaScript as a typed language, and reduce run-time errors. It's 2016, you should be adding some types to your JavaScript code. -Yes, you would be including the entire babel-core, and it wouldn't be efficient for production. On production you need to perform a series of pre-tasks to get your project ready that make the ritual to summon Satan look like a boiled eggs recipe. You need to minify assets, uglify them, inline css above the fold, defer scripts, as well as- -Flow as well, although it only checks for typing while Typescript is a superset of JavaScript which needs to be compiled. I got it, I got it. So if you wouldn't include the libraries directly in a CDN, how would you do it? -It's a static type checker made by some guys at Facebook. They coded it in OCaml, because functional programming is awesome. Typescript? I thought we were coding in JavaScript! -It's what the cool kids use nowadays man, you know, 2016? Functional programming? High order functions? Currying? Pure functions? I thought ES2016 was already a superset of ES6! WHY we need now this thing called Typescript? -No one does at the beginning. Look, you just need to know that functional programming is better than OOP and that's what we should be using in 2016. And Typescript obviously does that. -So was Java before being bought by Oracle. I mean, OOP was good back in the days, and it still has its uses today, but now everyone is realising modifying states is equivalent to kicking babies, so now everyone is moving to immutable objects and functional programming. Haskell guys had been calling it for years, -and don't get me started with the Elm guys- but luckily in the web now we have libraries like Ramda that allow us to use functional programming in plain JavaScript. Sigh... and Flow is? -No. Ramda. Like Lambda. You know, that David Chambers' library? OCaml? Functional programming? -David Chambers. Cool guy. Plays a mean Coup game. One of the contributors for Ramda. You should also check Erik Meijer if you are serious about learning functional programming. I have no idea what you just said. -Functional programming guy as well. Awesome guy. He has a bunch of presentations where he trashes Agile while using this weird coloured shirt. You should also check some of the stuff from Tj, Jash Kenas, Sindre Sorhus, Paul Irish, Addy Osmani- Wait, I learned OOP in college, I thought that was good? -Well, you actually don't fetch the data with React, you just display the data with React. Are you just dropping names for the sake of it? What the hell is Ramnda? -You use Fetch to fetch the data from the server. David who? -I know right? Fetch it's the name of the native implementation for performing XMLHttpRequests against a server. And Erik Meijer is...? -AJAX is just the use of XMLHttpRequests. But sure. Fetch allows you to do AJAX based in promises, which then you can resolve to avoid the callback hell. Ok. I'm going to stop you there. All that is good and fine, but I think all that is just so complicated and unnecessary for just fetching data and displaying it. I'm pretty sure I don't need to know these people or learn all those things to create a table with dynamic data. Let's get back to React. How can I fetch the data from the server with React? -Yeah. Every time you perform an asynchronous request against the server, you need to wait for its response, which then makes you to add a function within a function, which is called the callback pyramid from hell. Oh, damn me. So what do you use to fetch the data? -Indeed. By manipulating your callbacks through promises, you can write easier to understand code, mock and test them, as well as perform simultaneous requests at once and wait until all of them are loaded. I'm sorry? You use Fetch to fetch the data? Whoever is naming those things needs a thesaurus. -Yes, but only if your user uses an evergreen browser, otherwise you need to include a Fetch polyfill or use Request, Bluebird or Axios. Oh, so AJAX. -It's JavaScript. There has to be thousands of libraries that all do the same thing. We know libraries, in fact, we have the best libraries. Our libraries are huuuge, and sometimes we include pictures of Guy Fieri in them. Callback hell? -They are libraries to perform XMLHttpRequests that return promises. Oh, Ok. And this promise thing solves it? -We don't use the ""J"" word in 2016 anymore. Just use Fetch, and polyfill it when it's not in a browser or use Bluebird, Request or Axios instead. Then manage the promise with await within an async function and boom, you have proper control flow. And that can be done with Fetch? -Await allows you to block an asynchronous call, allowing you to have better control on when the data is being fetch and overall increasing code readability. It's awesome, you just need to make sure you add the stage-3 preset in Babel, or use syntax-async-functions and transform-async-to-generator plugin. How many libraries do I need to know for god's sake? How many are of them? -No, insane is the fact you need to precompile Typescript code and then transpile it with Babel to use await. Did you just say Guy Fieri? Let's get this over with. What these Bluebird, Request, Axios libraries do? -It does in the next version, but as of version 1.7 it only targets ES6, so if you want to use await in the browser, first you need to compile your Typescript code targeting ES6 and then Babel that shit up to target ES5. Didn't jQuery's AJAX method started to return promises as well? -Look, it's easy. Code everything in Typescript. All modules that use Fetch compile them to target ES6, transpile them with Babel on a stage-3 preset, and load them with SystemJS. If you don't have Fetch, polyfill it, or use Bluebird, Request or Axios, and handle all your promises with await. It's the third time you mention await but I have no idea what it is. -Is your application going to handle any state changes? This is insane. -Oh, thank god. Otherwise I would had to explain you Flux, and implementations like Flummox, Alt, Fluxible. Although to be honest you should be using Redux. Wat? It's not included in Typescript? -Oh, if you are just displaying the data you didn't need React to begin with. You would had been fine with a templating engine. At this point I don't know what to say. -I was just explaining what you could use. We have very different definitions of easy. So, with that ritual I finally fetched the data and now I can display it with React right? -I mean, even if it's just using templating engine, I would still use a Typescript + SystemJS + Babel combo if I were you. Err, I don't think so. I just need to display the data. -There's a lot, which one you are familiar with? I'm going to just fly over those names. Again, I just need to display data. -jTemplates? jQote? PURE? Are you kidding me? Do you think this is funny? Is that how you treat your loved ones? -Transparency? JSRender? MarkupJS? KnockoutJS? That one had two-way binding. Stop. Just stop. -PlatesJS? jQuery-tmpl? Handlebars? Some people still use it. I need to display data on a page, not perform Sub Zero's original MK fatality. Just tell me what templating engine to use and I'll take it from there. -Mustache, underscore? I think now even lodash has one to be honest, but those are kind of 2014. Ugh, can't remember the name. It was a long time ago. -Jade? DustJS? Err, doesn't ring a bell. Another one? Another one? -Nunjucks? ECT? Maybe. Are there similar to that last one? -Mah, no one likes Coffeescript syntax anyway. Jade? Err.. maybe it was newer. -I meant Pug. I meant Jade. I mean, Jade is now Pug. No. -Probably just ES6 native template strings. No. No. No, you already said Jade. Sigh. No. Can't remember. Which one would you use? Let me guess. And that requires ES6. Which, depending on what browser I'm using needs Babel. Which, if I want to include without adding the entire core library, I need to load it as a module from npm. Which, requires Browserify, or Wepback, or most likely that other thing called SystemJS. -Just don't forget to polyfill Fetch if it's not supported, Safari still can't handle it. Which, unless it's Webpack, ideally should be managed by a task runner. -That's fine, in a few years we all are going to be coding in Elm or WebAssembly. But, since I should be using functional programming and typed languages I first need to pre-compile Typescript or add this Flow thingy. -I hear you. You should try the Python community then. And then send that to Babel if I want to use await. -Ever heard of Python 3? So I can then use Fetch, promises, and control flow and all that magic. You know what. I think we are done here. Actually, I think I'm done. I'm done with the web, I'm done with JavaScript altogether. I'm just going to move back to the backend. I just can't handle these many changes and versions and editions and compilers and transpilers. The JavaScript community is insane if it thinks anyone can keep up with this. Why?",en,150
34,1273,1465223368,CONTENT SHARED,8890720798209849691,1895326251577378793,-1847389231177111235,,,,HTML,https://www.nngroup.com/articles/top-intranet-trends/?utm_source=Alertbox&utm_campaign=2449df7535-Diary_Studies_Top_Intranet_Trends_06_06_2016&utm_medium=email&utm_term=0_7f29a2b335-2449df7535-40280153,top 10 intranet trends of 2016,"Summary: Hero images, carousels, fat footers, video, minimalist design, and responsive navigation, are among some of the top feature trends of the best intranets of 2016. We even see a revival of online help that's actually helpful to employees exploring new features or attempting complex tasks. Each winning intranet has its own style, feature set, and personality, unique and special in its own way. But, great minds do think alike, and some themes and features are common in multiple or all of the winning intranets in our Intranet Design Annual 2016 . The 10 best-designed intranets for 2016 may be leading-edge cases, but the trends in their design should spread to more mainstream intranets in the next few years. Although every intranet feature won't work well at every organization, feature trends from outstanding intranets can inspire your intranet redesign. Stay ahead of the curve and consider taking on some of the following intranet design trends now: Help and tutorials Simple, minimalist-looking design Better photos Search evolution Carousels and heroes Fat footers Left-side navigation Social features targeted at particular topics or groups Video Business communication Help and Tutorials After many years of being chastised for being unhelpful, online help went out of fashion. This was reinforced by the idea that an interface should stand on its own, and not need help to be usable. This year, however, we saw a resurgence of Help that is helpful. While the winning-intranets' mostly employ nice-and-easy user interfaces, occasional more-involved or new interface elements do benefit from well-designed help. Help features can aid in discoverability of functionality, and expedite employees' learning and understanding of the intranet and its capabilities. At Intermountain Healthcare, employees who need a little assistance can refer to the Help section in the right rail. Similarly, The Co-operators provides thorough, wide-ranging guidance about how to make the most of its intranet. Simple, Minimalist-Looking Design The "" flat and boxy"" designs prevalent in years past are much less pronounced this year. Most of the designs, however, continue to boast a simple, sometimes minimalist , aesthetic. Some designs, such as those from Enbridge; Repsol; and Cadwalader, Wickersham & Taft, use generous white space. Intermountain Healthcare, NAV CANADA, Swedish Parliament, and (to a lesser degree) American Cancer Society all use rectangles for a boxy, easy-to-scan design. Better Photos Whether it indicates a strong commitment to photography , more people sharing photos, or simply today's better phone cameras, the photographs on the winning intranets are quite engaging. Photos typically relate strongly to the material they accompany and often show employees doing their work. For example, the Enbridge site shows an employee evaluating a particular job site. DORMA shows two employees joking around. Search Evolution Intranet search is a lion that the best intranets tame . The evolution of search on this year's winning sites is impressive. To enhance its intranet search, Cadwalader, Wickersham & Taft consolidates data sources to produce a single point of entry to knowledge resources. Salini Impregilo's search, which appears on every page, allows employees to search the entire intranet for news, people , projects, and documents. Carousels and Heroes Although the carousel is still a prominent feature on winning intranets, the hero is making a comeback. Some organizations, such as NAV CANADA and Repsol, still opt for multiple images and statements in one area, while others, such as Salini Impregilo and Intermountain Healthcare prefer one hero image to make the desired statement. Fat Footers Large footers at the bottom of pages became popular a few years ago and remain common today. Employees often know that the information they're seeking is on the intranet, but they can't always find it. When using a public-facing website, people may have the option to leave the site, and will. But employees often know that the intranet is the main source or the only place to turn to to find particular content. Providing organized links at the bottom of pages provides employees with one more chance to locate what they need. The content in these footers can be arranged in many different ways: repeat the global navigation suggest related content present popular links On the NAV CANADA intranet, the wide footer navigation repeats the global navigation topics and offers a list of the megamenu links. The Enbridge footer includes links to the public sites of Enbridge companies and information for contacting the Enterprise Service Desk. It also lists Ethics and Conduct information, along with a reminder: "" Let's work together to maintain a respectful workplace ."" All of these visually use an obvious aesthetic element to indicate the footer; this element can be: colored background that is different from that of the rest of the page border (line) delineating the footer from the page content combination of the above Side Global Navigation Because mega menus don't work in a phone UI, designers often use a mega menu for desktop navigation , and an accordion or some other option for mobile navigation . Some teams opt for menu UIs that translate easily from desktop to mobile . One such pattern popular this year is the vertical navigation bar down the left side of the page. The Swedish Parliament and Cadwalader, Wickersham & Taft are among the winning organizations that follow this navigation pattern. Targeted Social A great trend from past years continues this year: presenting social tools in an understandable, targeted way. Gone are the days of displaying a wall feed on the homepage or in personal profiles with no additional description or context. Great intranets use social features to encourage further communication about important or trending topics. DORMA's CEO participates in the social features, leading employees by example and playing a big role in the social features' success. Two major company events occurred near the features' launch; these events provided a source of content, increased interest, and natural momentum for success. The Cadwalader, Wickersham & Taft intranet lists recent hires and employee anniversaries on the homepage. Such seemingly simple features can go a long way toward building a sense of community and inclusiveness in an organization. The Co-operators offers a variety of social features, including the weekly Five Minutes With ... (an employee-profiles feature), polls, achievements, Popular Links , and the ability for users to submit news. Also, in the site's executive blogs, senior leaders share their knowledge and opinions and ask employees to do the same. The Trending Now section on the American Cancer Society intranet summarizes the site resources, search terms, and pages that receive the most traffic. This section is a simple and automated way to keep users informed of the site's most popular items. Enbridge employees can easily access the company's public information on social sites - including Twitter, Facebook, LinkedIn, and YouTube - via links in the middle of the homepage. This section also includes a link to the @enbridge blog. Providing quick access to external sites raises employee awareness about information that Enbridge is sharing with the public and about the conversations occurring on social-media sites. Video Tools to create, edit, and post videos have made them accessible and easy to deal with. Prevalence in social channels has lowered the expectation for high-quality video production . In fact, many people welcome the simplicity and folksiness of more realistic, just-shot-myself type of videos. With these changes, individuals, teams, and even high-level managers are taking advantage of video and sharing information in this way. Videos are often stored in their own section of the intranet, which allows employees to sort, filter, and search by topic. But videos, like written content, are also presented on the homepage, in news sections, and cross linked from related contend. Business Communication Business people have learned that the intranet is the perfect place to communicate their goals and statuses to all employees. This information helps employees realize how the organization is doing and motivates them to work toward and achieve organizational goals. It also adds a level of respect, signaling to all employees that they are important enough to know where the organization has set it sights, and that that each person can play positive a role in those plans. Conclusion Whether your organization is large or small; formal or informal; public, private or government; consider which of the trends here can be implemented, deployed, and used successfully on your intranet. Choose a few to do over the next year to enhance the user experience, and increase the business value of your intranet. Full Report For more information about themes, intranet best practices, and full-color screenshots of the 10 winners, download the 2016 Intranet Design Annual . The report download comes with a folder containing each image as a .png to make it possible to zoom in and study the designs in detail.",en,150
35,1887,1469561171,CONTENT SHARED,4105873627900556540,-2525380383541287600,8115788345630016900,,,,HTML,http://agiletesters.com.br/topic/103/a-arte-de-desenvolver-testes-cucumber-capybara,sua comunidade de teste.,"Esse é o segundo artigo sobre desenvolvimento de testes e dessa vez resolvi falar um pouco de cucumber + capybara e vem sendo o framework para meus testes. Hoje existem diversos artigos que falam sobre cucumber e minha intenção é mostrar como funciona os steps de forma mais fácil. Bem, vamos primeiro partir para instalação e configuração do ambiente para começarmos a desenvolver os testes. Como eu venho utilizando MAC para desenvolvimento, vou focar a maior parte do tempo nele, porém fiquem a vontade para perguntar algo caso utilizem Linux ou Windows (o conceito será o mesmo). Antes de mais nada, baixar o Xcode pelo link: . Aprendi que sempre que instalar o SO, a primeira coisa será instalar o Xcode. Bem, depois do Xcode, vamos baixar o Homebrew via terminal com o comando: ruby -e ""$(curl -fsSL )"" . Para explicar o que é o homebrew basta dizer: ""O Homebrew instala as coisas que você precisa que a Apple não forneceu para você."" - fonte: . Ou seja, é um gerenciador de pacotes. Reinicie a máquina (isso mesmo, para ""completar as instalações""). Com o Homebrew instalado, será necessário instalar o ""Qt"" - Framework multiplataforma para desenvolvimento de interfaces gráficas - fonte: ( ) e serve basicamente para podermos utilizar o selenium como driver padrão de execução para os testes. Para instalar o Qt, é bem simples: ""brew install qt"". Por default o ruby já vem instalado, mas vou deixar registrado os comandos para instalar a versao 1.9.3-p545: curl -sSL | bash -s stable --ruby=1.9.3-p545 Ps: Caso o ""curl"" não estiver instalado, basta instalar com ""brew install curl"" Bem, agora vamos ao que de fato insteressa, as gems, então vamos a lista das básicas: Com isso tudo já da para trabalhar um pouquinho \o/. Bem, a estrutura básica para um projeto em cucumber é essa: Para explicar, dentro da pasta ""specifications"" se encontra aquela famosa estrutura que vemos em todos os artigos por ai: language: pt (colocar cerquilha antes) Funcionalidade: Aprendendo a trabalhar com capybara Cenario: Exemplo basico de cadastro Dado que eu acesse o facebook Quando eu preencher os campos de cadastro E clicar em Abrir uma conta Então primeiro cadastro completo Até ai beleza e depois??? Depois salve o arquivo como ""cadastro.feature"". Depois que escrevemos, vamos a parte das ""configurações"" no arquivo ""env.rb"", pois é lá que definimos as gems que eu vou utilizar e claro, definir o driver que eu vou utilizar como base para execução dos testes. O modelo básico vai ficar da seguinte forma: Depois de configurar (por hora são apenas essas), vamos deixar o teste pronto para ""desenvolver"", então, basta ir no terminal, navegar até a pasta features de seu projeto e executar o comando ""cucumber"". É só isso??? Não!! O que fizemos apenas foi pedir para o cucumber ""ajustar"" os steps para podermos dar vida ao desenvolvimento, e vai ficar algo mais ou menos assim: language: pt (colocar cerquilha antes) Funcionalidade: Aprendendo a trabalhar com capybara Agora a brincadeira vai começar a ficar engraçada, como vocês viram, não executou por estar apenas em português estruturado como algumas pessoas já me perguntaram, pois eu preciso agora dar o caminho das pedras para que cada step possa fazer sentido, pois é a partir daí que vamos começar a desenvolver. A primeira dica que eu dou é seguir passo a passo, ou seja, sempre que você executar e terminar um passo, começe outro. Alguns lugares falam pra copiar tudo e colocar já em um arquivo.rb, mas faça isso não, vai um por um que você tem melhores resultados . Então, vamos pegar o primeiro passo e colocar em um arquivo novo: Considere cada passo como um método a ser executado, e em ruby todo método finaliza com um end, com cucumber, os inícios Dado, Quando, Então, E são os inícios desses métodos. A base de execução do cucumber é em cima de Expressões Regulares (regex), o que quer dizer que vamos nos deparar sempre com os caracteres (/^ $/), etc. Mas não vou entrar no contexto das expressões regulares agora, mas explicando a regex (/^que eu acesse o facebook$/), sempre que em alguma feature eu escrever ""Dado que eu acesse o facebook"", ele será executado, sem que eu precise escrever o código todo de novo, ou seja, economizo muito tempo =). Bom, mãos na massa, e para isso, a frase ""pending # express the regexp above with the code you wish you had"" será substituída por comandos da gem capybara ( ). Como eu falei no primeiro post A arte de desenvolver testes, o coração da automação está em fazer as tarefas que são repetitivas se tornarem automáticas e para isso eu tenho que encontrar e trabalhar em cima dos elementos da minha página e para isso eu tenho ferramentas que me facilitam, a que eu mais gosto é o Firebug (complemento do firefox) e posso encontrar os elementros apenas selecionando, e posso escolher id, css ou xpath para poder indicá-los. Partindo deste princípio, o desenvolvimento fica muito mais fácil, pois geralmente o que eu faço nos elementos é clicar, informar um valor e validar. No capybara basicamente se utilizam os comandos: visit ' ' - Para visitar alguma url. page.find(:id, ""id do elemento"").click - Clica em um elemento definido por ID. page.find(:css, ""css do elemento"").click - Clica em um elemento definido por CSS. page.find(:xpath, ""xpath do elemento"").click - Clica em um elemento definido por XPATH. page.all(:id, ""id do elemento"")[0].click - Clica no primeiro elemento dentro de uma lista definido por ID. page.all(:css, ""css do elemento"")[0].click - Clica no primeiro elemento dentro de uma lista definido por CSS. page.all(:xpath, ""xpath do elemento"")[0].click - Clica no primeiro elemento dentro de uma lista definido por XPATH. PS: Quando nos depararmos com um checkbox, radiobutton, utilizar da seguinte forma: page.find(:radio_button, 'nome do radiobutton').set(true) - Nesse caso, ele vai selecionar aquele radiobutton. page.find(:checkbox, 'nome do checkbox').set(true) - Nesse caso, ele vai selecionar aquele checkbox. fill_in 'nome do elemento para inserir valor', :with => ""Aprendendo Capybara"" - Irá inserir no elemento a string Aprendendo Capybara. select 'Nome do item no Drop Down', from: 'nome do elemento drop down' - Seleciona um item de um drop down. ex: select 'Apto', from 'tipo_moradia' click_button 'Cadastrar' - Clic no botão cadastrar. click_link 'Home' - Clica no link Home caso haja algum na página. expect(page).to have_content 'Cadastro efetuado com sucesso' - Procura a mensagem e caso tenha, será sucesso. Basicamente é isso =). Vou colocar todos os steps abaixo para mostrar como ficaria =) para não ficar maior que já está essa mini aula rs: Salve o arquivo com o nome cenario1.rb. No terminal, navegue até a pasta do projeto e digite cucumber. O resultado será esse: Note que o mais legal de trabalhar com cucumber é a facilidade que ele tem de chamar um cenário ou outro através de frases, frases essas que são nada mais nada menos que os próprios critérios de aceite de uma estória, ou seja, desenvolver o critério de aceite se torna rápido e fácil, e para o review, mostrar tudo verdinho é segurança de deploy =). Desculpem-me por alongar esse assunto, mas acho necessário passar uma visão mais simplista de como desenvolver os testes, não fizemos mágica alguma para realizar a primeira etapa do cadastro do facebook em 14 segundos. Para quem está começando, esse é o caminho das pedras!!! Há formas de rodar em background (sem abrir navegador), o que vai mais rápido ainda o teste, mas isso vai ficar para a próxima. Dúvidas, sugestões, críticas são muito bem vindas, meu email é thiagomarquessp@gmail.com caso alguém queira saber de cara como rodar em background. Desenvolver testes é muito mais legal do que parece!!! =) Algumas fontes legais: Capybara doc: Regex doc (as que eu mais gosto): e",pt,148
36,2994,1485194633,CONTENT SHARED,-6728844082024523434,801895594717772308,7194441186926042361,"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.95 Safari/537.36",MG,BR,HTML,http://merowing.info/2017/01/seniority/,seniority,"People use different words to classify Engineer skill, some companies give you more senior role just based on a number of years you have been working there, but what does it mean to be Senior? In this article, I composed a complete and final list of API's that you need to know to classify yourself as Senior Engineer. Here's a list of all the APIs you must know: * API's don't matter If you thought that what makes people Senior is memorizing API's, then this article is most definitely for you. Sure, experience matters and helps you be a better developer, but knowing any particular API is not of great importance, anyone can learn a new API given enough time. How each company gives title is very arbitrary, sometimes it's based on years of experience, other times, unfortunately, its a matter of how friendly you are with management. What matter is not the title you hold, but what you represent. Technical skills will usually come to you naturally with years. I want to focus on something many people miss: soft skills. Let's split this into Personal Principles and Working with Others. Personal Principles Reliable You need to be reliable, never promise you do something and then drop the ball. Others people work often depends on being able to gauge when your piece of the puzzles are ready, for that reason it is better to be on the safe side with estimates and over-deliver than to promise too much and fail to deliver. There are always tasks you can pick up if you finish your sprint sooner, seniors are proactive and do not sit on their hands until they are assigned new tasks. Accountable The only person that does not make mistakes is the person that does nothing. Making mistakes, or hitting roadblocks is normal. More experienced people know that when that happens they let other people know, and they are not afraid to ask for help. Don't be afraid to admit your mistakes, it is only human, and it will build more trust when working with others. Flexible Don't get attached to particular framework or language. Only one thing is sure: you will change them many times in your career . Some languages that I used early in my career are obsolete now. I would go further and suggest that you should embrace change , force yourself to challenge your assumptions, it makes you less ignorant of other opinions/ways. There is more than one way to skin a cat. I usually have some side-project going, and I have a rule that every six months I try using a different approach to things. It allows me to distinguish the best solutions from those I am used to. Endowment bias states that we value things we own/know more than they are worth. One thing to remember is that change is hard and requires time. Trying something for a couple of days just won't cut it. Pragmatic Many programmers would like to just focus on the engineering side of things, always do the cleanest code they can and ignore the business side of things. That is not how our jobs work. People hire you because they need to solve some specific problems for their core business, be it a new app or anything else, not to write code for the sake of code. Delivering things matters . You need to invest time into understanding the business requirements, only knowing them you can make well-informed decisions and focus your work on things that matter. Working with others A single person rarely creates software products on their own. You have to work with other people. Invest time into learning how to be a team player. This will make a huge difference in how your career progresses. Lead by example People will look up to you and how you behave will heavily influence team culture. If you are: Proactively grabbing new tasks from the backlog when you are free Looking for ways to improve the architecture and tooling Teaching and volunteering to pair program or help with issues Giving everyone space to state their opinion and discuss with arguments Treat everyone as an equal conversation partner Other people will adopt those behaviors, and your team will prosper. Listen and learn from others In engineering, there are many different ways to achieve the same end results. It's way too easy to get set in our ways, we have to exercise our flexibility. Be open to other opinions, do not dismiss other people ideas because they are different to your default thought. Spend some time discussing others' opinions to better understand what they mean and how their ideas would work, all the pros and cons, etc. Only then you can evaluate whose approach makes more sense. It does not matter whether the idea comes from people that are more experienced than you or less. I have learned plenty useful things from Junior developers in my teams, they often have fresh ideas and challenge our assumptions. Which bring me to another important point: Do not leverage your 'power' or 'seniority' . Never treat people as inferior and never use arguments like 'Let's do X because I've higher position than you / I've been programming for N years.' When an Engineer uses that kind of argumentation, it makes him look like a child from kindergarten, not a professional. Don't be that person. Use factual arguments. If you cannot find reasons behind your way of doing things, maybe it is not that good? Avoid being reactive, discussions can get heated, but a professional understands that a critique of an idea is not the same as a critique of a person. Pause and think before replying, and don't interrupt other people. Let them finish and focus on understanding their message before discussing it. Learning how to manage your impulses and emotions is crucial in living with others. Learning time management will allow you to maintain sanity. Take a look at some recommended reading. Become a mentor to other devs Mentoring other developers is incredibly rewarding, and it also helps foster the right team culture. I want everyone in the team to feel comfortable asking me any questions. Be it to learn how to do something or questioning my decisions. I have seen few teams that people were afraid to ask questions because managers were awful and used that to evaluate people performance based on that. Encourage your teammates to ask for help, or advice how to do something. If you want to be the mythical 10x engineer, help other engineers in your team grow. Remember: There are no stupid questions, and if people are asking about your code it usually means it can be simplified or improved in some way. Actively looking for ways to improve the process No process is ideal. You should always be on the lookout for improvements. If you criticize something, always propose alternatives. You can have excuses or results, but not both. If you see pain points when working on your code, or when observing your teammates, think about how they how they could be resolved. You do not immediately jump on writing another library or tool if something happens twice. However, when a pattern keeps repeating itself many times for different people, it means it is time to look at ways of improving it. iOS community is one of the best communities I had the privilege to be a part of. I would encourage you to open source those tools and libraries and share your experienced with others. Writing about things often makes you realize how much you still have to learn. Subscribe to newsletters like iOS Dev Weekly or Swift News for ideas. Courage to fight for healthy team culture I cannot stress this point enough, I have seen many people complain about their team culture over the years, my question is always 'What are you doing about it?' You need to fight for a healthy team culture, if you see that someone is misbehaving do not be afraid to speak up . Propose things like team contract or code of conduct. The way I think about this is simple: If I see something I do not like, then I question it in the team environment. Otherwise, the team might become toxic, and if that happens, I quit. If you are going to quit otherwise, why are you afraid of fighting for your team? Many people might be afraid to speak up, especially minorities and introverts. Often they will reach out to you in private to say thanks. Things to look for: Everyone is equal, make sure everyone can express his or her opinion, even those people that are shy. When people are interrupted by someone, make sure to make it easy for them to speak their mind e.g. ""Hey X, you were saying?"" Conclusion Being a good developer and team member is about so much more than programming. With years you can gain much technical experience, but if you do not invest time and energy into improving your soft skills, you can create a bottleneck for your career, one that you might not even see. Read more about how I work with clients I'd like to thank Paul Yorke , Cezary Bielecki , Gabriel Peart and Adam Shott from The New York Times for helping me with initial draft of this article.",en,148
37,1293,1465324965,CONTENT SHARED,2555983212310147009,7774613525190730745,5630253306675486090,,,,HTML,https://www.infoq.com/articles/Database-Version-Control,the definitive guide to database version control,"In the brave new world of big data and business intelligence, the only technology constant is change, especially when it comes to the database. Almost daily changes in business needs due to demographics, increased service delivery demand and the regulatory environment drive database change. So when it comes to database change, agility through automation - the ability to do more with less more rapidly to accelerate delivery - is what differentiates highly competitive, world-class enterprises from the rest of the crowd. If your competitor can deliver relevant features faster, with better quality, you will lose market share. Agile development arose from the need to move more quickly to deal with ever-changing requirements, ensuring optimum quality despite resource constraints. The big release approach is obsolete - waiting six months until the next rollout or release is self-defeating. Agile development reduces release scope to complete each change faster, minimizing the impact of each change. Agility is expected from tech companies and IT divisions to support changing business needs. The next logical step is to link development with operations - to adopt DevOps. To master Agile sprint deployments effectively with DevOps, you need the ability to implement deployment and process automation internally within development and QA, or to production. Otherwise, deployments and releases will require manual steps and processes, which are prone to human error and cannot be frequently replicated. The automation required relies on a version control repository that manages all software assets ready to be built and deployed to the next environment. The build process starts by cleaning the working space and getting the relevant files from the version control repository. This critical phase prevents out-of-process changes. These changes can still happen though they can be avoided if developers save their changes directly in the build server working space, instead of checking-in the changes to the version control repository. This example may sound absurd because developers know if they do so, their changes will be lost, as the technology enforces the process. This phase also prevents the build phase from accepting work-in-progress changes by referring to only those changes that were submitted to the version control repository in a check-in process. The version control repository acts as the single source of truth . The Database is a Key Component Most IT applications today have many components using different technologies; mobile, ASP, PHP, application servers, Citrix, databases, etc. These must all be in sync for the application to work. If, for example, a new column was added to a table, or a new parameter was added to a stored procedure, all other application components must be synchronized to the structure change to function correctly. If this synchronization breaks, the application can fail by calling the wrong parameters to the stored procedure, or by trying to insert data without the new column. The unique properties of the database component differentiate it from other components: A database is more than just SQL scripts. It has a table structure, code written in the database language within stored procedures, content saved in reference tables or configuration tables, and dependencies between objects. A database is a central resource. Several developers can work on the same object, so their work must be synchronized to prevent code overrides. Deploying database changes is not as simple as copying and replacing old binaries. Database deployment transforms version A into version B while keeping business data and transferring it to the new structure. Database code exists in any database, and can be directly modified in any environment. This is in contrast to other components, where everything starts from a clean workspace in the build server. Must-Have Requirements Several challenges must be addressed when managing database changes. You must: Ensure all database code is covered (structure, code, reference content, grants) Ensure the version control repository acts as the single source of truth Ensure the deployment script being executed knows environment status when the script is executing Ensure the deployment script handles and merges conflicts Generate a deployment script for only relevant changes Ensure the deployment script knows the database's dependencies There are four common approaches to managing database changes in development and deploying them internally (Dev, QA) or to the production environment. Utilizing SQL alter scripts generated during development Utilizing a changelog activities tracking system Utilizing simple compare & sync Utilizing a database enforced change management solution Utilizing SQL Alter Scripts Generated During Development The most basic method for managing database changes is to save the alter command in a script or set of scripts, and manage them in the exiting file-based version control. This guarantees a single repository that stores all the application component assets. Developers have the same functionality when checking-in changes for the database as they do when they check-in changes for .NET or Java, such as linking the change to the reason (CR, defect#, user story, etc.). Almost any modern file-based version control solution has a merge notification when several developers change the same file. But let's see if this solution actually overcomes the challenges for the database, and avoids the potential pitfalls: Ensures all database code is covered - since the developer or DBA writes the script, they can ensure it will handle all database code. Ensures the version control repository acts as the single source of truth - not really, as the developer/DBA can login directly to the database (in any environment) and make changes directly in the database. Manually-Written SQL Scripts Changes made to the deployment scripts as part of scope changes, branch merges, or re-works are done manually and require additional testing. Two sets of scripts must be maintained - the create script and the alter script for the specific change for the release. Having two sets of scripts for the same change is a recipe for disaster. Ensures the deployment script being executed knows the environment status when the script is executing - this depends on the developer and how the script is written. If the script just contains the relevant alter command, then it is not aware of the environment status when it is executed. This means it may try to add the column although it already exists. Writing scripts that will be aware of the environment status at execution time significantly complicates script development. Ensures the deployment script handles conflicts and merges them - although the file-based version control provides the ability to merge conflicts, this is irrelevant to the database as the version control repository is not 100% accurate and cannot be the single source of truth. The script might override a hot fix performed by another team, leaving no evidence that something went wrong. Generates deployment scripts for only relevant changes - scripts are generated as part of development. Ensuring the scripts include only relevant and authorized changes - based on the tasks being approved - requires changing the script(s), which creates more risk to the deployment and wastes time. Ensures the deployment script knows the database dependencies - developers must be aware of database dependencies during the development of the script. If a single script is being used, then the change is usually being appended. This can result in many changes to the same objects. If many scripts are being used, then the order of the scripts is critical and is maintained manually. Bottom line: not only does this basic approach fail to solve the database challenges, it's also error-prone, time consuming, and requires an additional system to keep track of the scripts being executed. Utilizing a Changelog Activities Tracking System Another common approach is to use XML files, which use an abstract language for the change and keep track of the execution. The most common open source solution for this is Liquibase. With XML files, Liquibase separates the logical change from the physical change, and allows the developer to write the change without knowing the database-specific command. At execution time, it converts the XML to the specific RDBMS language to perform the change. Changes are grouped into a changelog, and they can be in a single XML file or many XML files referred by a major XML file which contains the order of the changes. The XML files can be saved using the existing file-based version control, which offers the same benefits as the basic approach. In addition, based on the Liquibase execution track, it knows which changelog(s) have already been deployed and shouldn't run again, and which were not yet deployed and should be deployed. Let's see if Liquibase answers the challenges: Ensures all database code is covered - managing changes to reference content is not supported in the XML files used by Liquibase, and must be handled as an external addition, which can result in changes being forgotten. Ensures the version control repository acts as the single source of truth - Liquibase doesn't have any version control functionality. It depends on third-party version control tools to manage XML files, so you have the same challenges in making sure the file-based version control repository reflects the version that was tested. The process that will ensure the version control repository can be the single source of truth requires developers to check-in changes in order to test them. This can result in work-in-progress changes being deployed to next environment. Ensures the deployment script being executed knows the environment status when the script is executing - Liquibase knows which changelogs have been deployed and will not execute them again. However, if the logical change is to add a date column that exists in varchar format, the deployment will fail. Also, overrides of out-of-process changes cannot be prevented. Ensures the deployment script handles conflicts and merges them - any change being made to the database outside of Liquibase can cause a conflict, which will not be handled by Liquibase. Out of process changes are not handled Generates deployment scripts for only relevant changes - changes can be skipped at the changelog level, but breaking a changelog into several changelogs requires writing a new XML file, thus requiring more tests. Ensures the deployment script knows the database dependencies - the order of the changes is maintained manually during the development of the changelog XML. Bottom line: using a system that tracks change execution does not address the challenges associated with database development and, as a result, does not meet the deployment requirements. Utilizing Simple Compare & Sync Another common approach is to generate the database change script automatically by comparing the source environment (development) to the target environment (test, UAT, Production, etc.). This saves the developers and DBAs time because they don't have to manually maintain the script if it is a create script or an alter script for the release. Scripts can be generated when needed, and refer to the current structure of the target environment. Let's review the challenges and see if this approach overcomes them: Ensures all database code is covered - most compare & sync tools know how to handle the different database objects, but only a few have the functionality to handle compare & sync of the reference data. Ensures the version control repository acts as the single source of truth - simple compare & sync does not utilize the version control repository when performing the compare and generating the merge script. Ensures the deployment script being executed knows the environment status when the script is executing - the best practice is to generate the script just before executing it so it will refer the current environment status. Ensures the deployment script handles conflicts and merges them - simple compare & sync tools compare A to B (source to target). Based on the simple table at the right, the tool then generates a script to ""upgrade"" the target to match the source. Without knowing the nature of the change, the wrong script can be generated. For example, there is an index in the target that was created from a different branch or critical fix. If this index does not exist in the source environment, what should the tool do? Drop the index? If there is an index in development, but not in production, was it added in development? Dropped in production? Using such a solution requires deep knowledge of each change to make sure they are handled properly. Generates deployment scripts for only relevant changes - the compare & sync tools compare the entire schema and show the differences. They are unaware of the reason behind the changes, as this information is stored in the ALM, CMS, or version control repository, which is external to the compare & sync tool. You might get a lot of background noise, making it difficult to determine what you actually need to deal with. Ensures the deployment script knows the database dependencies - compare & sync tools are aware of database dependencies and generate the relevant DDLs, DCLs, and DMLs in the correct order. Not all compare & sync tools support generating a script that contains objects from several schemas. Bottom line: compare & sync tools satisfy some of the must-have requirements, but fail to deal with others. Scripts must be manually reviewed, and cannot be trusted in an automated process. Utilizing a Database Enforced Change Management Solution Database enforced change management combines enforcement of version control processes on database objects with generation of the deployment script when required, based on the version control repository and the structure of the environment at that time. This approach uses ""build and deploy on-demand,"" meaning the deploy script is built (generated) when needed, not as part of development. This allows for efficient handling of conflicts, merges, and out-of-process changes. Build & Deploy On-Demand How does database enforced change management handle the challenges? Ensures all database code is covered - structure, business logic written in the database language, reference content, database permissions and more are managed. Ensures the version control repository acts as the single source of truth - the enforced change policy prevents anyone using any IDE (even command line) from modifying database objects that were not checked-out before and checked-in after the change. This guarantees the version control repository will always be in sync with the definition of the object at check-in time. Single Process Enforcing Version Control Ensures the deployment script being executed knows the environment status when the script is executing - building (generating) the deployment script when needed (just before executing) guarantees it knows the current environment status. Ensures the deployment script handles conflicts and merges them - by using baselines in the analysis, the nature of the change is known and the correct decision whether to promote the change, protect the target (ignore the change), or merge a conflict is easy. Baseline Aware Analysis Generates deployment scripts for only relevant changes - the integration with application lifecycle management (ALM) and change management systems (CMS) enables you to assign a reason to the change, as is done in the file-based version control or task management system. Ensures the deployment script knows the database dependencies - the sophisticated analysis and script generation algorithm ensures the DDLs, DCLs, and DMLs will be executed in the correct order based on the database dependencies, including inter-schema dependencies. In addition to the must-have requirements, there are other requirements, such as supporting parallel development, merging branches, integrating with the database IDE, and supporting changes originating from data modeling tools. You must verify that these requirements will be addressed by whichever method you choose. Bottom Line The database component has special requirements, and therefore creates a real challenge for automation processes. In the old days when there were only a few releases per year, it was common and understandable to invest time manually reviewing and maintaining the database deployment scripts. Today, with the growing need to be agile and provide releases faster, the database must be part of the automation process. Developing SQL or XML scripts, or using simple compare & sync are either inefficient and/or risky approaches when it comes to automation. The most effective method is to implement database enforce change management . About The Author Uri Margalit is the Director of Product Management at DBmaestro , an Enterprise Software Development Company focusing on database development and deployment technologies. Uri has over 15 years' experience in enterprise software and systems management and has held senior Product Management and R&D Management positions at a number of leading software companies.",en,146
38,1359,1465736425,CONTENT SHARED,1862503310075246782,3915038251784681624,6469413037900917152,,,,HTML,http://cio.com.br/carreira/2016/06/10/cinco-competencias-essenciais-ao-it-leaders/,cinco competências essenciais ao it leaders - cio,"Quanto mais a empresa considera a TI como uma área estratégica, menos valoriza competências técnicas para o CIO. Isso não significa, no entanto, que o líder de TI possa se dar ao luxo de deixar de lado os conhecimentos específicos da sua área. Assim como os super-heróis das histórias em quadrinho, o CIO precisa ter várias identidades. No momento em que está sentado em frente ao board, deve assumir uma postura e um discurso totalmente orientados aos negócios. Já quando encontra-se na mesa de negociação com fornecedores ou conversa sobre o escopo de um determinado projeto com sua equipe, tem de resgatar a bagagem de conhecimentos técnicos. Essa multiplicidade de visões também se aplica às competências exigidas dos CIOs. Isso porque, além da identidade técnica e de negócios, os profissionais são cobrados por sua capacidade de atender às demandas das diversas áreas da companhia e por gerenciar a equipe de TI e os fornecedores. Além disso, eles precisam encontrar tempo para idealizar produtos e serviços inovadores. Equilibrar essas diferentes tarefas representa um fator crucial para o sucesso dos líderes de TI. A seguir, seguem as competências essenciais para os CIOs, na visão de especialistas e de profissionais que atuam no setor: Conhecimento do negócio - Por mais interessantes que as tecnologias pareçam para a equipe de TI, os argumentos técnicos não podem ser utilizados para justificar um projeto para a diretoria e as demais áreas da organização. Assim, os CIOs devem conhecer a fundo o negócio da companhia para entender como as iniciativas da sua área estão alinhadas aos objetivos da organização e quais os resultados práticos esperados. Um projeto de TI é um investimento como qualquer outro da empresa e, em muitas ocasiões, pode inclusive concorrer com as demais áreas. Uma reestruturação de parque tecnológico, por exemplo, necessita estar alinhada à necessidade de crescimento da empresa. Não faz mais sentido trocar só por trocar. Capacidade de comunicação - No dia-a-dia das organizações, boa parte das atividades de TI passa despercebida pelos funcionários da companhia. Na realidade, o CIO e a sua equipe só são lembrados em situações negativas, como quando o sistema cai ou o computador para de funcionar. Com isso, a imagem do trabalho da área de tecnologia da informação fica prejudicada dentro das organizações. E o pior, essa percepção chega até o board da companhia, o que reflete diretamente no humor de investimentos em novos projetos. O CIO que pretende reverter essa situação precisa estar preparado a estruturar uma melhor comunicação de sua área com todos os stackeholders da organização. Para tanto, precisa investir em ferramentas que o ajudem a divulgar as iniciativas de TI a toda a companhia, bem como criar um canal para que os diversos usuários consigam expressar opiniões sobre produtos e serviços oferecidos pela equipe de tecnologia. Gestão de pessoas - Os resultados da área de TI também estão diretamente relacionados à capacidade que o CIO tem para recrutar, reter e desenvolver seus colaboradores. Essa capacidade de gestão e motivação das equipes é essencial a qualquer profissional em posição de liderança, mas tende a ser ainda mais crítica na TI, uma vez que trata-se de um setor no qual faltam pessoas capacitadas e, portanto, a retenção de talentos é essencial. Perfil inovador - Quando buscam um profissional para ocupar a posição de CIO, as empresas buscam pessoas com postura voltada à inovação. Na prática, isso seria representado, por exemplo, por um CIO que, antenado aos lançamentos do mercado no qual atua, percebe uma nova maneira de se relacionar com os clientes e leva essa sugestão à área de marketing. Conhecimento técnico - Desde que o líder da área de tecnologia passou a exercer uma função estratégica nas organizações, existe uma dúvida a respeito sobre, até que ponto, o conhecimento técnico representa algo essencial para quem ocupa a posição de CIO. Todo o conhecimento técnico que o profissional levou anos para adquirir começa a parecer inútil e um pouco enferrujado. Mas os especialistas aconselham que revisitar essas habilidades é extremamente importante para que o líder cultive um repertório necessário para o relacionamento com os técnicos da sua área. Um dos pecados que o CIO comete é distanciar-se do conhecimento técnico. Sem essa habilidade, o profissional não consegue saber como o departamento de TI pode contribuir com as demais áreas da organização e não consegue liderar sua própria equipe. ""Durante os últimos 20 anos, o mercado deu muita ênfase à capacitação voltada aos negócios"", diz a fundadora e presidente da empresa de recrutamento de executivos Valuedance, Susan Cramm. Ela afirma que essa tendência produziu CIOs que, hoje, perderam completamente o contato com a parte técnica da operação de TI. ""E o desempenho desses líderes é afetado negativamente por isso"", afirma a especialista. Susan aconselha os gestores de tecnologia a buscarem maneiras para colocar em prática o conhecimento teórico adquirido na universidade e no início da carreira. ""Entretanto, esse reencontro com o repertório não pode tornar-se uma mais um item na lista de obrigações diárias do CIO"", informa, ao apontar que isso deve ser um exercício prazeroso.",pt,145
39,1034,1463694744,CONTENT SHARED,-1453783314552286835,-1032019229384696495,-3222296078930623200,,,,HTML,https://flights.airberlin.com/en-DE/progressive-web-app,progressive web app - first introduced on google i/o | airberlin,"airberlin is delighted to be the first airline in the world to develop a progressive web app. Hannes Schwager, Head of Mobile & Innovations at airberlin: ""The intelligent networking of mobile services in the airline industry will continue to become increasingly important and we know that our passengers, no matter where they are in the world, want to be able to check in easily and conveniently from their mobile devices. That is why we have worked with our internal innovations team to create an app that combines the best features of an app and a web application."" Progressive web app technology makes it possible for airberlin passengers to access their personal boarding pass and further travel information about their destination at any time, wherever they are and even without an internet connection, after a one-time web check-in. This means airberlin can give its passengers even more flexible travel comfort - and makes the mobile future a little more tangible today. The innovation will be presented exclusively at the prestigious Google I/O developer conference from 18 th to 20 th May 2016 in San Francisco. Google will present the latest hardware and software solutions to a professional audience of more than 7,000 developers and countless live stream viewers during the internet giant's most important technology conference. Progressive web app: all about it All the technical details What is a Progressive Web App? A website that provides an app-like experience Offline first approach using caching Service worker (JavaScript) Available for Chrome 40, Firefox 40 and Opera The benefits of a progressive web app Available offline or with a slow connection Add-to-home icon can be added in the dialogue as needed Push alerts are possible Very fast loading times Use case: beta version PWA look & feel Step by step through the progressive web app Dashboard - Add flight One-step check-in Boarding passes: always on hand Journey details at a glance Name: Schwager PNR: IO2016 How we built the airberlin PWA by Marian Pöschmann and Axel Michel What we used Web Components : Following the simple idea, that everything except the app shell is a component. Using Polymer 1.0 ( ) as a Polyfill, we created components for the slider, which contains all available board-cards, the board-card itself (displaying the destination image and flight information), the boarding pass (QR-code), journey details, destination information, as well as the check-in and confirmation forms. Custom Events : To interact between the different components as well as the the basic application script which handles / centralizes async requests, history, app data and date handling. WebSQL : In addition to the offline service worker we tried to improve offline experience by using localForage ( ) as a wrapper around IndexedDB, WebSQL, and / or localStorage. The complete communication between server and client is JSON based to simplify this kind of data storage. HistoryAPI : The PWA is a one-page application. Since service worker / cache does not work with a hash in an url, we decided to use GET parameter to identify the different states and views of the app. A decision, with which we ran into minor offline availability problems. As a learning - do not use keys only, always send a valid pair, if you want the request to be handled properly. Or you recreate your own request depending on the relevant data of the url (see the code example of the service worker further down). Service worker : As mentioned, it's a single page app with only one use case, that means - compared with other projects - adding on service worker offline support had been rather simple. The app-shell itself is cached inside the install routine, the outline and the data on its first request. Removing the correct data at the correct time was a bit more challenging. We also integrated push notifications to enable the two-tap check-in process. VanillaJS : For the rest. Some basic DOM-selectors, a few async requests, there was no need for additional libraries. Except moment.js ( ), since we wanted some kind of 'date awareness' for the app. It has to handle different timezones, e.g. to display the correct journey step or to disable the boarding pass. Manifest and Meta data : Allows the user to add the application to the home screen. Even if there is currently no offline support on iOS, we decided to provide a proper app icon, title and color theme on android and iOS. What we did Lazy loading - Don't block the DOM : Besides our major goals: A quick check-in and the possibility to complete boarding without any additional application, we wanted the app to be fast. Therefore we load everything except the critical CSS and some basic placeholders asynchron without blocking the DOM. Basic HTML structure: Initial JavaScript: So basically all what is sent with the first request is a skeleton of the app, with a menubar and colored placeholders for the dashboard, menu options, footer (the web components) and a short intro text in the visible area. The main CSS, the core script and the vendor scripts (polymer, moment, local forage) are loaded deferred, finally the core loads the different polymer elements. Paul Lewis wrote a great article on that: Comparing the actual check-in page of m.airberlin with the new approach the initial loading time is almost the same (about ~1.5s and ~2.5s on 3G), but the starting point of rendering is significantly lower. (~0.5s vs. ~1.2s), this is still (and will always be) work in progress. However, using lazy loading and placeholder styling to prevent FOUC (Flash of unstyled content) reduces the time of starring at a blank screen. Minimize - Blurry to hurry : Serving images in different sizes, depending on screen size and resolution should be common sense, but a lot of modern mobile devices require rather large images for a high quality result. Based on the concept used by Facebook in its native apps and an HTML/SVG/CSS adaption described in this article ( ) we use very small images (60x40 px) and load the optimized variant async - until it comes from cache. Get the Service Worker working : Our very first version of the service worker had been a few lines of code. We loaded all of our static assets when initializing the worker, and once installed it fetched and cached simply everything. For our demo this is sufficient, since we only provide one flight, one destination, there won't be any flight history and nothing which can be out of date. In real, we needed more. We have flight data, e.g. an e-ticket which has to be disabled or removed after the flight. In addition, we have related data for each flight, e.g. some information and images for each destination. Spamming the cache by storing all related data for all flights over all time isn't a good idea. Therefore we decided to remove everything with a delay of 48 hours after the flight. Since we also use WebSQL / local storage, we needed to do this twice. The following code fragments are showing our current implementation draft: Code Fragment app.js: Code Fragment service-worker.js: Inside the app.js (triggered from different polymer elements) a method tests, if the stored data is still valid or not. In case it is outdated, the script uses service worker post messages (see: ) to trigger the cache delete inside the worker and removes the data from local storage. Finally it triggers a custom event to tell all listeners (polymer elements) that the data has been changed. The sample code also contains part of the fetch handler. We reduced the cache ‚url' identifier to the check-in number by creating an own Request. Which allows us to easily identify the data when deleting and brought up a second benefit: The requested URL's might contain various parameters in future which change the url, but not the data itself (e.g. data for google campaigns), reducing the url to its relevant part provides a solution for this. One more thing I would like to mention: In our first draft we put all web-components inside the install routine of the service-worker, and we returned all this, so we blocked the service-worker until every asset had been loaded. At the very same time we do some lazy loading for these elements, the CSS and scripts. While fetching we stored 'everything' in cache. Scanning the timeline of the page, and the amount of requests, it was more efficient to reduce the number of required files and let fetch do the rest. Add to home screen - if useful : Currently you can't trigger ‚add-to-home-screen' message and there is some unknown magic in it under which conditions the message is shown. In our case we also have an additional condition when this message should be displayed. Going offline is possible after you checked-in, not before. Since you can't trigger the message, you could go for your own dialog, explaining the user how to do ‚add-to-home-screen' via the menu. To implement your own conditions, you'll need to defer the event: In our case the message pops up when the client closes the massage window, that the ticket has been saved. Process integration : Due to it's atomic structure, a polymer element normally comes with inline CSS and JavaScript. Surely you are able to integrate external stylesheets and scripts in polymer elements, but the inline approach is fast. Only downside, like this it is complicated to maintain the code, especially the CSS part. Our solution - a Grunt task named 'grunt-inline' ( ) and SCSS as CSS-precompiler. Each element gets its own SCSS file which includes basic settings (like variables and functions), as well as normalizing styles. The grunt task takes the generated CSS and writes the inline style and script tags to the element. For sure, the very same is possible with gulp and LESS. What we learned (so far) Web components : Using arrays of objects as a database for polymer elements can be quite tricky when updating the data. Especially when you work with nested components, in our case the slider which contains all board-cards. Since the rendering of all data went from server to client, we only need to transfer a bit of JSON data and some binary files, great for response time, great for the capacity utilization of the server. Measuring the loading time over different devices it gets a bit more ambivalent. Pre-rendering complex data on the server and serve the complete html is faster on older / less powerful devices (although the start of first rendering takes longer). If the client cache is working, this is only true for the very first request - if not you should do some serious testing what you do on server side and what not. Service worker : On the first view easy to integrate and the benefits are simply great: Not only the increase of speed and rendering, the offline availability, it also removes a lot of requests from the server (which helps to speed up thinks even further). The complicated part is not the script itself, it is the classic question how to cache properly. Also, for the moment the implementation of service workers differs a lot, only a subset of android users get the full benefit of this technology. However this is no argument for 'not implementing', but you still need an eye on the word progressive inside Progressive Web Apps. On iOS for example, you could add our PWA to the home screen too, due to browser caching and local storage you could open it (with some luck) even if your network is slow or down but it won't take long and you would see the dinosaur. Continue Reading: about airberlin airberlin is one of the leading airlines in Europe and flies to 147 destinations worldwide each year. This includes german destinations such as Berlin and Dusseldorf , popular european travel destinations such as Majorca , Paris und Istanbul as well as destinations in Italy and Spain . Germany's second largest airline carried more than 30.2 million passengers in 2015. airberlin offers a global route network through its strategic partnership with Etihad Airways, which has a 29.21 percent share in airberlin, and through membership of the one world ® airline alliance. topbonus, the frequent flyer programme of airberlin is one of the leading programmes in Europe with more than 4 million members. The airline with the award-winning service operates codeshare flights worldwide with 22 airlines. The fleet is among the most modern and eco-efficient in Europe. Together with other airlines, airberlin belongs to Etihad Airways Partners, a new brand with which Etihad has been uniting shared activities since the end of 2014.",en,144
40,2944,1484126235,CONTENT SHARED,3660989387512978561,9210530975708218054,1216354488778990261,"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36",SP,BR,HTML,https://www.thoughtworks.com/pt/insights/blog/discutindo-devops-na-pratica,discutindo devops na prática,"Práticas de DevOps e Entrega Contínua ajudam a aumentar a frequência de deploys na sua empresa, ao mesmo tempo aumentando a estabilidade e robustez do sistema em produção. Neste webinar, Danilo Sato, autor do livro ""DevOps na prática: entrega de software confiável e automatizada"" , discute princípios, práticas e ferramentas de DevOps, cobrindo: Como automatizar o build e deploy de uma aplicação web e o gerenciamento da infraestrutura Como monitorar o sistema em produção Como evoluir a arquitetura e migrá-la para a nuvem Quais ferramentas estão disponíveis para começar a prática DevOps na Prática webinar - 10-6-2014",pt,143
41,1079,1464055142,CONTENT SHARED,943818026930898372,-6946355789336786528,4492969654113742216,,,,HTML,http://exame.abril.com.br/pme/noticias/inovador-nubank-ganha-premio-no-vale-do-silicio,"inovador, nubank ganha prêmio no vale do silício | exame.com","São Paulo - O Nubank , startup brasileira que oferece um cartão de crédito sem anuidade gerido através de um aplicativo, recebeu nesta semana o prêmio Marketers That Matter, do Sage Group do Vale do Silício . A empresa tem tido enorme sucesso na divulgação de seu produto. Até agora, cerca de 3 milhões de pessoas já se cadastraram para receber o cartão Nubank. Detalhe: tanto sucesso aconteceu sem nenhum investimento em mídia tradicional, segundo Cristina Junqueira, diretora e co-fundadora do Nubank. ""Não tínhamos verba para marketing e, mesmo se tivéssemos, nossos concorrentes sempre teriam mais. Como concorrer com o Itaú?"", explica Cristina em entrevista a EXAME.com. Consciente da sua posição de desvantagem em termos financeiros, a startup decidiu apostar em uma estratégia mais moderna. A solução encontrada foi baseada principalmente nas redes sociais, no relacionamento com a imprensa e na propaganda boca a boca. ""Focamos em Facebook, Twitter, Youtube, Instagram"", exemplifica a co-fundadora. Foi justamente essa estratégia que chamou a atenção do Vale do Silício. Para Cristina, outro ponto fundamental para o sucesso da startup é a qualidade do serviço entregue ao cliente. ""Tem também o aspecto de entregar a promessa que estamos vendendo. Ter a expectativa do cliente atendida ou superada é muito importante"", afirma. A satisfação do cliente Nubank ajudou a criar o que Cristina chama de ""crescimento viral"", através da propaganda boca a boca. Atualmente a startup tem um índice de satisfação de 88% (medido através do Net Promoter Score - NPS) e recebe elogios dos clientes a todo momento. Alguns exemplos do tipo de atendimento oferecido são bem inusitados. Uma cliente que teve o cartão mordido por seu cachorro entrou em contato para pedir um novo e, ao recebê-lo, também ganhou um ossinho roxo para o seu cão ( conheça outros exemplos ). O prêmio Marketers That Matter é o segundo reconhecimento da startup só nesse mês. Ela também foi eleita a Melhor Empresa de B2C na América Latina pelo Latam Founders Network.",pt,142
42,1182,1464797377,CONTENT SHARED,2285214528595997209,-9009798162809551896,-1393306465829315553,,,,HTML,http://www.mundodocker.com.br/docker-e-dotnet/,docker e .net,"Oi pessoal! Rodar .Net em container? SIM!! é possível, e vamos mostrar hoje como você pode fazer esse ambiente funcionar e testar sua aplicação. Bem, antes de tudo você precisa entender todo o movimento que a Microsoft vem fazendo para que sua linguagem de programação torne-se cada vez mais utilizada pelos desenvolvedores, para isso eles repensaram tudo que já fizeram, e lançaram uma versão do .Net com features muito parecidas como as encontradas em node e ruby on rails, e a batizaram de ASP.NET Core. Conceitualmente o ASP.NET Core é uma plataforma open source de desenvolvimento leve e eficiente para aplicação escritas em .net, veja abaixo uma lista dos principais pontos do asp.net Core: Suporte integrado para criação e utilização de pacotes NuGet; Pipeline de request mais leve e modular; Ambiente configurado para cloud; Suporte a injeção de dependência; Novas ferramentas que facilitam o desenvolvimento web; Capacidade de hospedar o processo no IIS ou em um Self-Host; Suporte para desenvolvimento de aplicação asp.net em Windows, Linux e Mac; Open Source e com comunidade ativa; Todos os pacotes do .Net no NuGet; .Net Core construido com suporte a versionamento side-by-side; Uma unica Stack para Web UI e Web APIs; Por que a Microsoft vem tomando esse rumo? Simples, para atrair mais pessoas para o seu produto, tendo em vista que as principais linguagens, ou as que mais crescem tem nativas todas essas features, a melhor alternativa para a Microsoft foi mudar a abordagem e atrair pessoas para melhorar seu produto/serviço e claro, isso consolida ainda mais o Windows Azure, fazendo com o desenvolvimento e deploy de Apps em .net torne-se menos doloroso. Vamos ver isso rodando em um container? Simples, tendo o Docker instalado (em qualquer host, seja ele Windows, Linux ou Mac), execute: Dentro do container, você deve criar sua aplicação, de forma parecida com o que ocorre com o ruby on rails, para isso o .net core disponibiliza um utilitário chamado: dotnet, veja: O comando dotnet new criará toda a estrutura necessária para rodar uma aplicação hello world básica, ele criará um arquivo chamado project.json com o seguinte código: Esse arquivo project.json contém as informações de dependência que sua aplicação precisará para rodar, é fundamental que você informe nesse aquivo todos os pacotes que necessita. Nessa mesma estrutura será criado um arquivo chamado Program.cs com este código: Agora basta realizar a instalação das dependências: E pronto, sua aplicação estará pronta para rodar, basta executar o comando: Fácil certo? E é possível deixar ainda mais fácil, para isso, basta criar Dockerfile e gerar uma imagem com esse ambiente: Agora crie a imagem: docker build -t myapp . E crie um container baseado nessa nova imagem: docker run -d myapp , quer ver o que retornou nessa execução? docker logs iddocontainer , ele deverá retornar algo desse tipo: Hello World! . É claro que você pode aprimorar esse ambiente, principalmente se quiser rodar uma aplicação web, mas esse ambiente básico já serve como inicio para seus testes ;). Gostou? Então nos ajude divulgando o mundodocker.com.br , grande abraço!",pt,142
43,1834,1469097976,CONTENT SHARED,4259370161044254504,-2979881261169775358,6708890816588756654,,,,HTML,https://medium.com/unboxd/how-i-built-an-app-with-500-000-users-in-5-days-on-a-100-server-77deeb238e83,"how i built an app with 500,000 users in 5 days on a $100 server","How I built an app with 500,000 users in 5 days on a $100 server There seems to be a general consensus in the world of startups that you should build an MVP (minimum viable product) without caring too much about technical scalability. I've heard many times that the only priority is to just get the product out there. As long as your business model would work at scale, you're good. You shouldn't waste time and money on making a technically scalable product. All you worry about is testing your assumptions, validating the market and gaining traction. Scalability is a concern for later. Unfortunately, this somewhat blind belief has led to some terrible failures. And Pokémon GO reminded us of it. One person who won't make this mistake again is Jonathan Zarra, the creator of GoChat for Pokémon GO. The guy who reached 1 million users in 5 days by making a chat app for Pokémon GO fans. Last week, as you can read in the article, he was talking to VCs to see how he could grow and monetize his app. Now, GoChat is gone. At least 1 million users lost and a lot of money spent. A real shame for a genius move. The article states that Zarra had a hard time paying for the servers that were necessary to host 1M active users. He never thought to get this many users. He built this app as an MVP, caring about scalability later. He built it to fail. Zarra hired a contractor on Upwork to fix a lot of performance issues. The contractor stated that the server costs were around $4,000. Since my calendar says it's 2016, I assume he isn't talking about $4000 of hardware, but $4000 in monthly or yearly virtual server and traffic costs. I've been designing and building web platforms for hundreds of millions of active users for most of my career. I can say $4,000 is a totally unnecessary amount of money for 1M users in a chat app. Even for an MVP. It means the app's server tech was designed poorly. It's not easy to build a cost-efficient, scalable system for millions of monthly users. But it's also not terribly complicated to have some sort of setup that can handle at least a decent amount of users on some cheap servers in the cloud. You just have to take it into account when building the MVP, by making the right choices. GoSnaps: 500,000 users in 5 days on $100/month server Similarly to GoChat, I also launched a Pokémon GO fan app last week, called GoSnaps . GoSnaps is an app to share Pokémon GO screenshots and images on a map. The Instagram/Snapchat for Pokémon GO. GoSnaps grew to 60k users its first day, 160k users on its second day and 500k unique users after 5 days (which is now). It has 150-200k uploaded snaps now. It has around 1000 concurrent users at any given time. I built image recognition software to automatically check if an uploaded image is Pokémon GO-related, and resizing tools for uploaded images. We run this whole setup on one medium Google Cloud server of $100/month, plus (cheap) Google Cloud Storage for the storage of images. Yes, $100. And it performs well. Technical comparison of GoChat and GoSnaps Let's compare GoChat and GoSnaps. Both apps probably fire a lot of requests per second to fetch chats/images within a certain area of the map. This is a geospatial lookup in the database (or search engine), either by a polygon of latitude/longitude locations or by a specific point. We use a polygon and we fire this request every time someone moves the map. These types of queries are heavy operations on a database, especially in combination with sorting or filtering. We get this type of search request hundreds of times per second. GoChat probably did too. Unique to GoChat is that it had to fetch and post a lot of chat messages every second. The article about GoChat talks about 600 requests per second for the whole app. Those 600 requests are a combination of map requests and chat messages. These chat messages are small and could/should be done over a simple socket connection, but happen often and have to be distributed to other chatters. This is manageable with the right setup, but disastrous with a poor, MVP-like setup. GoSnaps, on the other hand, has a lot of images being fetched and 'liked' every second. The snaps pile up on the server, since old snaps stay relevant. Old chats do not. Since the actual image files are stored in the Google Cloud Storage, the amount of requested image files is not a concern for me as a developer. Google Cloud handles this and I trust Google. But the requested snaps on the map are my concern. GoSnaps has image recognition software that looks for patterns on all uploads to see if an image is Pokémon-related or not. It also resizes the images and sends them to Cloud Storage. These are all heavy operations in terms of CPU and bandwidth. Way heavier than distributing some small chat messages, but less frequent. My conclusion is that both apps are very similar in terms of scalability complexity. GoChat handles more small messages while GoSnaps handles larger images and heavier server operations. Designing an architecture for these two apps both require a slightly different approach, but are similarly complex. How I built a scalable MVP in 24h GoSnaps is built as an MVP, not as a professional business product. It was built entirely in 24 hours. I took a NodeJS boilerplate project for hackathons and used a MongoDB database without any form of caching. No Redis, no Varnish, no fancy Nginx settings, nothing. The actual iOS app was built in native Objective-C code, with some borrowed Apple Maps-related code from Unboxd , our main app. So how did I make it scalable? By not being lazy. Let's say I would consider an MVP as solely a race against the clock to build a functional app as quick as possible, regardless of technical backend quality. Where would I have put my images? In the database: MongoDB. It would require no configuration and almost no code. Easy. MVP. How would I have queried the snaps within a certain area that got the most likes? By just running a plain MongoDB query on the entire pile of uploaded snaps. Just one database query on one database collection. MVP. All of this would have destroyed my app and the app's feature. Look at the query I would have had to run to get these snaps: ""find all snaps within location polygon [A, B, C, D], excluding snaps marked as abuse, excluding snaps that are still being processed, ordered by number of likes, ordered by valid Pokémon GO snaps first and then ordered by newest first"". This works great on a small dataset, great, MVP. But this would have been totally disastrous under any type of serious load. Even if I would have simplified the above query to only include three conditions/sorting operations, it would have been disastrous. Why? Because this is not how a database is supposed to be used. A database should query only on one index at a time, which is impossible with these geospatial queries. You'll get away with it if you don't have a lot of users, but you'll go down once you get successful. Like GoChat. What did I do instead? After applying the CPU-expensive image recognition and doing resizing, the resized images are uploaded to Google Cloud Storage. This way the server and database don't get hit for requesting images. The database should worry about data, not images. This saves many servers by itself. On the database side, I separate the snaps into a few different collections: all snaps, most liked snaps, newest snaps, newest valid snaps and so forth. Whenever a snap gets added, liked or marked as abuse, the code checks if it (still) belongs to one of those collections and acts accordingly. This way the code can query from prepared collections instead of running complicated queries on one huge pile of mess. It's simply separating data logically into some simple buckets. Nothing complicated. But it allows me to query solely on the geospatial coordinates with one sorting operation, instead of a complex query as described above. In simple terms: it makes it straightforward to select data. How much extra time did I spent on all of this? Maybe 2 to 3 hours. Why I did this in the first place? Because that's just the way I set things up. I assume my apps will be successful. There's no point in building an app assuming it won't be successful. I would not be able to sleep if my app gains traction and then dies due to bad tech. I bake minimum viable scalability principles into my app. It's the difference between happiness and total panic. It's what I think should be part of an app MVP. Choose the right tools for your MVP If I would have built GoSnaps with a slower programming language or with a big framework, I would have required more servers. If I would have used something like PHP with Symfony, or Python with Django, or Ruby on Rails, I would have been spending my days on fixing slow parts of the app now, or adding servers. Trust me, I've done it many times before. These languages and frameworks are great in many scenarios, but not for an MVP with low server budget. This is primarily due to the many layers of code that are usually used for mapping database records to logic and unnecessary framework code. It just simply hits the CPU too hard. Let me give you an example on how much this actually matters. As said, GoSnaps uses NodeJS as the backend language/platform, which is generally fast and efficient. I use Mongoose as an ORM to make the MongoDB work straightforward as a programmer. I'm not a Mongoose expert by any means and I know the library by itself has a huge codebase. Therefore Mongoose was a red flag. But yeah, MVP. At one point last weekend, our server's 4 NodeJS processes were running at 90% CPU each, which is unacceptable to me for 800-1000 concurrent users. I realized that it had to be Mongoose doing things with my fetched data. Apparently I simply had to enable Mongoose's ""lean()"" function to get plain JSON objects instead of magical Mongoose objects. After that change, the NodeJS processes dropped to around 5-10% CPU usage. Just the simple logic of knowing what your code actually does is very important. It reduced the load by 90%. Imagine having a really heavy library, like Symfony with Doctrine. It would have required a couple of servers with many CPU cores to just execute the code alone, even though the database is supposed to be the bottleneck, not the code. Choosing a lean and fast language is important for scalability, unless you have a lot of money for servers. Choosing a language with a lot of useful available libraries is even more important, since you want to build your MVP quickly. NodeJS, Scala and Go are good languages that cover both of these requirements. They provide a lot of good tools with a lot of good performance. A language like PHP or Java by itself is not necessarily slow, but is usually used together with large frameworks and codebases that make the application heavy. These languages are great for clean object oriented development and well-tested code, but not for quick and cheap scalability. I don't want to start a big programming language argument, so let me just state that this is subjective and incomplete. I personally love Erlang and would never use it for an MVP, so all your arguments are invalid. My previous startup Cloud Games A few years ago, I co-founded Cloud Games , an HTML5 games publisher. When we started, we were a B2C gaming website focused on the MENA region. We spent a lot of effort on gaining users and reached 1M monthly active users (MAU) after a few months. At the time, I used PHP, Symfony2, Doctrine and MongoDB in a pretty simple and lean setup. I used to work for Spil Games with 200 million MAU, which used PHP at the time and then moved to Erlang. After Cloud Games reached approximately 100,000 MAU, we started to see real server pain with Doctrine and MongoDB due to the huge overhead of these PHP libraries. I did set up MongoDB, indexes and queries the right way, but the servers were having a hard time processing all the code. And yes, I used PHP's APC cache and so forth. Since cloudgames.com was still very static, I was able to migrate the MVP to NodeJS with Redis in a few days. Similar setup, different language. This led to an immediate decrease in load by about 95%. Granted, this had more to do with avoiding PHP libraries than with the actual language. But a minimalistic NodeJS setup makes more sense than a minimalistic PHP setup. Especially since MongoDB and frontend code are also 100% JavaScript, like NodeJS. PHP without its frameworks and libraries is just another language. We needed this cheap setup, since we were a self-funded, early-stage startup. Cloud Games is now doing well and still based on a cost-efficient NodeJS architecture. We might not have managed to be successful with a more costly tech setup, given the fact that we've been through some really tough times as a startup. Designing a low-cost, scalable architecture has been essential for success. MVP and scalability can coexist If there's an opportunity for your app to grow exponentially due to hype or possible media coverage, make sure to consider scalability as part of your MVP. The principles of minimum viable products and scalable tech can coexist. There's nothing sadder than building a successful app and seeing it fail because of technical issues. Pokémon GO itself has had a lot of issues, but is so unique and hyped that it didn't matter. Small startups don't have this luxury. Timing is everything . One million GoChat users and half a million GoSnaps users probably agree with me. Please like and follow! If you liked this article, please like it here below on Medium. This would mean a lot to me. Feel free to comment for advice on scalability. At Unboxd , we're always happy to see other apps grow! Thank you!",en,140
44,2797,1480504938,CONTENT SHARED,-7294716554902079523,4227773676394505435,6923518953276678853,"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.100 Safari/537.36",MG,BR,HTML,http://blog.caelum.com.br/usando-o-git-add-interativo/,usando o git add interativo,"Muitas vezes, quando estamos desenvolvendo alguma funcionalidade nova, precisamos criar novas classes, modificar outras e até mesmo mexer em algumas partes que não necessariamente são relacionadas com a funcionalidade nova que estamos implementando. Quando terminamos tudo, ficamos com um cenário assim no git: Como você pode ver, no desenvolvimento da funcionalidade de solicitar cotações em meu sistema, eu precisei modificar views, DAOs, modelos, criar novos arquivos css, javascript e um controller. Além disso, durante o desenvolvimento, eu arrumei alguns testes que outro desenvolvedor quebrou e precisei criar novos testes para o código que acabei de criar. O caminho mais rápido pra commitar tudo isso seria fazer tudo em um único commit, com uma mensagem genérica: O problema dessa abordagem é que ela pode tornar mais difícil encontrar quando um bug começou a acontecer, pode atrapalhar outro desenvolvedor que está tentando entender o que aconteceu em cada arquivo e também pois perde boa parte do propósito do git, que é manter um histórico de todas as alterações do projeto. Será que existe alguma coisa que pode nos ajudar dessas situações? Usando o git add interativo O git add interativo é uma ferramenta que nos ajuda a lidar com cenários como esse, ele nos permite adicionar arquivos específicos de forma mais rápida e até mesmo selecionar partes específicas de um arquivo pra commitar. Para usar ela, basta digitar o comando: Logo de primeira ela nos mostra quais são as opções, para usar cada uma delas, basta digitar o número correspondente e dar enter. 1. Status Nos mostra essa mesma tela, listando os arquivos que foram modificados 2. Update Nos permite adicionar as mudanças que fizemos em arquivos já existentes, ou seja, passar arquivos de not staged para staged . Por exemplo: ao digitar o número 2 e dar um enter, ele me mostra uma lista dos arquivos que foram modificados e um número em frente a cada um deles. Para adicionar um arquivo, eu posso tanto digitar apenas o número dele, como 8 ou, se eu quero adicionar mais de um arquivo, posso digitar um intervalo: 1-5 Como você pode ver, os arquivos que eu adicionei ficam com um * do lado. Assim que eu terminar de adicionar todos os arquivos, é só dar mais um enter sem inserir nenhum número para voltar ao menu principal. Pronto, eu adicionei apenas 6 de meus 9 arquivos que foram modificados e agora eu posso escrever uma mensagem de commit bastante detalhada sobre quais modificações eu fiz. Mais opções do git add interativo 3. Revert Permite voltar arquivos que estão staged para not staged . (o mesmo que git reset HEAD caminho/para/o/arquivo.java ) 4. Add untracked Permite adicionar novos arquivos no projeto. (equivalente a git add caminho/para/o/arquivo.java ) 5. Patch Permite escolher um arquivo e adicionar pedaços específicos dele. 6. Diff Nos mostra um diff dos arquivos que estão staged mostrando exatamente as diferenças de cada um dos arquivos que foram modificados. (equivalente ao git diff --cached ) As opções 3 - revert , 4 - add untracked e 6 - diff funcionam de forma idêntica à opção 2 - update , já a opção 5 - patch que nos permite escolher pedaços específicos de nossos arquivos para commitar, será tema de um próximo post. Se você usa bastante o git e quer aprender mais dicas como essa, dê uma olhada em nosso curso online de git",pt,139
45,2535,1475854719,CONTENT SHARED,5854206600849997966,7645894863578715801,-2165791432286103639,Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:49.0) Gecko/20100101 Firefox/49.0,SP,BR,HTML,http://blog.kaczmarzyk.net/2016/10/03/the-hardest-thing-in-computer-science/,the hardest thing in computer science,"I firmly believe that the hardest thing in computer science is naming things . I saw it many times during code reviews or analyzing legacy code, over and over: Wrong (vague or meaningless) names are often the consequence of bad design. For instance, it is impossible to find a good, concise name for a class/module which holds too many responsibilities. When something (e.g. a class) is named badly, programmers tend to care less about it. You have probably seen generic ""Service"" classes, that grow very fast, as more and more methods are added to them. Wrong names erase very important parts of the domain language from the code. The consequence is, that it is very hard to truly understand the domain model, especially when you are not the author of the code. This leads to even more shallow understanding of the system and the business. And that is the shortest route to even more bad code. ""(...) when you program, you have to think about how someone will read your code, not just how a computer will interpret it."" - Kent Beck Just imagine what would happen if, in real life, most of the names were vague, meaningless, or simply wrong. Not a good prospect, right? Yet you can see it in the source code very often. I decided to describe some common naming anti-patterns that I observed during my career. I think that eliminating them can really improve readability and, in turn, the overall quality of any codebase. Parameters and arguments are not the same thing Let's consider the following method signature: It is very common to see an usage similar to the following: The thing is, that the actual argument passed to a method is always much more specific than the corresponding parameter. A parameter might be generic, while an argument is a concrete value. It has its context, it has its intention. Therefore it deserves a much more specific name than just a repetition of the parameter name. Just see how much more information we get, if the above code is rewritten as follows: The idea is very simple - an argument is more than a parameter which it fits. Surprisingly often, though, programmers seem to forgot about it. Do not let an interface name pollute the name of your implementation The difference between parameters and arguments might be taken to another level when you consider interfaces and their implementations. If you read Dependency Inversion Principle ( SOLID ) carefully, it is clear that an interface is what a class (or a system or whatsoever) exposes for another components to express what it expects from them. Implementing an interface is meeting such expectations (i.e. making a class or another system something that you can ""plug in"" to this interface). Sadly, many programmers see it other way around, as if an interface was something that an implementation exposes to tell other classes about its capabilities. ""Could you pass me the charger, please?"" ""Sure!"" - real life conversation ""Could you pass me an electrical outlet implementation?"" ""WAT!?"" - if our daily language was as sloppy as the code often is This misunderstanding is another source of very bad names that lead to bad design. Let me make an example. I once dealt with two systems that were integrated with each other. The first system was sending some messages to the second one. The name of the second system was very vague, it just reflected a generic concept in the first system. The first system was basically a router which was preprocessing some messages and sending them to departments ( Department was an actual name used in the domain model of the first system). Most of these departments were external organizations with some web-service APIs. But here comes the second system. It wasn't 3rd party, it was maintained by the same group of people who maintained the first one. From the perspective of the first system, it was dealt with as any Department . So it was named ""MessageDepartment"" (seriously). The name reflected just the fact that it accepted messages and was plugged into a system that dealt with Departments . It wasn't any external department, just another subsystem of a larger whole. Yes, it accepted messages, but it would be much more informative to tell what it actually did with them. This extremely generic and vague name caused more and more functionality being added to the system. The developers ended up with adding many (too many) responsibilities to a single module. Among others, it included document/message browsing and management, review/verification process, notification feeds and duplicate detection. All of these were quite big and deserved a separate module/subsystem to preserve cohesion. It was clearly visible that something was wrong when you tried to describe that thing: it was difficult to find a concise definition/name. What the system became was just a big bag with different, unrelated functions. I firmly believe that the design would have been much better, if the system had been named accurately from the beginning. I.e. if the emphasis was put on what it really was (e.g. a notice board) rather than what it was integrated with. It would not have been so easy to randomly put new things into it so, hopefully, the functionality would have been split into separate modules. Express expectations through parameter names Let's consider the following code snippets: The good thing here is that a private method has been nicely extracted. Its name is quite OK too. Actually it could have been a proper code. The problem was that the logic in the private method was valid only for Notes (a type of Documents). It has not been used for any other documents (and should not have been). Therefore it would be great if the parameter was named more precisely, for example as follows: Variable or field is much more than its type How of then do you see code like this: In some cases it might be OK, but in many others variable names may (and should) carry additional information. For example: Another example I once found: Can you tell me what this person field really is? You probably could if you took a look on its usage, but you would not have to if somebody cared enough to write just: Another, more complex example: It is an actual snippet that I once saw during a code review. At least three names (marked as (1) , (2) and (3) in the comments above) could be improved here. But let's focus on the last one ( interviewStatus2InterviewResult ). It tells that the variable is a mapping of statuses to full interview results. It is true (at least), but do we really need that? The type of the variable tells the same and is clear enough, I think ( Map<InterviewStatus, List<InterviewResult>> ). The really important fact is that the map contains interview results for a single candidate. It is a consequence of the grouping operation few lines above. The code is not clear, so a good name would be even more helpful here. I think that this code requires much more refactoring in general, but even just naming improvements make it noticeably less ugly: Context of a method invocation is more than just the method itself In another system I worked on, I once found a method called advancedSearch . There was nothing wrong with the method itself (let's not worry about what ""advanced"" actually meant). The problem was in other place. The system had an on/off flag for duplicate detection (i.e. whether or not it should throw an exception if a duplicated item was submitted). The flag was named enableAdvancedSearch . How would you interpret it without knowing the explanation I gave above? I bet it would not be obvious. This advancedSearch method was actually used in other parts of the system too. The badly named flag wasn't about disabling/enabling the method/feature itself, but rather one particular context of using it. The intention (i.e. checking for duplicates) was much more important than the means of executing it (i.e. by invoking advancedSearch ). Therefore the proper name would be enableDuplicatePrevention . Name something if you can Logical expressions, literals, lambdas, even code snippets - they are all anonymous. Sometimes (very often, I think) it is a good idea to just name them. The easiest way to do that is to extract a method. Let's consider the following snippet as an example: Why should a reader be forced to even look on these 7 lines of code when all he or she wants is to get a general idea about what is going on at the high level? All that happens in these lines is just selecting the highest value out of a collection of status codes. It is a simple logic, but it would be easier to follow if it was explicitly named. And doing that is as easy as extracting a private method (e.g. status = theHighestStatusOf(statuses) ). Another example: Which in my opinion would be much better if a private method was extracted: The idea is simple. Just remember to be careful, because no name at all is actually better than a wrong name. Summary It is, for sure, not a complete list of naming anti-patterns. I hope it is just a good starting point for improving names in the code and finding more rules about them. Improving names improves readability, which positively affects the overall code quality. Which, I hope, is the goal for the most of us. Thanks for reading and I hope you will find it useful! Photo credit: Geoffrey Fairchild",en,139
46,2422,1474815847,CONTENT SHARED,9175693555063886126,1374824663945909617,-7520177282913389815,,,,HTML,https://bittencode.wordpress.com/2015/07/08/15-minutos-sobre-docker/,15 minutos sobre docker,Neste post eu compartilho um vídeo e uma apresentação sobre o que é o Docker e como começar a usar. O objetivo aqui é mostrar como é fácil começar a usar ao compartilhar a referencia da documentação e mostrando a execução do Docker em linha de comando. Este post serve pra quem já ouviu falar sobre Docker e quer então começar a brincar com ele. É um guia simples para os iniciantes. Descrevo então sobre: 1. Conceitos básicos 2. Instalação 3. Repositórios de images 4. Criar suas próprias images 5. Executar containers baseados nas images. Espero que gostem! Segue os slides: Referência:,pt,139
47,1306,1465396491,CONTENT SHARED,310515487419366995,-5527145562136413747,3192880191028684629,,,,HTML,http://blog.runrun.it/erros-de-portugues-em-e-mails/,71 erros de português que precisam sumir dos seus e-mails,"Escrever um e-mail não deveria ser uma coisa tão penosa. Não deveria ser aquele momento em que você excomunga o idioma porque hesita entre uma e outra forma de grafar as palavras. Não deveria ser como assumir um risco. Sobretudo, não deveria ser um novo 7 a 1 todos os dias. Por isso, preparamos uma lista com os 71 erros de português e dúvidasortográficasmais comuns em e-mails! Mais do que esclarecer suas dúvidas, você vai se espantar com algumas expressões que usa, mas que estão fora da norma gramatical. Guarde esta lista, caso se esqueça de algo, e aproveite para a compartilhar com seus colegas! 1. Ao invés de / Em vez de ""Em vez de"" é usado como substituição. Ex: São Paulo em vez de BH. ""Ao invés de"" é usado como oposição. Ex: Grande ao invés de pequeno. 2. De encontro a / Ao encontro de ""Ao encontro de"" expressa harmonia. Ex: Obrigada! Sua ajuda veio ao encontro do que eu precisava. ""De encontro a"" expressa embate. Ex: Brigaram porque a opinião dele ia de encontro ao que ela acreditava. 3. Através de / Por meio de ""Por meio de"" é o mesmo que ""por intermédio"". Ex: Conseguimos por meio de muito trabalho. ""Através de"" expressa a ideia de atravessar. Ex: Olhava através da janela. 4. Em princípio / A princípio ""A princípio"" equivale a ""no início"". Ex: A princípio, achei que não seria capaz. ""Em princípio"" equivale a ""em tese"". Ex: Em princípio, todo homem é igual perante a lei. 5. Se não / Senão ""Senão"" significa ""caso contrário"" ou ""a não ser"". Ex: Me avise, senão vou esquecer. Não fez senão o prometido. ""Se não"" é usado para expressar uma condição. Ex: Se não puder, nos avise antes. 6. Retificar / Ratificar ""Ratificar"" é o mesmo que confirmar. Ex: Os dados ratificaram a previsão. ""Retificar"" é o mesmo que corrigir, emendar. Ex: Vou retificar os dados da empresa. 7. À medida que / Na medida em que ""Na medida em que"" equivale a ""porque"". Ex: Cancelamos a reunião na medida em que a negociação havia sido adiada. ""À medida que"" mostra relação de proporção. Ex: A produtividade aumenta à medida que a equipe usa a ferramenta. 8. Eminente / Iminente ""Eminente"" significa ""excelente"". Ex: É uma professora eminente. ""Iminente"" significa deverá acontecer em breve. Ex: O sucesso do projeto é iminente. 9. Bastante / Bastantes ""Bastante"" concorda com o substantivo. Só não concorda com adjetivos e advérbios. Por isso se diz ""Há bastantes e-mails"" e ""Andei bastante rápido"". 10. Sessão / Seção ""Sessão"" com ""ss"" quer dizer o tempo de um evento. Ex: Sessão de cinema, ou sessão de acupuntura. ""Seção"" com ""ç"" quer dizer ""departamento"" ou ""divisão"". Ex: A seção de arte moderna do museu, ou a seção de carnes do supermercado. 11. Tachar / Taxar ""Tachar"" com ""ch"" é ""censurar"", ""rotular"". Ex: Foi tachado de louco. ""Taxar"" com ""x"" é receber taxa, imposto. Ex: Grandes fortunas serão taxadas. ""Trás"" só existe na expressão ""Para trás"". Se você está se referindo ao verbo ""trazer"", lembre-se da letra z nele e use sempre ""traz"". 13. Descrição / Discrição ""Descrição"" é o detalhamento de algo. Ex: Não havia uma descrição clara do trabalho. ""Discrição"" é a qualidade do que é discreto, não chamativo. Ex: É bom ter discrição durante a negociação. 14. Afim / A fim de ""A fim de"" indica ideia de finalidade. Ex: Irei ao evento a fim de praticar o networking. ""Afim"" é um adjetivo, o mesmo que ""semelhante"". Ex: Temos ideias afins. 15. Desapercebido / Despercebido ""Despercebido"" significa ""sem atenção"". Ex: A mudança passou despercebida. ""Desapercebido"" significa desprovido, desprevenido. Ex: Estava desapercebido de dinheiro. ""Demais"" significa ""excessivamente"". Também pode significar ""os outros"", na expressão ""os demais"". ""De mais"" se opõe a ""de menos"". Ex: Uns têm privilégios de mais; outros de menos. 17. Tão pouco / Tampouco Tampouco corresponde a ""também não"", ""nem sequer"". Ex: Ele não fez o que pedi, tampouco o que você pediu. Tão pouco corresponde a ""muito pouco"". Ex: O fim de semana foi delicioso, mas durou tão pouco. Mal opõe-se a bem. Ex: Acordo mal-humorada. Estava malfeito. Mau opõe-se a bom. Ex: Hoje é um mau dia para conversarmos. Homens dizem ""obrigado"". Mulheres dizem ""obrigada"". 20. Descriminar / Discriminar ""Discriminar"" significa ""separar"" e também ""discernir"". Ex: Discriminar por orientação sexual é desprezível. As notas fiscais já foram discriminadas. 21. A cerca de / Acerca de ""Descriminar"" significa ""inocentar"" e também ""descriminalizar"". Ex: A juíza descriminou o réu. ""Acerca de"" é o mesmo que ""a respeito de"". Ex: Deveríamos discutir mais acerca de política. Já ""a cerca de"" indica aproximação. Ex: Moro a cerca de 3Km daqui. b) A forma correta é a segunda Lembre-se que o sentido é ""dar resposta a alguém"", portanto, sempre acompanha a preposição ""a"". Anexo é um adjetivo e concorda em gênero e número com o substantivo a que se refere. No caso, a palavra ""imagens"" é feminina e está no plural. Além disso, a expressão ""em anexo"" é recusada pelos principais linguistas. 24. Visar o objetivo / Visar ao objetivo O verbo visar, no sentido de almejar, pede a preposição ""a"". No entanto, quando ele está junto de outro verbo, dispensa-se a preposição. Ex: Visamos viajar para o exterior este ano. 25. Precisar de fazer / Precisar fazer Assim como o verbo anterior, ""precisar"" só vem junto da preposição ""de"" quando há um substantivo. Ex: Precisamos de mais foco. Precisavam tirar umas férias. 26. Media a reunião / Medeia a reunião Lembre-se dos outros verbos irregulares com final ""-iar"": ansiar, incendiar e odiar. Por maior que seja seu ódio, você não diz: ""Eu odio"". 27. Interviu, interviram / Interveio, intervieram O verbo ""intervir"", assim como ""convir"", se conjuga como o verbo ""vir"". 28. Quando dispor / Quando dispuser O verbo ""dispor"", assim como ""repor"", ""propor"" se conjuga como o verbo ""pôr"". 29. Preveu, preveram / Previu, previram O verbo ""prever"", assim como ""rever"", se conjuga como o verbo ""ver"". ""Chego"" e ""trago"" só existem na expressões ""Eu chego"" e ""Eu trago"". O verbo ser pede o particípio irregular, que não termina em -do. Por isso se diz ""foi impresso"" e não ""foi imprimido"". O verbo ter pede o particípio regular. Por isso se diz ""tinha acendido"" e não ""tinha aceso"". 32. A curto, médio, longo prazo / Em curto, médio, longo prazo A expressão exige a preposição ""em"". A palavra ""ora"" não só existe como significa ""agora"". 34. Quando ver / Quando vir ""Quando vir"" se refere ao verbo ver no futuro e na condicional. Ex: Quando eu te vir, vou te dar um abraço apertado! Além disso, ""quando ver"" não existe. A menos que você se refira a um quiz ( aqui estão vários! ), escreva ""quis"". ""A nível de"" é uma expressão coringa, que não tem sentido próprio. Procure substituir, por ex., ""a nível de Brasil"" por ""a nível nacional"", ou ainda melhor, por ""com relação ao Brasil"". Em outros casos, a expressão é inútil. Em vez de dizer ""problemas a nível de foco"", diga apenas ""problemas de foco"". Mesmo após a última reforma ortográfica, a palavra continua sendo grafada com hífen. O verbo ""esquecer"" só é usado com a preposição ""de"" quando vem acompanhado de um pronome oblíquo (me, te, se, nos...). O mesmo vale para o verbo ""lembrar"". No sentido de tempo decorrido, o verbo ""fazer"" só é usado no singular. Para indicar tempo passado, usa-se o verbo haver. O ""a"", como expressão de tempo, é usado para indicar apenas tempo futuro ou distância. Ex: Falarei com o diretor daqui a cinco dias. Ele mora a duas horas do escritório. Além disso, se desejar usar a expressão ""atrás"", o verbo ""haver"" deve ser removido. ""Há dois anos atrás"" . No sentido de existir, o verbo ""haver"" fica sempre no singular. Já nas locuções verbais, ele concorda com o sujeito. Ex: Elas haviam feito um ótimo trabalho. ""Quite"" deve concordar com o substantivo a que se refere. ""Onde"" se refere a um lugar em que alguém ou alguma coisa está. "" Aonde"" é formado pela preposição ""a"", porque indica movimento. Quem vai vai a algum lugar. Nessa mesma lógica, não existe a expressão ""daonde"", pois quem vem vem de algum lugar. Existe apenas ""de onde"". O verbo assistir, no sentido de ver, exige a preposição ""a"". Caso contrário, significa ""ajudar"". Ex: A enfermeira assistiu o paciente por horas. Quem admite admite ""algo"". Isso classifica o verbo ""admitir"" como transitivo direto. Assim como quem ama ama algo. Quando isso ocorre, o verbo concorda com o sujeito. Quem precisa precisa ""de algo"". Isso classifica o verbo ""precisar"" como transitivo indireto. Quando isso ocorre, o verbo fica no singular. O verbo ""implicar"" tem sentido de ""requerer"" e também de ""acarretar"" e, em ambos casos, não admite preposição. Não se usa a preposição ""em"" nessa expressão. ""Entre eu"" só pode ser usado antes de um verbo no infinitivo. Ex.: ""Passou-se um bom tempo entre eu começar o trabalho e você me ajudar."" Tem refere-se à 3ª pessoa do singular do verbo ""ter"" no Presente do Indicativo. Têm refere-se ao mesmo tempo verbal, porém na 3ª pessoa do plural. Vêm, Convêm e Retêm Verbos de movimento exigem a preposição ""a"". A regência do verbo preferir é com a preposição ""a"" e não ""do que"". O correto é meio-dia e meia, pois o numeral fracionário concorda em gênero com a palavra hora. A menos que você esteja dizendo que a meia do seu pé está ansiosa, o correto é no masculino, sempre que quiser dizer ""um pouco"". No sentido de ""metade"", concorde com o substantivo. Ex: Meia hora, meia xícara de chá. ""Menas"" não existe. ""Perca"" é verbo. Ex: Não quero que você perca sua fé em mim. ""Perda"" é substantivo. Foi uma perda incalculável. c) Abandonando pleonasmos 57. Na minha opinião pessoal = Na minha opinião 58. Repetir de novo = Repetir 59. Multidão de gente = Multidão 60. Encarar de frente = Encarar 61. Duas metades iguais = Metades 62. Preferir mais = Preferir 63. Há anos atrás = Há anos ou Anos atrás d) Descomplicando o uso da crase Motivo de erros de português há gerações, a crase é simplesmente quando duas letras se fundem numa só: a preposição ""a"" e o artigo feminino ""a"". Algumas pessoas inclusive preferem ler ""à"" como ""a a"". Tendo isso em mente, fica bem mais fácil entender quando a crase é necessária. Veja alguns erros: 64. De segunda à sexta / De segunda a sexta Você está dizendo ""De segunda até sexta"" e não ""De segundo até a sexta"". Portanto, não há duas vezes o ""a"". Logo, não faz sentido haver crase. 65. Das 17 até às 18h / Das 17 às 18h É o mesmo caso que acabamos de explicar. 66. À partir de / A partir de Nenhum verbo exige crase antes. Prazo é uma palavra masculina e, portanto, não acompanha o artigo feminino ""a"", necessário para haver crase. 68. Refiro-me aquilo / Refiro-me àquilo As palavras ""aquilo"" e ""aquele"", masculinas, levam acento quando provêm da fusão do ""a"" preposição com a letra ""a"" de ""aquilo"". Ex: Refiro-me àquilo que você disse na reunião ontem. 69. Disse à você / Disse a você Não ocorre crase antes de pronome pessoais (eu, você, ele, ela, nós, vocês, eles, elas), uma vez que nenhum deles vem acompanhado do artigo feminino ""a"". 70. A vista, a disposição, a beira, a espera, a base / À, à, à, à, à Sem o acento grave, todas essas expressões são apenas substantivos. 71. Vou à Curitiba, Vou a Bahia / Vou a Curitiba, Vou à Bahia Quando estiver se referindo a cidades e países, lembre-se: Vou a, volte de... Crase pra quê? Vou a, volta da... Crase há! No exemplo: Vou a Curitiba (porque volto dE Curitiba) vs. Vou à Bahia (porque volto dA Bahia) e) Descomplicando os porquês Por fim, um dos mais célebres erros de português é a confusão que se faz entre os porquês. Veja como é mais simples do que parece! Sempre que a palavra ""motivo"" estiver implícita na expressão, use ""por que"". Mesmo que não seja uma pergunta. Caso haja pontuação (seja vírgula, ponto final, de exclamação ou interrogação) após, acentue a palavra ""quê"", ficando ""por quê"". Se é possível substituir por ""pois"", use ""porque"". Se é possível trocar pela palavra ""motivo"", use o substantivo ""porquê"". Exemplo: Você não sabe por que [motivo] eu fiz aquilo, e agora me pede que eu explique por quê. Mas eu não irei dizer o porquê! Ainda estou triste porque você me ofendeu. Agora que você deu um importante passo para não cometer mais erros de português em e-mails, é hora de avançar ainda mais. O número de e-mails e a forma como a troca de demandas fica desordenada provavelmente são um pesadelo diário para você e sua equipe. O fim do pesadelo está na contratação de uma ferramenta de gestão de projetos, tarefas e fluxo de trabalho, como o Runrun.it. Experimente grátis e sinta a diferença:",pt,136
48,1485,1466605328,CONTENT SHARED,2719909253419802298,-5527145562136413747,101989957506145943,,,,HTML,http://blog.runrun.it/mulheres-millenials-geracao-y/,as expectativas das mulheres da geração y são muitas,"Elas configuram uma nova era do talento nas empresas: alta escolaridade, confiança, ambição e objetivos claros em relação à carreira. Nascidas entre 1980 e 1995, as mulheres da Geração Y , ou millenials, têm levado novas demandas ao mundo corporativo. Elas são comunicativas, gentis, democráticas e participativas. E estão promovendo mudanças de comportamento e criando novas expectativas dentro do mercado de trabalho. Por outro lado, apesar da grande atenção que a Geração Y recebe , poucos estudos que mapearam as atitudes e valores profissionais deste grupo diferenciaram seus integrantes por gênero. E, apesar de e mpresas de todo o mundo enfrentarem os desafios de remodelar e rejuvenescer sua força de trabalho, lhes faltam mulheres em posições de liderança. Este post vai mostrar um pouco mais sobre a profissional millennial, suas capacidades e fragilidades, para você tentar trazê-la para a sua equipe. Em muitas empresas, as posições de cunho gerencial e alto poder decisório estão sendo igualmente disputadas por ambos os sexos. Entretanto, a discriminação ainda existe e fica exposta quando o assunto é salário. Segundo pesquisa publicada pela PUC-RIO em 2014, a diferença entre a remuneração de homens e mulheres com o mesmo cargo é de até 30%. Este quadro levou o Brasil a ocupar o 82º lugar em um ranking sobre desigualdade de sexos , com 135 países, conforme pesquisa realizada pelo World Economic Forum (WEF). A PwC, consultoria global, tem 50% de profissionais mulheres e 80% de profissionais da geração Milênio. Em parceria com o Instituto Optimum Research, divulgou a pesquisa "" The female millennial - A new era of talent "" em 2015. O estudo mostrou que 53% das mulheres classificaram as oportunidades de progresso na carreira como a característica mais atraente da empresa. Infelizmente, 71% das entrevistadas disseram não sentir que as oportunidades são realmente iguais para homens e mulheres. A densa participação da mulher no mercado de trabalho abre novas perspectivas tanto na esfera pública quanto na privada. Os números só crescem: elas já atingem escolaridade superior a dos seus colegas homens e, cada vez mais, investem na formação técnica e intelectual com o objetivo de alcançar novos espaços em todas as camadas organizacionais. As millenials demonstram grandes expectativas profissionais, procuram tarefas que proporcionem desafios, reconhecimento, crescimento e desenvolvimento, e que permitam trabalhar com autonomia. É a autoconfiança que abre portas para isso. Ou, em uma expressão muito usada hoje em dia, o ""empoderamento feminino"". Segundo a pesquisa da PWC , as jovens brasileiras (76%), Índia (76%) e Portugal (68%) são as mais confiantes em relação à carreira, enquanto que suas congêneres no Japão (11%) e Alemanha (19%) são as menos confiantes. Uma pesquisa realizada por Sylvia Ann Hewlett e Ripa Rashid em empresas do setor privado do Brasil confirma a tese: 80% das mulheres têm ambição de chegar ao topo da hierarquia organizacional. O estudo também identificou que as profissionais brasileiras possuem uma maior ambição pelo crescimento profissional em relação às americanas, inclusive. Por que elas escolherão sua empresa Com uma visão muito clara sobre a importância da experiência internacional, 71% das entrevistadas pela PwC gostariam de trabalhar fora do país. Portanto, oferecer a possibilidade de viagem profissional pode ser essencial para a contratação de uma millenial. Os resultados da pesquisa levaram a mais uma conclusão: elas verbalizam claramente que querem um trabalho que tenha propósito, querem contribuir de alguma maneira para o mundo e ter orgulho de seu empregador. Reputação, portanto, influencia bastante em suas decisões. As empresas devem se comunicar melhor e ser mais transparentes para atrair esses talentos. Para Juliana Albanez , coach especialista em comportamento e liderança feminina, as millenials ""são mulheres criativas e com propósito de vida. A vida profissional tem que fazer sentido, o trabalho tem que oferecer prazer e elas fazem questão do reconhecimento. São mulheres que farão história na luta pelo equilíbrio entre vida profissional e pessoal. E isso é só uma questão de tempo"". Outros aspectos que essas profissionais almejam são horário flexível, autonomia, boa remuneração e oportunidade de crescimento. Para o psicanalista e especialista em carreiras e gestão de pessoas, Paulo Paiva ""o modelo com rigidez de horário, deslocamento e processos está ficando obsoleto para as mulheres da nossa e próximas gerações. A questão do horário flexível está se tornando uma necessidade, não é mais um diferencial das empresas"". Para corrigir o défict de liderança de gênero, é preciso conduzir esforços paralelos: inserir diversidade na liderança em conjunto com mudanças na cultura organizacional. Inclusive, se interessar, fizemos um guia da cultura corporativa para a igualdade de gênero . E, para alcançar um resultado positivo, as organizações devem compreender quais são as expectativas de carreira dessas jovens profissionais para não só atrair, como desenvolver, contratar e reter o talento dessa geração. E ainda que o caminho para a igualdade de gêneros seja longo, as mulheres da Geração Y não encontram mais o mesmo ambiente profissional do qual fizeram parte suas mães e avós. Mais qualificadas e confiantes, elas impulsionam e fortalecem a demanda por novas estratégias de mercado e mudanças culturais em ambientes considerados engessados. Para manter a competitividade num mercado de constantes e rápidas mudanças, a adaptação das empresas acaba sendo inevitável. E essa já pode ser considerada uma grande vitória das millennials. As mulheres da Geração Y são multifuncionais, se preocupam em gerenciar o tempo da melhor forma e alcançar resultados consistentes. Conheça o Runrun.it , gerenciador de projetos e tarefas, que irá ajudar nessa tarefa, além de tornar a relação de trabalho e a comunicação muito mais transparentes. Experimente Grátis:",pt,134
49,887,1462920714,CONTENT SHARED,2072448887839540892,-3390049372067052505,-5764325092046707218,,,,HTML,https://medium.com/enrique-dans/welcome-to-googlebank-facebook-bank-amazon-bank-and-apple-bank-c9c3955006d4,"welcome to googlebank, facebook bank, amazon bank, and apple bank - enrique dans","Welcome to GoogleBank, Facebook Bank, Amazon Bank, and Apple Bank How would you like to bank with Apple, Google, Amazon, or Facebook? This is the question Fujitsu has just asked a sample of some 7,000 people throughout Europe : around a fifth said they were up for it. As the fintech environment garners an increasing number of experiences that tend to take place outside banking or insurance, and while the big tech companies push into financial products through payment initiatives, the idea seems to be gaining traction with more and more people . What is a bank anyway? If most of the business these days is mainly about moving bits around data bases, it's pretty clear that the big tech companies are going to have more expertise than banks that don't see technology as part of their business and tend to subcontract it out to the big consultancies or to simply continue doing thing like they did in the 1950s. Most banks are way behind the tech companies when it comes to dealing with clients, when it comes to their internet systems, and most definitely when it comes to analytics, as well as the assets they hold. Banks are generally still relatively small setups, less developed, and rooted in obsolete working practices. This is reflected in key areas such as risk analysis, which lags behind the big tech companies' use of machine learning, which in the case of outfits like Google are considered the basis for future growth . Until now, the tech sector has thought that it wasn't necessary to own a bank to disrupt the financial sector . But the signs are that some tech companies are increasingly taking on the role of banks . We can easily imagine a future in which these well-capitalized outfits realize that a banking license isn't such a big deal when it comes to building a sustainable competitive advantage and that they could integrate these services in the future if people accept the idea. The pundits have been asking for some time which sector Apple will disrupt next . The company would have no problem finding the money to meet the legal requirements, and with its history of cross selling, its network of shops and its client base, it would have few problems in becoming a powerful and profitable bank. What's more, the smartphone and companies like Venmo , Square , PayPal , Snapcash and many others, have pretty much put paid to the cheque book and the credit card. What's more, their target audience is the millennial generation born between 1981 and 1997 , is already the majority, and have no problems in accepting disruption to the financial sector. Let's be honest: the traditional banks do not enjoy sufficient loyalty from their customers that they could stand up to competition from new entrants from the technology sector. Could internet end up replacing the traditional banks ? For the moment, the most proactive are buying startups and companies with interesting business models , or those who are able to fit into their traditional ways of doing things , but... will such initiatives be enough to hold onto their places in an industry in which major disruption is coming?",en,134
50,2985,1484911446,CONTENT SHARED,-7113155163062752691,1374824663945909617,7124692265469031695,"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.100 Safari/537.36",SP,BR,HTML,http://sensedia.com/blog/apis/arquitetura-de-microservicos-habilitando-apis/,arquitetura de microserviços habilitando apis - sensedia,"Esse post foi realizado pelo nosso consultor Rafael Rocha, e adaptado para o nosso blog, caso queira acessar o original em inglês, basta acessar este link . Contexto Hoje em dia a arquitetura de Microserviços se tornou dentro do segmento de desenvolvimento de software. Esse modelo arquitetural é uma abordagem que prega pela decomposição de aplicações em uma gama de serviços. O time de desenvolvimento pode adotar as tecnologias mais apropriadas para resolver problemas específicos . A arquitetura de Microserviços também aumenta a escalabilidade por possibilitar o uso de abordagens como auto-scaling e micro-container . APIs e microserviços estão muito relacionados já que o padrão é utilizar uma interface de APIs que é totalmente voltada a RESTful. Porém, hoje em dia nós ainda sofremos alguns problemas quando precisamos integrar com sistemas legados para externalizar funcionalidades como serviços, uma vez que a maioria dos sistemas não possuem protocolos como WebServices ou interfaces RESTful, então vamos explorar esse tema. O Problema e a abordagem de Microserviços A maioria das companhias querem expor APIs de forma interna (dentro da corporação) e/ou externa (para parceiros ou clientes), no entanto, os seus sistemas ou aplicações não foram contruídas para este propósito. A maioria das aplicações são baseadas na seguinte arquitetura: Aplicações monolíticas para web usando um banco de dados único. Ex: Java (JSF) com banco de dados Oracle. Produto ou plataforma como por exemplo ERP's da SAP. Aplicações de alto nível em escritas em Cobol Aplicações Cliente-Servidor implementados em VB6 e SQL Server. Quando nos deparamos com esse tipo de cenário, a solução comum é construir um adaptador para a exposição de um protocolo padrão. Esse componente deve se parecer com o diagrama abaixo: O adaptador é o componente principal para a solução já que é isso que vai possibilitar a externalização do serviço. Para promover essa padronização, alguns padrões devem ser utilizados: Compatibilidade total com padrão RESTful Organização por domínios de negócio Ser facilmente escalável Pacotes leves e com inicialização rápida. Para ter uma aplicação com um modelo de integração simples e escalável é necessário possuir um estilo arquitetural que cumpra com todos esses requisitos, e o estilo arquitetural que mais se aproxima dessas característica é o estilo arquitetural de Microserviços. Lembre-se que essa recomendação se aplicada quando o seu backend não é exposto com o protocolo HTTP, porque a maioria das soluções de API Gateways irá conseguir rotear e transformar facilmente as mensagens para comunicação com o backend. Outros cenários que a arquitetura de Microserviços é muito recomendada são: Orquestração: em alguns casos é necessário rotear e/ou chamar outros serviços dependendo de alguma condição específica. Composição: algumas vezes é preciso chamar mais de um serviço para compor e devolver uma resposta. Transformação de Mensagens Complexas: quando é necessário utilizar de algoritmos mais complexos para conseguir devolver uma mensagem para o consumidor do serviço. Por fim, repare que os microserviços precisam ser organizados dentro dos domínios do seu negócio. Desta maneira, podemos ver que esse estilo arquitetural é uma oportunidade de quebrar os monolíticos de seu negócio e prover uma melhor arquitetura, como o desenho abaixo: Um artigo muito interessante de Microserviços é esse . Estratégia de Implementação de Microserviços Uma vez decidido que a implementação do adaptador será baseada em uma arquitetura de Microserviços, algumas características são necessárias: Pacotes leves e baixo consumo de memória A inicialização da aplicação deve ser rápida para criar rapidamente em novas instâncias de containers ou servidores. Desenvolvimento rápido e simples baseado na especificação Swagger Features de segurança como OAuth2 Alguns dos frameworks que indicamos são: JavaScript: Outra característica crucial quando quando implementamos APIs usando Microserviços é a capacidade de integração com sistemas legados. Esse tipo de feature requer um framework específico que implementa os padrões de integração empresarial (EAI patterns), a recomendação nesse caso é utilizar o framework Java Apache Camel . Estratégia de Implantação de Microserviços Uma vez que o pacote com Microserviços está implementado e pronto, ele precisa ser implantado para estar disponível para o uso. A estratégia mais recomendada é utilizar de serviços PaaS (Plataforma como serviço) para implantar os pacotes de Microserviços. Isto porque este tipo de serviço oferece algumas features muito interessantes como: Uso de containers Orquestração de containers Armazenamento (sistema de arquivos e banco de dados) Monitoramento em tempo real Logging e rastreamento Outras duas características importantes são: Ser capaz de escalar para suportar o tráfego Oferecer APIs para automatizar o processo de implantação As principais ofertas de PaaS do mercado devem ser avaliadas para uma estratégia de implantação, e são elas: Outras opções para serem consideradas são a Amazon Elastic Beanstalk e Google App Engine. Os dois são muito interessantes porque possuem uma integração nativa com os serviços de Cloud e Infraestrutura já que também oferecem os serviços de infraestrutura como serviço (IaaS). Porém na nossa visão, a melhor alternativa para implantação de microserviços são aquelas que provém uma integração completa com uma plataforma de gestão APIs, e nesse caso o Sensedia API Suite oferece uma funcionalidade chamada BaaS (Backend as a Service) que utiliza das mesmas características de serviços PaaS e realiza esta integração. Essa funcionalidade permite que você faça a implantação e rode os seus microserviços, já expondo diretamente as APIs dos seus sistemas legados. As tecnologias suportadas pelo BaaS são: Vale lembrar que se você pode utilizar desta plataforma para rodar seus microserviços que não seja somente estes que habilitam a integração com legados, poderá ser sua plataforma oficial de execução de microserviços. Microserviços e Plataformas de API Management Uma vez que os Microserviços estão rodando corretamente, as APIs que expõem os Microserviços devem ser bem gerenciadas e algumas das features essenciais para isso, que a maioria das plataformas deste tipo oferecem são: Segurança e Resiliências : necessário para proteger o seu backend de pessoas ou aplicações não habilitadas para consumir essas APIs. Quando uma API é aberta ou para parceiros, os seus Microserviços devem estar protegidos contra os picos de tráfego, para que não fique fora de serviço e para isso é necessário ter controle de limite de tráfego (Rate Limit e Spike Arrest) e limite de tamanho do corpo da mensagem (Payload Size). Controle de Acesso: os consumidores da API devem estar sobre o seu controle, para isso é necessário a utilização de padrões de mercado como OAuth 2.0 ou JSON Web Tokens. Monitoramento e Rastreamento: é necessário conseguir monitorar todos os tipos de chamada realizadas na sua plataforma, além disso é necessário ter um mecanismo de log poderoso para encontrar os erros que vêm acontecendo na sua API. Todas as capacidades listadas acima são comuns em uma solução de API Gateway, porém algumas outras features cruciais para o gerenciamento completo das suas APIs, são elas: Caching: deve ser capaz de evitar chamadas desnecessárias em sua API, provendo uma latência muito melhor para as suas chamadas e economizando até mesmo o custo de sua infraestrutura de backend. Analytics: o uso da API monitorado em tempo real é de extrema importância, tanto para acompanhar o consumo e até mesmo para ter insights de como vender, monetizar e utilizar da melhor forma a sua API. Como foi mencionado antes, algumas Plataformas de Gerenciamento de APIs oferecem uma integração total com a plataforma de execução de microserviços. Esse tipo de funcionalidade oferece um gerenciamento total de todas as partes da solução, não sendo necessário uma infra separada. Sendo assim, a sua arquitetura ficará como a imagem abaixo: Conclusão Utilizar arquitetura de Microserviços possibilita o desenvolvimento de interfaces RESTful que irão expor o seu legado que nativamente não possui uma interface HTTP, porém o primeiro desafio é escolher as ferramentas corretas essa implementação. Existem muitos frameworks e linguagens que podem ajudar na implementação de microserviços, a decisão depende muito do cenário que está se enfrentando, porém algumas das mais utilizadas são citadas neste artigo. Depois de escolher seu kit de desenvolvimento, a próxima decisão a ser tomada é estabelecer a plataforma de implantação e execução, mais uma vez, a decisão depende muito do cenário sendo encarado, porém neste caso, o principal objetivo é a exposição de RESTful APIs de forma consistente que atendam aos requisitos funcionais e não-funcionais. O Sensedia API Suite é uma ferramenta de gerenciamento de APIs que consegue prover a funcionalidade de Backend as Services (BaaS) que podem substituir as responsabilidades de um PaaS na implantação e execução dos Microserviços. Além disso, a ferramenta consegue prover todas as funcionalidades chave para o melhor gerenciamento de uma API, como por exemplo API Gateway com Caching e Analytics. Em resumo, a recomendação é usar uma ou mais plataformas integradas que consiga lhe entregar o gerenciamento total de seus Microserviços e bem como das APIs que eles irão expor. Referências The following two tabs change content below.",pt,131
51,2242,1472739274,CONTENT SHARED,-615912190028612956,6013226412048763966,1930561315994163469,,,,HTML,http://vocesa.uol.com.br/noticias/acervo/coragem-e-o-principal-requisito-para-ser-um-bom-lider-diz-o-autor-britanico-simon-sinek.phtml,"ser um bom líder depende de inúmeros fatores. mas um deles, segundo o inglês simon sinek, é importantíssimo: a coragem para proteger as equipes e para arriscar quando necessário","Crédito: Divulgação"" title=""""Nós seguimos os bons líderes não porque precisamos mas porque queremos"", diz o inglês Simon Sinek | Crédito: Divulgação""> ""Nós seguimos os bons líderes não porque precisamos mas porque queremos"", diz o inglês Simon Sinek | Crédito: Divulgação Qual o principal requisito de uma liderança inspiradora? Essa é uma pergunta que gera muitas respostas. E o inglês Simon Sinek, especialista em gestão, encontrou um adjetivo satisfatório: coragem. Para ele, o que faz com que os profissionais sejam vistos por suas equipes como excelentes líderes é a capacidade de proteger as equipes e a ousadia para arriscar. Simon está lançando no Brasil o livro Líderes se Servem Por Último (44,90 reais, HSM) - e a metáfora do título explica exatamente o que ele acredita ser o grande papel de um bom gestor de pessoas: desenvolver os times, fazendo com que os subordinados sejam os primeiros a ""se alimentar"". Só assim as equipes conseguem ter força para produzir e motivação para trabalhar. VOCÊ S/A - A sua teoria de liderança diz que grandes líderes colocam os interesses dos times em primeiro lugar e ""comem por último"". Por que essa atitude é importante? SIMON- A metáfora ""os líderes comem por último"" se relaciona com a dos pais, que sempre alimentam as crianças primeiro - não porque leram isso em um livro, não porque foram ensinadas, simplesmente porque usam o instinto paterno ou materno. Esse instinto de cuidar de pessoas que dependem de você pode ser aplicado à liderança. Liderar não é sobre estar no comando. É sobre cuidar das pessoas que estão sob o seu comando. Essa é a verdadeira liderança. Os líderes que comem por último até poderiam comer primeiro, pois o cargo permite. Eles poderiam, também, conseguir regalias e vantagens por conta da posição que ocupam. Mas não fazem isso. Os verdadeiros líderes preferem sacrificar seus interesses para cuidar bem das vidas das pessoas que fazem parte de sua equipe - e nunca sacrificam a vida do time para cuidar de seus próprios interesses. Eles escolhem comer por último por uma questão de honestidade e lealdade. Os líderes são aqueles que protegem. E quando as pessoas se sentem seguras, elas transmitem essa segurança para os colegas, clientes e consumidores. VOCÊ S/A - Seu livro mostra a evolução da liderança da era Paleolítica até os dias de hoje. Como eram os líderes do tempo das cavernas? SIMON- O que faz com que alguém seja um líder na era Paleolítica e hoje é exatamente a mesma coisa. O que evolui são as condições em que operamos. No paleolítico, vivíamos em cavernas em tribos que tinham entre 100 e 150 pessoas. Nossos líderes eram homens fortes e grandes - e havia os homens-alfa. Naquela época, a questão era sobreviver. Os homens mais fortes, os ""alfas"", eram capazes de suportar cargas pesadas, encontrar alimentos e proteger a tribo do perigo. Por conta disso, eles tinham o privilégio de possuir a primeira escolha de carne da tribo (para se sentirem mais fortes e continuar caçando) e a primeira escolha da companheira que gostariam de ter (para garantir que os melhores genes fossem perpetuados). VOCÊ S/A - Eles tinham esses privilégios por que cuidavam bem da tribo? SIMON - Havia um código antropológico do que significava liderar no Paleolítico, que é o mesmo até hoje: os liderados permitem um tratamento preferencial para o líder. Mas isso tem um custo, o líder tem que agir de acordo com o interesse coletivo. Se os liderados dão ao líder todos os benefícios e ele não os protege do perigo, não é um líder de verdade. A liderança é um serviço. Serviço vem com sacrifício. Se não há sacrifício, não há serviço, nem liderança. VOCÊ S/A - Quais características transformam alguém em um líder excepcional? SIMON - O requisito para ser um líder não é ter visão ou carisma. É ter coragem. Liderar significa que temos que dar o primeiro passo, que temos que colocar a corda no pescoço para defender aquilo que acreditamos. Todos os bons líderes são corajosos e coragem não é algo que, miraculosamente, surge dentro de você. Nossa coragem vem da coragem dos outros, daqueles que fizeram algo antes de nós e que nos olham nos olhos e dizem: ""Eu acredito em você. Você consegue fazer isso"". Características como egocentrismo e ganância acabam com o significado da palavra liderar. A liderança é sobre cuidar das pessoas pelas quais você é responsável. Aqueles que se esquecem disso não são líderes, são simplesmente autoridades que mandam usando ferramentas como medo ou a necessidade que o empregado tem de manter aquele trabalho. Mas nós não seguimos esses profissionais. Nós seguimos os bons líderes não porque precisamos, mas porque queremos. VOCÊ S/A - Quais as diferenças entre gerentes e líderes? SIMON- Gerentes se preocupam com melhorias. Líderes se preocupam com saltos à frente. Gerentes se concentram em sistemas, métricas, processos e resultados. Líderes mantém o foco na percepção de como as ações do time influenciam nos resultados. Gerentes olham para os números. Líderes olham para o ""nós"". Todos os gerentes de métricas têm a oportunidade de se tornar líderes de pessoas. VOCÊ S/A - O Brasil está enfrentando uma crise econômica e, ao mesmo tempo, uma crise de liderança institucional. Nas empresas, o que os líderes precisam fazer para combater a descrença de seus times? SIMON- Se tornar líderes melhores. Liderança não é uma campanha de marketing. Não se trata de convencer as pessoas, mas de cuidar das pessoas. Liderança não é sobre enviar uma mensagem. É sobre a mensagem em si. Pense no Dr. Martin Luther King: 250 000 pessoas foram ouvi-lo falar de perto. Não houve lembretes por e-mail, campanha no Facebook ou hashtags para atrair essa multidão. Todos eles se reuniram porque tinham algo em comum - acreditar em uma América em que as leis deveriam ser iguais para todos, independentemente de cor da pele, religião ou classe econômica. Dr. King foi capaz de criar um movimento forte em um momento de crise porque não ficou parado esperando por um futuro melhor. Ele não fez o discurso do ""Eu tenho um plano"". Ele fez o discurso do ""Eu tenho um sonho"" e transformou em palavras o que ansiava. Com isso, Dr. King nos deu um mundo do qual poderíamos fazer parte e no qual poderíamos contribuir. Ele não quis tentar provar como poderia resolver tudo sozinho. Ele criou as condições (e o desejo) para que nós fizéssemos isso juntos, ao lado dele.",pt,130
52,2556,1476360149,CONTENT SHARED,-3040610224044779845,801895594717772308,-6061043432083340716,"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.143 Safari/537.36",MG,BR,HTML,https://medium.freecodecamp.com/10-tips-to-maximize-your-javascript-debugging-experience-b69a75859329?gi=581529e210bf,things you probably didn't know you could do with chrome's developer console,"Chrome comes with built-in developer tools. This comes with a wide variety of features, such as Elements, Network, and Security. Today, we'll focus 100% on its JavaScript console. When I started coding, I only used the JavaScript console for logging values like responses from the server, or the value of variables. But over time, and with the help of tutorials, I discovered that the console can do way more than I ever imagined. Here are useful things you can do with it. If you're reading this in Chrome (Or any other Browser) on a desktop, you can even open up its Developer Tools and try these out immediately. 1. Select DOM Elements If you're familiar with jQuery, you know how important the $('.class') and $('#id') selectors are. They select the DOM elements depending upon the class or ID associated with them. But when you don't have access to jQuery in the DOM, you can still do the same in the developer console. $('tagName') $('.class') $('#id') and $('.class #id') are equivalent to the document.querySelector(' '). This returns the first element in the DOM that matches the selector. You can use $$('tagName') or $$('.class') - note the double dollar signs - to select all the elements of the DOM depending on a particular selector. This also puts them into an array. You can again go ahead and select a particular element among them by specifying the position of that element in the array. For example, $$('.className') will give you all the elements that have the class className, and $$('.className')[0]and $$('.className')[1] will give you the first and the second element respectively. 2. Convert Your Browser Into An Editor How many times have you wondered whether you could edit some text in the browser itself? The answer is yes, you can convert your browser into a text editor. You can add text to and remove text from anywhere in the DOM. You don't have to inspect the element and edit the HTML anymore. Instead, go into the developer console and type the following: This will make the content editable. You can now edit almost anything and everything in the DOM. 3. Find Events Associated with an Element in the DOM While debugging, you must be interested in finding the event listeners bound to an element in the DOM. The developer console makes it easier to find these. getEventListeners($('selector')) returns an array of objectsthat contains all the events bound to that element. You can expand the objectto view the events: To find the Listener for a particular event, you can do something like this: This will display the listenerassociated with a particular event. Here eventName[0] is an array that lists all the events of a particular event. For example: ...will display the listener associated with the click event of element with ID 'firstName'. 4. Monitor Events If you want to monitor the events bound to a particular element in the DOM while they are executed, you can this in the console as well. There are different commands you can use to monitor some or all of these events: monitorEvents($('selector')) will monitor all the events associated with the element with your selector, then log them in the console as soon as they're fired. For example, monitorEvents($('#firstName')) will log all the events bound to the element with the ID of 'firstName' . monitorEvents($('selector'),'eventName') will log a particular event bound with an element. You can pass the event name as an argument to the function. This will log only a particular event bound to a particular element. For example, monitorEvents($('#firstName'),'click') will log all the click events bound to the element with the ID 'firstName'. monitorEvents($('selector'),['eventName1','eventName3',....]) will log multiple events depending upon your own requirements. Instead of passing a single event name as an argument, pass an array of strings that contains all the events. For example monitorEvents($('#firstName'),['click','focus']) will log the click event and focus events bound to the element with the ID 'firstName' . unmonitorEvents($('selector')) : This will stop monitoring and logging the events in the console. 5. Find the Time Of Execution of a Code Block The JavaScript console has an essential function called console.time('labelName') which takes a label name as an argument, then starts the timer. There's another essential function called console.timeEnd('labelName') which also takes a label name and ends the timer associated with that particular label. For example: The above two lines of code give us the time taken from starting the timer to ending the timer. We can enhance this to calculate the time taken for executing a block of code. For example, let's say I want to find the time taken for the execution of a loop. I can do like this: 6. Arrange the Values of a Variable into a Table Let's say we have an array of objects that looks like the following: When we type the variable name into the console, it gives us the values in the form of an array of objects. This is very helpful. You can expand the objects and see the values. But this gets difficult to understand when the properties increase. Therefore, to get a clear representation of the variable, we can display them in a table. console.table(variableName) represents the variable and all its properties in a tabular structure. Here's what this looks like: 7. Inspect an Element in the DOM You can directly inspect an element from the console: inspect($('selector')) will inspect the element that matches the selector and take you to the Elements tab in the Chrome Developer Tools. For example inspect($('#firstName')) will inspect the element with the ID 'firstName' and inspect($('a')[3]) will inspect the 4th anchor element you have in your DOM. $0, $1, $2, etc. can help you grab the recently inspected elements. For example $0 gives you the last-inspected DOM element, whereas $1 gives you the second last inspected DOM Element. 8. List the Properties of an Element If you want to list all the properties of an element, you can do that directly from the Console. dir($('selector')) returns an object with all of the properties associated with its DOM element. You can expand these to view them in more detail. 9. Retrieve the Value of your Last Result You can use the console as a calculator. And when you do this, you may need follow up one calculation with a second one. Here's how to retrieve the result of a previous calculation from memory: Here's what this looks like: 10. Clear the Console and the Memory If you want to clear the console and its memory, just type: Then press enter. That's all there is to it.",en,130
53,1539,1467026915,CONTENT SHARED,4084131344684656470,5127372011815639401,5134361217529549945,,,,HTML,https://deis.com/blog/2016/kubernetes-illustrated-guide/,the children's illustrated guide to kubernetes,"Kubernetes, Book Introducing Phippy, an intrepid little PHP app, and her journey to Kubernetes. What is this? Well, I wrote a book that explains Kubernetes. We posted a video version to the Kubernetes community blog. If you find us at a conference, you stand a chance to pick up a physical copy. But for now, here's a blog post version! And after you've finished reading, tweet something at @opendeis for a chance to win a squishy little Phippy toy of your own. Not sure what to tweet? Why don't you tell us about yourself and how you use Kubernetes! The Other Day... The other day, my daughter sidled into my office, and asked me, ""Dearest Father, whose knowledge is incomparable, what is Kubernetes?"" Alright, that's a little bit of a paraphrase, but you get the idea. And I responded, ""Kubernetes is an open source orchestration system for Docker containers. It handles scheduling onto nodes in a compute cluster and actively manages workloads to ensure that their state matches the users' declared intentions. Using the concepts of ""labels"" and ""pods"", it groups the container which make up an application into logical units for easy management and discovery."" And my daughter said to me, ""Huh?"" And so I give you... The Children's Illustrated Guide to Kubernetes Once upon a time there was an app named Phippy. And she was a simple app. She was written in PHP and had just one page. She lived on a hosting provider and she shared her environment with scary other apps that she didn't know and didn't care to associate with. She wished she had her own environment: just her and a webserver she could call home. An app has an environment that it relies upon to run. For a PHP app, that environment might include a webserver, a readable file system, and the PHP engine itself. One day, a kindly whale came along. He suggested that little Phippy might be happier living in a container. And so the app moved. And the container was nice, but... It was a little bit like having a fancy living room floating in the middle of the ocean. A container provides an isolated environment in which an app, together with its environment, can run. But those isolated containers often need to be managed and connected to the external world. Shared file systems, networking, scheduling, load balancing, and distribution are all challenges. The whale shrugged his shoulders. ""Sorry, kid,"" he said, and disappeared beneath the ocean's surface. But before Phippy could even begin to despair, a captain appeared on the horizon, piloting a gigantic ship. The ship was made of dozens of rafts all lashed together, but from the outside, it looked like one giant ship. ""Hello there, friend PHP app. My name is Captain Kube"" said the wise old captain. ""Kubernetes"" is the Greek word for a ship's captain. We get the words Cybernetic and Gubernatorial from it. Led by Google, the Kubernetes project focuses on building a robust platform for running thousands of containers in production. ""I'm Phippy,"" said the little app. ""Nice to make your acquaintance,"" said the Captain as he slapped a name tag on her. Kubernetes uses labels as ""nametags"" to identify things. And it can query based on these labels. Labels are open-ended: You can use them to indicate roles, stability, or other important attributes. Captain Kube suggested that the app might like to move her container to a pod on board the ship. Phippy happily moved her container inside of the pod aboard Kube's giant ship. It felt like home. In Kubernetes, a Pod represents a runnable unit of work. Usually, you will run a single container inside of a Pod. But for cases where a few containers are tightly coupled, you may opt to run more than one container inside of the same Pod. Kubernetes takes on the work of connecting your pod to the network and the rest of the Kubernetes environment. Phippy had some unusual interests. She was really into genetics and sheep. And so she asked the captain, ""What if I want to clone myself... On demand... Any number of times?"" ""That's easy,"" said the captain. And he introduced her to the replication controllers. Replication controllers provide a method for managing an arbitrary number of pods. A replication controller contains a pod template, which can be replicated any number of times. Through the replication controller, Kubernetes will manage your pods' lifecycle, including scaling up and down, rolling deployments, and monitoring. For many days and nights the little app was happy with her pod and happy with her replicas. But only having yourself for company is not all it's cracked up to be.... even if it is N copies of yourself. Captain Kube smiled benevolently, ""I have just the thing."" No sooner had he spoken than a tunnel opened between Phippy's replication controller and the rest of the ship. With a hearty laugh, Captain Kube said, ""Even when your clones come and go, this tunnel will stay here so you can discover other pods, and they can discover you!"" A service tells the rest of the Kubernetes environment (including other pods and replication controllers) what services your application provides. While pods come and go, the service IP addresses and ports remain the same. And other applications can find your service through Kurbenetes service discovery. Thanks to the services, Phippy began to explore the rest of the ship. It wasn't long before Phippy met Goldie. And they became the best of friends. One day, Goldie did something extraordinary. She gave Phippy a present. Phippy took one look and the saddest of sad tears escaped her eye. ""Why are you so sad?"" asked Goldie. ""I love the present, but I have nowhere to put it!"" sniffled Phippy. But Goldie knew what to do. ""Why not put it in a volume?"" A volume represents a location where containers can access and store information. For the application, the volume appears as part of the local filesystem. But volumes may be backed by local storage, Ceph, Gluster, Elastic Block Storage, and a number of other storage backends. Phippy loved life aboard Captain Kube's ship and she enjoyed the company of her new friends (every replicated pod of Goldie was equally delightful). But as she thought back to her days on the scary hosted provider, she began to wonder if perhaps she could also have a little privacy. ""It sounds like what you need,"" said Captain Kube, ""is a namespace."" A namespace functions as a grouping mechanism inside of Kubernetes. Services, pods, replication controllers, and volumes can easily cooperate within a namespace, but the namespace provides a degree of isolation from the other parts of the cluster. Together with her new friends, Phippy sailed the seas on Captain Kube's great boat. She had many grand adventures, but most importantly, Phippy had found her home. And so Phippy lived happily ever after.",en,128
54,2316,1473686450,CONTENT SHARED,-3716447017462787559,-1578287561410088674,1736179400644080759,,,,HTML,http://hipsters.tech/squads-nao-suicidas-hipsters-08/,squads não-suicidas - hipsters #08,"Como as equipes são divididas em uma empresa? Funciona bem? Eles tem o mesmo objetivo ou a mesma função? Nosso host Paulo Silveira conversa sobre mais um assunto da moda: Squads . Os esquadrões são uma forma que o Spotify encontrou para escalar Agile em empresas maiores, fazendo com que os times foquem em um objetivo comum e tenham autonomia... hummm ou algo assim. Muitas empresas abraçaram a técnica, com maior e menor sucesso. Vamos conversar sobre essa cultura e as dificuldades no processo de adoção. Participantes: Paulo Silveira , host do hipsters e gerente de produtos na Alura Pedro Axelrud , cuspidor de fogo e gerente de produtos no Nubank Gabriela Rojas , gerente de produtos no Nubank Marcell Almeida , gerente de produtos no Vivareal Pois é, não faltava gerente de produtos e startupeiro na sala! Links que apareceram: Produção e conteúdo: Alura Cursos online de Tecnologia Caelum Ensino e Inovação Edição e sonorização: Radiofobia Podcast e Multimídia",pt,128
55,3060,1486469778,CONTENT SHARED,-14569272361926584,-8781306637602263252,9157733567505152711,"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36",SP,BR,HTML,http://guirmendes.blogspot.com.br/2017/02/java-8-streams-deeper-approach-about.html,java 8 streams - a deeper approach about performance improvement,"Introduction Java 8 was released almost three years ago, but it still lacks articles with deeper approach through Stream API. There are some good articles about it, but not a single one showing a real world example and comparing its performance against Java 7 style of coding. This article assumes that the reader already has some knowledge about Stream API, so many simple code will not be explained in here. To start learning about Stream API, I suggest this article , from Benjamin Winterberg. This article shows a real world alike example, with several different approach of coding, always comparing performance against Java 7. It is result of a study over Stream API's performance when dealing with file processing. The main goal is to use as many Streams and Lambdas as possible into Java 8's code and not to use a single line of Java's new features into Java 7's code, in order to learn how to use Java 8's new features. This project was my first contact with Java 8 at all and I will show how the application evolved to current status. Be ready to see some bad code as well! The text is divided in sessions and pretty much follows time line from the project's development. All benchmark results are posted at the end and previous sessions talk about how I implemented and improved the project. Please note that the article is long, so please be patient and reserve some time to learn where I made mistakes and maybe to help me find any error I may have implemented in the code. Input Data As input data for processing, I used a Brazilian government data file, known as Sigtap (can be found here ). It is a zip archive, containing lots of text files, divided into two groups: layout files and data files. Data files contains positional based information, where each line represents a database register. Layout files contains information about each position in data files. Those files looks very alike a database export, which is the way we'll treat them in this article. It is important to note that there is a layout.txt file containing all other layout files' information. Let's call this file the general layout. It'll be used later in this article. There is an example data into Resources' folder inside the project. Please take a look into those files to better understanding about the implementation decisions to write the code. The Project The objective is to convert the input data files into SQL inserts. The inserts are not printed or saved anywhere, just generated in memory, because of volume of data. It uses only Java default libraries to process everything, except the benchmarks itself. For that, the JMH library was used. Because of JMH, the jar of the project excpects a JDK argument called path to run properly. This argument is the path to Sigtap file's extraction folder, for example: '/path/to/Stream_Study/src/main/resources/Sigtap/'. Development Phase - Java 8 In this session, I'll present how the code was written, using some subsessions to make reading easily. Although it's not strictly necessary, it's recommended to read the next subsessions with this source file for consulting. Reading files The project started with Java 8 implementation. The first surprise was newer Files API. One code line to read all file's lines is impressive! Check out the code inside Image 1. Please nothe that Paths.get may expect the file's encoding. It's important to inform in this case because the input data is encoded with ISO 8859-1. Also note that exceptions are ignored because of example purposes. In this case, an empty List will be returned to prevent NullPointerException to happen. Processing files With this very nice start, let's start file processing. The strategy is read the general layout file to detect all tables and its columns. So, let's use a Map, where the key is the table's name and the value is the table's columns, as a List of String. This strategy brings a easy way to work with layout's information and to access desired data from data files. First nightmare: how to perform this conversion using Stream API? None of default options could do the job. So, let's Google it a bit. After Googling, I came to this StackOverflow . Its accepted answer does the job, but hey! What an Alien code! At this point, I was not ready to understand such a complex code. No problem understanding that it was the implementation of a custom Collector, but the Collector code itself. Note that at this moment, I inserted lots of waste into code. The strategy is to split the original file's list into sublists (Image 2). The sep argument is String::isEmpty , because I want to split my list when I find an empty line. The custom Collector receives three arguments: a Supplier, an Accumulator and a Combiner. The Supplier object will be returned after a call to Stream's collect using the custom Collector. The Accumulator function (actually a BiConsumer) will be called for each element of the stream and creates an output element. The Collector function (a BinaryOperator) will only be called if this Collector is used into a ParallelStream and is used to combine multiple results from Accumulator into one only result. Note that for serial Stream this function will never be called. The above code is not ready to ParallelStream, but in this case there is no difference, because we need the list to be processed serially, otherwise we won't get the expected result. After splitting the list into a List<List<T>>, it is time to convert it into a Map<T, List<T>>, as desired. Let's take a look into Image 3's code. The list argument is the original file's List of lines and the sep argument is String::isEmpty , because I want to split my list when I find an empty line. After calling the collect using the above splitBySeparator Collector, a new Stream is started, filtering empty lists (there are empty lists at this point) and collecting to a Map using the default Collector implementation that generates a Map. Note that this code is generic enough to receive any List and any Predicate that it'll work. Just note that the first sublist's element will be used as the Map's key, so some adjust may be needed at this point. Save this comment for Java 7's implementation. Validating Although it is an example project, it follows real world applications, so it needs some kind of data validation. In this case I choosed to simply compare if general layout and table's specific layout has same content. To do so, the simplest way is to convert the List of columns into a single String taking care of using the same sorting for both files. The Map.Entry in above code is an entry from the Map of general layout. The key is the table's name and the value is the List of this table's columns (with extra column processing information). This snippet just starts a Stream from each List of columns, sorts it and collect to String, using default joining Collector. Then, it throws an Exception if both are not the same. Please note the call of readFile method. It guarantees the equals' argument is from another file. Generating SQL inserts The next step is generate the SQL inserts desired. This is the core of processing in this project. At this point the data files are processed using the positional information contained into layout files. The Map.Entry in Image 5's code is an entry from the Map of general layout. The key is the table name and the value is the List of this table's columns (with extra column processing information). The first thing to do is validate the entry using the Image 4's validate method. Once it's validated, it's important to remove some header information from file, to prevent misbehaviour of the application. Some extra information about the layout now became important: this file's columns can be splitted using a comma, here convenientely replaced by SEPARATOR constant. After splitting the layout, there are only three information positions interesting for this project: the index 0, containing the column name; the index 2, which informs the position to start reading data in data file; and the index 3, indicating the position to stop reading data in data file. Next, it's the moment to get a list of each column's name. Getting a Stream from layout's List, I mapped each splitted layout line to its 0 index, then sorted the result and then collected using default Collector's toList method. After extracting column information, it's time to iterate over data file's content. Note that this is the first time I used a parallelStream in entire code so far. I used it here because this is the first point where there is no harm using parallelism and benchmarks showed parallelStream is faster at this point. And it is a major tip : test your own case to see if parallelStream is the best option. I used a forEach loop here to populate a List of Map to represent each line of the file. The Map is a key value representation of data, where the key is the column's name and value is the content of this column in this line of the file. Note that for each file line there is lots of columns so I used a new forEach inside the first, this time into layout's content, to convert each line into the Map I wanted. At this point, the processedDataList variable is a List of each data file's lines converted into a map of columns to its data. Now I used another parallelStream over processedDataList to in fact generate the insert. After removing null elements, I map the line to the SQL insert text and then collect it to a List, using default toList Collector. To produce the SQL insert, I used code shown in Image 6. There is a StringBuffer to build my final String and two Stream operations inside the method. The first one I already explained before. The second one is the most interesting. Starting a Stream from the list of columns, each column is mapped to its data value. Then I use Java 8's Optional to treat null elements, returning SQL's keyword NULL String when there is no data for that column in the Map. Also, it is important to map each empty value to SQL's NULL String. Then I used the default Collector's joining method to create a String. Orchestrating everything Now that I showed how to process the entire file it's time to understand how to orchestrating all this methods calls. Image 7's code shows how to orchestrate everything. This is the entry point method to all processing and it is very simple. First I use readFile to read general layout file. Then I use listToMap to transform the List of the file's lines into a Map where the key is the table's name and value is the List of table's columns. The next step is to get the Map's entry set, which is an information where I can get a Stream, to start processing. After getting Stream from Map's entry set, it is important to remove null elements. Then, each Map.Entry is converted to process method's output result using flatMap . Flat mapping is an extremely important concept of Stream. It allows you to convert one input line into multiple output lines transparently. In this case, each entry of the Map, representing a table, is converted to a List of Strings, representing all SQL inserts I need to run on that table. Last, it's time to produce the algoritm's output: a List of String. The default Collector's toList method is used here. Comments over this implementation The code presented in this session is the result of my first contact with Java 8. It contains lots of problems I'll discuss in the session called Improvement Phase - Java 8 . At this point, a good exercise is to understand this code and try find any performance problems to compare with my results to be shown. This way you may produce an even better optimized implementation than mine's. Development Phase - Java 7 After first implementation using Java 8, it's time to talk about first implementation using Java 7. The first version of Java 7's code was basically a translation of Java 8's implementation, to test performance with a consistent base. Although it's not strictly necessary, it's recommended to read the next subsessions with this source file for consulting. Reading files In this version, I used most common implementation to read files: Java 6 style. Please note the amount of extra code, comparing to Java 8's version. Processing files The same strategy of Java 8's version was used at this point: create a Map where the key is the table's name and value is a List of table's columns. No problem to write the Java 7 version, so no Googling needed. Please note that this method is much more readable. Also, there is no need to implement a two method solution, only one can do the job. Reading the code, the first loop does the same thing the custom Collector does in Java 8's version and the second loop generates the output Map, just like Java 8's default Collector's toMap method. Note that this implementation also has problems with empty Lists. Pay attention that this code is not completely generic and only Strings can be processed without modification. I choosed not to use Java 8's functional interface Predicate here to stay strictly into Java 7's default code style, exactly to show where Java 8 is better or worse, speaking of readability and usability. Validating Following Java 8's strategy again, let's validate input's content comparing the String generated from columns' List from both general layout and each table's layout file. Comparing to Java 8's code in Image 4, there is plenty more code in here. It's easy to understand, but Stream implementation also is easy to read. Here I use two for loops to convert each List into a String and then I check if both are equal. An inconvenient here is the need to remove the last SEPARATOR from output String, made through substring calls. I choosed removing last SEPARATOR's character to keep the method's comparing objects exactly the same as generated by Stream solution and keep consistency. Generating SQL inserts As of Java 8's version, here is the core of Java 7's implementation. Read carefully the Image 11's code. This is the most similar code snippet between versions. Almost every line of this code does the same thing as Java 8's version. Main difference is that I don't use parallelism in here, just sequential processing. Generating the SQL insert with Java 7 is a lot more code to write, but it is easy to read. The same approach here: a StringBuilder to construct the output String, two loops, one for column list and other for data list. Same problem here to remove final SEPARATOR character. It is really annoying, but does the job. Orchestrating Everything Again let's talk about how to orchestrate everything shown until here on Java 7's version. Again, very similar code between versions. In fact, the only differences are the listToMap specialized implementation (without a Predicate argument) and the loop instead of Stream's flatMap. This method also is the entry point to all processing. Java 8's version uses less code to do the same processing, but this code is easier to understand. Comments over this implementation This session's code is just a translation of Java 8's version. In fact, any methods can be replaced from both implementations. The only real difference over methods' interfaces is the use of Predicate as separator function on Java 8's listToMap , something that Java 7 can't provide to you and that can be very usefull when reusing code. Also is clear that Java 8 can be used to produce a lot less code instead of Java 7. At this point, there is a lot of waste code on both implementations. As spoiler, this Java 7 implementation is faster than Java 8's. More about it into Benchmark Results' session, after discussing improvements to each version. Again I recommend understanding all code shown in this session and try to remove waste code and improve performance. Improvement Phase - Java 8 At this session, the main goal is to show how the code evolved, after studying more and rethinking about the problem. Here I'll talk just about Java 8's implementation improvements and next session will talk about Java 7's version. Again, the sequence of facts is how I really developed the project. I developed two major evolutions using Java 8 and I'll call them V2 and V3, to simplify writing and understanding. I'll refer first version as V1 when needed. Same subsessions from above sessions will be used to show how each part evolved. Skipped subsessions has not been changed since previous version. Java 8's V2 Processing Files Validating Generating SQL inserts Lots of improvements here. First, all synchronized stuff has been removed to simplify and increase readability. After, one less Stream processing step is needed, joining last two parallelStreams into only one parallelStream. It means one less iteration over results and more performance! One important change is the use of a Supplier variable. It is very usefull when there is need to reuse a Stream. Java's Streams are not reusable, once it is processed with a terminal operation, the Stream can't be used again. But we still can set a Stream to a variable in order to reuse code using the Supplier functional interface. Look into columnSupplier variable's accesses. It has one get method which returns a new instance of the desired Stream each time it's called. It helps the readability of the code. Also note that this method now returns a Stream instead of a List. Java's Streams are only processed when a terminal operation is called, such as forEach , collect and reduce . The less calls needed to terminal operations the better, because this way Java can optimize code's execution time. More about how this can optimize your code's execution time on next session. Please come back and take a look into Image 7's code. Flatmap needs that lambda's output to be a Stream . The V1's collect approach was very wrong, since the process method's output List was only used to turn into a Stream again. Generating the insert text also was improved. Compare it to Image 6's code. Now I used a joining Collector's different constructor to inform a prefix and a suffix to output String. Note that baseInsertText variable's value does not changes for each processed data line. Now it's generated once for each table and reused for each data file's lines. Less code to process, more performance. Orchestrating everything Only change is to not need to start a Stream inside flatMap because process method already returns a Stream. Comments over this implementation After this improvement round, the code became more concise and easy to understand. Lots of wasting operations were removed. Looking at the code, doesn't seems to be such a huge improvement, but believe me, it is. This version made me start to think about how to develop considering the use of Stream API and helped me understand how this new world about Java programming works. Java 8's V3 Improvement Phase - Java 7 Java 7's version of this project also has been improved, again taking base at Java 8's V2 implementation. But the second version of Java 7's code is not just a translation of Java 8's second version, some other improvements has been applied too. There is only one improvement version based on Java 7, called V2. First version will be reffered as V1. Java 7's V2 This version brings all shareable optimizations used in Java 8's V2 plus some exclusive changes. Although it's not strictly necessary, it's recommended to read the next subsessions with this source file for consulting. Reading files This time, I decide to use Java 7's default implementation for reading files, using try-with-resource. It shortened a lot the code, also making it more readable. Compared to Image 8's code, there's a lot of improvement here on readability. Please note that multiple try blocks are not strictly necessary, but to use only one block implicates constructing objects passing a constructor as argument, which reduces readability. The most advantage of try-with-resource is that it's not necessary closing resources initialized inside the parenthesis. Processing files This subsection contains a lot of improvement, both in performance and readability. A new strategy was used to convert a List into a Map. The new approach needs only one loop over input List to generate the Map. There is an extra attempt to improve this section that won't be discussed in this topic, only in Benchmark Results' session. The code can be found here . Note how is possible to construct efficiently the desired Map using a simple flag. This solution also has no problems with empty Lists. This code is much more readable than Java 7's V1 implementation and much much more readable than Java 8's versions. Validating No significantly changes has been made into validate method in terms of performance. But readability has been very improved with use of a helper method to convert a List into a String using a default separator. Comparing to Image 10's code, the main change is just the use of a helper method for grouping List into a String. It's very clear how the code became cleaner and now there is some reusability. The helper method has no changes comparing with Image 10's code that it replaces. Generating SQL inserts There are some improvements here already known from Java 8's V2, such as eliminating one loop over data and processing only once the SQL insert's base text. Some waste code was removed, but general structure of this method was not changed. Compare it with Image 11's code, there's not so much changes in here. Note that generateInsert method has evolved a lot. Reusing baseInsertText variable's content simplified very much this method. Compare it with Image 12's code. Both readability and performance became better at this point. Comments over this implementation The major improvement into this version was the listToMap method. Other changes were just about removing unnecessary code, while listToMap is a really new code. But even with few changes, all waste code was decreasing significantly how performant this implementation can be. I hasn't found any new improvements to apply into this code at the moment and certainly some improvement may still be possible. Anyway, I'm very happy with the performance results for this implementation. Benchmark Results Now that all code has been properly explained, let's see how each version performate in a consistent test. As already told, I used JMH library to benchmark all code. It makes test results consistent and provides serious metrics. All benchmark code is availiable in this file and will not be explained here. Image 30 contains all benchmark results implemented in this project. All them will be properly explained in next subsessions in details, properly splitted to better readability. About table's columns: Benchmark column displays benchmark's name; Mode column displays which test this results refers - avgt means Average Time; Cnt column displays how many tests was performed to compute the score; Score column is the test's result and the most important column to compare; Error column is the error margin of the test; Units column is the measurement unit of Score and Error columns. Please note all benchmarks here used the Average Time method. Also, note that Benchmark name explains which version of Java is used and which version explained here the test refers. Also note that benchmark results are machine dependant, so running in different machine will produce different results. Read file results Let's start talking about reading files. Both Java 7's implementations has exaclty the same performance. Java 8's first and second implementations shows pratically the same results, a little bit slower than Java 7's versions, and Java 8's V3 implementation is very faster than anything! Why? Because readFile 's implementation in Java 8's V3 just reads the file into a Stream, which is just closed here, with no processing realized. In fact, this test only shows how much time takes to prepare a Stream, not to read the file itself. Analyzing the results, the verdict is to use Java 8's versions, V3 if a Stream is required or V1 if a List is required, except if performance is a very critical issue to your application and file reading operations are very common. Otherwise, Java 8's facility to write and read and the little difference between results are really big convincing arguments. Conversions from List to Map Please note here that some benchmarks does not refers to one version explicitly. Those tests are described partially back in Improvements' sessions as partial improvement code. Note also that Java 8's V3 splitList method is not tested here. It happens because this method throws lots of exceptions of to many open files while running benchmark tests, even collecting Stream result. Looking into Java 7's results, the partial study method using edges approach, not shown in this article, looks faster, but its more complex to understand code and its greater error margin was decisive to choose V2's implementation. Because of error margin, in past tests the results were different, with V2 implementation faster. Now looking into Java 8's results, the faster implementation is definitely the one using ForEach approach. This version is basically Java 7's V2 replacing traditional for loop by Java 8's forEach call and lambda argument. This code works, but has some inconvenient global variables because lambda can't access local non final variables. That is why I discarted this solution. Talking about the edges' approach, the same argument from Java 7's edges approach not has been choosen is applicable. V2's implementation is as fast as edges approach, but shows more stable results. In this case, Java 7's code is faster than Java 8's implementation. Also, the second Java 7 version's code is more easy to understand, so it's better to use Java 7's style at this point. Although Java 8's version has a very close performance, it lacks in readability. Full processing Surprise! Java 8's V3 optimized implementation is faster than Java 7's V2 optimized implementation?! Yes! But please notice that Java 8's V3 has lots more error margin than Java 7's V2. It means at long term, Java 7's V2 can be faster, or even that is more likely that Java 7's single shot calling may be faster than Java 8's single shot calling. I prefer to say both results are in a tie. But hey! That's awesome! It means that Java 8's Stream can be as fast as Java 7's default concepts and this is a great result! Looking towards other results, it's impressive how much performance can be obtained by just removing waste code. It also shows that Java 8's Streams need to be correctly written to obtain satisfatory performance. Conclusion This project shows a bit about Java 8 Stream's optimization. It's clear that is very easy to made a mistake when working with Streams and it may slow down your application's performance in a concerning way. It just means that Stream processing must be very carefully analyzed before releasing the code. It is important to note that a real world application would use a hybrid approach, mixing Java 7's concepts with Java 8's new Stream processing, and not this completely heterogeneous solution as shown here. This article is the proof that Streams can be used alone, but Java's default style is still easier to use and usually has better performance. Please use comments box or Github's issues/pull requests to interact about the results and suggest improvements. This way, all of us learn together how to improve development with Java 8's new features. Thank you for your time!",en,127
56,2680,1477670394,CONTENT SHARED,7814856426770804213,-9016528795238256703,9100012288934953337,"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.143 Safari/537.36",MG,BR,HTML,https://blog.devteam.space/new-macbook-pro-is-not-a-laptop-for-developers-anymore-d0d4b1b8b7de?gi=6d5f52f5f44,new macbook pro is not a laptop for developers anymore,"Here's why: #1. No Escape and function keys Today's Apple Event confirmed many of the rumors surrounding the long-awaited refresh of the Macbook Pro line. The Escape and Function keys on the laptops have been abandoned in favor of a touch bar that changed depending on the application that is being used. The last the Macbook Pro got a major update was a shocking 4 years ago and many publications are celebrating the new design. However, the lack of physical Escape and Function keys is a disaster for one major set of Apple's customers - Developers. Let's take a look at numbers: There are ~ 19 million developers in the world. And Apple has managed to sell ~19 million Macs over the past 4 quarters. What a coincidence! Yes, developers are drawn towards Apple products primarily for software reasons: the Unix-like operating system and the proprietary development ecosystem. But developers need to have a functional keyboard to make use of that software and now they don't. Why Tim Cook , why? This isn't to say that the touch bar is an inherently bad idea. You could locate it on top of the Esc and function keys instead of eliminating them entirely! Something like this: #2 Power. Almost no improvement for RAM and a processor The 2016 MacBook Pro ships with RAM and processor specs that are nearly identical to the 2010 model. Deja vu? RAM: At least it feels like that, because the MacBook Pro has had options of up to 16 GB of RAM since 2010. The only difference now is that you pay for the update. Processors: The MacBook Pro had options with 2.4 gigahertz dual-core processors back in 2010. Anything new in 2016? Not really, well... nope. That feels strange too, especially if you compare the price and the hardware quality to Apple's competitors. These days it's easy to find a Windows or Linux machine comparable to the Macbook Pro for $1,000 - $1,500. You don't need to go far to find them, notable brands like Lenovo, HP, Asus, Samsung, and Dell all offer them. And for those who aren't fans of Windows, Linux is always an option. With more resources poured into Microsoft developers ecosystem and Linux distributions, developers may soon have a wide range of great operating systems that can run on pretty cheap laptops. And to nail it: #3. What people are saying about MacBook Pro 2016 If you liked the post, click the �� below so more people will see it! :)",en,127
57,1312,1465424159,CONTENT SHARED,-2948321821574578861,7527226129639571966,3989456265529686673,,,,HTML,http://arquiteturadeinformacao.com/usabilidade/quando-usar-paginacao-e-quando-user-scroll-infinito/,quando usar paginação e quando usar scroll infinito?,"Scroll infinito é uma técnica que permite que o usuário continue rolando por uma infinidade de conteúdos sem precisar em nenhum momento apertar nenhum botão para carregar mais. Depois exemplos famosos desse tipo de interação são o newsfeed do Facebook, e o seu feed pessoal do Pinterest: você continua rolando a página indefinidamente, sem nunca precisar trocar de página ou apertar um botão para carregar mais itens. Aliás, essa técnica está totalmente alinhada com a estratégia desses serviços: eles não querem que você se dê conta que já está rolando a página há um tempão e que você chegou ao final dela; quanto menos você perceber isso, melhor . Por isso o scroll infinito é um ótimo alinhado de serviços que medem seu sucesso pelo tempo de engajamento do usuário no site ou aplicativo. O lado negativo do scroll infinito Na verdade é plural: negativos. Falta de sensação de controle. Alcançar um ""ponto final"" dá ao usuário a sensação de controle. Quando você tem um número de resultados limitados, você consegue facilmente se o resultado que você procura está ali ou não. No scroll infinito isso fica impossível, porque você não faz ideia do que vem pela frente. Falta de localização. Porque o feed é infinito, o usuário não consegue determinar um ponto específico na página onde está a informação que ele está procurando. Mesmo que ele consiga na primeira visita; em sua segunda visita ele já não consegue mais localizar onde estava aquela informação, já que a localização geográfica do conteúdo varia muito no scroll infinito. Além dos problemas acima, o scroll infinito ainda pode trazer um problema de performance para sua página ou app, já que carregar muito conteúdo na mesma página requer muita memória do seu browser. Por fim, se o usuário está tentando chegar até o rodapé do site para encontrar um link (termos de serviço, contato, etc.), ele pode se sentir frustrado com o fato de a página carregar mais conteúdo toda vez que ele volta a scrollar. A alternativa para isso é simplesmente não ter um rodapé, ou colocar os links que iriam rodapé em outro lugar do site. Quando usar paginação e quando usar scroll infinito? Scroll infinito funciona melhor para sites com conteúdo gerado por usuários (exemplo: Twitter, Facebook), conteúdo visual (exemplo: Pinterest, Google Images) ou sites que pretendem equilibrar a quantidade de tráfego em seu conteúdo. Já a paginação é mais universal, e funciona melhor para plataformas onde o usuário tem uma tarefa bem específica em mente (exemplo: busca do Google). Concorda? Discorda?",pt,126
58,1063,1463933351,CONTENT SHARED,5658521282502533116,-1561982036714087726,-6957674286814133812,,,,HTML,http://engenhariae.com.br/mais/colunas/ita-esta-oferecendo-10-cursos-gratuitos-a-distancia/,ita está oferecendo 10 cursos gratuitos a distância - engenharia é:,"O Instituto Tecnológico de Aeronáutica (ITA) , fundado em 1950, está oferecendo 10 cursos gratuitos por meio da plataforma de ensino on-line Coursera. A plataforma conta conta com mais de 12 milhões de usuários e dispõe de mais de mil cursos de instituições renomadas do Brasil e do mundo. Os professores que ministram os cursos são todos do ITA, instituto de ensino superior do Comando da Aeronáutica (COMAER), localizado no Departamento de Ciência e Tecnologia Aeroespacial (DCTA). Confira cada um dos cursos e faça sua inscrição: 1. Introdução ao Controle de Sistemas 2. Controle Usando a Resposta em Frequência 3. Arquitetura de Software em Projetos Ágeis 4. Desenvolvimento Ágil com Padrões de Projeto 5. Desenvolvimento Ágil com Java Avançado 6. 7. Princípios de Desenvolvimento Ágil de Software 9. TDD - Desenvolvimento de Software Guiado por Testes Projeto Final: Aplicativo para Web com Componente Gamificado 8. Técnicas Avançadas para Projeto de Software 10. Orientação a Objetos com Java PS: O curso é oferecido inteiramente de graça! Logo, caso você queira o certificado, é cobrado uma pequena taxa. Achou útil essa informação? Compartilhe com seus amigos! xD Deixe-nos a sua opinião aqui nos comentários. Nascido no interior de Minas Gerais, foi seminarista em uma congregação francesa, mas viu que que sua vocação era a de ser engenheiro. Criou o Engenharia é: exatamente às 11:28, do dia 2 de agosto de 2011. Você pode falar comigo pelo email: ademilson@engenhariae.com.br ;)",pt,126
59,2967,1484651225,CONTENT SHARED,-3191013159715472435,-1393866732742189886,2008706978002473337,"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36",MG,BR,HTML,http://www.otempo.com.br/capa/pol%C3%ADtica/rob%C3%B4-acha-de-almo%C3%A7o-de-12-kg-a-cervejas-pagas-pelo-cidad%C3%A3o-1.1424088,robô acha de almoço de 12 kg a cervejas pagas pelo cidadão,"Em apenas dois meses, Rosie descobriu 3.553 casos suspeitos envolvendo a cota parlamentar dos deputados federais de todo o país. Rosie não é nenhuma funcionária da Ouvidoria da Casa, mas um robô criado por oito amigos que atuam na área de ciência de dados para fiscalizar as despesas dos deputados. A ferramenta cruza informações das notas apresentadas para reembolso com outras, como as da Receita Federal, a presença em plenário, a localização e as característica do lugar onde a compra foi feita. Assim, já foi encontrada uma nota de R$ 170 de um restaurante onde o quilo da refeição custa R$ 14. Ou seja, teriam sido consumidos 12 kg em um dia. Ainda tem o reembolso de um almoço de R$ 41 feito em São Paulo, apenas 35 minutos depois de o deputado ter discursado em Brasília. Só na última semana, 849 casos foram auditados. Destes, 629 resultaram em denúncias envolvendo R$ 378,8 mil pagos com dinheiro público por 216 deputados. O projeto responsável pelos levantamentos recebeu o nome de Operação Serenata de Amor. O nome revela a intenção de desvendar fraudes com valores pequenos. ""Na Suíça, uma ministra perdeu o cargo por ter comprado com dinheiro público um chocolate Toblerone. A ideia é ter um detalhamento tão preciso que seja capaz de detectar um chocolate pago irregularmente"", explica um dos membros, o publicitário Pedro Vilanova. A base de dados considera todos os reembolsos dos deputados que passaram pela Câmara Federal desde 2011. ""Alguns dos denunciados, dessa forma, já não estão mais no cargo, mas terão que prestar contas igualmente"", diz Vilanova. Por orientação jurídica, eles só divulgam o nome dos parlamentares depois de terem tido um retorno da Câmara. Entre os episódios em que já conseguiram não só resposta da Casa, mas a devolução do dinheiro pago equivocadamente está o do então deputado Odelmo Leão (PP-MG), eleito prefeito de Uberlândia. O mineiro gastou R$ 190,05 da cota parlamentar com envio de correspondência da sua campanha. Em agosto passado, Marco Maia (PT-RS), por exemplo, pediu ressarcimento de R$ 154,50 por duas refeições. Ele devolveu, em dezembro, R$ 77,25, referentes a um almoço. De acordo com as regras da Cota de Atividade Parlamentar, as despesas são autorizadas apenas para os parlamentares. O robô ainda encontrou um reembolso de R$ 135,15 para Vitor Lippi (PSDB-SP) na compra de cinco cervejas durante uma viagem à Califórnia, nos Estados Unidos. O tucano devolveu o valor e pediu desculpas. ""Aproveito para assumir a responsabilidade pelo erro cometido, é de praxe dessa assessoria pedir a glosa de itens não autorizados, como bebidas alcoólicas, mas infelizmente dessa vez não identifiquei o produto, já que estava em outra língua"", alegou o parlamentar no mês passado. Já o deputado Rocha (PSDB-AC) apresentou três notas de alimentação do mesmo dia. Duas delas, de R$ 52 e R$ 43, foram emitidas em Rio Branco, no Acre. A outra, de R$ 148, a 4.000 km, em Caxias do Sul, no Rio Grande do Sul. Ele alegou que o segundo reembolso foi de uma despesa feita uma semana antes. Como estava com pressa no dia, pediu a um assessor que voltasse outro dia para emitir o cupom, o que, segundo ele, explicaria a coincidência de data. Sem convencer ninguém, devolveu R$ 148. Criadores buscam mais financiamento Os dados brutos levantados pelo robô Rosie na plataforma Jarbas, ambos desenvolvidos pelos oito jovens na operação Serenata de Amor depois são analisados pelos profissionais e denunciados à Câmara dos Deputados, no próprio site do Legislativo. Como o volume de dados é muito grande, a equipe aceita a colaboração de outras pessoas que tenham conhecimento técnico ou simplesmente que estejam dispostas a divulgar a ideia, além, claro, de doações em dinheiro. Foi graças a um financiamento coletivo que eles conseguiram arrecadar R$ 80 mil e custear a investigação sobre as despesas da Copta de Atividade Parlamentar. O valor angariado foi superior à meta inicial de R$ 60 mil. ""O projeto prevê trabalho até o fim de janeiro e o começo de fevereiro, mas nós já estamos buscando novas formas de viabilizá-lo para que consigamos não parar de trabalhar. Continuamos precisando da ajuda de pessoas"", diz o publicitário Pedro Vilanova, um dos integrantes da ação. Na última semana, eles conseguiram organizar um mutirão com vários colaboradores. Os dados da Operação Serenata de Amor são públicos. Com algumas orientações da equipe, qualquer pessoa pode acompanhar os gastos de seu deputado. A cada semana um relatório novo do trabalho é publicado na página deles no Facebook, onde também é possível contatá-los. Seleção. A ação é uma das 20 escolhidas para a Hack Brazil, que acontece na Brazil Conference at Harvard & MIT, e elege projetos resolutivos em tecnologia, criatividade e inovação.",pt,124
60,1358,1465677689,CONTENT SHARED,-330801551666885085,7774613525190730745,-5387740831049193382,,,,HTML,http://blog.tiingo.com/switched-away-aws-packet-net-benchmarking-networking-disk-processing-speeds/,"aws vs packet.net why we left aws benchmarking aws's network, disk, and cpu performance - tiingo thoughts","If this sounds like a glowing review of Packet.net - it is. I found myself re-reading this post over and over, trying to make it sound less shrilly - but I can't. It's just a ridiculously good product and value - EC2 containers just don't make sense anymore. A friend once told me, ""Rishi - sometimes if you don't advocate a product aggressively - you can be doing society a disservice in your attempt to be neutral. If the value is so good, you must tell everybody about it."" This is one of those times. EDIT: Feeling really grateful the HackerNews community decided to link to Tiingo a second time. In the first HackerNews posting many of you asked for an API, which is what led to me finding the AWS bottleneck. The API launched [quietly] this week at: where Tiingo is now the first company to bring IEX (anti-HFT exchange/darkpool) data to mainstream FinTech. Kind of went full-circle as this post wouldn't have existed without the original HN coverage . TL;DR: The performance of AWS on network speed, disk speed, and CPU performance are quantitatively just ""not good,"" for what we needed. When we introduced real-time market data, we were in search of our bottleneck and realized it was AWS. We made the decision to switch to Packet.net and the below reflects on our decision and explains why. The benchmarks continue to reaffirm our decision. Having said all of this, certain features of AWS remain incredibly convenient like S3, Cloudfront, and Route53 - but we can't justify using EC2. In Networking : Packet is significantly faster, more stable, and 15%-44% cheaper In Disk Usage : Packet is more performant and 92% cheaper In CPU: Packet is 30-40% more performant and 15% cheaper In machines: Packet's systems are all bare-metal/dedicated, whereas AWS charges extra for dedicated machines If you've noticed Tiingo being particularly snappy these days, it's because I couldn't stand it anymore. I had tried everything - buying more expensive instances on AWS, allocating more space, scaling horizontally, but it wasn't matching up to my local dev machines. And so I started searching for the bottleneck - only to realize it was AWS. I started researching AWS, I found I wasn't alone. Many people experienced what I had but I tried prolonging the switch. Trying to change cloud service providers is frustrating: scripts break, performance temporarily suffers, you experience downtime, and you know there will be unknown-unknowns. Recently we just got real-time market data and this exacerbated the issues. Our websockets were being overwhelmed in queues and throwing more nodes at the problem was becoming expensive. We were trying to put a bandaid over a burst pipe. I finally decided on Packet.net and I want to share the reasons why. I've included benchmarking results to help emphasize the point. Our search was motivated by two major reasons: The costs were getting out-of-hand After reading the below Reddit post on AWS's [lack of] network stability, we started asking around and realized the experts were right... AWS's network is slow. If we are going to give our users real-time data directly from the exchanges, that's a heck-of-a-lot of data and we need it to be as fast as possible. The Reddit/Blog Post was from an engineer at Stack Overflow. . More specifically, this Reddit comment on AWS's network stability that seemed echo'd by many: We explored options like DigitalOcean , but Tiingo, like all financial data/analytics companies, is very data heavy and their plans didn't allow for flexible data storage (EBS on Amazon for example). We looked into Rackspace and Azure, but the cost differentials didn't make it seem worth the transition. Admittedly, having used Rackspace in the past - I've always loved their customer support and was personally disappointed I couldn't justify the cost. Eventually I came across Packet and spoke to their engineers since I hadn't heard of them before. I took a chance. It paid off. I told them my concerns and what I was trying to solve (market data connectivity and high data transfer rates). One of the co-founders, who was a networking engineer, personally oversaw my connectivity project to the data exchanges. I'm pretty sure this was Paul Graham 101 on start-ups and customer service. Ultimately though - I'm a data nut and so I decided to benchmark AWS vs Packet and was really curious about the Reddit comments on AWS's network stability. The benchmarks closed the deal for us. It was a no-brainer. Part of the major reason being that Packet.net is bare metal (dedicated physical machines) whereas AWS tends to be focused on virtual machines. The hardware/pricepoint is actually even cheaper on Packet. We are paying 1/3rd of what it would cost to get a similar, less performant, system on AWS. SO here you have it! The tests below compare AWS vs Packet for disk, network, and CPU benchmarking - and also cost. I've outlined and commented the results below so you can reproduce the tests. Hardware Since we are testing Packet vs AWS, we started off with the Packet hardware and found the AWS price equivalent. We started with the Type 1 and worked backwards to find the equivalent in performance/price on AWS. Note: For the network test, we also test a smaller machine. The reason for the lighter hardware is for load balancing (HAProxy in this sense). If all of the back-end servers can have high network throughput, but we need to send it to the end-user, the load-balancer's networking performance will be the determining factor. This is especially important in cases like real-time data. Packet: Type 1 (Server) (4 core, 8-threaded) 3.4ghz Intel Xeon E3-1240 v3 32gb $0.40/hr $0.37/hr if reserved for 1 month $292.80/month Type 0 (Load Balancer) (4 core) 2.4ghz Intel Atom C2550 8gb $0.05/hr $0.0459/hr if reserved for 1 month $36.60/month What somebody may choose as their load balancer *Note:We assume 732 hours in a month; but if you reserve a Packet instance for a month, they will only charge you 672 hours per month. However, to make apples-to-apples comparisons, all calcs in Price/Month assume you choose hourly pricing (732 hours for 1 month) to keep things normalized. AWS: m4.2xlarge (Server) 8 VCPU (2.4ghz Intel Xeon E5-2676) 32gb $0.479/hr $350.63/month xlarge was chosen for it's optimized network performance t2.medium (Load Balancer) 2 VCPU (Xeon processors burstable to 3.3ghz) 4gb $0.052/hr $38.07/month What somebody may choose as their load balancer OS : Ubuntu 14.04 server Network: For this test, we used iperf3 as per the AWS documentation ( ) We wanted to simulate a very real-world network configuration for ourselves - basically what our site looks behind a load balancer. Load balancers tend to require very low processing power, and serve as a network bottleneck to the user. We are testing: Internet -> Load-balancer (Haproxy) Load-balancer (HAProxy) -> Server Server -> Server The ""Internet"" machine used was an Azure machine. Not perfect, but we figured it was a good 3rd party control. You can view the detailed methodology in the Appendix below. Results: Performance: AWS came out incredibly inconsistent - with a high std. deviation and low mean transfer rates. What AWS considered a ""High"" performance network tier, was the least expensive tier on Packet. Why didn't we use AWS Elastic-Load-Balancer (ELB)? For our use case with websockets, - we found ELB to be lacking what we needed. This will be a blog post for a later day. What was particularly interesting was the inconsistency of the lower tier machines. We ran our benchmarks over an hour, and here is what the rates looked like when making requests to-and-from the lower tier (t2.medium) EC2 Instance. This seems consistent with their ""burstable"" instance - which is great and all...except Packet's lowest tier outperforms it: Pricing: The above AWS configuration is $.081/hour more expensive than Packet and also less performant. Another consideration is bandwidth costs. AWS charges $0.09/GB (for the first 10TB) out to the internet. Packet.net charges $0.05/GB out to the internet. Within the same data centers (availability zones in AWS), both Packet and AWS are free. However, when transferring to a different availability zone, AWS charges $0.02/GB and Packet.net charges $0.05/GB. Conclusion: Packet is the clear winner in this. In both absolute speed and stability. In terms of price, Packet is cheaper by $.081/hour in the above configuration, or 15% cheaper - and for the majority of our bandwidth we go external to the internet. In outbound internet traffic, Packet is 44% cheaper. Disk: Packet offers two storage types: Basic (500 IOPS) and Performance (15,000 IOPS). We created a EBS volume on both Packet & AWS with provisioned IOPS of 500 and then 15,000. Then we used sysbench to run an I/O test (see Appendix below for methodology). Results: Performance: When getting to the 15k IOPS, we saw a more significant performance differential favoring Packet. At Tiingo we used the performance tier given the amount of data we store and calculate. Price: Provisioning 15,000 IOPS on AWS @ $0.065/IOPS = $975. But wait, that's not all! They also charge $0.125/hour per GB. So a 15k IOPS 500GB HDD on AWS would be $1037.50 On Packet it would be 500GB * $0.15 = $75. Doing a bit of algebra, the cost for 15k IOPS on AWS would be cost effective if you have >39TB of storage. That's right - Packet is cheaper until you hit 39TB of storage.... Conclusion: Packet is literally 92.3% cheaper than AWS for 15k IOPS performance, and Packet is even more performant. It's the victor in disk performance as well. CPU: CPUs cannot be benchmarked purely on the speed of the processor [clock] alone. For these reasons, we ran a sysbench test as well on different threads. Results: Performance: The results are damning for AWS. On an 8 processor machine, the benchmark ran slower on 8 cores than 4. I ran this multiple times, double checked to make sure this was an m4.2xlarge. Then I spun up another m4.2xlarge and the results were more in line with what I expected (still slower than Packet). However, I am going to keep the original instance's benchmark below to highlight the point of noisy neighbors . With AWS, you can get a shared machine with other neighbors who are processor intensive and reduce your performance. This is what virtualization is. With Packet, you get a dedicated system. What most likely happened was that our original machine had a noisy neighbor. Here are the results - you can see at 8 threads Packet performs 4x faster than AWS. OK OK - I will show the second instance's performance - even when there are no noisy neighbors. Even with a non-noisy neighbor machine, Packet is 30-40% faster in processor benchmarks. EDIT: A user asked me to run the benchmark using a compute-optimized EC2 instance. I decided on c4.2xlarge which has 8 threads, but half as much memory (16gb). It cost $0.419/hour ($0.019/hr more expensive than a Type1 Packet server). Here are the results (Packet wins again but less drastic of a margin) Price: On the above setup, Packet is $0.079/hour cheaper. Conclusion: There really is no way around it - the above benchmarks show the issues with virtualization. Even with those issues aside, AWS is slower and more expensive. Packet wins this one again. Conclusion Even giving AWS the benefit of the doubt, there is no way around it - Packet is faster and SIGNIFICANTLY cheaper. Let's take a very real-world example of our server set-up: Packet: *Note:We assume 732 hours in a month; but if you reserve a Packet instance for a month, they will only charge you 672 hours per month. However, to make apples-to-apples comparisons, all calcs in Price/Month assume you choose hourly pricing (732 hours for 1 month) to keep things normalized. AWS: Packet is literally less than 1/3rd the price and is more performant than AWS. It's allowed us to deploy resources we didn't think would be affordable before. Thank you to everyone @ Packet for making this product possible. Further Steps: If anybody wants to continue this study, I would love to hear your results. AWS does allow you dedicated machines for extra $, but we didn't bother testing them since Packet is already cheaper than their virtual machines. Methodology: Networking: Setting up AWS: We want to make sure we give AWS the best chance. First, we have to make sure enhanced networking is enabled. Running the command: Will give us the output, and look for ""version"". In our instance we have version 2.11.3-k. Amazon recommends we upgrade. For the ubuntu users out there, follow this gist and run the commands: ixgbevf 2.16.1 upgrade for AWS EC2 SR-IOV ""Enhanced Networking"" on Ubuntu 14.04 (Trusty) LTS After rebooting run: Again to make sure the version now reads: 2.16.1 Let's also check via command line to make sure enhanced networking is supported (Ubuntu 14.04): If you get the output, you're good: Next, we used iperf3 to run the diagnostic scripts and scrapy bench . iperf3 is a common network benchmarking tool and scrapy is the framework that powers Tiingo's scraper farm. We figured Scrapy would be another real-time test to see how things flow. the iperf3 command was: Meaning we ran the tests for one hour (3600 seconds), and with 10 processors in parallel. Also note to set the -B option on Packet machines as it takes advantage of the full bonding algo and increases thoroughput. Note: make sure to use the internal IP addresses to give the best benefit of doubt �� Disk: First install/update sysbench on your Ubuntu machine using the code: Then we used the command: The file size must be greater than the RAM size for this test to properly work. CPU: See the above ""Disk"" section to set up sysbench. We then ran the command below, replacing ""num-threads"" with 1, 4, and 8 respectively",en,122
61,2964,1484579813,CONTENT SHARED,569574447134368517,-4465926797008424436,6992931052309362824,"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2982.0 Safari/537.36",SP,BR,HTML,https://www.blockloop.io/mastering-bash-and-terminal,mastering bash and terminal,"If there is one tool that every developer uses regardless of language, platform, or framework it's the terminal. If we are not compiling code, executing git commands, or scp-ing ssl certificates to some remote server, we are finding a new version of cowsay to entertain ourselves while we wait on one of the former. As much as we use the terminal it is important that we are efficient with it. Here are some ways I make my time in the terminal efficient and effective. Assumed Settings Some of these commands list alt as a prefix character. This is because I have manually set alt as a meta key. Without this setting enabled you have to use the esc key instead. I recommend enabling the alt key. In Mac Terminal.app this setting is Preferences > Profiles tab > Keyboard sub-tab > at the bottom ""Use option as meta key."" In iTerm2 the setting is at Preferences > Profiles tab > Keys sub-tab > at the bottom of the window set ""left/right option key acts as"" to ""+Esc"". In GNOME terminal Edit > Keyboard Shortcuts > uncheck ""Enable menu access keys."" I also assume you're using bash. I know there are some cool newcomers out there like zsh and fish, but after trying others out I always found that some of my utilities were missing or ill-replaced. If you are not using bash then YMMV. I also assume you are using at least bash version 4. If you're on a Mac then, unless you have manually installed bash with homebrew, you are using an old version. Install bash with homebrew and include it in /etc/shells . Repeat Commands I spend a lot of my time in terminal repeating commands that I have previously run. One thing I noticed a lot of people do is use the up and down arrows to navigate their history of commands. This is terribly inefficient. It requires repositioning your hands and often times removing your eyes from the computer screen. Also, your history (depending on your HISTSIZE) can be very long. Using the up and down arrows is almost like searching through a terrible version of the Oxford English Dictionary which has one word per page. Instead of searching line-by-line I use search history ( ctrl-r and ctrl-s ). In your terminal window, before you type any text press ctrl-r and you should see your prompt change to (reverse-i-search): . Now begin typing any part of any previous command you have executed and you will see the most recent command which matches your search. If this is not the one you want, press ctrl-r again to search incrementally. For example, if you are searching for kubectl delete pods -l=app=nginx you would type kubectl or kubectl del . You should land on that command. If, while incrementally searching backward, you pass the one you're looking for, press ctrl-s to go the other direction and you will see your prompt change to (i-search): . Once you find the command you want press enter to execute it or move the cursor left/right to modify the command first. NOTE ctrl-s probably won't work by default for most terminals. You will need to add stty -ixon to your ~/.bashrc ( ~/.bash_profile for Mac). Sometimes you know that the command that you want to repeat is only two or three places back in history. In these cases it is sometimes easier to move up to that command directly. But you still should not use the arrow keys. Bash has keyboard shortcuts for this too! Here is where we use ctrl-p for ""previous"" or ctrl-n for ""next."" Pressing ctrl-p moves to the previous command in history (replacing the up arrow), and ctrl-n moves to the next command (replacing the down arrow). In mose cases your history will probably be set to record duplicates. This gets pretty annoying for me so I use the following setting to make sure my history doesn't get flooded with duplicate entries. Add this to your ~/.bashrc or ~/.bashprofile and your history will only keep the newest versions of commands. If you typed git status seven times, it will only record the latest one and delete the previous entries. Movements Now that we know we don't need the up and down arrow keys, what about the left and right? Unfortunately, these keys are still needed for single character movements, but I find myself using them less often. Here are some key combinations to move your cursor a little more efficiently. ctrl-a - move the cursor to the beginning of the current line ctrl-e - move the cursor to the end of the current line alt-b - move the cursor backwards one word alt-f - move the cursor forward one word ctrl-k - delete from cursor to the end of the line ctrl-u - delete from cursor to the beginning of the line alt-d - delete the word in front of the cursor ctrl-w - delete the word behind of the cursor The last four aren't necessarily movements, but I use them in conjunction most of the time. Copy / Paste One of my favorite MacOS command line utilities is pbcopy / pbpaste . I like them so much that I created the aliases for my linux machines using xclip (shared below). These two commands use your system clipboard (also called the pasteboard, hence the names). You can pipe data to pbcopy to copy something to your system clipboard or you can pipe pbpaste to another command to paste from your clipboard. Here are some examples that I use: In linux, I put the following in my ~/.bashrc to create the same effect. Changing Directories cd is one of my most used commands according to my bash history. One thing I find myself doing a lot is changing between two directories or briefly changing from directory a to directory b and then back to a. Depending on the reason I'm changing directories I will use either cd - or a combination of pushd and popd . If you type cd - and press enter, you will change to your previous working directory. On the other hand, sometimes I know that I want to go to some directory in a different place, but I might cd a few times to get there, but I want to mark my place so that I can get back quicker. In this case, you would use pushd like this. You can pushd multiple times to build a stack. I don't find myself doing this much, but it's there if you need it. Background Processes One of my pet peves about working with other software developers is that they almost always have ten or more terminal windows open at all times. Usually, they will have one terminal per directory they are working with (this can be avoided by using pushd , popd , and cd tricks mentioned above). But often they will have a few windows open that are running processes which have locked the window. This is difficult to work with because it requires flipping back and forth and knowing where everything is. For executing processes I like to use a mixture of some commands. If you need to run a command indefinitely you can send it to the background by first running it and then pressing ctrl-z . This will suspend or pause the process. After it has been suspended, type bg and press enter. This will move it to a running state, but it will no longer have control of your terminal window. However, if you close the terminal that job will terminate. To avoid this, you disown the process by typing disown and pressing enter. At this point the process is no longer a child of your current terminal process. I often use this to run kubectl proxy or python -m SimpleHTTPServer . ctrl-z - move the current process to the background in a suspended state. jobs -l - list the current background processes for the current tty session. bg - tell the most recent background process to continue running in the background fg - bring the most recent background process back to the foreground disown -h - disown the most recent background job. This will remove it from your current tty session. It will not be able to be brought back to the foreground. You will have to control it either with kill or something else. bg , fg , and disown can be used with the job number found in jobs -l . If you run jobs -l you will see the job number at the beginning of the line. If you want to bring the 2nd job to the foreground you run fg %2 . If you want to disown the fourth job then you run disown -h %4 , and so on. The plus sign (or minus sign) at the bigging of the line has meaning as well. A plus sign indicates that the job is the most recently used, or the one that will be targeted if you type any of the commands without a job ID. The minus sign is the second most recently used. I use ctrl-z a lot because I use a single terminal window for vim and as my command line interface. When I'm writing code in vim and I need to get back to my shell prompt I use ctrl-z to suspend vim. NOTE this will still print stdout and stderr to your command window. If you want to change that then you can redirect to files I modified my PS1 to show my current background job count. Working With Files Several times throughout the day I want to view the contents of a file. Before, I would cat the file or open it in vim . cat was annoying because it flooded my terminal history. This is when I learned to use less to open files with pagination. When you open a file with less the contents of the file become paginated and you start at page one. What's great about less is that many of my favorite key combinations work. You can use ctrl-u to page up, ctrl-d to page down, ctrl-p to scroll up one line, ctrl-n to scroll down one line, g goes to the top of the file, G goes to the bottom of the file, and / searches the file. While less is great for opening files, I may not know where the file is in the first place. Say I have a file named ""auth.py"" but I don't remember exactly where I put it. I could cd back and forth until I find it, or do what some people do and run start . and browse for it in a UI window (terrible workflow). Instead, I use either find , ag (silver searcher), or tree . find is great for searching by file name. You can run find . -type f -name auth.py to search the current directory for a file named ""auth.py"". tree is great for listing a directory in a tree format (much like how you see it in a UI). ag is an applciation called ""the silver searcher."" It is essentially a modernized version of grep and I find myself using it quite often. It's better than grep in that it automatically ignores commonly ignored files such as the .git directory, virtualenv, and anything listed in your .gitignore. I like silver searcher because the command line arguments are very similar to grep so my flags are generally transferrable. Note It's best to combine these commands with less because they will likely flood your terminal history . Choose a Few to Start With I did not use all of these when I first started using bash, nor did I memorize them all at once. I picked up one or two here and there over the years. It's difficult to memorize key combinations, especially when there are so many of them. Pick one or two shortcuts and focus on using them. I find myself using these commands by muscle memory, not by memorizing each keyboard shortcut. In fact, once I started to write this I had to open up the terminal and work around to remember which shortcuts I use. I hope these help you work with bash and terminal more effectively. It is easy to learn one new trick and force yourself to use it for a few days until you get used to it. Once you are comfortable with that command, pick up another one.",en,120
62,1419,1466082624,CONTENT SHARED,319340024738595907,4670267857749552625,7965979047714660786,,,,HTML,http://www.webmotors.com.br/comprar/mitsubishi/asx/2-0-4x4-awd-16v-gasolina-4p-automatico/4-portas/2013-2014/17115683,mitsubishi asx 2.0 4x4 awd 16v gasolina 4p automático - webmotors - 17115683,"Opcionais Airbag, Alarme, Ar condicionado, Ar quente, Banco com regulagem de altura, Bancos dianteiros com aquecimento, Bancos em couro, CD e MP3 Player, CD Player, Computador de bordo, Controle automático de velocidade, Controle de tração, Desembaçador traseiro, Direção hidráulica, Encosto de cabeça traseiro, Freio ABS, Limpador traseiro, Rádio, Retrovisores elétricos, Rodas de liga leve, Sensor de estacionamento, Tração 4x4, Travas elétricas, Vidros elétricos, Volante com regulagem de altura Observações do vendedor IPVA pago, Licenciado, Não aceita troca, Todas as revisões feitas pela concessionária Carro super novo,sou o segundo dono,excelente procedência!!",pt,120
63,1869,1469469603,CONTENT SHARED,-2447632164766022033,881856221521045800,-1412399050403999340,,,,HTML,https://tecnoblog.net/198814/github-estereotipos-programadores/,o que o github tem a nos dizer sobre os estereótipos entre programadores | tecnoblog,"O ser humano possui uma característica, talvez de ordem evolutiva, que nos torna em verdadeiras máquinas de classificação e padronização. Sentimos constantemente o impulso de agrupar e rotular qualquer coisa que pareça semelhante entre si, seja para nos igualarmos, seja para nos distinguirmos. No mundo da tecnologia, e cultura pop, isso não seria diferente. Seja num nível mais abrangente, com frases sem sentido como ""quem gosta de não conhece nada de computador"", ""quem joga RPG é virgem"" ou ""engenharia não é área para mulheres"", como em camadas bem mais específicas, a exemplo de estereótipos que ditam que Swift é coisa de hipster, C++ é uma linguagem de programação para velhos ou que ninguém de fato gosta de codificar em Java. Foi pensando nisso que um engenheiro de computação chamado Jeff Allen, do Trestle Tech , resolveu tirar isso a limpo usando ferramentas de estatística e linguagem R para cruzar informações pessoais de um pequeno número de homens e mulheres, todos com certo renome em suas áreas e linguagens de programação, e o que existe em seus repositórios no GitHub. Aliado a isso, ele usou o API Face , do sistema cognitivo de reconhecimento de faces da , para avaliar e classificar as fotos dessas pessoas. Como o site R-bloggers deixa claro, não podemos dizer que esse é um processo lá muito científico. Além disso, por questões de performance, Jeff restringiu seu estudo às seguintes linguagens: E ele chegou a alguns resultados bem curiosos. Por exemplo, esta é a distribuição de linguagens de programação entre as mulheres, com maior número de programadoras em JavaScript e R, e bem menos em C++, Java e Python: Ainda assim, quando a gente compara ao número de homens que programam, ou ao menos ao que o Face API identificou como homem, essa distribuição se torna bem disforme: É importante citar aqui que o processo envolve um conceito de gênero simplificado por limitações computacionais, que por ora avalia somente feições estéticas e não de identidade. Fica aí uma dica de evolução para o futuro dessa API. Mas voltando, vamos falar de uma distribuição estatística mediana em relação à idade dessas pessoas e a linguagem de programação que elas trabalham: E quais são as linguagens de programação que deixam os profissionais mais felizes? Para saber isso ao certo, somente com um estudo muito mais completo, mas Jeff resolveu analisar de uma forma mais poética: sorrisos! E essa coisa de que hipsters barbudos preferem programar em Swift, seria verdade? Veja bem... E já que chegamos nesse nível de maluquice, por que não cruzar tipos de linguagem de programação por bigodes e costeletas? Conclusão Se você for participar de um projeto em C++, deixe seu bigode crescer. Mentira. Na verdade, o cara precisaria de uma amostragem muito maior para que esse estudo tivesse o mínimo de precisão com relação a comunidade de programadores, e a Face API precisaria ser melhor ajustada para avaliar essas nuances de gênero, personalidade e identidade. Mas foi uma abordagem extremamente interessante do ponto de vista de análise de dados. Para quem se interessa por esse tipo de assunto e quiser ver a fundo como as consultas foram realizadas, Jeff disponibilizou o código-fonte, claro, em seu perfil no GitHub - note que você precisará de uma chave do Azure para a API da Microsoft.",pt,120
64,1592,1467288669,CONTENT SHARED,-5014627593450767720,-5380862725077089346,-5535680675523205102,,,,HTML,http://buytaert.net/drupal-is-for-ambitious-digital-experiences,drupal and ambitious digital experiences,"What feelings does the name Drupal evoke? Perceptions vary from person to person; where one may describe it in positive terms as ""powerful"" and ""flexible"", another may describe it negatively as ""complex"". People describe Drupal differently not only as a result of their professional backgrounds, but also based on what they've heard and learned. If you ask different people what Drupal is for, you'll get many different answers. This isn't a surprise because over the years, the answers to this fundamental question have evolved. Drupal started as a tool for hobbyists building community websites, but over time it has evolved to support large and sophisticated use cases. Perception is everything Perception is everything; it sets expectations and guides actions and inactions. We need to better communicate Drupal's identity, demonstrate its true value, and manage its perceptions and misconceptions. Words do lead to actions. Spending the time to capture what Drupal is for could energize and empower people to make better decisions when adopting, building and marketing Drupal. Truth be told, I've been reluctant to define what Drupal is for, as it requires making trade-offs. I have feared that we would make the wrong choice or limit our growth. Over the years, it has become clear that not defining what Drupal is used for leaves more people confused even within our own community. For example, because Drupal evolved from a simple tool for hobbyists to a more powerful digital experience platform, many people believe that Drupal is now ""for the enterprise"". While I agree that Drupal is a great fit for the enterprise, I personally never loved that categorization. It's not just large organizations that use Drupal. Individuals, small startups, universities, museums and non-profits can be equally ambitious in what they'd like to accomplish and Drupal can be an incredibly solution for them. Defining what Drupal is for Rather than using ""for the enterprise"", I thought ""for ambitious digital experiences"" was a good phrase to describe what people can build using Drupal. I say ""digital experiences"" because I don't want to confine this definition to traditional browser-based websites. As I've stated in my Drupalcon New Orleans keynote , Drupal is used to power mobile applications, digital kiosks, conversational user experiences , and more. Today I really wanted to focus on the word ""ambitious"". ""Ambitious"" is a good word because it aligns with the flexibility, scalability, speed and creative freedom that Drupal provides. Drupal projects may be ambitious because of the sheer scale (e.g. The Weather Channel ), their security requirements (e.g. The White House ), the number of sites (e.g. Johnson & Johnson manages thousands of Drupal sites), or specialized requirements of the project (e.g. the New York MTA powering digital kiosks with Drupal). Organizations are turning to Drupal because it gives them greater flexibility, better usability, deeper integrations, and faster innovation. Not all Drupal projects need these features on day one -- or needs to know about them -- but it is good to have them in case you need them later on. ""Ambitious"" also aligns with our community's culture. Our industry is in constant change (responsive design, web services, social media, IoT), and we never look away. Drupal 8 was a very ambitious release; a reboot that took one-third of Drupal's lifespan to complete, but maneuvered Drupal to the right place for the future that is now coming. I have always believed that the Drupal community is ambitious, and believe that attitude remains strong in our community. Last but not least, our adopters are also ambitious. They are using Drupal to transform their organizations digitally, leaving established business models and old business processes in the dust. I like the position that Drupal is ambitious. Stating that Drupal is for ambitious digital experiences however is only a start. It only gives a taste of Drupal's objectives, scope, target audience and advantages. I think we'd benefit from being much more clear. I'm curious to know how you feel about the term ""for ambitious digital experiences"" versus ""for the enterprise"" versus not specifying anything. Let me know in the comments so we can figure out how to collectively change the perception of Drupal. PS: I'm borrowing the term ""ambitious"" from the Ember.js community . They use the term in their tagline and slogan on their main page.",en,120
65,3066,1486553183,CONTENT SHARED,4876769046116846438,9109075639526981934,1133978080643679329,"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36",MG,BR,HTML,http://ciclovivo.com.br/noticia/shopping-em-bh-tera-fazenda-urbana-de-2-700-m%C2%B2/,shopping em bh terá fazenda urbana de 2.700 m²,"O projeto apoiado no conceito 'farm to table' tem previsão de inaugurar em março. 1 de fevereiro de 2017 * Atualizado às 15 : 08 Além da estufa, o espaço contemplará uma loja para a venda dos produtos cultivados e um restaurante. | Foto: Divulgação Em uma parceria inédita entre o Boulevard Shopping e a Startup BeGreen, a capital mineira receberá sua primeira Fazenda Urbana - um espaço no próprio Boulevard para a produção de hortaliças sem agrotóxicos (orgânica) com um sistema inovador de cultivo em consórcio com a criação de peixes. Só existem oito unidades deste tipo no mundo. O novo projeto, que tem previsão de inaugurar na segunda quinzena de março, funcionará na área externa do shopping e trará todo um conceito alicerçado na sustentabilidade. No espaço haverá utilização de composto proveniente do lixo orgânico da Praça de Alimentação do Boulevard como substrato para o crescimento das verduras; e redução de consumo de água com captação da chuva. Além disso, não haverá emissão de CO², pois além da autossuficiência elétrica do projeto, o consumidor final irá adquirir os produtos da fazenda in loco, sem serviços de logísticas e entrega. A Fazenda Urbana sediada no Boulevard irá ocupar uma área de 2.700 m², com uma estufa de 1.500 m² e capacidade para produzir até 50 mil pés de alfaces baby e ervas por mês. Além da estufa, o espaço contemplará a loja Casa Horta, para a venda dos produtos cultivados e de produtores locais; A Casa Amora, restaurante conceito 'farm to table' (da fazenda para a mesa) com pratos que utilizem as hortaliças e legumes orgânicos e da produção local; e um espaço de convivência com mesas, deck e palco para realizações de eventos relacionados à vida mais saudável e à conscientização da nova agricultura. A Fazenda Urbana BeGreen Boulevard O projeto ainda contempla várias ações integradas, como a realização de ações de conscientização de crianças e jovens de escolas públicas e privadas, e eventos e treinamentos de produção sustentável que pretendem atingir pelo menos 1 milhão de pessoas por ano. ""Os visitantes terão a oportunidade de fazer visitas guiadas, comprar os produtos cultivados na fazenda e até consumi-los na hora."", conta Paulo Ceratti, gerente de marketing do Boulevard. ""Nosso principal objetivo é demonstrar que é possível ser sustentável e produtivo gerando mais empregos, menos lixo e sem prejudicar o meio ambiente. É um projeto inovador, que segue um movimento global que aproxima a produção do consumidor final."", explica um dos idealizadores do projeto, Giuliano Bitencourt. Outros produtores locais poderão vender seus produtos agroecológicos na loja Casa Horta. ""A intenção é que aconteça um comércio justo para os produtores e clientes que procuram por alimentos saudáveis, frescos e locais."", acrescenta Paulo. A sustentabilidade também fez parte da construção do espaço. O projeto primou por utilizar o mínimo de novos produtos, tendo como norte a reutilização de materiais. Para isso, toda a obra será feita de containers que virariam sucata; as mesas e cadeiras do espaço terão como matéria prima a madeira plástica, que sofre um processo de transformação do plástico jogado fora; e todo o piso será feito com material de rejeito de mineração. Por que uma fazenda urbana? A motivação original deste projeto se deve ao fato que atualmente 80% de todo produto fresco cultivado no Brasil é desperdiçado e acaba tendo como destino o lixo. Estes são dados da FAO - Organização das Nações Unidas para Alimentação e Agricultura. Tamanho desperdício tem origem na estrutura da cadeia de suprimentos agrícolas brasileira, que faz com que os alimentos colhidos frescos só cheguem à mesa do consumidor com no mínimo dois dias de colhidos. Além disso, o excesso de transporte e baldeações entre os diversos elos da cadeia aumenta ainda mais o desperdício. Esta complexa cadeia de suprimentos além de prejudicar a qualidade do produto final, também impacta no valor total pago pelo consumidor. Um exemplo disso é a trajetória de uma alface até chegar ao prato do consumidor final. Cultivada em uma grande fazenda, a alface é colhida e armazenada de forma incorreta para então ser transportada por centenas de quilômetros em um caminhão, no qual outros pés ficam pelo caminho, até chegar a um centro de distribuição (CEASA). Só então ela será entregue a uma feira ou supermercado e ficar à disposição do cliente final. E se a alface tivesse sido cultivada bem ao lado da casa deste consumidor? Esta é a proposta da Begreen e do Boulevard: uma fazenda bem perto do consumidor final, sem intermediários, sem transporte, com um produto retirado direto do cultivo para a mesa. A ideia de se criar este projeto surgiu quando Giuliano Bitencourt, um dos empreendedores, voltou do MIT em 2014 apaixonado pela ideia de poder cultivar dentro das grandes cidades. Para criar a BeGreen, convidou seu sócio Pedro Graziano, com uma trajetória profissional no ramo tecnológico, para se juntar ao projeto. Por meio do uso de tecnologia inovadora, a parceria entre o Boulevard e a BeGreen torna capaz de produzir de forma automatizada produtos agrícolas frescos e sem agrotóxicos. Tudo isso dentro de centros urbanos, com produtividade superior por metro quadrado, consumo reduzido de água e sem intermediários na distribuição dos alimentos. As fazendas urbanas parecem ser tendência no mundo. Recentemente, um shopping em Israel também inaugurou uma em sua cobertura ( veja aqui ) e no Brasil, o Shopping Eldorado, em São Paulo, também foi pioneiro em suas ações e abriga uma horta e um sistema de compostagem em sua cobertura desde 2012. ( veja aqui ) (25932)",pt,119
66,1459,1466443111,CONTENT SHARED,4788854083489560153,5127372011815639401,-5274409486477114174,,,,HTML,https://blog.risingstack.com/node-js-examples-how-enterprises-use-node-in-2016/,node.js examples - how enterprises use node in 2016 | @risingstack,"Node.js had an extraordinary year so far: npm already hit 4 million users and processes a billion downloads a week, while major enterprises adopt the language as the main production framework day by day. The latest example of Node.js ruling the world is the fact that NASA uses it ""to build the present and future systems supporting spaceship operations and development."" - according to the recent tweets of Collin Estes - Director of Software Technologies of the Space Agency. Fortunately, the Node Foundation's ""Enterprise conversations"" project lets us peek into the life of the greatest enterprises and their use cases as well. This article summarizes how GoDaddy, Netflix, and Capital One uses Node.js in 2016. GoDaddy ditched .NET to work with Node.js Charlie Robbins is the Director of Engineering for the UX platform at GoDaddy. He is one of the longest-term users of the technology, since he started to use it shortly after watching Ryan Dahl's legendary Node.js presentation at JSConf in December 2009 and was one of the founders of Nodejitsu. His team at GoDaddy uses Node.js for both front-end and back-end projects, and they recently rolled out their global site rebrand in one hour thanks to the help of Node.js. Before that, the company primarily used .NET and was transitioning to Java. They figured out that despite the fact that Microsoft does a great job supporting .NET developers and they've made .NET open source, it doesn't have a vibrant community of module publishers and they had to rely too much on what Microsoft released. ""The typical .NET scenario is that you wait for Microsoft to come out with something that you can use to do a certain task. You become really good at using that, but the search process for what's good and what's bad, it's just not a skill that you develop."" Because of this, the company had to develop a new skill: to go out and find all the other parts of the stack. As opposed to other enterprise technologies like .NET where most of the functionality was included in the standard library, they had to become experts in evaluating modules. GoDaddy started to use Node for the front-end and then ended up using it more in the back-end as well. The same .NET engineers who were writing the back-end code were writing the JavaScript front-end code. The majority of engineers are full stack now. The most exciting things for Charlie about Node.js are being handled mainly by the working groups . ""I'm very excited about the tracing working group and the things that are going to come out of that to build an open source instrumentation system of eco-tooling."" Other exciting things for him are the diagnostics working group (previously: inclusivity) and the Node.js Live events - particularly Node.js communities in countries where English is not used. Places like China, for example, where most of the engineers are still primarily speaking Chinese, and there's a not a lot of crossovers. ""I'm excited to see those barriers start to come down and as those events get to run."" As of talking about GoDaddy and Node: they have just released the project that they've been working on pretty extensively with Cassandra. It was an eight-month long process, and you can read the full story of ""Taming Cassandra in Node.js"" at the GoDaddy engineering blog. Netflix scales horizontally thanks to its Node container layer The next participants in Node Foundations enterprise conversation series are Kim Trott , the director of UI Platform Engineering and Yunong Xiao , Platform Architect from Netflix. Kim's been at Netflix for nine years - she just arrived before the company launched its first streaming service. It was the era when you could only watch Netflix with Windows Media Player, and the full catalog consisted only 50 titles. ""I've seen the evolution of Netflix going from DVD and streaming to now being our own content producer."" Yunong Xiao, who's well known for being the maintainer of restify arrived two years ago, and just missed the party the company held for reaching 15 million users - but since they are fastly approaching their 100 millionth subscribers, he'll have a chance to celebrate soon. Yunong previously worked at Joyent on Node.js and distributed systems, and at AWS as well. His role at Netflix is to have Node up and running in scale and making sure it's performing well. Kim manages the UI platform team within the UI engineering part of the organization. Their role is to help all the teams building the Netflix application by making them more productive and efficient. This job can cover a wide range of tasks: it could be building libraries that are shared across all of the teams that make it easier to do data access or client side logging, and building things that make easier to run Node applications in production for UI focused teams. Kim provided us a brief update on how the containerization of the edge services have been going at Netflix - since she talked about it on Node Interactive in last years December. When any device or client tries to access Netflix, they have to use something what's called edge services, which is a set of endpoint scripts - a monolithic JVM based system, which lets them mutate and access data. It's been working really well, but since it's a monolith, Netflix met some vertical scaling concerns. It was a great opportunity to leverage Node and Docker to be able to scale horizontally all of this data access scripts out. ""Since I've spoken at Node Interactive we've made a lot of progress on the project, and we're actually about to run a full system test where we put real production traffic through the new Node container layer to prove out the whole stack and flush out any problems around scaling or memory, so that's really exciting."" How Node.js affected developer productivity at Netflix? The developer productivity comes from breaking down the monolith into smaller, much more manageable pieces - and from being able to run them on local machines and do the containerization. We can effectively guarantee that what you're running locally will very closely mirror what you run in production and that's really beneficial - told Kim. ""Because of the way Node works we can attach debuggers, and set breakpoint steps through the code. If you wanted to debug these groovy scripts in the past, you would make some code changes upload it to the edge layer, run it, see if it breaks, make some more changes, upload it again, and so on.."" It saves us tens of minutes to test, but the real testament to this project is: all of our engineers who are working on the clients are asking: when do we get to use this instead of the current stack? - told Yunong. The future of Node at Netflix Over the next few months, the engineering team will move past building out the previously mentioned stack and start working on tooling and performance related problems. Finding better tools for post-mortem debugging is something that they're absolutely passionate about. They are also planning to be involved in the working groups and help contribute back to the community and so that they can build a better tool that everyone can leverage. ""One of the reasons why Node is so popular is the fact that it's got a really solid suite of tools just to debug, so that's something that we're actually working contributing on."" Node.js brings joy for developers at Capital One Azat Mardan is a technology fellow at Capital One and an expert on Node.js and JavaScript. He's also the author of the Webapplog.com, and you've probably read one of his most popular book: Practical Node.js. ""Most people think of Capital One as a bank and not as a technology company, which it is. At Capital One, and especially this Technology Fellowship program, we bring innovation, so we have really interesting people on my team: Jim Jagielski and Mitch Pirtle. One founded Apache Software Foundation and the other, Joomla!, so I'm just honored to be on this team."" Azats goal is to bring Node.js to Capital One and to teach Node.js courses internally, as well as to write for the blog, and provide architectural advice. The company has over 5,000 engineers and several teams who started using Node.js at different times. Capital One uses Node.js for: Hygieia, which is an open-source dashboard for DevOps. It started in 2013 and was announced last year at OSCON, and it has about 900 GitHub stars right now. They're using Node.js for the frontend and for the build too. Building the orchestration layer. They have three versions of the Enterprise API, and it's mostly built with Java, but it's not convenient to use on the front end. Capital One uses Angular mostly, but they have a little bit of React as well. In this case, the front-facing single page applications need something to massage and format the data - basically to make multiple codes to the different APIs. Node.js works really great for them for building this orchestration layer. ""It's a brilliant technology for that piece of the stack because it allows us to use the same knowledge from the front end, to reuse some of the modules, to use the same developers. I think that's the most widespread use case at Capital One, in terms of Node.js."" The effect of Node.js on the company Node.js allows much more transferable skill-sets between the front end and some of the back-end team, and it allows them to be a little bit more integrated. ""When I'm working with the team, and whether it's Java or C# developers, they're doubling a little bit on front ends; so they're not experts but once they switch to the stack where Node.js is used in the back end, they're more productive because they don't have that switch of context. I see this pure joy that it brings to them during development because JavaScript it just a fun language that they can use."" From the business perspective: the teams can reuse some of the modules and templates for example, and some of the libraries as well. It's great from both the developers and from the managerial perspective. Also, Node has a noticeable effect on the positions and responsibilities of the engineers as well. Big companies like Capital One will definitely need pure back-end engineers for some of the projects in the future, but more and more teams employ ninjas who can do front-end, back-end, and a little bit of DevOps too - so the teams are becoming smaller. Instead of two teams, one is a pure back end, and one is a pure front end - consisting seven people overall - a ninja team of five can do both. ""That removes a lot of overhead in communication because now you have fewer people, so you need fewer meetings, and you actually can focus more on the work, instead of just wasting your time."" The future of Node.js Node.js has the potential to be the go-to-framework for both startups and big companies, which is a really unique phenomenon - according to Azat. ""I'm excited about this year, actually. I think this year is when Node.js has gone mainstream."" The Node.js Interactive in December has shown that major companies are supporting Node.js now. IBM said that Node.js and Java are the two languages for the APIs they would be focusing on, so the mainstream adoption of the language is coming, unlike what we've seen with Ruby - he told. ""I'm excited about Node.js in general, I see more demand for courses, for books, for different topics, and I think having this huge number of front-end JavaScript developers is just a tremendous advantage in Node.js."" Start learning Node! As you can see, adopting Node.js in an enterprise environment has tremendous benefits. It makes the developers happier and increases the productivity of the engineering teams. If you'd like to start learning it I suggest to check out our Node Hero tutorial series . Share your thoughts in the comments.",en,119
67,2256,1472833539,CONTENT SHARED,-8381230866408697127,1766257854965201953,9211894396612985323,,,,HTML,http://m.olhardigital.uol.com.br/pro/noticia/inteligencia-artificial-da-ibm-criou-trailer-perfeito-para-filme/61735,inteligência artificial da ibm criou 'trailer perfeito' para filme,"A produtora 20th Century Fox fez uma parceria com a IBM para criar o trailer do filme Morgan . O filme fala sobre um super ser humano com modificações cibernéticas; por isso, a equipe de pesquisa da IBM usou a plataforma de inteligência artificial Watson para editar o ""trailer perfeito"" para o filme. Ele pode ser visto no final desta nota. Para realizar essa tarefa, o Watson ""assistiu"" aos trailers de 100 filmes de suspense e terror, divididos em cenas. Em cada cena, o sistema avaliou separadamente os sons, a composição das imagens e as emoções geradas pelo filme. Para analisar esse último aspecto, a inteligência artificial examinou as expressões faciais dos atores, os objetos usados nas cenas e a gradação de cores do filme. Em seguida, o sistema assistiu ao filme Morgan e separou as 10 cenas que seriam as melhores candidatas para o trailer. Segundo a IBM, o Watson conseguiu escolher cenas que nenhum dos editores tinha pensado em incluir; por outro lado, uma das dez cenas selecionadas pela máquina acabou não sendo incluída no trailer. Colaboração O processo também contou com a participação de humanos no processo de edição, já que o Watson não era capaz de fazer isso. Um editor colocou os letreiros, ordenou os momentos e selecionou a música para o trailer. Com a ajuda da inteligência artificial, o processo, que segundo a IBM pode levar de 10 a 30 dias, foi completado em 24 horas. Esse tempo começou a ser contado no momento em que o editor viu as imagens selecionadas pelo Watson até a hora em que o trailer ficou pronto. Uma utilização artística ou estética de inteligências artificiais não é exatamente novidade. O Google, por exemplo, está usando sua plataforma open-source Tensorflow para ensinar computadores a fazer arte , e esse processo já deu resultados . A empresa também usa essa tecnologia para combater cegueira e até ajudar a tratar câncer . O própri Watson já foi usado para criar desde sistemas de combate ao cibercrime até um chapéu seletor do Harry Potter . Dirigido por Luke Scott (o filho de Ridley Scott, diretor de Alien ), Morgan conta a história de um ser humano artificial criado ""em laboratório"". Morgan, como ele é chamado, é mantido em cativeiro e estudado por cientistas; no entanto, ele acaba se desenvolvendo mais rapidamente e melhor do que o esperado, e então se volta contra seus criadores. O trailer do filme, marcado para estrear dia 2 de setembro nos EUA, pode ser visto abaixo:",pt,118
68,1175,1464785508,CONTENT SHARED,-7463305179076477879,2416280733544962613,5253606329297313273,,,,HTML,http://techcrunch.com/2016/06/01/salesforce-buys-demandware-for-2-8b-taking-a-big-step-into-e-commerce/,"salesforce buys demandware for $2.8b, taking a big step into e-commerce","Salesforce made its name originally with cloud-based software to help salespeople manage their leads and close deals; and today the company took a big step into the business of sales themselves. Today the company announced that it would spend $2.8 billion to acquire Demandware, a cloud-based provider of e-commerce services to businesses big and small, which will spearhead a newer business area: the Salesforce Commerce Cloud. Demandware is a publicly-traded company, and Salesforce says that it will commence a tender offer for all outstanding shares of Demandwarefor $75.00 per share, in cash. This is a big premium on the company's current valuation - which was $1.87 billion at close of trade yesterday . The transaction is expected to close in Salesforce's Q2 2017, which ends July 31, 2016. ""Demandware is an amazing company-the global cloud leader in the multi-billion dollar digital commerce market,"" said Marc Benioff, chairman and CEO, Salesforce, in a statement. ""With Demandware, Salesforce will be well positioned to deliver the future of commerce as part of our Customer Success Platform and create yet another billion dollar cloud."" ""Demandware and Salesforce share the same passionate focus on customer success,"" said Tom Ebling, CEO, Demandware, also in a statement. ""Becoming part of Salesforce will accelerate our vision to empower the world's leading brands with the most innovative digital commerce solutions that enable them to connect 1:1 with customers across any channel."" This is a huge deal for Salesforce, as it extends the types of transactions that it will be open to with existing customers, and also gives it a new group of customers to upsell for other services that it offers, from marketing and online analytics through to back-office software for sales and other IT functions. Demandware customers include Design Within Reach, Lands' End, L'Oreal and Marks & Spencer, the company said. It also opens up Salesforce to competition with the likes of Amazon, who not only provides third parties with commerce software, but would prefer to be the go-to platform for transactions. Gartner estimates that worldwide spending on digital commerce platforms is expected to grow at over 14 percent annually, reaching $8.544 billion by 2020, according to figures provided by salesforce. Salesforce says its new Commerce Cloud ""will be an integral part of Salesforce's Customer Success Platform, creating opportunities for companies to connect with their customers in entirely new ways. Salesforce customers will have access to the industry's leading enterprise cloud commerce platform, and Demandware's customers will be able to leverage Salesforce's leading sales, service, marketing, communities, analytics, IoT and platform solutions to deliver a more comprehensive, personalized consumer experience."" more to come.",en,118
69,1424,1466115486,CONTENT SHARED,7534917347133949300,3891637997717104548,1233433224046298458,,,,HTML,https://www.sitepoint.com/the-importance-of-code-reviews/,the importance of code reviews,"I recently read this on Twitter : Sadly, it seems code reviewing is a practice that's foreign to many students, freelancers and agencies. [Translated] Apparently, it's not obvious to everyone that code reviews are actually helpful. Call me naive, but I really thought it was a process used in all IT companies. Apparently I was wrong, and it scares me. In this article, I'd like to give my thoughts on code reviews, why I believe they're an important part of the code shipping process, and how to get started with them. If you don't do code reviews, or if you feel like you could do better, I hope this write-up will help! What Is a Code Review? We live in the era of Wikipedia, so allow me to begin by quoting the definition given on the Code review entry: Code review is systematic examination (sometimes referred to as peer review) of computer source code. It is intended to find mistakes overlooked in the initial development phase, improving the overall quality of software. Reviews are done in various forms such as pair programming, informal walkthroughs, and formal inspections. A code review, as the name states, is the process of reviewing some code in order to make sure it works, and in order to improve it where possible. Ways to Do a Code Review As the Wikipedia definition notes, there are various ways to perform code reviews. However, in a world where so much code lives on GitHub , code reviewing often goes hand-in-hand with what we call a ""pull request"". A pull request is a request to introduce changes to a code repository using a distributed version control system (Git, SVN, Mercurial etc.). It works by ""pulling"" the original code, applying changes, then submitting a request to merge the changes in. GitHub made this process particularly easy and efficient thanks to its friendly user interface, abstracting most of the Git knowledge requirements. Why Reviewing Code Matters So, why does code reviewing matter? After all, we're all competent here. Surely we can ship code without having someone metaphorically standing over our shoulder, watching everything we do. In theory, yes. But in practice, there are many reasons why having an established code reviewing process helps. Let's look at a few of them. It limits risks This is probably the most important reason of all. Having someone double-checking our work never hurts, and limits the risk of unnoticed mistakes. Even good developers get tunnel vision sometimes. It's always good to make sure not to forget anything. For instance, proper keyboard navigation, screen reader accessibility, flexibility for internationalization and friendly, non-JavaScript behavior are often forgotten topics in the front-end world, to name but four. It dramatically improves code quality Let's make something clear: this is not about standards and code linting (at least not exclusively). It's about making code more efficient. In a team where everybody has their own background and strong suits, asking for improvements (because that's what it's about) is always a good idea. Someone could suggest a smarter solution, a more appropriate design pattern, a way to reduce complexity or to improve performance. It makes everyone better By joining forces, everyone can learn and get better. The code submitter is likely to receive feedback on their work, making them aware of possible problems and areas for improvement. The reviewers could well learn new things by reading through the code, and figure out solutions applicable to their own work. It helps being familiar with the project When a team works on a project, it's highly unlikely that every developer is working on every part of the application. Sometimes a developer will heavily work on one large part for a while, while another one is working on something else entirely. Doing code reviews helps people familiarize themselves with code they haven't written but might be asked to maintain in the future. It promotes knowledge of the codebase across the team, and is likely to speed up future development. How To Do It Properly Again, having an established code reviewing process is both extremely useful and important. Every team producing code should have some code review, one way or the other. That being said, doing meaningful and helpful code reviews is not always as straightforward as it might seem. Worry not, it's not like it's going to bite you if it's done poorly. It simply won't be useful, and could feel like a waste of time. Recently at my workplace, we had a retrospective about our code reviewing process. We knew some things were wrong when we realized only 3 out of 12 developers were engaging in code reviews. To help us change this, one of our Scrum Masters organized a retrospective to determine where there was room for improvement, and how we could bring it about. Planning ahead The most recurrent argument to justify the lack of participation in code reviews was that it takes time - time that people can't or aren't willing to take. I must say I don't really understand this argument myself, because I picture it like this: if a colleague comes to me directly and asks me to help them with something, I'm not going to say - ""Don't have time, not interested."" I'm going to find time to help. Maybe not right now, maybe in an hour - but I will obviously take time for them. Why? Because this is what being part of a team means if they want my opinion, it's because they value it one way or another, and therefore it makes only sense to give them. ""Why don't you take part in the code reviewing process?"" ""I don't have time."" To me, a pull request is no different from a coworker asking for help. Saying you don't have time is perfectly fine from time to time, but by systematically refusing to help, you're actively pulling yourself out of the team. This behavior is neither friendly nor positive. Take the time to help. To make it possible for developers to find time, we started taking into account that every developer will spend a bit of time (maybe 30 minutes) reviewing code every day. No more surprise when we end up spending half an hour on a large code review: it's part of the day. We also tried to dramatically reduce the amount of code constituting a pull request. We used to have mammoth pull requests - of thousands of changes across dozens of files. We try not to do that anymore. By making smaller pull requests, we make them easier to review, the feedback more relevant, and developers more willing to engage in this process. ""Ship small and often."" Giving context The second biggest problem we found was that we usually lacked an understanding of the code's context, which is needed if you're going to provide helpful feedback. When missing the context, we usually did no more than a syntax review - which, though useful to some extent, is not enough. You simple become what we call a ""human linter"". Fortunately, the solution to this problem is relatively simple: add a description to the pull request to explain what the objective is, and how to get there. It doesn't have to be a wall of text; just a few lines are usually enough. It also helps to add a link to the issue and/or story. Liv Madsen , one of our developers, even adds screenshots - or screencasts when relevant - to illustrate what she's done, which is amazing. Actually asking The third problem we pointed out was that we sometimes simply didn't realize there was something to review. Let's face it, we're flooded with tons of emails and notifications every day - so much so that it can be hard to keep track. We're only human, after all. Here, again, the solution is pretty simple: actually ask someone for a review. There are many ways to do that, from honking a horn in the office to pinging someone on Slack; to each team their own. We created groups on GitHub based on our activity, and when submitting a pull request, we always ping a group. Members of this group will receive a notification and are free to tackle it as soon as they have time. Sometimes, we ping a developer (or several) directly when it's more specific to someone's work. That also works. From there, pinged people can review the code and leave comments. We try to leave a comment even when there's nothing specific to report - if only to indicate that the code is ready to be merged. Because we had some pull requests blindly merged regardless of given comments, we established a strict ""reply or fix everything"" policy. When receiving feedback, either you fix it or you reply to explain why you didn't. In any case, you never leave a comment pending, and you certainly don't merge your pull request with non-handled comments. Wrapping Things Up Having a regular and efficient code reviewing process is essential to maintain high-quality code standards, grow as a team and share knowledge between developers. Asking for a code review is not a mark of weakness. There's nothing embarrassing about asking for help, and certainly not in the form of a code review. Accept all feedback given to you, and offer constructive (ideally positive) comments to people submitting pull requests. Find what works for you. Reviewing code should be a large part of the code shipping process, so you should tailor it to your team. Make it the way you want so that it's helpful and positive for everybody. Happy reviewing!",en,117
70,2375,1474371643,CONTENT SHARED,-4145260601063545880,-1393866732742189886,5698474775920535228,,,,HTML,https://hackernoon.com/why-learning-angular-2-was-excruciating-d50dc28acc8a?gi=13bcfa27581a,why learning angular 2 was excruciating,"A few months ago I decided to try my hand at web development. Angular 2 was new and shiny at the moment, so I decided to download it and start building a website. Now, as someone who is primarily a Java developer, this was an altogether new and very interesting experience. I followed the alleged ""5 Minute Quick Start Guide"" and after an hour and a half of wrangling with Angular 2 and its plethora of dependencies, I was able to get something up and running. Next, I started to build a real app. I decided to build a personal blog platform from scratch, mostly for the learning, and partially because I've been meaning to start a blog for ages. The desire to have a blog would be the carrot keeping me motivated to learn new technologies and keep building my project. Over the months, I've been keeping up with the Angular 2 releases and, each weekend, I've been chugging along on the blog. Oh, did I say chugging? I meant banging my head vigorously against the wall over and over and over again trying to understand and deal with the freaking Javascript ecosystem. Maybe I'm just used to the slow paced, sturdy, battle tested libraries of the Java world. Maybe learning web development starting with Angular 2 is like trying to learn a new video game and starting on hard core mode. I don't know. All I know is that the Angular 2 release was, from my perspective as a Java engineer, a freaking disaster. Every single minor release made lots of breaking changes. Every single time I checked back on the accursed ""5 Minute Quick Start"", huge swaths of it had completely changed. Don't even get me started on the whole Angular 2 router business. That was truly a fiasco, and agonizing to deal with as a user, particular one who is less familiar with the Javascript world. I find myself writing this post in a moment of passion, fuming over the latest Angular release which was announced a few days ago. Angular 2, for rulz this time, has been released and you can upgrade to it and not have to deal with breaking changes for a whopping 6 months! Well then, I naively thought to myself, I should upgrade my blog from @angular 2.0.0-rc.4 to @angular 2.0.0. This was the journey that I just underwent: Upgrade to @angular 2.0.0 Remove all 'directives' fields from my Components. Apparently Angular2 has decided that Modules are the way to go. Get rid of all imports anywhere that end with _DIRECTIVES. Upgrade @angular/forms from 0.3.0 to 2.0.0. For some reason, @angular/forms was super behind the rest of the versioning for angular. Until this release. Upgrade angular-material to 2.0.0-alpha.8-2 (can we just pause for a second and appreciate the ridiculousness of a version called 2.0.0-alpha.8-2??). Upgrade to typescript 2.o, which, as I was unpleasantly surprised by, is currently in beta. Having finally reached a version of relative stability in Angular, it was dismaying to realize that angular-material, a key tool in my stack, has unstable dependencies that are ahead of Angular 2's dependencies. By this point, `npm start` was working. This is where the hard part began, because now I had to deal with tremendously cryptic error messages that have been plaguing me ever since I started learning Angular 2. Such as this one: After some troubleshooting (by now I've gotten okay at debugging System JS's useless error messages), the issue was caused by this errant line in my systemjs.config.js file: I guess @anguler/router has a umd now. Whatever a umd is......... The next issue I encountered after that was this: Great, a freaking syntax error from a random file in angular-material. There is no helpful error message, no line numbers. There is precious little to help me figure out what to do next. I don't know whether to upgrade dependencies, downgrade dependencies, install new dependencies, change the syntax of my tsconfig.js file (which undoubtedly changed when I upgraded to typescript 2.0). I'm lost in a sea of confusion and frustration. Now, those of you who are seasoned web developers can probably troubleshoot an issue like this fairly easily. That's not what I'm getting at. It's not about the particular issue I just faced. It's that the Javascript ecosystem is utter chaos. Every new version of every new library comes with a slew of breaking changes. New libraries will be released before their APIs are nailed down. Libraries in beta are now old news because only alpha libraries are new and interesting. Stackoverflow posts that are older than 6 weeks old are no longer relevant and probably deal with an issue from an old version that I'm no longer using. The Java engineer in me is screaming with the frustration of this ecosystem. What sort of madness has web development devolved into? What the f**cking f**ck is going on with Javascript these days??? Okay, I think it's time to take a step back and say that I actually love Angular 2. When I get to a point where all the dependencies have been upgraded, and everything is working, when Intellij is underlining the correct things and my typescript compiler is hooked up right, Angular 2 is awesome. To be fair, I have been using versions of the library that have thus far not been officially released. Maybe, you say, it's my fault for trying to download and use a version of the library that is still in alpha/beta/release candidate and expecting it to work and be relatively easy to use. Perhaps you are right. But, considering the fact that hundreds of thousands of developers are already using Angular 2, we should ask ourselves the question: is it responsible to release libraries that are still very much a work in progress? Does it make sense to announce a release candidate and then make tons and tons of breaking changes over the course of evolving from rc.1 to rc.6? How many hundreds of thousands of human hours were wasted dealing with the pain of upgrading a version of Angular 2 and all of its related dependencies? And, as most engineers will attest to, developer hours are a precious commodity in today's job market. How many developers have been utterly burned by the experience of trying to use Angular 2 and have sworn off Angular forevermore in favor of React? How many other Javascript libraries are out there that have also caused such frustration and undue anguish for their users? Perhaps the Javascript ecosystem is just experiencing intense growing pains. Maybe developers are quickly understanding the errors in their initial designs and brazenly correcting them as they blaze on in iterating through better and better versions of their libraries. Maybe the Javascript world is where the vast majority of software engineers will live soon, since the tools there will have evolved rapidly and effectively. Perhaps people will accomplish more in Javascript and the tools written in Javascript will be easier to use than the tools in any other software engineering landscape. Or, maybe, just maybe, the Javascript world will always be sort of a joke, a place where engineering hipsters go to waste time and feel like they're on the cutting edge of innovation when really, they are living in a world of madness and chaos, throwing hours of productivity down the drain so that they can use the latest and 'greatest' tools out there. But I am just a humble Java engineer. What I'm most interested in is: what do you think?",en,117
71,1398,1465992728,CONTENT SHARED,-4888170580455425266,-8424644554119645763,-1573551387441998743,,,,HTML,https://www.jetbrains.com/help/resharper/2016.1/Speeding_Up_ReSharper.html,speeding up resharper (and visual studio),"This document presents a list of performance optimizations that can be applied if you experience performance issues with Visual Studio and ReSharper. Some of the tricks presented are ReSharper-specific, whereas others will affect Visual Studio performance whether you have installed ReSharper or not. We constantly make sure that ReSharper works fine on modern hardware and with medium- and large-size solutions without any tweaking. We believe that Visual Studio developers are working towards the same things. By trying to speed up ReSharper on outdated hardware, you deprive yourself of great features that can speed up your development performance. In this topic: Speeding up ReSharper Disable code analysis for current file You can temporarily disable design time code inspection for the current file by pressing Ctrl+Shift+Alt+8 . Pressing the shortcut again will re-enable the inspection. You can spot the status of code analysis in the current file by the status indicator : If you want to bind a different shortcut for this operation, look for the ReSharper_EnableDaemon command. Disable code analysis for specific files You can tell ReSharper to skip analyzing certain files without opening them. For example, you can skip files that contain well tested algorithms and that do not change much. To do so, go to and scroll to the Element to skip section, where you can pick the files and folders to skip. You can also specify a file mask for skipping files. You will also notice that all files where you disabled code analysis with Ctrl+Shift+Alt+8 are already in the list of ignored files. Turn off Solution-Wide Analysis On very large projects, turning on Solution-Wide Analysis may cause performance degradation, particularly on less powerful hardware. If you find this analysis to be taking up too many resources, simply switch it off: right-click the SWA circle in the bottom right corner of Visual Studio window and choose Stop Solution-Wide Analysis or Pause Analysis. A dialog box will pop up asking whether you want to turn off SWA. Say Yes and you're done. Disable context actions In ReSharper options, go to and , and uncheck the that are less helpful to you. Speed up typing If you experience slowdown while typing, you can turn off member signatures, symbol types and summary in completion lists - go to and clear the corresponding check-boxes: If this doesn't help, switch back to built-in Visual Studio IntelliSense under : Disable auto-formatting To speed up typing, you can also disable auto-formatting options under to avoid reformatting code while typing: Speed up code templates To speed up expanding code templates , you can turn off the Reformat and Shorten qualified references options for templates that you use: Disable unit testing If you don't use the ReSharper unit test runner , you can save some processing time by turning it off. Go to and clear the Enable Unit Testing check-box. Disable the navigation bar If you use File Structure Window , then you probably don't use the navigation bar on top of the editor. If so, you can disable the navigation bar by clearing the corresponding check-box in Visual Studio options: . If nothing helps If you've tried out everything described above and the performance is still down, you can temporarily disable ReSharper and check whether it was the cause of the slowdown. To disable/enable ReSharper, go to and click Suspend Now/Resume Now. If suspending ReSharper helps improve the performance but you still want to use it occasionally for code analysis , code cleanup or reformatting code , you might want to have a shortcut that quickly switches ReSharper on and off. Here is how to do it: go to and find the ReSharper_ToggleSuspended command, then press a shortcut of your choice and click Assign. Known Performance Problems The following is a list of known performance problems and their solutions. VS2010 with ReSharper on Windows XP slowness This known issue can be resolved by installing the Windows Automation API 3.0. For further details, see this article . Please note that this fix applies to Windows XP only - later Windows operating systems already have this API installed. Performance degradation after ReSharper upgrade If you have recently updated ReSharper and observe performance degradation with solutions that were opened with previous versions, you can attempt to speed thing up by clearing the ReSharper caches and deleting the solution .suo file. To clear the caches, go to and click Clear Caches. You can also tweak the Store solution caches in selector on this page: performance can be improved if your selected caches storage is mapped to a faster storage medium, such as a high-performance SSD or a RAM disk. Known Compatibility Problems Other Visual Studio extensions Major compatibility issues have been observed with the following products: DevExpress CodeRush/Refactor Pro (incompatible) Telerik JustCode (incompatible) Whole Tomato Visual Assist Productivity Power Tools Performance degradation has been observed with the following products: Some versions of the StyleCop ReSharper plug-in PowerCommands for Visual Studio There are also reports on Web Essentials contributing to low performance while editing .cshtml files. If you're affected by this problem, consider going to and setting Auto-format HTML on Enter to False. Parallels Desktop for Mac If you're running Visual Studio in a Windows virtual machine on your Mac using Parallels Desktop, ReSharper IntelliSense lists might be very slow to render. If this occurs in your setup, consider switching from Coherence mode to Full Screen mode. For guidelines on switching between the two modes, please see this Parallels Knowledge Base entry . Improving Visual Studio Performance Before starting to tweak Visual Studio settings, check that the most recent service pack and hot fixes are installed. Speed up editor scrolling The problem with editor scrolling arises due to hardware-accelerated editor rendering. If you experience this, try turning off the following options under : Automatically adjust visual experience based on client performance Use hardware graphics acceleration if available Save time on startup Turning off the start page and the news channel might save some time on startup. To do so, go to and choose to show empty environment at startup. Clean web cache If you work with web projects, web cache might slow down Visual Studio. To clean it, delete everything under %LOCALAPPDATA%\Microsoft\WebSiteCache . Disable unused extensions Go to , go through the list and check if you really need each of them. You can uninstall or disable the unused ones. Unload unused projects If you are not working on some projects, you can unload them from Visual Studio and reload them back when needed. Right-click on the project or a solution folder in the Solution Explorer and choose Unload Project or Unload Projects in Solution Folder - this will speed up both Visual Studio and ReSharper. By the way, ReSharper navigation features will work even for unloaded projects. Disable visual XAML editor On large projects, editing XAML files can feel slow even on good hardware. If you don't use visual XAML editor, you can partly solve the problem by disabling it. To do so, right-click on a XAML file in the Solution Explorer and choose Open With. In the dialog box that appears, select Source Code (Text) Editor and click Set as default. Last modified: 24 May 2016",en,117
72,2372,1474307005,CONTENT SHARED,-4084394822880420062,-1836083230511905974,949170573697719844,,,,HTML,http://android-developers.blogspot.com.br/2016/09/android-studio-2-2.html,android studio 2.2,"By Jamal Eason , Product Manager, Android Android Studio 2.2 is available to download today. Previewed at Google I/O 2016, Android Studio 2.2 is the latest release of our IDE used by millions of Android developers around the world. Packed with enhancements, this release has three major themes: speed, smarts, and Android platform support. Develop faster with features such as the new Layout Editor, which makes creating an app user interface quick and intuitive. Develop smarter with our new APK analyzer, enhanced Layout Inspector, expanded code analysis, IntelliJ's 2016.1.3 features and much more. Lastly, as the official IDE for Android app development, Android Studio 2.2 includes support for all the latest developer features in Android 7.0 Nougat, like code completion to help you add Android platform features like Multi-Window support , Quick Settings API , or the redesigned Notifications , and of course, the built-in Android Emulator to test them all out. In this release, we evolved the Android Frameworks and the IDE together to create the Constraint Layout. This powerful new layout manager helps you design large and complex layouts in a flat and streamlined hierarchy. The ConstraintLayout integrates into your app like a standard Android support library, and was built in parallel with the new Layout Editor. Android Studio 2.2 includes 20+ new features across every major phase of the development process: design, develop, build, & test. From designing UIs with the new ConstraintLayout , to developing C++ code with the Android NDK, to building with the latest Jack compliers, to creating Espresso test cases for your app, Android Studio 2.2 is the update you do not want to miss. Here's more detail on some of the top highlights: Design Layout Editor: Creating Android app user interfaces is now easier with the new user interface designer. Quickly construct the structure of your app UI with the new blueprint mode and adjust the visual attributes of each widget with new properties panel. Learn more . Constraint Layout: This new layout is a flexible layout manager for your app that allows you to create dynamic user interfaces without nesting multiple layouts. It is backwards compatible all the way back to Android API level 9 (Gingerbread). ConstraintLayout works best with the new Layout Editor in Android Studio 2.2. Learn more . Develop Improved C++ Support: You can now use CMake or ndk-build to compile your C++ projects from Gradle. Migrating projects from CMake build systems to Android Studio is now seamless. You will also find C++ support in the new project wizard in Android Studio, plus a number of bug fixes to the C++ edit and debug experience. Learn more . C++ Code Editing & CMake Support Samples Browser: Referencing Android sample code is now even easier with Android Studio 2.2. Within the code editor window, find occurrences of your app code in Google Android sample code to help jump start your app development. Build Instant Run Improvements: Introduced in Android Studio 2.0, Instant Run is our major, long-term investment to make Android development as fast and lightweight. Since launch, it has significantly improved the edit, build, run iteration cycles for many developers. In this release, we have made many stability and reliability improvements to Instant Run. If you have previously disabled Instant Run, we encourage you to re-enable it and let us know if you come across further issues. (Settings → Build, Execution, Deployment → Instant Run [Windows/Linux] , Preferences → Build, Execution, Deployment → Instant Run [OS X]). For details on the fixes that we have made, see the Android Studio 2.2 release notes . APK Analyzer: Easily inspect the contents of your APKs to understand the size contribution of each component. This feature can be helpful when debugging multi-dex issues. Plus, with the APK Analyzer you can compare two versions of an APK. Learn more . Build cache (Experimental): We are continuing our investments to improve build speeds with the introduction of a new experimental build cache that will help reduce both full and incremental build times. Just add android.enableBuildCache=true to your gradle.properties file. Learn more . Test Virtual Sensors in the Android Emulator: The Android Emulator now includes a new set of virtual sensors controls. With the new UI controls, you can now test Android Sensors such as Accelerometer, Ambient Temperature, Magnetometer and more. Learn more . Android Emulator Virtual Sensors Espresso Test Recorder (Beta): The Espresso Test Recorder lets you easily create UI tests by recording interactions with your app; it then outputs the UI test code for you. You record your interactions with a device and add assertions to verify UI elements in particular snapshots of your app. Espresso Test Recorder then takes the saved recording and automatically generates a corresponding UI test. You can run the test locally, on your continuous integration server, or using Firebase Test Lab for Android . Learn more . Espresso Test Recorder GPU Debugger (Beta): The GPU Debugger is now in Beta. You can now capture a stream of OpenGL ES commands on your Android device and then replay it from inside Android Studio for analysis. You can also fully inspect the GPU state of any given OpenGL ES command to better understand and debug your graphical output. Lean more . To recap, Android Studio 2.2 includes these major features and more: Learn more about Android Studio 2.2 by reviewing the release notes and the preview blog post . Getting Started Download If you are using a previous version of Android Studio, you can check for updates on the Stable channel from the navigation menu (Help → Check for Update [Windows/Linux] , Android Studio → Check for Updates [OS X]). You can also download Android Studio 2.2 from the official download page . To take advantage of all the new features and improvements in Android Studio, you should also update to the Android Gradle plugin version to 2.2.0 in your current app project. Next Release We would like to thank all of you in the Android Developer community for your work on this release. We are grateful for your contributions, your ongoing feedback which inspired the new features in this release, and your highly active use on canary and beta builds filing bugs. We all wanted to make Android Studio 2.2 our best release yet, with many stability and performance fixes in addition to the many new features. For our next release, look for even more; we want to work hard to address feedback and keep driving up quality and stability on existing features to make you productive. We appreciate any feedback on things you like, issues or features you would like to see. Connect with us -- the Android Studio development team -- on our Google+ page or on Twitter . What's new in Android development tools - Google I/O 2016",en,117
73,2149,1471924347,CONTENT SHARED,-2479936301516183562,-1032019229384696495,-1085681130193775758,,,,HTML,http://venturebeat.com/2016/08/22/gitlab-issue-boards/,"gitlab launches issue boards, an open-source task management tool that resembles trello","Source code repository software startup GitLab today is introducing Issue Boards, an open-source task-management tool that will be integrated into the existing web service for all users, free of charge. The tool provides a visual interface where team members can track the status of their projects. Rather than require users to list out all the tasks that have been completed and have yet to be done, the tool can take in existing issues from teams' repositories and drop them in automatically using the labels that have previously been applied to issues. All boards have two lists enabled by default: Backlog and Done, GitLab said in a blog post . Users can add unlimited lists - each one representing a step in the development process - but just one board, a spokesperson told VentureBeat in an email. Enterprises customers could get unlimited boards in the future, the spokesperson said. Issues can be filtered by their corresponding label, milestone, author, or assignee. Of course, developers have always been able to check out issues for projects hosted on GitLab. Issue Boards offer a different representation of things that people need to do. GitLab says the tool can help teams with thousands and thousands of issues do a better job of figuring out what to focus on. The thing is, many other tools help people manage tasks - Trello, Wrike, Asana, the recently launched Microsoft Planner , and so on. There are also open-source task management tools, including Kanboard, Restyaboard, and Wekan. GitLab's Issue Boards bear a striking similarity specifically to Trello, which had more than 12 million users as of January. But GitLab, which competes with Atlassian's Bitbucket and GitHub, has been seeking to make its offering more well rounded. Not only is it core functionality open source unlike GitHub. It now packages up additional tools like Mattermost, an open-source alternative to team communication app Slack, and also it now lets users easily build and run code inside a Koding cloud-based integrated development environment (IDE) . Founded in 2011 by Dmitriy Zaporozhets and Sytse ""Sid"" Sijbrandij, GitLab is currently used by more than 100,000 organizations, including Alibaba, CERN, Expedia, IBM, NASA, and SpaceX. You can find the code for Issue Boards here . Get more stories like this on Twitter & Facebook",en,116
74,972,1463359748,CONTENT SHARED,-6255158415883847921,6013226412048763966,7846471679835395183,,,,HTML,http://m.folha.uol.com.br/ilustrada/2016/04/1766160-para-neurociencia-motivacao-nao-e-fator-principal-em-mudanca-de-habitos.shtml,"para neurociência, motivação não é fator principal em mudança de hábitos","Ao contrário do que pregam livros de autoajuda, querer não é poder. A neurociência mostra que ação é mais importante que motivação na hora de dar uma virada na vida. Quer dizer: não dá para esperar ter desejo de comer salada para fazer dieta. ""A vontade anda de mãos dadas com a preguiça, o cérebro sempre quer gastar menos energia. É mais efetivo agir primeiro, até que a repetição leve a um novo hábito"", diz Pedro Calabrez, pesquisador do Laboratório de Neurociências Clínicas da Unifesp. Mudar é agir contra o script. Como os hábitos já estão gravados na memória, preferimos o piloto-automático a criar novas rotinas. ""É como um rio que corre pelo mesmo caminho. O cérebro tem a tendência de reagir sempre do mesmo jeito"", diz Paulo Knapp, psiquiatra e terapeuta cognitivo-comportamental. Não é por mal. Hábitos só existem para facilitar o cotidiano, lembra o neurocientista Jorge Moll, diretor-presidente do Instituto D'Or. ""Comportamento automatizado faz o cérebro mais eficiente. Não conseguiríamos fazer tanta coisa se fosse preciso pensar em cada ação."" Segundo ele, mais de 99% dos hábitos são bons. O problema é o 1% que sobra. ""Da procrastinação ao uso de cocaína, o hábito é formado graças a uma recompensa ou punição. Todos são vícios em potencial"", diz Knapp. Quanto mais rápida for a resposta de uma atitude, mais facilmente ela vira hábito, e mais difícil é mudar. É outra pegadinha do cérebro, que prefere recompensas a curto prazo. Por isso a meta de ficar em forma é dura de ser cumprida. ""A recompensa está distante, e comer traz prêmio imediato"", diz Moll. PLANO DE GUERRA Para driblar o comodismo e instituir novos hábitos, existem técnicas, algumas controversas. A mais célebre envolve repetir o novo comportamento por 21 dias. Mas, segundo um estudo do University College de Londres são necessários, em média, 66 dias para mudar. ""Isso é baboseira, não há tempo certo. Depende de quão antigo o hábito é. Mas há técnicas de psicologia comportamental, como colocar um alarme para comer na hora certa"", diz Moll. Calabrez explica que o hábito tem três dimensões: a ação em si, a recompensa e um gatilho que leva àquele comportamento, tipo o café que convida ao cigarro. Para desconstruir um vício, então, é preciso mudar ao menos uma das três etapas, como descobrir o gatilho e eliminá-lo, dividir o objetivo em metas menores e criar prêmios imediatos para ações com efeitos a longo prazo. Foi o que fez a blogueira Luiza Ferro, 26. Toda vez que ia ao shopping, comprava ""uma besteirinha"". O hábito passou a incomodar quando ela percebeu que gastava demais com coisas inúteis. Luiza deixou de ir ao shopping -o gatilho das compras. Também cancelou newsletters de lojas. E instituiu uma recompensa se conseguisse poupar: um curso no exterior. ""Demorou, mas deu certo. Depois, me interessei por sustentabilidade na moda e pela onda 'slow fashion'. Hoje tenho 40 itens no armário."" Para a psicanálise, a mudança não passa por treinar novos comportamentos. ""O indivíduo não deve ser condicionado. Precisa compreender que não pode se furtar da escolha de seus hábitos e da responsabilidade por eles"", diz o psicanalista Jorge Forbes. A virada é difícil porque envolve surpresa, acrescenta. ""Mudar é angustiante. As pessoas preferem um sintoma conhecido do que um desconhecido."" Fronteiras do Pensamento Fale com a Redação - leitor@grupofolha.com.br Problemas no aplicativo? - novasplataformas@grupofolha.com.br",pt,116
75,2024,1470832126,CONTENT SHARED,-3678789633202302491,7774613525190730745,6403122829531428804,,,,HTML,https://dzone.com/articles/do-you-suffer-from-deployment-anxiety-1,do you suffer from deployment anxiety? - dzone devops,"Whether you suffer from a diagnosed anxiety disorder or not, many of us who are responsible for deployments become uneasy when deploying code to production. Did my tests catch everything? What if something happens during a migration and I can't rollback? Will that small code change create an unstoppable reaction destroying everything in my database? Okay, maybe that's a little aggressive, but you get my point. Deploying to production creates anxiety among developers all over the world. So, how do we combat this anxiety without a prescription for Xanax? We've gathered a couple tricks to help ease your anxiety and optimize your release pipeline all at the same time. Automate Configuration and Deployment By automating a process, you are turning it into a constant. This is the best way to make sure your deployments behave the same every time you deploy an application. Like we learned during our 4th grade science fair, the control variable is unchanging and will give you confidence to learn from future breaks and failures (dependent variables). Create a Testing Environment QA testing should have its own environment and it should be production-like. This is the first major step towards Continuous Delivery and an overall healthier state of mind when deploying to production. Hopefully, you can automate your code's transition from a dev environment to a QA environment where it can run suites of functional and performance tests. Each QA environment should be complete with the right configurations for each new code release. Work With Your Support Team It's all too easy to get involved with your next project and forget to check in on the apps you already have running. DevOps principles often talk about breaking down the walls between departments and, in this case, that means working with your support team to ensure the health of your existing apps. If you can automate the way existing apps are tested and monitored by your dev and support teams, you will begin deploying to production with a lot more confidence. Use Canary Releases Danilo Sato's definition of a Canary Release on MartinFowler.com is the best I've found: "" Canary release is a technique to reduce the risk of introducing a new software version in production by slowly rolling out the change to a small subset of users before rolling it out to the entire infrastructure and making it available to everybody."" By releasing to a small portion of users, you can build confidence in your deployment before pushing it into production completely. Run Fire Drills Crashes, outages, breaks, and failures are all terrifying what-ifs. One way to become slightly more comfortable with these uncertainties is to run controlled fire drills with your teams. Come up with some creative, albeit terrifying, ideas like database crashes or datacenter outages, and run through those scenarios and your related response plan with your team. Incremental Deployments You can read a full blog on Incremental vs. Full Deployments here , but the basics of it are, by running incremental deployments you are updating only the assets on the site that are being changed, instead of re-creating the configuration of the site as a whole. They are faster deploys and they don't touch the systems that are already OK. Dependable Fast Rollbacks Nothing is more comforting than knowing you can roll back your release. Rollbacks are another process that can be automated so that if/when your deployments break, you'll notice serious errors quickly. It should be noted that you'll want to make sure your old code still works on the new database layout, if you were to rollback. Don't be afraid to re-test your old code before rolling back. Deploy Often The most comfortable developers I've met deploy to production a dozen or more times a day. This is the result of a Continuous Delivery pipeline. By automating the right processes and deploying often, you will be able to tweak and refine your pipeline to run smoothly. Am I saying you should start pushing out 30 deploys a day? No. But increasing your deployment rates over time will help create a release process that flows with far fewer bottlenecks and failures. One of the best ways to keep anxiety at bay is by learning from others mistakes. Check out our on-demand webinar: Lessons Learned: Scaling DevOps and Continuous Delivery for the Enterprise . continuous delivery, qa, pipeline, environment, deployment, canary release, rollback",en,115
76,2077,1471352498,CONTENT SHARED,-2584174137395076448,534764222466712491,-2603418925334381846,,,,HTML,http://epoca.globo.com/vida/noticia/2016/08/elektro-google-e-sama-lideram-entre-melhores-empresas-para-trabalhar-em-2016.html,"elektro, google e sama lideram entre as melhores empresas para trabalhar em 2016","Sama, primeiro lugar entre as empresas médias nacionais da lista ÉPOCA GPTW 2016 (Foto: Luís Lima) Época e Great Place to Work realizaram há pouco a cerimônia de premiação das melhores empresas para trabalhar de 2016. A pesquisa está completando 20 anos e é publicada com exclusividade por Época no Brasil. Como vem acontecendo nos últimos anos, o número de inscrições foi recorde: 1563 organizações disputaram um lugar entre as 150 vencedoras, média de 10,4 empresas por vaga. Em 2015 foram 1454 participantes e em 2014, 1276. Elektro, primeiro lugar entre as grandes na lista de ÉPOCA GPTW Brasil 2016 (Foto: Luís Lima) As empresas estão divididas em três categorias: Grandes, com 80 empresas que têm mil funcionários ou mais; Médias Multinacionais, com 35 organizações que possuem entre 100 e 999 empregados, e Médias Nacionais, também um total de 35 empresas com número de funcionários entre 100 e 999. As campeãs nas três categorias do ano passado conseguiram repetir o mesmo feito em 2016, destacando-se novamente como as melhores em suas categorias. São elas: * Grandes - Elektro * Médias Multinacionais - Google * Médias Nacionais - Sama Google, primeiro lugar na categoria médias multinacionais da lista Época GPTW 2016 (Foto: Luís Lima) Leia a lista completa:",pt,115
77,596,1461689512,CONTENT SHARED,6989198691754522425,7890134385692540512,-2241007388742885017,,,,HTML,https://www.sympla.com.br/front-in-bh-2016__54441,front in bh 2016,"Descrição do evento O Front in BH nasceu da necessidade de fortalecer a comunidade Front-end local, através de um circuito de palestras focado em promover o relacionamento e a integração entre os profissionais de Front-end e das áreas a afins. Além de profissionalizar ainda mais o mercado com a contribuição das palestras para a formação de profissionais mais críticos e capacitados, o Front in BH é pioneiro na abordagem de temas atuais e de interesse comum às áreas de desenvolvimento, design, métricas e afins, através de palestrantes renomados no mercado de desenvolvimento Front-end no Brasil e no exterior. Este pioneirismo impulsionou outros grupos a se organizarem, levando o conceito do Front in BH para pelo menos 5 regiões do país. Hoje o Front in BH chega à sua quinta edição com um passado de grande sucesso e a reputação de ser o melhor evento de Front-end do Brasil novamente.",pt,114
78,1976,1470253169,CONTENT SHARED,3170775058142440102,-7606731662737258050,6878120212962380972,,,,HTML,http://revistatrip.uol.com.br/tpm/milly-lacombe-fala-sobre-patriarcado-e-casamento,criadas para casar,"Eu tinha 9 anos quando fui arrancada de um jogo de futebol no recreio porque a diretoria de um dos colégios mais tradicionais de São Paulo havia se reunido e decidido que eu, uma menina, deveria brincar de boneca e não bater bola. Lembro da sensação de ser conduzida por uma das professoras a uma sala escura dentro da qual as garotas da classe celebravam o aniversário de uma boneca pálida, loira e de plástico. Sempre tive medo de bonecas porque a falta de vida delas me soava petrificante. Como petrificante foi ter que passar muitos minutos escutando ""parabéns a você"" para um ser inanimado enquanto ainda era capaz de ouvir os meninos gritando de alegria do lado de fora, o que indicava que embora eu fosse um dos melhores jogadores em campo minha ausência não era sentida. Da mesma forma, minha presença na sala escura tampouco era reconhecida. É preciso que entendamos a violência para além da delinquência: o que a diretoria daquela escola fez comigo no outono de 1976 foi uma violência. Assim como é uma violência que o recado dado a meninas desde o berço seja: o importante é casar e ter filhos. E ele vem de todos os lados: das lojas de brinquedo, dos pais, dos professores, da comunidade, da propaganda. Ainda que muita coisa tenha mudado desde aquele recreio há 40 anos, a condição feminina segue lutando para se livrar dos resíduos de séculos de patriarcado. LEIA TAMBÉM: Todos os textos de Milly Lacombe na Tpm ""Fascismo é a derradeira expressão da hierarquia patriarcal"", escreveu Virginia Woolf há mais de meio século. Para ela, a escola da vida não ensinaria a arte da dominação, ou a arte do poder e da matança, ou como adquirir terras, propriedades, capital. Num mundo mais feminino, a escola imaginada por ela ensinaria artes como medicina, matemática, música, pintura, literatura, e também a arte do sexo e do entendimento a outros seres humanos. O objetivo não seria segregar ou especializar, mas misturar. O objetivo não seria casar, mas amar. Só que, como escreveu Nicholas Kristof, colunista do New York Times, a grande ameaça a extremistas não são drones jogando bombas, mas meninas lendo livros. Em democracias, onde extremistas são mais raros, a perpetuação do patriarcado se faz pela propaganda e pela doutrinação, e segue funcionando como sempre. Politicamente, somos moldadas desde pequenas: casar é transmitido como objetivo final. A mensagem vem das formas mais singelas e doces, como por exemplo dos filmes românticos que terminam sempre com um casamento. Não há conversas sobre sexo, relações humanas, convívio, e eles terminam onde deveriam começar porque todos os que já casaram sabem que é aí que a coisa exige atenção, dedicação, esforço. Não temos isso, temos o casamento como destino, os afazeres do lar como brincadeiras de criança e, depois, como realidade do adulto que ""deu certo socialmente"". Em 1910, Emma Goldman, misturou casamento e política quando escreveu: ""Casamento nesses termos é como aquele outro arranjo patriarcal, o capitalismo. Ele rouba homens e mulheres de dignidade, de brilho, poda seu crescimento, envenena seu corpo, os mantém na ignorância, na pobreza, na dependência, e então institui a caridade, que rouba os últimos vestígios de autorrespeito"". E o professor britânico Timothy Morton disse a respeito do aquecimento global: ""Colocar a natureza num pedestal e admirá-la de longe faz pelo meio ambiente o que o patriarcado faz pela mulher"". Oprimidas e domesticadas A crueldade do patriarcado se desdobra em muitas áreas do cotidiano e é assimilada sem que percebamos. Somos, por exemplo, encorajadas a acreditar que não emagrecemos porque não nos exercitamos o suficiente, mas não se fala do açúcar e de como as legislações aplicadas a ele nos matam lentamente. Quando as taxas de obesidade nos EUA ultrapassaram todos os limites aceitáveis teve início a febre do low fat, mas o que não contaram é que ao tirar a gordura eles acrescentaram açúcar. Claro que homens são vítimas também, mas a eles não está imposta a ditadura da magreza e da gostosura. E, depois disso tudo, o recado que chega é: você precisa malhar, manter-se decente para poder casar, ter filhos e seguir casada. O patriarcado é bom em nos culpar por suas mazelas. Ao nos recusarmos a encarar o debate político que envolve o patriarcado, seguimos vivendo dentro desta distopia: oprimidas, domesticadas, doutrinadas para sermos mães e mulheres dedicadas, magras e obedientes. Alguns dos direitos mais fundamentais de todo ser humano são a criatividade e a liberdade, mas o patriarcado nos priva disso enquanto tenta convencer que a verdadeira liberdade é poder comprar dois sapatos pelo preço de um. Liberdade é a possibilidade concreta de todos nós desenvolvermos as faculdades, capacidades e habilidades com as quais a natureza nos dotou e convertê-las em valor social. É o que desejamos àqueles que amamos, mas é o que deveríamos desejar a todo ser humano se percebêssemos que somos parte de uma mesma substância. Demorei muitos anos para me livrar da ferida deixada pela diretoria da escola em que estudei, e incontáveis invernos para entender que eu não precisaria casar, ou sequer casar com um homem, para cumprir uma suposta função social e me sentir bem-sucedida. Foi uma longa travessia essa que me ensinou que casar não é fundamental, mas que amar é. A menos que não nos acanhemos de encarar o debate político, arregacemos as mangas e lutemos contra a crueldade do patriarcado, não seremos capazes de sair dessa armadilha em que nos meteram. Nada menor do que uma revolução pode nos tirar daqui. Virginia Woolf fica com a palavra final: ""Como mulher, não tenho um país; como mulher, não quero um país; como mulher, meu país é o mundo inteiro"".",pt,113
79,1487,1466608308,CONTENT SHARED,-4278025512576376201,-5527145562136413747,101989957506145943,,,,HTML,http://cannes.meioemensagem.com.br/cobertura2016/diario-de-cannes/2016/06/22/sentaquelavemtextao,#sentaquelávemtextão,"""Me cansei de lero-lero! Rita Lee e Roberto de Carvalho Dá licença, mas eu vou sair do sério. Quero mais saúde! Me cansei de escutar opiniões De como ter um mundo melhor."" É aquela coisa, né? Você tem que escrever o seu ""Diário"" e, de repente, entra aqui e tem tanta gente falando tudo sobre tudo e mais um pouco. Você se pergunta: sobre o que vou escrever? Sobre verdades. Já foram quatro dias da minha parte do Festival. É muita informação. É informação demais. Hoje, quarta-feira, são quatro eventos num só: tem saúde, inovação, entretenimento e criatividade. Tudo ao mesmo tempo agora. No meio desse tanto, você espreme, joga fora o bagaço dos jabás e sobra alguma coisa para pensar. Isso se você entendeu tudo o que foi dito, tudo o que foi mostrado. Se você prestou atenção. Se se interessou de verdade. Difícil acompanhar. Mas vale a pena. De verdade. Poderia falar sobre as tecnicidades do momento. A realidade virtual. O vídeo. O vídeo 360º. A programática. O algoritmo. O engajamento. O storytelling. O social. A inteligência artificial. O universo Makers - que devia estar mais presente - ou a Internet das Coisas. Poderia falar sobre a aplicação dos conceitos do digital no dia a dia das agências. Coisas como o pensamento beta, o jeitão ""startup"" de ser, o design thinking e etc. Palestras mais interessantes e palestras menos interessantes na mesma medida sobre tudo isso. Esses discursos todos para mudar o jogo, mudar o jeito de fazer, são lindos, interessantíssimos, mas a gente precisa realmente estar muito a fim de botar na prática. Precisa muito ""combinar com os russos"", como diria Mané Garrincha. Precisa quebrar barreiras, mudar as cabeças, especialmente dos cabeças. E entender que o jogo é outro. Bom, a revolução já começou faz tempo e muita gente vai ficar pra trás. Verdade. Isso tudo é importante, sim. Mas acho que os recados sociais e humanos que o mundo está deixando aqui e ali - para o mundo - são mais tocantes, emocionantes, fortes. O que importa de verdade. Nos últimos anos, o canto que mais escutamos é o da diversidade. Do respeito à diversidade. Em Cannes, 2016 é muito o ano das mulheres. Mas, se você parar e pensar, vai ver que isso não se restringe apenas ao Festival Internacional de Criatividade de Cannes. As discussões em relação à igualdade de gêneros, empoderamento feminino e a violência extrapolante do universo machista são cada vez mais assunto constante em tudo o que é lugar. Está todos os dias no Twitter, no Facebook, no Oscar®, no Festival de Cinema de Cannes, no esporte, no jornalismo - procure saber sobre o movimento ""Jornalistas contra o assédio"" -, na política, no trabalho e nas ruas de todo o mundo. Se você não sacou isso ainda, amigo, amiga, tem algo muito errado. Tão errado quanto todas as desigualdades e todo tipo de violência. Olhe ao redor, a mulher é o assunto da vez no mundo. Mas, até aí, temos uma estrada longa para correr em relação ao #RespeitoAsMinas. Já rolaram diversas palestras e momentos importantes no Festival que trazem o debate do feminino dentro da indústria da comunicação. Seja mostrando as iniciativas, botando as mulheres na frente das discussões ou até apoiando as causas. O app do Festival tem um botão onde você pode escolher pra qual organização internacional da causa feminina você quer doar 25 dinheiros por dia. Mas ainda tem muito o que fazer. Algumas participações já entraram pra história, se não do Festival, na vida de algumas pessoas. Por exemplo: Cindy Gallop, logo após mandar a real sobre o sexo real, deu uma bela cutucada no Festival por distribuírem um livro sobre criatividade que só tem homens como criativos. O próprio autor James Hurman reconheceu o erro de não ter entrevistado mulheres para a obra. A presença de Claudia Gonzalez da ONU no painel com Sir John Hegarty e os produtores de cinema Lawrence Bender e Steve Golin, apresentando em primeira mão, o comercial #WhatIReallyReallyWant feito para a campanha do Global Causes para meninas e mulheres. Em julho, o filme vai pro ar. Enfim. Respira bem fundo e entenda: nada pode se comparar com a participação de Madonna Badger. Uma mulher extraordinária. Responsável pela belíssima campanha #WomenNotObjects, que luta contra a objetificação da mulher na propaganda. Sua história pessoal, duríssima, fez todo mundo se acabar em lágrimas. Vi muita gente derretendo. Eu já estava preparado pelo amigo Rodrigo Maroni que me cantou a bola no dia anterior. Mas não podia imaginar o que edificante seria aquele momento. O jeito como ela conduziu sua apresentação de forma brilhante, mostrando coragem, sabedoria e sua determinação incansável de deixar um legado inspirado em suas três filhas. São verdades. Ditas de verdade. Tudo o que Madonna Badger falou, acabou reverberando dentro de mim com a ótima apresentação de Kim Getty, que tratou muito bem sobre a influência da publicidade na cultura do gênero. Leia mais aqui . As duas apresentações mostravam como é a presença do feminino na publicidade. Mas,no caso de Madonna, o bicho pega mais forte através do apelo sexual. Foi uma catarse. E, na minha cabeça, automaticamente, me veio a lembrança de um fato muito antigo que aconteceu no Brasil: em 1996, Claudia Liz, uma das modelos mais lindas do País, viveu um triste episódio ao ficar em coma após complicações numa cirurgia de lipoaspiração. Um trauma terrível que quase lhe tirou a vida. Na época, Claudia era casada com Celso Loducca e eu nunca me esqueci de uma entrevista dada por ele no calor tenso daquele momento, onde comentava sobre como esse mercado das modelos e da beleza podia ser irresponsável e mal. E, naquele dia, me lembro de olhar para o Celso e pensar: ""Cara, na boa, você também é muito responsável por isso."" Isso é um fato. Todos somos. E, sim, existe a #culturadoestupro. E a gente tem que lutar muito contra isso. Todos os dias. Aqui mesmo no ""Diário de Cannes"" já vejo resultados de que as mulheres não podem se calar, tem que falar muito! A colega Daniela Schmitz, inspirada no que viu e sentiu, colocou para fora sua própria história recente do assédio absurdo vivido por sua filha, participante do programa MasterChef Jr, de forma igualmente tocante. Para quem não sabe, o fato que ela cita, acontecido nas redes sociais por conta do programa, foi o que inspirou o movimento do #primeiroassédio que moveu a internet no Brasil. Nunca me esquecerei daquele dia. Como vi o movimento crescendo no Twitter e no Facebook e como fiquei tão chocado com os absurdos ditos sobre a filha da Daniela e sobre outras crianças que acabei não conseguindo mais assistir ao programa na TV. Deu bode total pra mim. E, sim, Daniela, precisamos falar sobre isso. De verdade. Para terminar, queria falar sobre saúde. Sim. Queria mesmo era poder falar mais e mais sobre saúde aqui no Festival Internacional de Criatividade de Cannes. Mas meu ingresso não permitia acompanhar os conteúdos. Espero ter acesso ao site do Festival. Sabe por que? Minha última experiência num evento internacional desse tipo e tamanho foi num congresso de medicina. Em 2014, enquanto estava fora do mercado, participei em Toronto do SIOP, o maior congresso de oncologia pediátrica, por conta da ONG que faço parte, o Beaba . Como em Cannes, ali tive a oportunidade de estar ao lado de gente do mundo todo, dividindo experiências sobre assuntos importantes. E lá também, acabei me envolvendo menos nas questões técnicas - até porque de medicina entendo pouco - para mergulhar na verdade da empatia. O que aprendi e aprendo junto ao ""mundo paralelo do câncer"" - definição precisa da minha amiga Simone Mozzilli, outra mulher sensacional que vocês deviam conhecer! -, enfim, a experiência vivida na relação com pacientes, ex-pacientes, médicos e enfermeiros, transformaram a relação que tenho com meu trabalho de publicidade. Quando você é exposto à verdade da vida real, não dá pra ser mais do mesmo. Por isso é libertadora a fala de Sir John Hegarty: ""We need to get out of this bubble that is advertising because, to be honest with you, it's very boring"". O cara fala sobre trabalhar com propaganda, e não viver a propaganda. É abrir os olhos para um mundo muito maior que tudo isso. Um mundo de verdade. Que precisa nos inspirar mais e mais e fazer com que nós movimentemos para trazer novas perspectivas para ele. De verdade! P.s: No Tinder de Cannes, um young de algum país diferente me manda uma verdade: ""Isso aqui é muito fake, né? Parece que todo mundo pretende salvar o mundo, mas, no fundo só quer pegar uns prêmios."" Há esperança!",pt,113
80,1395,1465962769,CONTENT SHARED,6062146090334604102,-8020832670974472349,-2734800878000447382,,,,HTML,https://paul.kinlan.me/serverless-sync-in-web-apps/,serverless data sync in web apps with bit torrent,"Our team has built a lot of Progressive Web Apps recently to demonstrate how we think they can be built: Airhorner , Voice Memos , Guitar Tuner, SVG-OMG are a few that spring to mind. One thing that is common across all of these sites is that they have no sever component to store and synchronise data. We built these sites as examples and reference implementations of the types of experiences that we can deliver on the web, they were never intended to be full ""Apps"" that you would build as a business. I recently had an idea for another Web App that was inspired by Ben Thompson's Future of Podcasting PodCast. I wanted to make Pod Casting as simple as visiting your site and pressing a record button on the page (I did something similar years ago called FriendBoo but that had to use the PSTN telephone system). Technically we have all the parts of the web platform to help us: Service Worker to make it work offline, Blob and IndexedDB to store chunks of data locally and MediaStream Recorder to take Microphone input and record it to a file. Finally we have the power of the URL to allow us to access the web app from anywhere there is a browser and an internet connection. Once you have access to the web, you would expect that the data you recorded from one device would be stored on a server somewhere and then be available again later from any machine that you access the site. Traditionally you create a web service that will store the data on a server somewhere and you would manage all that infrastructure to support all your users. However, I'm not in the mood to create a service that requires storage and retrieval of data from a service (getting that through our legal teams will be a nightmare) so everything would have to be stored locally inside the web app. The big question is how do you get your data out and shared to another instance of your web app on another device? Voice Memos created by Paul Lewis is another example of this. All the data is local to the app The data is recored and stored locally which means if you want to sync it with your other devices or share it with a friend, you can't. There are solutions, for example we could dynamically encode the audio file as base64 and create a custom URL - but thats not scalable. It was a bit of a conumdrum and one that I wanted to solve. I set up some simple requirements for what I would like to see: The user should be able to share a simple url that would point to their local data, The user should not have to manually save the data outside of their browser or web app, There should be no ""backend"" that stores the data, Synchornisation should ideally happen peer to peer. Now. Step back 6 months. I was at ColdFront conference last year and I saw a talk by Feross Aboukhadijeh about Web RTC Data Channel, BitTorrent and how he started a project called WebTorrent . It was a great talk, but I didn't hook things up in my head until recently about the potential of what he was talking about. Now I think I have the start of a solution through the use of WebTorrent.io. Web Torrent I won't explain WebTorrent too much other than to say that it combines BitTorrent style distributed data delivery with WebRTC and it is rather spectacular. I do encourage you all to check it out though. The theory I had was that if the user's client could act as a peer in the Torrent network, it would be then able to seed some of the data that is local to the web site, the user could generate a torrent link that could be shared either with another one of their devices or with another user and then the ""remote"" instance of the web app will fetch the data from the ""client"" instance. User vists page User does some work User saves it to IndexedDB User clicks ""Share"" and it generates a ""magnet:"" URL and then starts to seed the Audio Blob. User shares or opens the URL in another browser. Site parses magnet URL and connects to tracker. Site finds peers from the tracker, connects to peer(s) and downloads data. Rough flow of data For 1:1 connections this might be a bit of an overkill, I could just create a WebRTC signalling service and deliver the messages to the other instance to get the data from one client to another. The interesting thing with this approach is that it scales nicely when sharing data to more than one person. Applying this to a real world sample I briefly mentioned Voice Memos earlier. It was a great reference application form me to try and integrate my theory in to a working app. It is close to my idea of a podcasting app and it is also in need of a way to synchronise data between clients because it has no server based backing store for the recordings. Voice memos I didn' make too many changes other than adding in a ""Share"" button. You can check out the demo on BitTorrent Voice Memos . Record a simple audio file, save it, then share it - the ""Share"" will generate a URL that you can send to another device or send to another person. I'm quite pleased with the output. In all this demo took only a couple of hours to get ready. Here is a quick run down of some of the major things that I had to do. Add in the webtorrent.min.js script used to power all the magic. Create a singleton instance of the WebTorrent API that can be used across all of my other classes (inspired by Paul Lewis). Seed all the memos - this is currently a little overkill, but it got the demos working. The this.memos is an Array Instance of VoiceMemos that were stored in our IndexedDB. When the user clicks the Share button, create a custom URL that will contain all the details needed to get access to the torrent and stream the data into another client. We need to run some custom logic to fetch the torrent when we detect that the user has entered a URL that contains the torrent information. The Voice Memos app has a custom router which looks for a url that starts with '/share'. Now we need to parse out the torrent information, and then using the WebTorrent API fetch the file from the BitTorrent network. Once it has been fetched we have to then get the file data out of the Blob using a rather hackey XMLHttpRequest. Wrapping up I will end this all by saying that this is a massive hack and it shouldn't be used for actual private data - right now if goes out on the network if people have the URL to the torrent then it will be accessible. I do think the concept is important, the ability to synchronise data and have no middle-man or server logic in our web apps is an important concept and we should actively consider how we build such experiences. I'm off to keep playing with this...",en,113
81,1228,1464892323,CONTENT SHARED,6783622248311192269,3609194402293569455,6147896666617046192,,,,HTML,http://www.infomoney.com.br/negocios/como-vender-mais/noticia/5062199/par-aliancas-impede-que-seu-parceiro-assista-netflix-sem-voce,par de alianças impede que seu parceiro assista netflix sem você,"SÃO PAULO - Não são poucos os casais que criaram o costume de assistir séries juntos, mesmo que nem sempre consigam manter o mesmo ritmo. Pensando em ""evitar as brigas de casais"" causadas pelo fato de o parceiro ter assistido a um episódio sozinho, a Cornetto criou um par de alianças que só permite que uma pessoa assista à Netflix se a outra estiver por perto. As alianças possuem a tecnologia NFC, mesma utilizada no Pay e meios de pagamentos móveis. Por conta dela, o aplicativo para smartphone que conecta as plataformas de streaming - Netflix, Hulu, Amazon, entre outras - permite que as séries sejam assistidas somente quando ambas as alianças estiverem próximas do celular. Caso contrário, o app é bloqueado. ""Cornetto, o sorvete que uniu casais por décadas, criou um dispositivo para prevenir a causa mais comum de brigas de casais atualmente: assistir episódios à frente de seu parceiro"", explica no site que anuncia o produto. Ainda não se sabe em quais países o produto será comercializado. No portal, os interessados podem registrar seu e-mail para ter acesso a uma espécie de pré-venda, também sem data de início definida. Confira o vídeo que mostra como as alianças devem funcionar:",pt,113
82,1523,1466862481,CONTENT SHARED,4419562057180692966,-1032019229384696495,6917925448322846329,,,,HTML,https://techcrunch.com/2016/06/25/latin-americas-chronic-inefficiency-could-drive-more-o2o-commerce-growth/,latin america's chronic inefficiency could drive more o2o commerce growth,"After 45 minutes on the phone with a travel agent, I've grown too frustrated with trying to figure out different alternatives for an international flight to the Maldives that stops in Paris on the way back. Within seconds, I'm compelled to pick up my mobile phone where Skyscanner and Kayak can help me much more efficiently. Tired and hungry, I find the refrigerator is empty. When I call in my order to my favorite Italian restaurant, the representative on the line doesn't recognize me and asks how she can help me. Yet, it's so obvious why I'm calling. Then she asks me for my phone number and address, and has no record of what I typically order when I call in for food delivery two to three times a week. The truth is that Latin America is extremely inefficient and its technology is behind North America. Yet, in Brazil, one of the world's largest economies despite the current crisis, e-commerce sales are expected to reach more than $22 billion this year (up more than 13 percent over 2015), and m-commerce in Brazil continues to rise at a fast pace, according to recent data from eMarketer . The mobile market opportunity is massive: more than 100 million people in Brazil are already online today, and there are nearly another 100 million more to go. There are numerous daily tasks that can be solved with ease in the U.S. In comparison, the lack of training, standardization and process in Brazil creates chronic inefficiencies in the service sector that push consumers to prefer and use mobile applications that provide everyday services in a standardized manner. In many cases, the experiences between online and offline service in the U.S. can be marginal. However, in Latin America it can be a significant and annoying difference. When you try to buy auto insurance with a broker by telephone, for example, the time you can expect to waste is much higher in Latin America. The lack of visibility into your options is huge. The bureaucracy in terms of the number of necessary documents and data required is exponentially higher. After headache-causing calls, the chances of getting only one quote without a comparison are big. The broker will likely just push their favorite insurance company. Brazil leads O2O commerce sector growth in Latin America Intense traffic in big metropolitan areas have helped create a new market for on-demand delivery by couriers on motor bikes. Crossing the city at high speeds while dodging cars and buses, they deliver documents and small parcels. \ There are now nearly one million ""motoboys"" in Brazil, and more than 200,000 of them work in the city of Sao Paulo alone. To make this fleet more efficient, motoboy hailing apps such as Rapiddo and Loggi are growing fast. On-demand pickup and delivery of laundry is another fast-growing O2O commerce service offering in big cities. Based on subscription fees, on-demand laundry operators are becoming an important service for the always-busy, always-late ""Paulistas"" (those born in Sao Paulo). ALavadeira is one example of a stand-out company in this new laundry service segment. With unemployment reaching a record high in Brazil, DogHero , which is similar to DogVacay or Rover, is having no problem finding new hosts for its popular pet-sitting platform. Whoever loves dogs with available time can apply to become a DogHero host for ""man's best friends."" In Brazil's larger cities, there is a huge shortage of hotels for dogs or kennels, making it difficult for dog owners to travel because they have no one to take care of their pets while they're away. In addition to new consumer offerings, the O2O commerce trend is helping reduce bureaucracy for small business owners. A new phenomenon is occurring in Brazil in response to the high unemployment rate: the number of individual micro-entrepreneurs (MEIs) are skyrocketing. More than 5.5 million of them now operate in Brazil. They offer a variety of services and goods from plumbing to manicures. In response, a government program was introduced at the end of 2009 to track and formalize them as business owners and to collect taxes. Prior to that, the MEIs never paid taxes or social security before. To help them navigate though it, Qipu is a new mobile app and service that helps MEIs manage taxes and finances. It also controls and automates access to social benefits. Qipu has grown fast with more than 200,000 active users. Mobile innovation will create Latin American service sector efficiencies The current experiential difference between the U.S. and Latin America creates two huge opportunities for O2O commerce growth. The first is the rapid adoption of new services that will balance it with the U.S. experience by creating more efficiencies where they're badly needed, led by fast mobile app adoption. T he second is we'll probably see new models in Brazil that didn't necessarily work as well in the U.S. Imagine a model like the instacart, in which you can use a mobile app to make grocery shopping that's delivered to your place in just a few hours. When you analyze the unit economics, Brazil could actually be an even better market than the U.S. for new on-demand businesses. Delivery person salaries, for example, are much lower in Brazil. There are fewer comparable services for the Brazilian to choose from, which provides an advantage for early-mover brands. Plus, the Brazilian consumer would likely have to travel more kilometers to find the products, most likely through heavy traffic or using poor public transport in large cities. For these reasons in particular, a Brazilian is even more likely to pick up a cell phone and request through a mobile app for a bicycle deliveryman do this work instead. It wouldn't be the first time that a model that did not work as well in the U.S. would work here. This happened in comparison e-commerce, for example, Buscapé for many years represented a large share of the domestic e-commerce. And the price-comparison model has never been more relevant in any other e-commerce market than Brazil. Thus, new eServices led my mobile innovation and rapid consumer adoption will fuel a growing on-demand economy in Brazil and Latin America through increased consumer convenience, the third most important vector driving consumer purchases after selection and price, by creating more efficiencies and less work. For this reason, I predict that innovative startups and larger companies that see the potential of solving inefficiencies will push growth of more than 50 percent year-over-year, even despite the current economic crisis in Brazil. This growth rate is already occurring today with iFood for food, MinutoSeguros for insurance, ViajaNet for airline tickets, Nubank with credit cards, to name a few examples of which I am very familiar with today. But be sure: the eServices revolution is coming soon for your plumber, your babysitter and your bank manager. Some more Brazilian startups that are disrupting inefficiencies with eServices include: Disclosure: Of the 22 Latin American companies mentioned in article, three of them (Magnetis, MinutoSeguros and ViajaNet) have received investments from Redpoint eventures, where Rodrigues is a partner and managing director. He's served as a board member, or is currently an advisor, shareholder or personal investor in six of the other companies (Buscape, Cheftime, iFood, Qipu, Rapiddo and Truckpad).",en,112
83,2243,1472744003,CONTENT SHARED,3495098006178009360,-1032019229384696495,4845766491468696400,,,,HTML,http://www.businessinsider.com/google-silicon-valley-tabs-spaces-debate-2016-8,a googler analyzed a billion files to settle the programming dispute made famous by hbo's 'silicon valley',"If you watch HBO's ""Silicon Valley,"" you may remember this now-classic scene from the most recent season, where our hero Richard Hendricks ends his relationship with a Facebook engineer over her programming style: The debate over tabs and spaces , as presented in HBO's Silicon Valley, is real: Developers have been arguing over using tabs versus spaces for formatting their code for almost as long as the concept of programming has existed. At stake is the aesthetics of the code itself - does putting a tab after each new line make it more readable? Or do you just push the space bar a few times? (A more detailed explanation of the tabs vs. spaces debate can be found here .) And so Google Developer Advocate Felipe Hoffa stepped in . By analyzing a billion files, taken from the 400,000 top programming projects on the GitHub social network for software developers, representing 14 terabytes of data all told, he was able to see who was using tabs, and who was using spaces, in most major programming languages. You can read his full results and methodology here , but the end result is bad news for the tabs-loving Hendricks. Check it out: Felipe Hoffa/Google As you can see, spaces far outpace tabs in every major programming language with the exceptions of C, one of the very oldest programming languages still widely used , and Google's Go, an upstart of a programming language that's finding fans among people writing software for the server. This doesn't exactly prove which one is better - but it certainly shows the way that programmers are working in real life. And in most ways that matter, it seems the debate is already over. SEE ALSO: 'Silicon Valley' built an entire episode around one of the most obscure fights in programming NOW READ: One of the oldest ways to write software is finally starting to fade NOW WATCH: Bumble founder: Here's what's seriously wrong with the growing trend in Silicon Valley called 'brogramming'",en,112
84,880,1462909635,CONTENT SHARED,3149164017776669829,-3390049372067052505,-5764325092046707218,,,,HTML,http://www.smartinsights.com/mobile-marketing/mobile-marketing-analytics/mobile-marketing-statistics/,mobile marketing statistics 2016,"Statistics on consumer mobile usage and adoption to inform your mobile marketing strategy mobile site design and app development "" Mobile to overtake fixed Internet access by 2014 "" was the huge headline summarising the bold prediction from 2008 by Mary Meeker, an analyst at Kleiner Perkins Caufield Byers who reviews technology trends annually in May. The mobile statistics that the team at Smart Insights curate in the regular updates to this article include: Ownership of smartphone vs Desktop Mobile vs desktop media and website use Mobile advertising response Smartphone vs Tablet vs Desktop conversion rates Well, we're now past the mobile Tipping Point as this report from comScore shows. So it's no longer a case of asking whether mobile marketing important, we know it is! It's now a question of using the statistics to understand how consumers behave when using different types of mobile devices and what their preferences are. To help you keep up-to-date with the rise in consumer and company adoption of mobile and its impact on mobile marketing, I will keep this post updated throughout 2016 as the new stats come through to support our 120 page Expert members Ebook explaining how to create a mobile marketing strategy . We also have this free summary mobile strategy briefing for Basic members. Our Mobile Marketing Strategy Briefing explains the key issues to plan for in mobile marketing. Download our Free Mobile Marketing Guide . We have grouped the latest mobile stats under these headings for key questions marketers need to answer about mobile to help them compete: Q1. Time spent using mobile meida Q2. Percentage of consumers using mobile devices Q3. How many website visits are on mobile vs desktop devices? Q4. Mobile device conversion rates and visit share for Ecommerce sites? Q5. Mobile - app vs mobile site usage? Q6. How important are mobile ads OK, let's go! Q1. How much time do consumers spend using mobile media? Mary Meeker's annual spring updates on mobile are a must-read if you follow consumer adoption of technology platforms, so we have used some of the key findings from the latest KPCB mobile technology trends by Mary Meeker. Her deck is nearly 200 slides, so we have selected the statistics which best summarise the importance of mobile devices today. The latest data shows that we are now well past the tipping point mentioned at the top of this post. Mobile digital media time in the US is now significantly higher at 51% compared to desktop (42%). The implications are clear - if you're not able to reach your audience through mobile search or display, or you're not providing a satisfactory mobile experience you will miss out compared to competitors who are. The trend in mobile device usage ('vertical screens') compared to all screen use again shows that we're well past the tipping point. Q2. Percentage of consumers using mobile devices? We've created a new summary showing the global popularity of using different digital devices using data from Global Web Index to include in for our State of Digital Marketing 2015 infographic. It clearly shows the popularity of smartphone ownership and emerging mobile devices like Smartwatches. Insight from comScore published in their February 2014 market review shows the picture that marketers need to build up. This panel data shows that the majority of consumers are ""multiscreening"", accessing retail sites on mobile or desktop, so consistent experiences across device need to be deployed. You need to answer this for your own site. As Rob Thurner explained in his post on KPIs to review mcommerce effectiveness , it's important to keep track of the split between users of mobile and desktop devices visiting your site(s). Using advanced segments in Google Analytics is the best way to do this. Q3. How many website visits are on mobile vs desktop devices? However, we need to be careful with interpreting data on hours spent, since we spend most of our time on smartphones checking email and using social media. This has led to the common mantra of 'mobile-first' design which I think is dangerous. Although mobile is growing in importance, this data from Adobe's latest Digital Index shows that in all industries the majority of visits are still on desktop. So with so many site visits still on desktop, it's important when designing using a responsive web design that the desktop experience isn't degraded and this has led to many larger businesses using an adaptive web design where layout and content are tailored for desktop, tablet and smartphone screen dimensions. Q4. Mobile device conversion rates and visit share for Ecommerce sites? We have a separate compilation of Ecommerce conversion stats if you're creating a business case for mobile optimised sites as explained in our mobile marketing strategy guide, this data is also valuable since it shows the variation in conversion rate by mobile type. This is the latest data from Monetate for their retail clients showing conversion rates. The data clearly shows that Smartphone add-to-cart and conversion rates are much lower than for desktop - important if you're making the business case for a mobile responsive site. This source is useful since it's a regular survey showing the growth in use of mobile site visitors. enables you to drill down to see usage by device type, for example iPad is still the dominant tablet, but Kindle Fire and Android tablets now account for over 10% of tablets. You can see that tablet and smartphone use nearly doubled in the year based on 500 million plus visits for these retail clients (see link above for methodology). Q5. Mobile media time - app vs mobile site usage? Consumer preference for mobile apps vs mobile sites should also be thought through as part of mobile strategy. This data from Nielsen on mobile media time shows the consumer preference for mobile apps which account for 89% of media time in mobile as might be expected from the use of the most popular social network, email and news apps. App usage (90% of time) dominates browsers in mobile usage We reported comScore data in May 2012 that showed that on smartphones 82% of mobile media time is via apps . Today, the latest data from Yahoo's Flurry analytics shows that 90 percent of consumer's mobile time is spent in apps . As they put it,put it: It's an App World. The Web Just Lives in It . This is a key insight as companies decide whether to develop mobile apps or create mobile device specific apps. This 90% figure is a key insight as companies decide whether to develop mobile apps or restrict themselves to mobile optimised sites. You do have to be careful about interpreting this since, as the chart below shows, Facebook, messaging, games and utility apps will naturally have the greatest time spent and browser use is still significant by volume if not proportion. But this has implications for advertising on mobile to reach consumers using apps like Facebook and Gmail. Q6. Mobile Ad Spend still lags behind Mobile Media Consumption So, how have advertisers responded to the change in mobile media time? The next chart shows that despite the growth in media time above, some advertisers are missing out since the right-most bar shows that there is a huge missing opportunity on mobile advertising This research sourced from a 2015 study by eMarketer into mobile ad budgets shows a different view. In 2015 mobile ad spending accounts for 49% of digital ad spending, which is only slightly behind the trends of how people are using their devices. These stats also show projections for future growth, which is important as it shows where the market is going. It is clear that mobile is the future, and within 3 years it will come to dominate digital ad spending. Q2. How consumers research products using mobile search and review sites Google's mobile path to purchase report surveyed 950 US consumers across 9 diﬀerent verticals (Restaurants, Food & Cooking, Finance, Travel, Home & Garden, Apparel & Beauty, Automotive, Electronics, Health & Nutrition) to assess how they researched purchases via mobile. A key finding is the starting point for mobile research. As might be expected search was the most common starting point, but it's lower than desktop showing the importance of branded apps and mobile sites. The 5 best sources for mobile marketing statistics? This update to this post features some of the latest updates on mobile statistics from 2014 and highlight some of the best sources to make the business case for investment in mobile marketing in your presentations and business cases to colleagues or clients. 1. Google Mobile Planet. A regular survey for different countries starting in 2011, this enabled you to prepare your own reports. Now this has been replaced by Google's Consumer barometer which enables you to create similar reports. In addition to downloads for each country, you can also create your own charts focusing on KPIs of interest. For example, if you're based in Australia you can look at usage by demographic. The weakness of the current data is that it focuses on Smartphones, not tablets. It may be useful for pushing back against over-enthusiastic colleagues or understanding consumer barriers. For example, less than a third of Australians have ever bought on a smartphone and you can see there are barriers of security and preference for desktop purchases. 2. ITU . The International Telecoms Union data reports mobile usage including mobile broadband subscriptions to show growth in use of mobile. This reported at country, continent and overall levels, so is the best overall source for mobile penetration worldwide. Much of the information is free - see their free mobile statistics section . 3. Flurry Mobile Analytics . This is a great source for showing the overall level of app usage across the four major mobile app platforms by country and drilling down into the popularity of individual apps for different sectors like retail, banking and travel. For example, the latest mobile app growth figures from Flurr y show growth of category use by more than 50% in many categories. Comscore is one of the key worldwide sources useful for marketers to help us find out about the changes in use of mobile media by consumers. This graph shows the pattern across Europe - follow the link above for US and other country breakdowns. The report shows much lower levels of adoption in other European countries though - not even a fifth in most. So extrapolating UK behaviour to other countries would seem to be a mistake with the mobile figure still key. The report also has useful summary of dayparts of different device behaviour, similar to others published. Retail mobile use Mobile was again the focus of the section on retail statistics. Audience growth rate is 80% + on mobile in these UK sites, but lower on grocer sites for obvious reasons. 5. Ofcom Internet usage report . Ofcom's Eighth International Communications Market Report was published in December 2014, this examines take-up, availability, price and use of broadband, landlines, mobiles, TV, radio and post across 17 major countries. As example, here's the picture of desktop vs mobile device in the UK showing that when you look at most important device, desktop and laptop remain important. We hope this compilation of statistics about mobile usage and effectiveness continues to be useful - please share if it is and we'll continue to update it in 2015. If you want a single source of the latest stats across the whole of digital marketing, for Expert members, we compile a regularly updated set of usage statistics to use in presentations - it's updated each quarter so all the latest stats are categorised in a single place for including in presentations. Our ""one-stop"" download includes the latest stats to include in presentations to make the case for digital investment. Download our Online marketing statistics compilation .",en,110
85,3026,1485525147,CONTENT SHARED,991271693336573226,3106760029136205156,-5854718852758691534,"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.95 Safari/537.36",NJ,US,HTML,http://www.brasilpost.com.br/2017/01/23/meme-luiza-esta-atenta-machismo-area-de-ti_n_14336764.html,este é o melhor jeito de entender como as mulheres sofrem machismo na área de ti,"Publicado: A conversa de WhatsApp entre duas amigas explicando o poder da família Kardashian , ""Luiza, você está atenta"", é o meme perfeito para explicar qualquer situação, inclusive as mais delicadas, como o preconceito. E foi com esta ideia que o PyLadies Teresina reformulou o meme para debater o machismo no mercado de TI (Tecnologia da Informação). O projeto foi criado para estimular mulheres a seguirem carreira no mercado de computação, uma área predominantemente masculina. Segundo dados do estudo Stack Overflow Developer de 2015, 92,1% dos profissionais de TI são homens . O baixo índice de mulheres no mercado não é justificado pela falta de interesse. De acordo com o PrograMaria, na escola, 74% das meninas demonstram interesse nas áreas de Ciência, Tecnologia Engenharia e Matemática, mas na hora de escolher a graduação, apenas 0,4% delas realmente rseguem essas áreas, o que significa que há uma grande barreira social nestas profissões. Pensando nisso, o projeto utilizou o meme ""Luiza, você está atenta?"" como ferramenta para propagar como a área de TI pode ser hostil com as mulheres. ""Você tá achando que as mulheres vão deixar o machismo na área de tecnologia barato? BANG, Luiza! A gente cria várias redes de mulheres pra ocupar esses espaços!"", escreveu o projeto no post do Facebook, que acabou viralizando : LEIA MAIS: - País com menor desigualdade de gênero, a Islândia está chocada com a morte desta jovem - Esta é a melhor explicação para quem não entende nada da família Kardashian",pt,110
86,1674,1467822323,CONTENT SHARED,5338677278233757627,-4627026983118548639,-3005922752286260244,,,,HTML,http://www.nytimes.com/2014/02/23/opinion/sunday/friedman-how-to-get-a-job-at-google.html,how to get a job at google,"MOUNTAIN VIEW, Calif. - LAST June, in an interview with Adam Bryant of The Times, Laszlo Bock, the senior vice president of people operations for Google - i.e., the guy in charge of hiring for one of the world's most successful companies - noted that Google had determined that ""G.P.A.'s are worthless as a criteria for hiring, and test scores are worthless. ... We found that they don't predict anything."" He also noted that the ""proportion of people without any college education at Google has increased over time"" - now as high as 14 percent on some teams. At a time when many people are asking, ""How's my kid gonna get a job?"" I thought it would be useful to visit Google and hear how Bock would answer. Don't get him wrong, Bock begins, ""Good grades certainly don't hurt."" Many jobs at Google require math, computing and coding skills, so if your good grades truly reflect skills in those areas that you can apply, it would be an advantage. But Google has its eyes on much more. ""There are five hiring attributes we have across the company,"" explained Bock. ""If it's a technical role, we assess your coding ability, and half the roles in the company are technical roles. For every job, though, the No. 1 thing we look for is general cognitive ability, and it's not I.Q. It's learning ability. It's the ability to process on the fly. It's the ability to pull together disparate bits of information. We assess that using structured behavioral interviews that we validate to make sure they're predictive."" The second, he added, ""is leadership - in particular emergent leadership as opposed to traditional leadership. Traditional leadership is, were you president of the chess club? Were you vice president of sales? How quickly did you get there? We don't care. What we care about is, when faced with a problem and you're a member of a team, do you, at the appropriate time, step in and lead. And just as critically, do you step back and stop leading, do you let someone else? Because what's critical to be an effective leader in this environment is you have to be willing to relinquish power."" What else? Humility and ownership. ""It's feeling the sense of responsibility, the sense of ownership, to step in,"" he said, to try to solve any problem - and the humility to step back and embrace the better ideas of others. ""Your end goal,"" explained Bock, ""is what can we do together to problem-solve. I've contributed my piece, and then I step back."" And it is not just humility in creating space for others to contribute, says Bock, it's ""intellectual humility. Without humility, you are unable to learn."" It is why research shows that many graduates from hotshot business schools plateau. ""Successful bright people rarely experience failure, and so they don't learn how to learn from that failure,"" said Bock. ""They, instead, commit the fundamental attribution error, which is if something good happens, it's because I'm a genius. If something bad happens, it's because someone's an idiot or I didn't get the resources or the market moved. ... What we've seen is that the people who are the most successful here, who we want to hire, will have a fierce position. They'll argue like hell. They'll be zealots about their point of view. But then you say, 'here's a new fact,' and they'll go, 'Oh, well, that changes things; you're right.' "" You need a big ego and small ego in the same person at the same time. The least important attribute they look for is ""expertise."" Said Bock: ""If you take somebody who has high cognitive ability, is innately curious, willing to learn and has emergent leadership skills, and you hire them as an H.R. person or finance person, and they have no content knowledge, and you compare them with someone who's been doing just one thing and is a world expert, the expert will go: 'I've seen this 100 times before; here's what you do.' "" Most of the time the nonexpert will come up with the same answer, added Bock, ""because most of the time it's not that hard."" Sure, once in a while they will mess it up, he said, but once in a while they'll also come up with an answer that is totally new. And there is huge value in that. To sum up Bock's approach to hiring: Talent can come in so many different forms and be built in so many nontraditional ways today, hiring officers have to be alive to every one - besides brand-name colleges. Because ""when you look at people who don't go to school and make their way in the world, those are exceptional human beings. And we should do everything we can to find those people."" Too many colleges, he added, ""don't deliver on what they promise. You generate a ton of debt, you don't learn the most useful things for your life. It's [just] an extended adolescence."" Google attracts so much talent it can afford to look beyond traditional metrics, like G.P.A. For most young people, though, going to college and doing well is still the best way to master the tools needed for many careers. But Bock is saying something important to them, too: Beware. Your degree is not a proxy for your ability to do any job. The world only cares about - and pays off on - what you can do with what you know (and it doesn't care how you learned it). And in an age when innovation is increasingly a group endeavor, it also cares about a lot of soft skills - leadership, humility, collaboration, adaptability and loving to learn and re-learn. This will be true no matter where you go to work.",en,108
87,2624,1477304783,CONTENT SHARED,-8954346068661072425,-6627505417926774253,7151797190085189711,"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.59 Safari/537.36",MG,BR,HTML,https://code.facebook.com/posts/991252547593574/the-technology-behind-preview-photos/,the technology behind preview photos,"First impressions matter, whether you're on a first date, in a job interview, or just choosing new decorations for your house. Some of the first things you see when you visit someone's profile or page on Facebook are the pictures. These pictures are an integral part of the Facebook experience, but sometimes they can be slow to download and display. This is especially true on low-connectivity or mobile networks, which often leave you staring at an empty gray box as you wait for images to download. This is a problem in developing markets such as India, where many people new to Facebook are primarily using 2G networks. Our engineering team took this on as a challenge: What could we design and build that would leave a much better first impression? We initially focused on the cover photo, the beautiful, high-resolution picture at the top of profiles and pages. The cover photo is one of the most visible parts of these surfaces, yet it's also one of the slowest to load. There were two big reasons for this. First, cover photos often reach 100 KB, even after JPEG compression. That's a lot of data when you realize that 2G connections might be transferring data as slowly as 32 KB/second. The second reason is subtler. Before downloading a picture, the application makes a network request for the picture's URL from the GraphQL server. Then, to actually get the image, it uses that URL to make a second network request to the CDN to get the image bytes. The latency of this second network request can be quite long, typically much longer than the first network request. We needed to attack both of these problems simultaneously. 200 bytes To address these issues, we asked ourselves if we could create a visual impression of the image using only 200 bytes. Why 200 bytes? In order to remove that second network request, we needed to include some facsimile of the image itself in the initial network request. This in turn meant that the image had to be part of the GraphQL response, but GraphQL isn't designed to handle full-size image data. It was determined that if we could shrink a cover photo down to 200 bytes, it could efficiently be delivered via the GraphQL response. The cool thing about this solution, if successful, was that it addressed both the small byte requirement and the need for a second network request in one fell swoop. We estimated that this would allow us to display the preview photo in our very first drawing pass, reducing the total latency to display profiles and page headers significantly. Eventually, we still want to download and display the full-size image from the CDN, but this can be done in the background while ensuring that the user experience still feels snappy and enjoyable. The challenge now became how to squeeze a cover photo into 200 bytes! Displaying the right image We felt that a frosted-glass ""impression"" of an image would provide something both visually interesting and consistent with the original image. Having decided on the desired user experience, we needed to figure out the technical details to make it happen. What was the lowest resolution we could use? How would we compress the image? How would we display that image on the client? This is where things got interesting. Displaying the image was the most straightforward part: The frosted-glass look is relatively easy to achieve with a Gaussian blur filter. The nice thing about this blurring filter, besides looking good, is that it ""band-limits"" the signal. Band-limiting essentially means throwing away detail and quickly changing information in the original source image. The more we are willing to blur the displayed image, the smaller our source image can be. Image resolution and compression Clearly, the more we blur our images, the lower resolution we need and the more we can compress. On one extreme, if we send down just the average color of all the pixels in an image (aka the DC components of an image), that would require only a single ""pixel"" of 3 bytes - one byte each for RGB! We knew we needed higher resolution than just one pixel, but how few pixels could we get away with? Given the final frosted-glass effect we want to achieve on the client, we can determine the required blur radius for our Gaussian filter. From that blur radius, we were then able to compute the lowest-resolution image that would still give us the desired final image. For the display size of our cover photos, we found that this resolution was about 42 pixels. Above a 42x42-pixel image, we would get no additional fidelity in the displayed image; essentially, we would be wasting data. But assuming 3 bytes per pixel (for RGB components), that would still be 42x42x3, or 5,292 bytes - much higher than the desired 200-byte target. We started evaluating standard compression techniques to find the best way to compress this data to 200 bytes. Unfortunately, simply entropy encoding the image, with, say, zlib, gets you only a factor of 2. Still too big. We then evaluated a bunch of nonstandard techniques, but we decided it was better to leverage other code/libraries that we had. So, we looked at JPEG image encoding, which is a very popular image codec. Especially since our image is going to be blurred heavily on the client, and thus band-limiting our image data, JPEG should compress this image quite efficiently for our purposes. Unfortunately, the standard JPEG header is hundreds of bytes in size. In fact, the JPEG header alone is several times bigger than our entire 200-byte budget. However, excluding the JPEG header, the encoded data payload itself was approaching our 200 bytes. We just needed to figure out what to do about that pesky header! JPEG to the rescue (mostly) There are a few tables within the JPEG header, which accounts for its size. The question then became: Would it be possible to generate a fixed header that could be stored on client and therefore not need to be transmitted? In that scenario, only the payload would need to be sent, which would make this the winning format. The investigation began. For a given Q value, the quantization table is fixed; through experimentation and measurement, Q20 produced an image that would meet our visual needs. So that was a good start to a fixed header. Our images were not a fixed size but capped at 42x42 (we retain the aspect ratio in the reduced format). This amounted to 2 bytes that we could prepend to the payload and that could be placed by the client in the correct spot to make the header valid. As we looked through the rest of the standard JPEG header, the only other table that could change with different images and options was the Huffman table. This required a bit more work, because there is a trade-off between changes to Q, image data, and image size, which meant different frequency values within the Huffman table, which would lead to different levels of compression and different final payload byte count. Compressing quite a few images while trading off each of those took time, but in the end we had a Huffman table that we could use as a standard that would get us the byte count we wanted across the test images. Since we deal with a large number of images, it was always possible that the solution wouldn't scale, there might be extreme edge cases, we didn't have a real representative sample set, etc. To that end, a version number was added to the beginning so that the format would be future-proof. If we find any extreme cases or better tables in the future, we can update the version number for those images and ship new tables on the clients. So the final format became one byte for version number, one byte each for width and height, and finally the approximately 200 byte payload. The server would just send this format as part of the GraphQL response, and then the client could simply append the JPEG body to the predefined JPEG header, patch the width and height, and treat it as a regular JPEG image. After the standard JPEG decoding, the client could run the predetermined Gaussian blur and scale it to fit the window size. With this, we finally had a format that met our requirements - a highly effective solution that allowed us to reuse the relatively sophisticated JPEG image encoding scheme while transmitting only the data unique to each cover photo. In our data, we saw big improvements. For people on a slow connection, this helped speed up profile and page loads by 30 percent. Even on the fastest connections, this ensured that people would always see a cover photo preview immediately, making their overall experience more seamless. It took a lot of creativity to make it happen, but thanks to everyone's hard work, it paid off!",en,108
88,826,1462537265,CONTENT SHARED,-6542996094878850014,7890134385692540512,2046518916076451829,,,,HTML,https://medium.com/javascript-scene/how-one-jira-ticket-made-my-employer-1mm-month-7-metrics-that-actually-matter-ffb5b2376a6b,how one jira ticket made my employer $1mm/month: 7 metrics that actually matter - javascript scene,"The essence of agile is the continual process of improving efficiency. This is the only thing you really need to know about agile in order to put it to (productive) work in your organization. You should value: Individuals and interactions over processes and tools Working software over comprehensive documentation Customer collaboration over contract negotiation Responding to change over following a plan If that doesn't look familiar, you really need to read this . I'll add some more of my favorite dev team values: Skills over titles Continuous delivery over deadlines Support over blame Collaboration over competition Why is your team's agile process so dysfunctional? Chances are you're losing track of those values, and you're trying to cram the same old chaotic waterfall into agile trappings like scrum meetings and retrospectives. Face to face scrum meetings can be counter-productive because they encourage competition over collaboration (employees compare themselves to each other), blame over support, deadlines over continuous delivery (""we have to close x tickets before Friday or we'll have to work the weekend!""), and a pecking order that infects a team like cancer. Where in the agile manifesto does it say, ""stand up for 15 minutes every day and feel slow-shamed because the coder across from you finished 6 tickets yesterday and you only finished one""? There's always the obnoxious over-achiever who comes in early and goes home late every day, and closes 2x as many tickets as the rest of the team. Note: almost always a relatively inexperienced, but super eager and super impressive. Too bad they're slowing down the rest of the team . Likewise, there's always the slow-poke who closes one or two tickets. Odd. They're a great mentor, and they're always chiming in on other people's tickets, helping them get unstuck, teaching, and giving great advice. They should be able to close 10x as many tickets as the rest of us, right? They must simply be terrible at time management. (Hint: They're not. You just suck at evaluating employees ). Ticket counting and ""velocity tracking"" are the worst ideas in software development management since waterfall chaos. Forget Counting Tickets Forget points. Forget estimates and commitments. Estimates are all worthless lies. Try weekly demos, instead. Get the marketing team to hype the features you finished last month, not the features you think you might be able to finish next month. Build yourself a good feature toggle and marketing release management system and you can still release features according to a marketing hype schedule, but you'll be hyping finished features, and your team will never have to burn out on night & weekend crunch times again. Tip: Your marketing and sales teams should never be allowed to discuss features ""in the pipeline"", and your sales & biz dev teams should never be able to commit to a deadline without a really flexible MVP (minimum viable product) plan. When I say flexible, I mean flexible: e.g., Elon Musk is taking us to Mars. Initial sales MVP: get a balloon into the clouds. Engineering estimates are usually wrong by orders of magnitude , but nobody wants to face up to that fact and deal in reality. What Should We Measure? The only thing that matters in software development is that the users love your software. Everything else in this list serves that purpose. Remember that. Now bring on the real metrics! The first 5 of these metrics are all essential business key performance indicators (KPIs) for nearly every app developer. You're going to wonder why I'm sharing these with a bunch of developers and telling you that these are the metrics you need to focus on , but bear with me. The remaining metrics will clear that up for you. 1. Revenue None of the other metrics mean a damn thing if you go broke. If you run out of fuel, it's game over. You're done. Pack up your office and go home. Core Tactics: Conversion rate optimization Crawlable content Sharable content Page load & perf Optimize to keep the lights on. 2. Monthly Active Users (MAU) Do you have any users? Do you have more than last month? If you're a venture funded startup, you'd better pray you're growing fast enough. In the beginning you can double month over month if you work hard. Of course, all hockey-stick curves are really S curves in disguise, but chances are good you have plenty of room left to grow. Core Tactics: Page load & perf Sharable content TDD & code review Optimize for growth. 3. Net Promoter Score (NPS) Remember when I said if you run out of fuel, it's game over? I tricked you, didn't I? You thought I was talking about money. Money isn't your fuel. Fans are your fuel. Fans are the key to unlocking more money. More sharing. More growth. More joy. More magic. Core Tactics: TDD & code review Page load & perf Collaboration with support & QA staff Optimize to turn users into evangelists. 4. Viral Factor Also known as k-factor or viral quotient. If you're not measuring this, start right now: i = number of invites (e.g., shares) per user c = conversion rate per share k = 1 is steady. No growth. No decline. k > 1 means exponential growth. k < 1 means exponential decline. You should have a giant screen in the middle of the office with a k-factor gauge in bright red for <1 , green for >1 , overlaid on your 3-month MAU growth chart. Core Tactics: Sharable content Integrate sharing into core product Conversion rate optimization Page load & perf Optimize for sharing and new visitor conversion. 5. Support tickets Nothing says ""this garbage is broken"" like an email to customer support. When was the last time somebody contacted support just to tell you how cool you are? Support tickets are your canary in the coal mine. When somebody says something is broken, don't think, ""it works for me!"". Even if it's user error, it's not user error. Chances are there's a flaw in the design and 1,000 other people are bothered by it too. 1,000 other people for every one person who cares enough to write you and complain about. 1,000 people who'd rather hit the back button than waste their time on your app for one more second. Ideally, you should aim for zero support tickets. You'll never reach that metric (if you're lucky), but you should consider every support ticket to be a bug report. Start categorizing the common ones. Count them and use them to prioritize fixes. I'm not saying the customer is always right. Sometimes customers don't know what they want until you give it to them. I am saying that if it's in your inbox, you're doing something wrong. Core Tactics: TDD & code review CI/CD Feature toggle & rollout Periodic bug burndown hackathons Collaboration with support & QA staff Optimize for problem-free customer experience. Key Engineering Focus Metrics As promised. The keys to unlocking the mysteries of the business KPIs. As it turns out, you can move all of the above needles a lot with two levers: 6. Bug Count Here's a shock. Some ticket counts are good for something. Be careful to categorize all the bug tickets as bugs, and then you can see a bug count. All software has bugs, but not all bugs need fixing. If a bug appears only on an ancient phone, and that phone is only used by one user of your software, and that user isn't even a paying customer, do you need to fix that bug? Probably not. Close it and move on. Prioritize the bugs that are hurting your users the most. Get busy and squash them. Core Tactics: TDD & code review Periodic bug burndown hackathons Collaboration with support & QA staff Optimize for a bug-free experience. 7. Performance I'm cheating a little this time. This one is going to contain 3 more critical metrics: Load time: The time it takes for your app to be usable after the user clicks the icon or hits your URL. Aim for <1 second. Beyond that, you lose users. Every ms you can trim off load time comes with measurable benefits to every metric above. Response time: The time from user action (like a click) to a visible response in the application. Aim for <100ms . Any more than that feels like lag. Animation time: The maximum time it takes to draw one animation frame. Aim for 10ms. Any more than 16 ms will cause noticeable jank, and may even make the user feel a bit queasy. You'll need a little breathing room for this one. Keep it under 10ms. Load time is by far the most important mini metric in this list. It will move the business KPI needles like nothing else I've mentioned. But response time and smooth animations cause a magical side-effect. Users are happier after using your app. You see, every little janky glitch, every little delayed response feels jarring to users on an almost subconscious level. Give the same user the same app with performance issues fixed, and they report much higher satisfaction ratings, even if they can't put their finger on why. Our little secret. Core Tactics: Periodic performance hackathons In-depth performance audits 10ms, 100ms, 1000ms, repeat Optimize for a jank-free experience. There's A Lot More To It Of course, this little rant can't go into great depth on how developers can directly manipulate viral factor and MAU numbers, but I'll leave you with a hint: You can. And when those numbers are staring you in the face every day, and you know that it's your job to move them - not management's job, not marketing's job -  your job, I'm sure you'll come up with some creative ideas to make it happen. If your manager thinks you have better things to do, send them this link. Now go out there and move the needle on some metrics that actually matter.",en,108
89,1762,1468438220,CONTENT SHARED,882422233694040097,-4092545774372727680,-3323769179501075221,,,,HTML,https://www.infoq.com/br/news/2016/07/infografico-machine-learning,infográfico: algoritmos para aprendizado de máquina,"Quais são os mais importantes algoritmos a serem utilizados em projetos com aprendizado de máquina? Qualquer pessoa inserida em projetos que façam uso desta tecnologia normalmente se questiona em algum momento, o que utilizar em decorrência de um projeto que utilize aprendizado de máquina. Anubhav Srivastava é um cientista de dados que trabalha e escreve sobre modelos de decisão utilizando Big Data e em seu campo de atuação busca aplicar estes modelos em diversas áreas de negócio da indústria, auxiliando empresas em suas tomadas de decisão. Srivastava, que também atua como escritor para o site thinkbigdata.in , publicou recentemente um infográfico com os principais algoritmos utilizados em aprendizado de máquina. O infográfico destaca alguns dos algoritmos mais utilizados, resumindo cada tipo de aplicabilidade com os disponíveis para uso. Trata-se de um material estruturado que possibilita uma consulta rápida para sanar dúvidas. Segundo o autor, é importante destacar que não há um algoritmo vencedor e que seja melhor em relação aos demais, o infográfico tem a intenção de ser apenas uma referência de uso. Srivastava destaca em sua publicação que: Para situações diferentes, e com diferentes algoritmos, mesmo que sejam concebidos para apresentar resultados semelhantes, o resultado normalmente será apresentado de forma diferente. Dependendo do que se deseja alcançar com a análise dos dados, um algoritmo pode lhe atender de uma forma melhor quando comparado a um outro que faz exatamente a mesma coisa. Outro fator importante é que o tamanho do conjunto de dados o qual estamos trabalhando faz uma diferença determinante no modelo que se quer aplicar. Além disso, a possibilidade de interações com os algoritmos existentes permite o aumento da relevância em uma variedade de aplicações. É importante lembrar que os algoritmos mais simples não são ruins ou obsoletos. Assim, a sugestão que dou é ao invés de buscar os melhores algoritmos, devemos nos concentrar em ganhar a consciência sobre os fundamentos destes diferentes algoritmos e suas aplicações. As técnicas cobertas no infográfico incluem entre outros: O infográfico completo com todos os algoritmos divididos por tipo de técnica de aprendizado de máquina pode ser visualizado na imagem abaixo: A intenção deste infográfico não é a de detalhar os algoritmos e suas técnicas de aprendizado de máquina e sim disponibilizar a informação de uma forma estruturada, fácil e acessível como um quadro que possa ser colado na parede e sempre estar a mão para uma rápida consulta. Para uma referência detalhada com alguns algoritmos, o autor cita ainda a lista disponibilizada na conferência de mineração de dados da IEEE de 2006.",pt,107
90,3097,1487257346,CONTENT SHARED,-8627051188605351707,-4465926797008424436,-5996702544087841012,"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.37 Safari/537.36",SP,BR,HTML,https://dev.to/gonedark/when-to-make-a-git-commit,when to make a git commit,"You don't have to look through too many commit histories on GitHub to see people are pretty terrible about making commits. Now I'm not going to talk about writing commit messages. Here's the post on that. I want to talk about the equally important topic of when to make commits. I get asked this a lot at conferences. Enough to where I made two rules I've continually put to the test. I make a commit when: Anytime I satisfy one of these rules, I commit that set of changes. To be clear, I commit only that set of changes. For example, if I had changes I may want to undo and changes that completed a unit of work, I'd make two commits - one containing the changes I may want to undo and one containing the changes that complete the work. I think the second rule is pretty straightforward. So let's tackle it first. Over the course of time, you'll make some changes you know will be undone. Be it a promotional feature, patch, or other temporary change someday soon you'll want to undo that work. If that's the case, I'll make these changes in their own commit. This way it's easy to find the changes and use git revert . This practice has proven itself time and again, I'll even commit changes I'm simply uncertain about in their own commit. So back to the first rule. I think generally most of us follow this rule. However, you don't have to scroll through too many repositories on GitHub to see that we're pretty bad with commits. The discrepancies come from how we define a unit of work . Let's start with how not to define a unit of work . A unit of work is absolutely not based on time. Making commits every X number of minutes, hours, or days is ridiculous and would never result in a version history that provides any value outside of a chronicling system. Yes, WIP commits are fine. But if they appear in the history of your master branch I'm coming for you! A unit of work is not based on the type of change. Making commits for new files separate from modified files rarely makes sense. Neither does any other type abstraction: code (e.g. JavaScript vs HTML), layer (e.g. Client vs API), or location (e.g. file system). So if a unit of work is not based on time or type , then what? I think it's based by feature . A feature provides more context. Therein making it a far better measurement for a unit of work . Often, implicit in this context are things like time and type , as well as the nature of the change. Said another way, by basing a unit of work by feature will guide you to make commits that tell a story. So, why not just make the first rule: I make a commit when I complete a feature ? Well, I think this a case where the journey matters. A feature can mean different things, even within the context of the same repository. A feature can also vary in size. With unit of work , you keep the flexibility to control the size of the unit . You just need to know how to measure . I've found by feature gives you the best commit. Enjoy this post? Check out my comprehensive video series Getting Git .",en,107
91,1481,1466563387,CONTENT SHARED,-3285397592982852407,-2626634673110551643,3199267762118537905,,,,HTML,http://www.valor.com.br/empresas/4609273/bradesco-vai-testar-tecnologia-por-tras-do-bitcoin-em-sao-paulo,bradesco vai testar tecnologia por trás do bitcoin em são paulo,"SÃO PAULO - O Bradesco vai fazer, até o fim do ano, em São Paulo, um teste de uso do ""blockchain"", a tecnologia por trás da moeda virtual bitcoin. Se o cronograma for mantido, será o primeiro teste prático da tecnologia feito por um banco brasileiro. O ""blockchain"" funciona como um livro-caixa de transações feitas no mundo digital. A tecnologia tem atraído a atenção dos bancos ao redor do mundo por duas razões principais. As transações registradas não podem ser apagadas nem alteradas, o que garante segurança ao sistema. O funcionamento também independe de grandes estruturas de tecnologia, o que pode representar uma redução de custos em relação às formas tradicionais de processamento de transações. Segundo Marcelo Frontini, diretor de pesquisa e inovação do Bradesco, o teste com o ""blockchain"" será feito em Paraisópolis, Zona Sul da capital paulista, em parceria com a startup eWally. A empresa foi uma das selecionadas da mais recente turma do InovaBRA, programa de apoio a startups do Bradesco. A empresa desenvolveu uma carteira digital que pode ser usada com um smartphone para pagamentos e transferência de dinheiro. O ""blockchain"" é a tecnologia por trás do sistema. De acordo com o executivo, o sistema será integrado à rede de correspondentes bancários Bradesco Expresso. Assim, quem quiser fazer um depósito, retirar dinheiro ou fazer um pagamento usando a eWally, poderá ir a um desses pontos. ""Usando o Bradesco como entrada e saída, conseguimos monitorar o uso, o que dá segurança ao teste"", disse. O Bradesco está levando o ""blockchain"" bastante a sério. O banco acabou de se integrar ao consórcio R3, que reúne 43 bancos de todo o mundo para estudar e criar padrões para o uso do sistema. É o segundo banco nacional a entrar no consórcio. O Itaú foi o primeiro, há dois meses. Além da carteira digital da eWally, o Bradesco também vai fazer testes com o sistema de transferência de dinheiro por meio de remessas internacionais da startup BitOne, outra recém-aprovada no InovaBRA. Nesse caso, no entanto, as avaliações ainda são iniciais, para verificar a viabilidade do modelo. A BitOne desenvolveu um sistema de proteção cambial para as transferências (""hedge""), que tem o objetivo de reduzir o impacto de conversão do bitcoin para a moeda do destinatário da transferência. De acordo com Frontini, é preciso avaliar se o custo desse hedge não será alto demais, inviabilizando o sistema. O banco também está em conversas com a startup Ripple Labs, a principal referencia em remessas internacionais usando ""blockchain"". O ""blockchain"" é uma das vertentes do grupo de trabalho criado pelo Banco Central (BC) para discutir as ""fintechs"", como vêm sendo chamadas as tecnologias voltadas ao setor financeiro.",pt,105
92,1999,1470416057,CONTENT SHARED,7526364197140419661,6013226412048763966,-1503381279074578444,,,,HTML,http://www.jjhws.com/solutions/digital-health-coaching,digital health coaching,"Skills for Life: A Scalable Approach to Behavior Change We believe that inside each individual lies the power to change-to be healthier and happier. Behavior change can be incredibly challenging. That's why we're proud to have been one of the first to offer Digital Health Coaching, which combines advanced technology and behavioral science to emulate a live health coach. Our programs offer an individually tailored and scalable approach to coaching, with a demonstrated ability to deliver results across large populations. We bring this approach to health plans to support their pursuit of the Triple Aim goals-achieving better health outcomes, at lower cost, all while enhancing the member experience. Drawing upon a wide array of validated behavior science models, our Digital Health Coaching solution can help empower your members by helping them build the skills they need most. You are striving for lower costs and satisfied members who are engaged, motivated, and confident in their ability to make healthy behavior change-our solution can help you achieve that. Being engaged with your health means being engaged with your life. Digital Health Coaching can help your members engage with their health-every day. Scientifically Based Created by behavioral scientists, health professionals, and content tailoring experts, our programs are built upon a foundation of tailored guidance and actionable steps to develop skills. This skills-based approach can help people dealing with multiple challenges (diabetes, depression, sleep problems, and more). Individually Tailored We recommend concrete, tailored action steps based on user-identified health and wellness goals. The personalized experience is designed to help increase success by tapping into a user's 'core values'. Skills and action steps also easily map to real-world activities to help foster behavior change. A Compelling Experience For Participants Our programs are designed to provide an engaging and interactive user experience-holistically connecting users' health and behavior with their core values and life goals. Tailored coaching content is delivered in small packages to help create a quick yet effective interaction and help create healthy habits. Designed to be responsive for desktop, tablet, and phone. Click here to schedule a product demonstration.",en,105
93,1494,1466631321,CONTENT SHARED,-820343972901090172,3302556033962996625,3775313930401213418,,,,HTML,https://medium.com/@shanselman/stop-saying-learning-to-code-is-easy-659c5f4c0d7,stop saying learning to code is easy.,"Stop saying learning to code is easy. I saw this tweet after the Apple WWDC keynote and had thought the same thing. Hang on, programming is hard. Rewarding, sure. Interesting, totally. But ""easy"" sets folks up for failure and a lifetime of self-doubt. When we tell folks - kids or otherwise - that programming is easy , what will they think when it gets difficult? And it will get difficult. That's where people find themselves saying ""well, I guess I'm not wired for coding. It's just not for me."" Now, to be clear, that may be the case. I'm arguing that if we as an industry go around telling everyone that ""coding is easy"" we are just prepping folks for self-exclusion, rather than enabling a growing and inclusive community. That's the goal right? Let's get more folks into computers, but let's set their expectations. Here, I'll try to level set. Hey you! People learning to code! Programming is hard. It's complicated. It's exhausting. It's exasperating. Some things will totally make sense to you and some won't. I'm looking at you, RegEx. The documentation usually sucks. Sometimes computers are stupid and crash. But. You'll meet amazing people who will mentor you. You'll feel powerful and create things you never thought possible. You'll better understand the tech world around you. You'll try new tools and build your own personal toolkit. Sometimes you'll just wake up with the answer. You'll start to ""see"" how systems fit together. Over the years you'll learn about the history of computers and how we are all standing on the shoulders of giants. It's rewarding. It's empowering. It's worthwhile. And you can do it. Stick with it. Join positive communities. Read code. Watch videos about code. Try new languages! Maybe the language you learned first isn't the ""programming language of your soul."" Learning to program is NOT easy but it's totally possible. You can do it. More Reading Sponsor: Big thanks to Redgate for sponsoring the feed this week. How do you find & fix your slowest .NET code? Boost the performance of your .NET application with ANTS Performance Profiler . Find your bottleneck fast with performance data for code & queries. Try it free !",en,104
94,2702,1478176600,CONTENT SHARED,-6642751159620064055,3891637997717104548,-5518716332871437149,"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.143 Safari/537.36",SP,BR,HTML,https://capgemini.github.io/drupal/what-to-look-for-in-code-review/,what to look for in a code review ,"In a previous article on this blog, I talked about why code review is a good idea , and some aspects of how to conduct them. This time I want to dig deeper into the practicalities of reviewing code, and mention a few things to watch out for. Code review is the first line of defence against hackers and bugs. When you approve a pull request, you're putting your name to it - taking a share of responsibility for the change. Once bad code has got into a system, it can be difficult to remove. Trying to find problems in an existing codebase is like looking for an unknown number of needles in a haystack , but when you're reviewing a pull request it's more like looking in a handful of hay. The difficult part is recognising a needle when you see one. Hopefully this article will help you with that. Code review shouldn't be a box-ticking exercise, but it can be helpful to have a list of common issues to watch out for. As well as the important question of whether the change will actually work, the main areas to consider are: I'll touch on these areas in more detail - I'll be talking about Drupal and PHP in particular, but a lot of the points I'll make are relevant to other languages and frameworks. Security I don't claim to be an expert on security, and often count myself lucky that I work in what my colleague Andrew Harmel-Law calls ""a creative-inventive market, not a safety-critical one"" . Having said that, there are a few common things to keep an eye out for, and developers should be aware of the OWASP top ten list of vulnerabilities . When working with Drupal, you should bear in mind the Drupal security team's advice for writing secure code . For me, the most important points to consider are: Does the code accept user input without proper sanitisation? In short - don't trust user input. The big attack vectors like XSS and SQL injection are based on malicious text strings. Drupal provides several types of text filtering - the appropriate filter depends on what you're going to do with the data, but you should always run user input through some kind of sanitisation . Are we storing sensitive data anywhere we shouldn't be? Security isn't just about stopping bad guys getting in where they shouldn't. Think about what kind of data you have, and what you're doing with it. Make sure that you're not logging people's private data inappropriately, or passing it across network in a way you shouldn't. Even if the site you're working on doesn't have anything as sensitive as the Panama papers , you have a legal, professional, and personal responsibility to make sure that you're handling data properly. Performance When we're considering code changes, we should always think about what impact they will have on the end user, not least in terms of how quickly a site will load. As Google recently reminded us , page load speed is vital for user engagement. Slow, bloated websites cost money, both in terms of mobile data charges and lost revenue . Does the change break caching? Most Drupal performance strategies will talk about the value of caching. The aim of the game is to reduce the amount of work that your web server does. Ideally, the web server won't do any work for a page request from an anonymous user - the whole thing will be handled by a reverse proxy cache, such as Varnish. If the request needs to go to the web server, we want as much of the page as possible to be served from an object cache such as Redis or Memcached, to minimise the number of database queries needed to render the page. Are there any unnecessary uses of $_SESSION ? Typically, reverse proxy servers like Varnish will not cache pages for authenticated users. If the browser has a session, the request won't be served by Varnish, but by the web server. Here's an illustration of why this is so important. This graph shows the difference in response time on a load test environment following a deployment that included some code to create sessions. There were some other changes that impacted performance, but this was the big one. As you can see, overall response time increased six-fold, with the biggest increase in the time spent by the web server processing PHP (the blue sections on the graphs), mainly because a few lines of code creating sessions had slipped through the net. Are there any inefficient loops? The developers' maxims ""Don't Repeat Yourself"" and ""Keep It Simple Stupid"" apply to servers as well. If the server is doing work to render a page, we don't want that work to be repeated or overly complex. What's the front end performance impact? There's no substitute for actually testing, but there are a few things that you can keep an eye out for when reviewing change. Does the change introduce any additional HTTP requests? Perhaps they could be avoided by using sprites or icon fonts. Have any images been optimised? Are you making any repeated DOM queries ? Accessibility Even if you're not an expert on accessibility, and don't know ARIA roles , you can at least bear in mind a few general pointers. When it comes to testing, there's a good checklist from the Accessibility Project , but here are some things I always try to think about when reviewing a pull request. Will it work on a keyboard / screen reader / other input or output device ? Doing proper accessibility testing is difficult, and you may not have access to assistive technology, but a good rule of thumb is that if you can navigate using only a keyboard, it will probably work for someone using one of the myriad input devices . Testing is the only way to be certain, but here are a couple of simple things to remember when reviewing CSS changes: hover and focus should usually go together, and you should almost never use outline: none; . Are you hiding content appropriately? One piece of low-hanging fruit is to make sure that text is available to screen readers and other assistive technology. Any time I see display: none; in a pull request, alarm bells start ringing. It's usually not the right way to hide content . Maintainability Hopefully the system you're working on will last for a long time. People will have to work on it in the future. You should try to make life easier for those people, not least because you'll probably be one of them. Reinventing the wheel Are you writing more code than you need to? It may well be that the problem you're looking at has already been solved, and one of the great things about open source is that you're able to recruit an army of developers and testers you may never meet . Is there already a module for that? On the other hand, even if there is an existing module, it might not always make sense to use it. Perhaps the contributed module provides more flexibility than our project will ever need, at a performance cost. Maybe it gives us 90% of what we want, but would force us to do things in a certain way that would make it difficult to get the final 10%. Perhaps it isn't in a very healthy state - if so, perhaps you could fix it up and contribute your fixes back to the community, as I did on a recent project . If you're writing a custom module to solve a very specific problem, could it be made more generic and contributed to the community? A couple of examples of this from the Capgemini team are Stomp and Route . One of the jobs of the code reviewer is to help draw the appropriate line between the generic and the specific. If you're reviewing custom code, think about whether there's prior art. If the pull request includes community-contributed code, you should still review it. Don't assume that it's perfect, just because someone's given it away for nothing. Appropriate API usage Is your team using your chosen frameworks as they were intended? If you see someone writing a custom function to solve a problem that's already been solved, maybe you need to share a link to the API docs for the existing solution. Introducing notices and errors If your logs are littered with notices about undefined variables or array indexes, not only are you likely to be suffering a performance hit from the logging, but it's much harder to separate the signal from the noise when you're trying to investigate something. Browser support Remember that sometimes, it's good to be boring . As a reviewer, one of your jobs is to stop your colleagues from getting carried away with shiny new features like ES6, or CSS variables. Tools like Can I Use are really useful in being able to check what's going to work in the browsers that you care about. Code smells Sometimes, code seems wrong. As I learned from Larry Garfield's excellent presentation on code smells at the first Drupalcon I went to, code smells are indications of things that might be a deeper problem. Rather than re-hash the points Larry made, I'd recommend reading his slides , but it is worth highlighting some of the anti-patterns he discusses. Functions or objects that do more than one thing A function should have a function. Not two functions, or three. If an appropriate comment or function name includes ""and"", it's a sign you should be splitting the function up. Functions that sometimes do different things Another bad sign is the word ""or"" in the comment. Functions should always do the same thing. Excessive complexity Long functions are usually a sign that you might want to think about refactoring. They tend to be an indicator that the code is more complex than it needs to be. The level of complexity can be measured , but you don't need a tool to tell you that if a function doesn't fit on a screen, it'll be difficult to debug. Not being testable Even if functions are simple enough to write tests for, do they depend on a whole system? In other words, can they be genuinely unit tested? Lack of documentation There's more to be said on the subject of code comments than I can go into here, but suffice to say code should have useful, meaningful comments to help future maintainers understand it. Tight coupling Modules should be modular. If two parts of a system need to interact, they should have a clearly defined and documented interface. Impurity Side effects and global variables should generally be avoided. Sensible naming Is the purpose of a function or variable obvious from the name? I don't want to rehash old jokes , but naming things is difficult, and it is important. Why would you comment out lines of code? If you don't need it, delete it. The beauty of version control is that you can go back in time to see what code used to be there. As long as you write a good commit message, it'll be easy enough to find. If you think that you might need it later, put it behind a feature toggle so that the functionality can be enabled without a code release. Specificity In CSS, IDs and !important are the big code smells for me. They're a bad sign that a specificity arms race has begun. Even if you aren't going to go all the way with a system like BEM or SMACSS , it's a good idea to keep specificity as low as possible. The excellent articles on CSS specificity by Harry Roberts and Chris Coyier are good starting points for learning more. Standards It's important to follow coding standards . The point of this isn't to get some imaginary Scout badge - code that follows standards is easier to read, which makes it easier to understand, and by extension easier to maintain. In addition, if you have your IDE set up right, it can warn you of possible problems, but those warnings will only be manageable if you keep your code clean . Deployability Will your changes be available in environments built by Continuous Integration? Do you need to set default values of variables which may need overriding for different environments? Just as your functions should be testable, so should your configuration changes. As far as possible, aim to make everything repeatable and automatable - if a release needs any manual changes it's a sign that your team may need to be thinking with more of a DevOps mindset . Keep Your Eyes On The Prize With all this talk of coding style and standards, don't get distracted by trivialities - it is worth caring about things like whitespace and variable naming, but remember that it's much more important to think about whether the code actually does what it is supposed to . The trouble is that our eyes tend to fixate on those sort of things, and they cause unnecessary cognitive load . Pre-commit hooks can help to catch coding standards violations so that reviewers don't need to waste their time commenting on them. If you're on a big project, it will almost certainly be worth investing some time in integrating your CI server and your code review tool, and automating checks for issues like code style , unit tests, mess detection - in short, all the things that a computer is better at spotting than humans are. Does the code actually solve the problem you want it to? Rather than just looking at the code, spend a couple of minutes reading the ticket that it is associated with - has the developer understood the requirements properly? Have they approached the issue appropriately? If you're not sure about the change, check out the branch locally and test it in your development environment. Even if there's nothing wrong with the suggested change, maybe there's a better way of doing it. The whole point of code review is to share the benefit of the team's various experiences, get extra eyes on the problem, and hopefully make the end product better. I hope that this has been useful for you, and if there's anything you think I've missed, please let me know via the comments.",en,103
95,2350,1473970449,CONTENT SHARED,-6603970730135147059,-4028919343899978105,5740617098389918193,,,,HTML,http://canaltech.com.br/noticia/geek/pesquisadores-do-mit-desenvolvem-tecnologia-que-digitaliza-livros-fechados-79769/,pesquisadores do mit desenvolvem tecnologia que digitaliza livros fechados - geek,"Por Redação | em Uma nova tecnologia desenvolvida por pesquisadores do MIT promete revolucionar a maneira de digitalizar documentos e livros para o formato digital. Publicado na revista Nature Communications, a tecnologia propõe que livros possam ser digitalizados sem precisar escanear página por página, bastando inserir qualquer livro fechado para que o processo seja feito. Caso a tecnologia seja aperfeiçoada, ela poderá agilizar o trabalho de bibliotecas, por exemplo. A maneira de conseguir uma versão digital de documentos e livros se tornará muito mais fácil e rápida. No vídeo abaixo você pode verificar com mais detalhes como a tecnologia desenvolvida no MIT funciona: O processo utiliza radiação tetrahertz, que é absorvida pelo papel e tinta de uma maneira diferente. A câmera é tão precisamente ajustada que os pesquisadores conseguem detectar a diferença entre uma página e outra, apesar de haver apenas 20 micrômetros de ar entra cada uma. As páginas digitalizadas são lidas através de softwares ajustados para o processamento da imagem gerada. Ainda há um longo caminho a ser percorrido. Neste momento, a tecnologia permite a leitura de apenas nove páginas agrupadas. Mas, novas pesquisas poderiam maximizar este número, tornando os resultados surpreendentes, principalmente para arquivistas e bibliotecários. Com a nova tecnologia, será possível obter acesso a livros restritos e antigos através de uma cópia digital e também agilizar consideravelmente o tempo de digitalização de documentos. Assine nosso canal e saiba mais sobre tecnologia!",pt,101
96,1211,1464875897,CONTENT SHARED,3306277069425849869,9109075639526981934,3973684612244026225,,,,HTML,http://computerworld.com.br/google-segue-microsoft-e-lanca-ferramenta-analitica-gratis,google segue microsoft e lança ferramenta analítica grátis,"O Google está intensificando seu compromisso com o mercado de ferramentas analíticas corporativas. A companhia acaba de lançar uma nova ferramenta gratuita para visualização de dados. O Data Studio, versão grátis de um Sistema de visualização de dados, chega como parte da suíte de tecnologias de analytics apresentadas pela empresa há alguns meses. A solução inclui uma variedade de conectores que permite que usuários conectem outras ferramentas como o AdWords e Sheets, por exemplo. O sistema também se integra ao BigQuery. A empresa planeja, ainda, disponibilizar conectores com bancos de dados SQL ainda esse ano. O produto concorre contra o Microsoft Power BI, um dos principais lançamentos da fabricante do Windows sob o comando de Satya Nadella. O Google afirma que o Data Studio garante a empresas coletarem dados de uma variedade de fontes e, a partir disso, gerar relatórios para serem compartilhados interna e externamente, fornecendo melhor compreensão de grandes volumes de dados. Os relatórios incluem gráficos, planilhas, mapas de calor ou outros modelos visualmente úteis aos usuários. Empresas que quiserem recursos mais amplos poderão comprar licenças do serviço Data Studio 360. A camada gratuita, porém, dá aos clientes uma forma de testar a tecnologia e ver se é aderente a suas demandas. A grande diferença entre os dois produtos, de fato, é o número de reports que cada usuário pode criar - clientes da versão grátis podem gerar cinco relatórios.",pt,101
97,2234,1472606340,CONTENT SHARED,-5488842573681626972,-4465926797008424436,4923575442991716043,,,,HTML,http://www.testingexcellence.com/bdd-guidelines-best-practices/,bdd best practices and guidelines - testing excellence,"BDD Best Practices BDD Introduction BDD (Behaviour Driven Development) is a methodology for developing software through continuous example-based communication between developers, QAs and BAs. In this article we discuss some BDD Best Practices to get the most benefit. More than anything else, the primary purpose of BDD methodology is to encourage communication amongst the stakeholders of the project so that the context of each feature is correctly understood by all members of the team (i.e. shared understanding), before development work starts. This helps in identifying key scenarios for each story and also eradicate ambiguities from requirements. In BDD, Examples are called Scenarios. Scenarios are structured around the Context-Action-Outcome pattern and are written in a special format called Gherkin . The scenarios are a way of explaining (in plain english) how a given feature should behave in different situations or with different input parameters. Because Gherkin is structural, it serves both as a specification and input into automated tests, hence the name ""Executable Specifications"". What is a feature file and what does it contain Feature files are text files with .feature extension, which can be opened by any text editor as well as readable by any BDD-aware tool, such as Cucumber, JBehave or Behat. Feature files should start with the context of the feature (which is essentially the story), followed by at least one scenario in the following format Feature: Some terse yet descriptive text of what is desired In order to realize a named business value As an explicit system actor I want to gain some beneficial outcome which furthers the goal Scenario: Some determinable business situation Given some precondition And some other precondition When some action by the actor And some other action And yet another action Then some testable outcome is achieved And something else we can check happens too Scenarios in feature files should focus on the ""what"" rather than the ""how"". The scenarios should be concise and to the point, so that the reader can quickly grasp the intent of the test without having to read a lot of irrelevant steps. Why should we write feature files As mentioned before, the primary aim of the BDD methodology is to encourage communication amongst the delivery team. The aim of the feature files is to document the scenarios talked through in order to give an indication of how much work is involved in delivering the feature. The feature files are also the drivers for the automated tests. Feature files also serve as a definition of done (DoD), meaning that when all the scenarios have been implemented and tested successfully, we can mark the story as done. Who should write feature files It doesn't really matter who actually writes/types the feature files, it could be any member of the delivery team, however, the contents (scenarios) which are discussed by a trio of Dev-QA-BA are the essential part of feature files. Getting the shared common understanding of the feature is the key element. When should feature files be written Feature files should be written during the story grooming sessions where the details of each story is discussed. Feature files containing scenarios should be written before development starts so that developers as well as QA have a clear understanding of the intent of the story. There should be a shared understanding of the story. The scenarios serve as requirements to development. Where should feature files be kept There should be one source of truth serving both as specification and automated execution, therefore should be kept somewhere where every member of the team has easy access. Having said that, because the feature files are the drivers of the automated tests, they should ideally be kept in source control system (GitHub) so that any updates to the feature files is immediately reflected on to the tests. For non-technical members who have no experience with Git, we can always execute a dry-run of the feature files which will then output the list of all existing scenarios without actually exercising the feature files. How should we write feature files There are generally two ways of writing feature files - Imperative and Declarative Imperative style of writing a feature file, is very verbose, contains low level details and too much information. Pros: person reading the feature file can follow the step-by-step Cons: Because of too much detail, the reader can lose the point of the story and the tests. The feature file becomes too big, difficult to maintain and likely to fail due to UI updates. Declarative style of writing a feature file is concise and to the point, contains only relevant information about the story. Pros: The declarative style is more readable as it contains less steps in the scenario. The reader can easily understand the scope of the test and quickly identify if any key elements are missing.",en,100
98,1065,1463934631,CONTENT SHARED,5206308811707978799,-1032019229384696495,7846322948118114767,,,,HTML,http://techcrunch.com/2016/05/21/the-rise-of-apis/,the rise of apis,"It's been almost five years since we heard that "" software is eating the world ."" The number of SaaS applications has exploded and there is a rising wave of software innovation in the area of APIs that provide critical connective tissue and increasingly important functionality. There has been a proliferation of third-party API companies, which is fundamentally changing the dynamics of how software is created and brought to market. The application programming interface (API) has been a key part of software development for decades as a way to develop for a specific platform, such as Microsoft Windows. More recently, newer platform providers, from Salesforce to Facebook and Google, have offered APIs that help the developer and have, in effect, created a developer dependency on these platforms. Now, a new breed of third-party APIs are offering capabilities that free developers from lock-in to any particular platform and allow them to more efficiently bring their applications to market. The monolithic infrastructure and applications that have powered businesses over the past few decades are giving way to distributed and modular alternatives. These rely on small, independent and reusable microservices that can be assembled relatively easily into more complex applications. As a result, developers can focus on their own unique functionality and surround it with fully functional, distributed processes developed by other specialists, which they access through APIs. Faster, cheaper, smarter Developers realize that much of the functionality they need to build into an app is redundant to what many other companies are toiling over. They've learned not to expend precious resources on reinventing the wheel but instead to rely on APIs from the larger platforms, such as Salesforce, Amazon and, more recently, specialized developers. We're still in the early innings of this shift to third-party APIs, but a number of promising examples illustrate how developers can turn to companies such as Stripe and Plaid for payment connectivity, Twilio for telephony, Factual for location-based data and Algolia for site search. Indeed, the area is booming. On last check, ProgrammableWeb was providing searchable access to almost 15,000 APIs, with more being added on a daily basis. Developers can incorporate these APIs into their software projects and get to market much more quickly than going it alone. While getting to market more quickly at a lower cost is a huge advantage, there is an even more important advantage: Companies that focus on their core capabilities develop differentiated functionality, their ""secret sauce,"" at higher velocity. The benefits for the rest of the software development ecosystem are profound. Another advantage is third-party APIs are often flat-out better. They work better and provide more flexibility than APIs that are built internally. Companies often underestimate the amount of work that goes into building and maintaining the functionality that they can now get as a third-party API. Finally, third-party API developers have more volume and access to a larger data set that creates network effects. These network effects can manifest themselves in everything from better pricing to superior SLA's to using AI to mine best practices and patterns across the data. For example, Menlo's portfolio company Signifyd offers fraud analysis as an API. They aggregate retail transactions across hundreds of companies, which allows them to understand a breadth of fraud markers better than any individual customer could. A new breed of software companies Releasing software as an API allows those companies to pursue a number of different adoption routes. Rather than trying to sell specific industry verticals or use cases, often the customer is a developer, leading to an extremely low-friction sales process. The revenue model is almost always recurring, which leads to an inherently scalable business model as the end customers' usage increases. While the ecosystem of API-based companies is early in its evolution, we believe the attributes of these companies will combine to create ultimately more capital-efficient and profitable business models. This opportunity is not limited to new upstarts. Existing developers may have the opportunity to expose their own unique functionality as an API, morphing their product from application to platform. Some outstanding companies have built API businesses that match or exceed their original focus: Salesforce reportedly generates 50 percent of its revenues through APIs, eBay nearly 60 percent and Expedia a whopping 90 percent. The model is attractive to entrepreneurs and investors. Rather than trying to create the next hot app and having to invest heavily in marketing and distribution before validating scalable demand, it may make more sense to build a bounded set of functionality and become an arms merchant for other developers. The API model creates a compelling route to market that if successful can scale capital efficiently and gain a network effect over time. Currently, there are 9 million developers working on private APIs; as that talent sees the opportunity to create companies versus functionalities, we may see a significant shift to public API development (where there are currently only 1.2 million developers). Rethinking the value chain In the past, the biggest companies were those closest to the data (e.g. a system of record), able to impose a tax, or lock-in to their platform. In the API economy, the biggest companies may be the ones that aggregate the most data smartly and open it up to others. This enables new types of competitive barriers, as in Twilio's ability to negotiate volume discounts from carriers that no individual developer could obtain, or the volume pricing that Stripe enjoys by pooling payments across many developers. Companies like Usermind (a Menlo Ventures portfolio company) show great promise in allowing enterprises to move beyond their single-application silos by creating workflows and simplifying the API connections between their existing SaaS applications. While the ecosystem for API startups is attractive today, we believe it will only become stronger. Over the last five years there's been a broadening of interest in enterprise-oriented technologies like SaaS, big data, microservices and AI. APIs are the nexus of all four of those areas. As the world of enterprise software development further embraces third-party APIs, we expect to see a number of large companies emerge. The low-touch sales model, recurring revenue and lack of customer concentration lead to a very attractive business model. In addition, the benefits for the rest of the software development ecosystem are profound, as app developers can focus on delivering the unique functionality of their app and more quickly and less expensively deliver that ever-important initial product. Featured Image: Bloomua / Shutterstock",en,99
99,1702,1467992229,CONTENT SHARED,-5879360586463363298,3891637997717104548,-3159742872959650153,,,,HTML,https://medium.com/@hichaelmart/lambci-4c3e29d6599b,introducing lambci - a serverless build system,"Introducing LambCI - a serverless build system I'm excited to announce the first release of LambCI , an open-source continuous integration tool built on AWS Lambda �� LambCI is a tool I began building over a year ago to run tests on our pull requests and branches at Uniqlo Mobile . Inspired at the inaugural ServerlessConf a few weeks ago, I recently put some work into hammering it into shape for public consumption. It was borne of a dissatisfaction with the two current choices for automated testing on private projects. You can either pay for it as a service (Travis, CircleCI, etc) - where 3 developers needing their own build containers might set you back a few hundred dollars a month. Or you can setup a system like Jenkins, Strider, etc and configure and manage a database, a web server and a cluster of build servers . In both cases you'll be under- or overutilized, waiting for servers to free up or paying for server power you're not using. And this, for me, is where the advantage of a serverless architecture really comes to light: 100% utilization, coupled with instant invocations. Systems built on solutions like AWS Lambda and Google Cloud Functions essentially have per-build pricing. You'd pay the same for 100 concurrent 30 second builds as you would for 10 separate 5 minute builds. The LambCI Advantage From an ops perspective, all of the systems and capacity are managed by Amazon (SNS, Lambda, DynamoDB and S3), so LambCI is far simpler to setup and manage than Jenkins - especially given that you get 100 concurrent builds out of the box. From a cost perspective, it's typically far cheaper for private builds than the various SaaS offerings because you only pay for the time you use (and the first 4,444 mins/mth are free): So if you had 2 developers, each simultaneously running sixty 4-min builds per day (ie, 4 hrs each), LambCI would be more than 8 times cheaper per month than Travis ($15 vs $129). It's only if you need to be running builds 24/7 that SaaS options become more competitive - and of course if you're wanting to run builds for your open source projects, then Travis and CircleCI and others all have great (free) options for that. Performance-wise, Lambda reports as a dual Xeon E5-2680 @2.80GHz. If you have checked-in dependencies and fast unit tests, builds can finish in single-digit seconds - but a larger project like dynalite , with 941 HTTP-to-localhost integration tests, builds in about 70 seconds. 43 secs of that is actually running the tests with the remainder being mostly npm installation. On my 1.7GHz i7 MacBook Air the npm install and tests complete about 20% faster, so there's definitely an element of ""cloud"" speed to keep in mind. The public Travis option takes only a few seconds longer than LambCI to run dynalite's npm install and tests, but the overall build time is larger due to worker startup time (22 secs) and waiting in the queue (up to several mins - I assume this only happens if you don't have enough concurrency). What does it look like? Here's what it looks like in action - this is building a project with only a handful of tests and checked-in dependencies, so this is definitely faster than it is when building our typical projects, but I promise this is real and all running remotely on AWS Lambda:",en,99
100,998,1463503800,CONTENT SHARED,-1038011342017850,6735372008307093370,841012281439613986,,,,HTML,https://universidadedocotidiano.catracalivre.com.br/para-entender/para-entender-o-dia-internacional-contra-homofobia/,para entender o dia internacional contra a homofobia,"O Dia Internacional contra a Homofobia é comemorado em 17 de maio para lembrar a data em que a OMS (Organização Mundial da Saúde) retirou o homossexualismo da Classificação Estatística Internacional de Doenças e Problemas Relacionados à Saúde, deixando de considerar essa tendência um desvio, além de abolir o termo. Na área de saúde, o sufixo ""ismo"" caracteriza uma condição patológica. Entre 1948 e 1990, a OMS classificou a tendência como um transtorno mental. Em 17 de maio de 1990, uma assembleia geral aprovou a retirada da condição da Classificação Internacional de Doenças, com a alegação que ""homossexualidade não constitui doença, nem distúrbio e nem perversão"". A nova classificação entrou em vigor nos países-membros da ONU em 1993. A exclusão marcou o fim de um ciclo de 2.000 anos em que a cultura judaico-cristã encarou a homossexualidade como um pecado, depois como um crime e, por último, como uma doença. Dizer que a condição é um vício, uma tara ou uma doença a ser curada passou oficialmente à categoria de preconceito e ignorância. E foi por isso que 17 de maio foi declarado o Dia Internacional de Combate à Homofobia, quando pessoas do mundo inteiro se mobilizam para discutir a diversidade e a tolerância.",pt,98
101,713,1462202338,CONTENT SHARED,-4754223659064624252,5127372011815639401,1894192985308020631,,,,HTML,https://www.box.com/blog/effective-learning-through-code-workshops/,effective learning through code workshops | box blog,"At Box, we're very interested in the quality of our code, which is why we're constantly evaluating our processes to figure out how we can do better. We even have a team of Code Reliability Engineers (CREs) that help others write better code and provide training both internally and externally . Recently, we've turned a critical eye towards code reviews and have been implementing a new process called code workshops. The trouble with code reviews Code reviews are a tough topic to approach as many have had bad experiences in the past. The first code review I ever personally participated in was, in fact, horrible. It was organized by the senior members of a team I was on and the process was strange. We all sat in a room, spending the first 20 minutes staring at printouts of someone's code and scribbling notes on them. After that, we went around the room and everyone shared what they thought of the code while the author of the code listened intently. Then, the author was allowed to defend the code. I hated that experience. In my mind, we have just wasted time sitting together and making someone incredibly uncomfortable for no good reason. We didn't actually learn anything and it was time spent away from being productive. I felt like there was some value to doing code reviews, but this process felt wrong in every way: There was an overall accusatory tone that encouraged people to find things that were wrong and trivialize things that were right. People seeing code for the first time in the review meeting was a complete waste of time. Why weren't we prepared? The negative comments put the code author on the defensive, which led to less-than-productive conversations. Over the course of my career, I've heard a lot of feedback around these points whenever engineers say they hate code reviews. The kicker is that engineers actually love talking about code. We talk about it on Twitter. We talk about it over email and IM. We talk about it over lunch. We think some people write code better than others. We like semicolons. We hate semicolons. We love talking about this stuff, so why is it that we hate code reviews so much? The problem is typically the code review process. Too many times they are set up as adversarial (as in my first experience), which leads to defensiveness and bad feelings. When processes are too loose, people don't feel like there's much value in participating. When processes are too strict, people feel judged. Unfortunately, the best way to learn about code is to discuss it with other people, so how can you create a system that people actually enjoy and learn from? Building a better code review Over the years, I've worked with a process that I've come to call code workshops (since code reviews have a negative connotation, a different phrase eliminates some bias). Code workshops are a group session that are intended to socialize team expectations and best practices, as well as uncover issues and points that need to be addressed by the team as a whole. Code workshops take place on a weekly basis for one hour and with a maximum of 20 people in the room. The session is split in half, with two reviewers each reviewing a file (or set of files, if small enough) for 30 minutes. The key pieces to the meeting are: A reviewer is assigned (or selects) a piece of code to review ahead of time. The reviewer must choose a file that he or she didn't write. It's the reviewer's job to present the work of someone else. The reviewer reviews the entire file, not just diffs. Diffs are okay for bug fixes but to get a deep understanding of the code, you need the extra context. The goal of the reviewer is to learn as much about the code as possible. The reviewer reviews the code before the workshop. I typically recommend people set aside an hour for this. During that time, the reviewer adds comments directly into the file marked with REVIEW . The meeting begins with everyone closing their laptops except for the reviewer and one other person to take notes. The point is to have everyone pay attention to what's being discussed so we don't waste anyone's time. In the meeting, each reviewer goes over the file with their comments, focusing on the interesting pieces of code. The code is projected so that everyone can follow along. The reviewer cannot mention people's names during the review. The point is to review code, not people. Everyone is considered an owner of the code so there is no need to get defensive. If there are problems in the code, then the group needs to get better as a whole. It's the reviewer's job to encourage discussion through findings in the code. Look for interesting or problematic patterns, opportunities to establish new conventions, or pieces of the code that need more explanation. After the meeting, notes are sent out to all of the participants and relevant documentation is updated (if, for example, new conventions emerge). Code workshops improve code quality I've seen the benefits of this approach firsthand. The most obvious benefit is having a group align their expectations together. Code quality ramps up very quickly when everyone understands what is expected of them and can hold each other responsible for decisions that were made by the group. This is much more effective than sending out a document or wiki page that no one stops to read. Another benefit of this approach is that certain types of bugs become immediately apparent, whereas the same issues can easily be missed by simply reviewing diffs. Looking at the file as a whole means getting a better understanding of what the code is doing (or should be doing). Strange patterns are more easily perceived this way. Perhaps the most important benefit is giving people a forum in which to communicate effectively and safely with one another. It's my firm belief that most code quality problems can be traced back, in one way or another, to poor communication. When communication improves, and expectations are explicit instead of implicit, that's when real change happens. We've been doing code workshops at Box for almost two months and have seen some excellent improvement in how code is being written. Along with the identification of problems that had been missed for a long time, we also have started to formalize conventions that were previously implied, increased understanding of older parts of the code base, and cleaned up a lot of bugs. As a result, we've also added tools to help identify common issues we were seeing (such as adding JSHint for JavaScript). Getting a consensus understanding of what is good and what is bad has made new code less error-prone and more maintainable in the long-run. What's even more important is that engineers don't hate the process. It's an opportunity to do what we love to do, and that's talk about code with each other. Now that we have a way to express ourselves that is non-confrontational and structured with positive outcomes in mind, we're free to continue evolving our discipline and making improvements week over week.",en,98
102,2035,1470916417,CONTENT SHARED,-8992803137960175254,6013226412048763966,716425472356502779,,,,HTML,https://hbr.org/2015/05/4-reasons-managers-should-spend-more-time-on-coaching,4 reasons managers should spend more time on coaching,"There are managers who coach and managers who don't. Leaders in the latter category are not necessarily bad managers, but they are neglecting an effective tool to develop talent. We've been researching managers who coach and what distinguishes them. What has stood out in our interviews with hundreds of managers who do coach their direct reports is their mindset: They believe in the value of coaching, and they think about their role as a manager in a way that makes coaching a natural part of their managerial toolkit. These are not professional coaches. They are line and staff leaders who manage a group of individuals, and they are busy, hard-working people. So why do they so readily give coaching an important place in their schedule? Here are four reasons: They see coaching as an essential tool for achieving business goals . They are not coaching their people because they are nice - they see personal involvement in the development of talent as an essential activity for business success. Most managers will tell you that they don't have the time to coach. However, time isn't a problem if you think coaching is a ""must have"" rather than a ""nice to have."" Whether it's because they are competing for talent, operating in a highly turbulent market place, trying to retain their budding leaders, or aiming to grow their solid players, they believe that they simply have to take the time to coach. There are two assumptions behind this belief. First, that extremely talented people are hard to find and recruit. If you are known as a manager who will help those people thrive, they will gravitate to you. Second, that an organization cannot be successful on the backs of the extremely talented alone. You need solid players just as you need stars, and they will need a manager's help to build skills and deal with the changing realities of their marketplace. They enjoy helping people develop. These managers are not unlike artists who look at material and imagine that something better, more interesting, and more valuable could emerge. They assume that the people who work for them don't necessarily show up ready to do the job, but that they will need to learn and grow to fulfill their role and adapt to changing circumstances. Coaching managers see this as an essential part of their job. They believe that those with the highest potential, who can often contribute the most to a business, will need their help to realize their often-lofty ambitions. As one manager told us recently, ""Isn't helping others to be more successful one of the key roles of a manager?"" The manager must adapt his or her style to the needs and style of each particular individual. This of course takes a good deal of work on the part of the manager, but again, this is perceived as being part of the job, not a special favor. They are curious. Coaching managers ask a lot of questions. They are genuinely interested in finding out more about how things are going, what kinds of problems people are running into, where the gaps and opportunities are, and what needs to be done better. Typically, they don't need to be taught how to ask questions because it's a natural strength. This curiosity facilitates the coaching dialogue, the give-and-take between coach and learner in which the learner freely shares his or her perceptions, doubts, mistakes, and successes so that they together reflect on what's happening. They are interested in establishing connections. As one coaching manager stated, ""That is why someone would listen to me, because they believe that for that time, I really am trying to put myself in their shoes."" This empathy allows the coaching manager to build an understanding of what each employee needs and appropriately adjust his or her style. Some employees might come to coaching with a ""Give it to me straight, I can take it"" attitude. Others need time to think and come to their own conclusions. A trusting, connected relationship helps managers better gauge which approach to take. And coaching managers don't put too much stock in the hierarchy. As a coaching manager recently told us, ""We all have a job to do, we're all important, and we can all be replaced. Ultimately, no one is above anyone else. We just need to work together to see what we can accomplish."" Achieving this mindset is doable. It comes down to whether the business case is sufficiently compelling to motivate a manager to develop a coaching mindset. Managers need to ask themselves a few questions: Does your organization (or group or team) have the talent it needs to compete? If not, why not? Have you done a poor job hiring, or are people not performing up to their potential? It's really either one or the other. If the latter is true, it's your job to help get them to where they need to be. For managers who want to start coaching, one of the first steps is to find someone who is a good coach in your organization and ask her or him to tell you about it. What do they do? Ask why they coach. Listen and learn. Second, understand that before you start coaching, you need to develop a culture of trust and a solid relationship with the people you will be coaching. In spite of your good intentions, all the techniques in the world will make little difference if those you are trying to coach don't feel connected to you in some way. The relationship you develop is more important than the all of the best coaching methods that are available. Third, learn some of the basic principles of managerial coaching that will help you develop your own expertise as a coach. One of the core lessons for managers is that coaching isn't always about telling people the answer. Rather, it is more about having a conversation and asking good, open-ended questions that allow the people you are coaching to reflect on what they are doing and how they can do things differently in the future to improve performance. Finally, the mindset should be focused on the people you are coaching. Always remember the main principle: coaching is about them, not about you.",en,97
103,1014,1463602049,CONTENT SHARED,-4509487968959834430,6013226412048763966,5768100664019329427,,,,HTML,http://hbrbr.com.br/o-valor-de-seu-trabalho-de-coaching-vai-depender-de-suas-habilidades-de-acompanhamento-2/,o valor de seu trabalho de coaching vai depender de suas habilidades de acompanhamento - harvard business review brasil,"Independente do quanto uma sessão de coaching possa parecer bem-sucedida, enquanto em andamento, se ela não levar a uma mudança após ter sido concluída, ele não foi eficaz. Infelizmente é grande o número de gestores que não dão continuidade, desperdiçando o tempo importante que investiram nessa atividade . É possível tornar o processo mais produtivo ao se adotar as seguintes práticas após cada sessão. Utilize essa lista de dicas e perguntas para ajudá-lo a monitorar o progresso de todos de sua equipe, para quem você está atuando como coach. Ela lhe dará subsídios significativos para as reuniões de acompanhamento assim como nos intervalos entre as sessões. Logo após a reunião: Faça anotações. Você não se lembrará de tudo que vê, ouve e pensa sobre o progresso de seus coachees , então anote tudo em um local próprio. Considere também utilizar um template padrão para armazenar essa informação. Boas anotações lhe permitirão fornecer feedback relevante à medida que vai progredindo. Após cada sessão, pergunte-se: O que posso fazer para dar assistência ao desenvolvimento desse coachee no intervalo entre esta sessão de coaching e a próxima? O que aprendi desta sessão que não sabia antes de começar? O que a pessoa para quem estou atuando como coach aprendeu? Quais mensagens-chave foram reforçadas na sessão? De modo contínuo: Se ainda não são, essas tarefas devem se tornar parte da rotina de sua prática de gestão. Reserve um momento em sua agenda para trabalhar nas tarefas que tem maior dificuldade em lembrar ou concluir. Observe sinais de crescimento . A fim de fornecer um feedback relevante, é preciso saber o que está acontecendo. Faça um esforço deliberado para observar os tipos de interações ou tarefas que seus coachees priorizaram em suas sessões de coaching . Se você atuar na empresa, esteja em contato diretamente. Institua uma política explícita de portas abertas que encoraje seus coahees a procurá-lo se tiverem dúvidas. Saber que podem buscar ajuda pode motivá-los a perseverar quando se sentem emperrados. Comunique o impacto . À medida que observa a mudança das pessoas comunique o impacto de seu crescimento de forma explícita. Ouvir diretamente de você aumentará sua motivação (e as deixará satisfeitas). Fique atento a mudanças no relacionamento . Preste atenção à dimensão emocional de suas interações. Se perceber uma mudança que suscite inquietação, intervenha logo. Ainda que não possa fazer nada para ajudar, sua preocupação provavelmente será apreciada. Faça uma auto avaliação . Avalie periodicamente seu próprio desempenho como coach ao se fazer essas duas perguntas: Estou atendendo às necessidades de cada pessoa em minha equipe? Trate o seu papel com uma mentalidade de tentativa e erro e mostre flexibilidade para fazer ajustes no decorrer do percurso. Periodicamente entre em contato direto com todos para quem está atuando como coach para saber o que está funcionando e o que não está - mas confie no seu taco. Estou cumprindo minha obrigação? Coaching é uma via de mão dupla, então seja honesto consigo mesmo em relação a estar ou não atrapalhando o progresso das pessoas ou passando mensagens confusas quanto às suas expectativas. Faça o que puder para tornar todos em sua equipe mais bem-sucedidos. É claro, a tarefa de acompanhamento nunca acaba: oferecer apoio e dar a conhecer e justificar o que se passa são as tarefas perpétuas de um gestor. À medida que se sentir mais à vontade com o coaching , pode ser que não precise mais de uma lista como essa. Mas tenha a lista por perto caso precise se relembrar. Esse artigo foi adaptado a partir da série HBR Guide to Coach da Harvard Business Review.",pt,97
104,2235,1472612022,CONTENT SHARED,9042192299854648021,-1032019229384696495,-7722399387279263781,,,,HTML,https://www.thestreet.com/story/13689830/1/google-s-cloud-platform-is-now-a-force-to-be-reckoned-with.html,google's cloud platform is now a force to be reckoned with,"Though the Google Cloud Platform (GCP) isn't as large and doesn't get as much publicity as Amazon Web Services (AWS) and Microsoft Azure, it's quickly putting its larger rivals on notice with a series of impressive customer wins. And the big investments Alphabet ( GOOGL ) is making in GCP should yield additional big deals in the coming months. CNBC reported this morning Google is ""the front-runner"" to land PayPal ( PYPL ) as a cloud infrastructure client. The online payments giants currently relies heavily on its own infrastructure, but has tapped AWS to provide services for its Braintree and Venmo units. Alphabet and PayPal are holdings in Jim Cramer's Action Alerts PLUS Charitable Trust Portfolio . Want to be alerted before Cramer buys or sells GOOGL or PYPL? Learn more now . The report comes five months after it was learned Apple has begun using GCP to help power iCloud services -- Apple's cloud services also rely on AWS and Azure, as well as the company's own infrastructure. Not long before that, Spotify announced it's migrating its infrastructure to GCP. Other big Google cloud clients include Snapchat, Best Buy ( BBY ) and (thanks to the fact the company is a Google spinoff) Pokémon Go developer Niantic. PayPal's size alone would make it one of Google's most impressive cloud wins -- the company handled $81.5 billion worth of transactions in 2015 (up 23% annually), and claimed 188 million active customer accounts as of the second quarter.",en,96
105,585,1461620499,CONTENT SHARED,8586403905004879205,6013226412048763966,-2657475163512388247,,,,HTML,http://www.cio.com/article/3047154/leadership-management/how-to-augment-your-career-with-leadership-coaching.html,how to augment your career with leadership coaching,"Burnout . Pipeline problems . Harassment . The laundry list of issues for women's leadership in tech is long and the list of solutions seems much shorter. One of the popular pieces of advice for women in tech looking to advance their careers is to find a mentor. Sheryl Sandberg, in her book Lean In , talks about her mentors who encouraged her, challenged her, and advocated for her throughout her career. ARA Mentors , founded by IT recruitment specialist Megan McCann in 2013, has the admirable goal of connecting women in technology to one another through events and one-on-one mentoring sessions. Mentoring is one way to build and maintain community, as well as for women in tech leadership to receive meaningful advice as they advance their careers. However, finding the right fit in a mentor - someone who has the right time, experience, and willingness can be a challenge, even with great programs like ARA in the mix. [ Related: Top 10 U.S. cities for women in technology ] To overcome obstacles for women in tech, women can augment their professional lives and mentoring relationships with one of the most powerful tools for clarifying ambitions and reaching goals: executive coaching. Normally, the context of coaching is organizational; an organization hires a coach for either and at-risk or high-potential employee for development. However, executive coaching may be sought by an individual to increase her leadership skills and gain valuable tools to reach her professional ambitions, and research is starting to show that coaching is highly effective . For more details about executive coaching and its potential impact on women in tech, I interviewed Kelly Ross, who runs a niche coaching and consulting firm, Ross Associates , that focuses on leadership development and talent management. In addition to her professional work, Ross is a certified Hudson Institute Coach , holds an International Coach Federation Professional Certified Coach credential, and teaches at Northwestern University in their graduate coaching program . Corpolongo : Why would a woman in tech want to hire an executive coach? Ross : Hire a coach if you are in a new, bigger role and trying to figure out how you want to show up as a leader; if you are leading a team or organization going through change, especially if you personally find the change a challenge but have to lead others through that same change; if you have received feedback about something that could be professionally limiting and need help sorting out how to take action and change your behavior. Those leaders who are not executives yet, and know they need to build presence and own their stories to make the leap to senior leadership, will find a coach helpful. Corpolongo : Why does executive coaching work? What is your coaching philosophy? Ross : Coaching is about the client and the coach partnering to help the client make the change they desire. The client brings the agenda, [and] the coach manages the process through great questions. That sounds very simple but in my experience great coaching is magical, it helps the client discover answers, make a change that sticks, and understand what has been getting in their way. The client has to want to invest time, energy and money into coaching, it is not going to have an impact if the client isn't clear about what they want to get from the coaching or is not willing to do the work. Many times the breakthrough happens between sessions when something the client and coach discussed ""clicks"" for the client. I customize coaching for each client - some need more process than others. I'll ask you how you want to receive feedback - ""tough love"" or more gently. Many leaders are lonely and appreciate coaching for the thought partnership and the safe space to not have to know every answer immediately. Many of us are our own biggest obstacle and coaching can help you articulate your ""lines in the sand"" and sticking to the boundaries you establish. Coaching is not therapy, which often looks back into the past; coaching is present and future focused. Coaching focuses on understanding patterns and taking action while therapy is about an analysis of the past. Coaching is solution focused. Corpolongo : What do you think are any special considerations for women in technology in their leadership development? Ross : As women, working in technology or elsewhere, we are often hard on ourselves. We don't always invest in ourselves as we might for others. Coaching is personal, it is focused on you being the best version of yourself, and it is one of the best investments in yourself I know of. My own coach is really important to my success as she challenges me, and she supports me. For example, one of my clients, a sales leader at a leading technology organization, wanted to be promoted to senior leadership and needed to increase her presence and own her story. We worked together for a year, meeting every two to three weeks, to articulate her values, build and tell her story, and increase her presence and influence. Alternately, one of my clients is a leader in a tech organization working in product marketing. She used coaching to support her leaving her organization and launching her own business. Initially, we worked together every other week for six months as she envisioned the business she would launch, the skills she had or needed to build, and her plan for making the change from employee to business owner. We are in the midst of our second engagement focusing on the legacy and leadership she wants to leave in her current organization and the ideal life she will create when she launches her business. This article is published as part of the IDG Contributor Network. Want to Join?",en,96
106,2108,1471550601,CONTENT SHARED,-8190931845319543363,-1032019229384696495,-478224908960553328,,,,HTML,https://arc.applause.com/2016/08/17/gartner-hype-cycle-2016-machine-learning/,machine learning is at the very peak of its hype cycle - arc,"Machine learning has earned a prestigious honor in its march to technological relevance ... The peak of its hype cycle. According to the 2016 Gartner Hype Cycle for Emerging Technologies , machine learning is at the very ""peak of inflated expectations,"" the highest point in the S-curve that Gartner awards technologies in its Hype Cycle reports. 2016 has been full of machine learning and artificial intelligence news as companies like Google, Facebook, Microsoft, Intel and IBM announce and release new tools to take advantage of neural networks that can train computers. Many machine learning advocates may feel betrayed to think that machine learning is at the peak of its hype cycle, thinking that analysts like Gartner believe machine learning to be all fluff and no substance. But that is not the case. Many emerging technologies never actually make it to the peak of the hype cycle, fizzling out long before they can make a true impact. In reality, technologies that make it to the peak of the hype cycle are almost ready for universal deployment. Much of the research and development of the technology has already been done and the real implementation (and all the headaches that go with it) is about to begin. That's why technologies fall from the peak of inflated expectations to ""the trough of disillusionment"" as developers go through the painful process of tightening the nuts and bolts to make the technology easier to use for the average person or enterprise. Machine learning will be extremely valuable to enterprises over the next decade thanks to radical computer power and endless data, which will allow companies to adapt quickly to new situations, Gartner said. Immersive experiences such as virtual and augmented reality have already begun a move towards the mainstream, with both consumers and enterprises adopting them within the next five to 10 years. The Hype Cycle Offers A Glimpse Of The Future Companies that invest in immersive experiences, smart machines and ecosystem-enabling platforms are likely to have a significant advantage over their rivals in the very near future. According to Gartner's Hype Cycle, these are the three key technology trends that enterprises and developers need to succeed (or at least be aware of) in the digital economy. The report is the longest running of Gartner's annual hype cycles and is considered to be an accurate barometer as to where technology is heading over the next decade or so. At the same time, the so-called platform revolution will shift the emphasis from a technical infrastructure to a dynamic ecosystem that relies on both internal and external algorithms to drive value, said the report. Companies need to redefine their business strategies to become more dynamic, especially as emerging technologies are redefining how established platforms are used. ""These trends illustrate that the more organizations are able to make technology an integral part of their employees', partners' and customers' experience, the more they will be able to connect their ecosystems to platforms in new and dynamic ways,"" said Gartner research director Mike J. Walker, in a press release. ""Also, as smart machine technologies continue to evolve, they will become part of the human experience and the digital business ecosystem."" All three of the identified trends have been part of the hype cycle for quite some time, so it is not really a surprise to see them featured so prominently in the latest emerging technologies report. Gartner's reports concentrate on five phases of development for a given technology, all of which match expectation with time to mainstream adoption. The phases are a must-know reference point for companies that are engaged in strategic planning. Some technologies are obviously further developed than others, but all are-in theory-capable of giving companies a competitive advantage, said Gartner. The phases are as follows: The Innovation Trigger , where the technology moves from the realms of science fiction to a potential reality. Peak of Inflated Expectations , the period when the technology will be the best thing since sliced bread ... one day. Trough of Disillusionment , which turns excitement into the realization that the innovation is still a long way off. Slope of Enlightenment , or in other words when the innovation is mature enough to provide real-world business uses. Plateau of Productivity , when the technology is actually mature and being used by companies or industries on a regular basis. For example, machine learning currently sits at the top of the Peak of Inflated Expectations, with mainstream adoption expected within two to five years. Advances in neural networks and the ubiquitous availability of big data have made machine learning one of the major concepts in IT, so much so that it has the capacity to transform companies, the report said. See also: Smart Dust May Be The Pinnacle Innovation Of The Internet Of Things And then there is virtual and augmented reality. The technology has been around for decades but has often veered between the Peak of Inflated Expectations and the Trough of Disillusionment. Over the last year, the landscape has changed dramatically, thanks mainly to the fact that virtual reality experiences and hardware have started to flow into the consumer world. The latest hype cycle shows that virtual reality is now firmly ensconced in the Slope of Enlightenment and should reach mainstream adoption in five to 10 years. Augmented reality-which many people consider to be a more viable option-is slowly traversing the trough, with the expectation that this will happen over a similar timeframe. Why Technology Needs To Be Constantly Tracked Other sci-fi technologies from the 2015 version of the hype cycle that were on the upward slope-smart dust, human augmentation, brain-computer interface-are at least ten years from maturity, the report said. Autonomous vehicles , which occupied machine learning's position at the Peak of Inflated Expectations a year ago, have started the slow decline into disillusionment ... irrespective of the current bullish attitude of the auto industry. ""To thrive in the digital economy, enterprise architects must continue to work with their CIOs and business leaders to proactively discover emerging technologies that will enable transformational business models for competitive advantage, maximize value through reduction of operating costs, and overcome legal and regulatory hurdles,"" said Walker. ""This Hype Cycle provides a high-level view of important emerging trends that organizations must track, as well as the specific technologies that must be monitored."" Lead image: "" Machine Learning "" by Flickr user Thomas Hawk , Creative Commons . Stay in the know! Subscribe to ARC and keep up-to-date with a daily or weekly subscription.",en,96
107,2852,1481543955,CONTENT SHARED,-2097075598039554565,3609194402293569455,4506332254516554808,"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.116 Safari/537.36",SP,BR,HTML,http://tutorialzine.com/2016/12/the-languages-frameworks-tools-you-should-learn-in-2017/,"the languages, frameworks and tools you should learn in 2017","The Languages, Frameworks and Tools You Should Learn in 2017 Martin Angelov The software development industry continues its relentless march forward. In 2016 we saw new releases of popular languages, frameworks and tools that give us more power and change the way we work. It is difficult to keep track of everything that is new, so at the end of every year we give you our take on what is important and what you should learn during the next twelve months. The Trends Progressive Web Apps In 2016 we saw the rise of the Progressive Web App concept. It represents web applications that work offline and offer a native, app-like experience. They can be added to your smart device's homescreen and can even send you push notifications, bridging the gap with native mobile apps. We think that in 2017 PWA are going to become even more important and are well worth investigating. See our overview here . The Bot Hype Everybody is talking about bots right now. From platforms for running them, to frameworks for building them, the community is buzzing with activity ( read our intro here ). Bots are the new mobile apps, and if you hurry up you can catch the wave while everyone is excited. Once the novelty wears off, bots will probably be relegated to some boring role such as automated customer support. But hey, we can dream! Consolidation of Frontend Frameworks In the JavaScript community we have an incredible churn of frameworks and tools, with new ones being born almost every week. Until recently, the expectation was that the old tools would just be replaced by the new, but this is not what we saw in 2016. Instead, we saw the popular frameworks exchanging ideas and incorporating the innovations put forth by newcomers. So in 2017 it won't matter much which of the major JS frameworks you choose, their features are mostly comparable. The Cloud Companies and developers everywhere are embracing ""the cloud"". This is virtualized computer infrastructure that is available on demand and fully configurable from a control panel. The big three cloud providers are AWS, Google Cloud and Azure. Thanks to their ongoing competition prices have been falling, bringing it within the budgets of smaller companies and individual developers. Familiarizing yourself with the cloud workflow would be a good investment for 2017. Machine Learning Machine Learning (ML) has exploded in popularity during the last twelve months. And with the historic AlphaGo vs Lee Sedol match in March, it entered the mainstream. Smart computer systems that learn from raw data are revolutionizing the way we interact with our mobile devices. By the looks of it, ML will be an even bigger factor in 2017. Languages JavaScript continues its incredible pace of innovation. Catalyzed by the quick release schedules of web browsers, the JS standard is updated every year. The next edition, ES2017 , is expected to be finalized in mid 2017. It will bring the dream feature of many JS developers - аsync/аwait for working with asynchronous functions. And thanks to Babel , you can write ES2017 in every browser even today. TypeScript 2.1 was released in late 2016, bringing async/await for old browsers and improved type inference. TypeScript is a statically typed language which compiles to JavaScript. It adds powerful features like a classic OOP model and optional static typing to make large codebases easier to maintain. It is the preferred language for writing Angular 2 apps, and we recommend giving it a try. Here is our quick start guide about it. C# 7.0 is expected in 2017 and will enhance an already excellent language. Microsoft surprised everyone when they introduced the open source Visual Studio Code editor and .Net Core. Both of these run on Linux, Windows and macOS and allow you to write fast and performant applications in C# (read more here ). A vibrant community is forming around both of these tools, and we are confident there is an exciting year ahead of them. Ruby 2.3 was released earlier this year with a number of performance improvements. Ruby is also a good choice as a general purpose scripting language, but it shines when paired with Rails. The Ruby 3×3 initiative was announced, which will attempt to make the upcoming Ruby 3 release 3 times faster that the current version, opening the doors to using Ruby in more contexts. If you are looking for something more exciting, you can try out Crystal and Elixir , which both combine a friendly ruby-like syntax with superior performance. Or you can look into a functional language like Haskell or Clojure . Two other fast languages are Rust and Go which we recommend. Learn one or more of these: JS (ES2017), TypeScript, C#, Python, Ruby, PHP7, Java/Kotlin/Scala. Frontend The web platform made two major advancements recently - Web Assembly and Service Workers . They open the gates for fast and performant web applications that bridge the gap with native compiled applications. Service Workers in particular are the enabling technology for Progressive Web Apps and bring support for Notifications to the web platform, with more APIs to follow in the future. Angular.js 2 was released this year. The framework is backed by Google and is very popular with enterprises and large companies. It has a vast number of features that make writing everything from web to desktop and mobile apps possible. The framework is written in TypeScript, which is also the recommended language to write applications in. There is a lot to read about, but we think learning Angular 2 in 2017 would be a good investment. Ember is another solid choice for a JavaScript framework. It supports data bindings, auto-updating templates, components and server-side rendering. One benefit that it has over its competitors, is that it is more mature and stable. Breaking changes are much less frequent and the community values backwards compatibility. This makes the framework a good choice for long-lived applications. Two other frameworks that are worth a look are Aurelia and React . The ecosystem around React has grown considerably more complicated in the last year, making it difficult to recommend for beginners. But experienced devs can combine the library with GraphQL , Relay , Flux and Immutable.js into a comprehensive full stack solution. No frontend compilation would be complete without mentioning Bootstrap . Version 4 is currently in Alpha and a release is expected in 2017. Notable changes are the new versatile card component and the flexbox grid (see our comparison with the regular grid here ), which modernize the framework and make it a joy to work with. SASS and LESS remain the two most popular CSS preprocessors today. Although vanilla CSS is finally getting support for variables, SASS and LESS are still superior with their support for mixins, functions and code organization. If you haven't already, take a look at our SASS and LESS quick start guides. Learn one or more of these: Angular 2, Vue.js, Ember, Bootstrap, LESS/SASS. Backend There is plenty of choice for the backend, all coming down to your preference of a programming language or specific performance needs. An ongoing trend in web development is business logic to move away from the backend, turning that layer into an API which is consumed by the frontend and mobile apps. But a full stack framework is often simpler and faster to develop in, and is still a valid choice for a lot of web apps. Node.js is the primary way for running JS outside the browser. It saw many new releases this year which increased performance and added coverage for the entire ES6 standard. Node has frameworks for building fast APIs, servers, desktop apps and even robots, and a vast community creating every kind of module imaginable. Some frameworks that you may like to look into: Express , Koa , Next , Nodal . PHP is a web language first and foremost, and has a large number of web frameworks to choose from. Thanks to its excellent documentation and futures, Laravel has formed an active community around it. Zend Framework released version 3 which marks a great upgrade for this business oriented framework. Symfony also saw a lot of new releases this year, making it an even better choice as a full stack solution. Python has its own full stack/minimal framework combo in the form of Django and Flask . Django 1.10 was released in August introducing full text search for Postgres and an overhauled middleware layer. For the enthusiasts there is also Phoenix , which is written in Elixir and attempts to be a feature complete alternative to Rails with superior performance. If Elixir is one of the languages you would like to learn in 2017, give Phoenix a try. Learn one of these: A full stack backend framework, a micro framework. Databases MySQL 8.0 is going to be the next major release of the database. It is expected sometime in 2017 and it will bring a lot of improvements to the system. MySQL is still the most popular database management system and the entire industry benefits from these new releases. PostgreSQL 9.6 was released in September. It brought better full text search and sped up the database system with parallel queries and more efficient replication, aggregation, indexing and sorting. Postgres is used for massive, terabyte scale datasets, as well as for busy web apps, and these optimizations are welcome. For NoSQL fans, we can recommend CouchDB . It is a fast and scalable JSON storage system which exposes a REST-ful HTTP API. The database is easy to use and offers great performance. PouchDB is a spiritual counterpart to CouchDB that works entirely in the browser and can sync with Couch. This allows you to use Pouch in an offline ready web app, and get automatic syncing once internet connectivity is available. Redis is our favorite key value store. It is small, fast and versatile. You can use it as a smart memcache alternative, as a NoSQL data store or a process messaging and synchronization channel. It offers a large number of data structures to choose from, and the upcoming 4.0 release will have a module system and improved replication. Learn one of these: MySQL, Postgres, CouchDB, Redis. Tools Yarn is an alternative package manager for Node.js which is developed by Facebook. It is an upgrade over the npm command line tool and provides faster installs, better security and deterministic builds. It still uses the npm package registry as its backend, so you have access to the same incredible ecosystem of JavaScript modules. Yarn is compatible with the package.json format that npm uses, and is just a quick install away. The two most popular open source code editors - Visual Studio Code and Atom have seen an incredible amount of innovation in the past 12 months. Both of these projects are built using web technologies and have attracted huge communities of fans. The editors have plugins available which bring syntax checking, linting and refactoring tools for a large number of languages. Git is the most popular source code version control system out there. It is serverless and you can turn any folder on your computer into a repository. If you wish to share code, you have many options like GitLab , Bitbucket and Github , to name a few. For 2017 we suggest that you familiarize yourself with the git command line , as it will come in handy more times than you think. Desktop applications are not dead yet. Even though web apps are becoming more and more capable, sometimes you need powerful capabilities and APIs that are simply not available to the web platform. With tools like Electron and NW.js you can write desktop applications by using web technologies. You get full access to the operating system and the breadth of modules available to npm. To learn more about these tools, read our tutorials about Electron and NW.js . A recent trend in software team organization is to have developers who are in charge of their own software deployment. Also called DevOps, this leads to quicker releases and faster fixes of issues in production. Developers with operations experience are highly valued by companies, so familiarity with the technologies that enable it is going to be a huge plus from now on. Some of the tools that we recommend are Ansible and Docker . Experience with the Linux command line and basic system administration skills will also serve you well. Try out one or more of these: Yarn, Git, Visual Studio Code, Electron, Ansible, Docker. Tech The cloud has won over the entire software industry, with large companies closing down their datacenters and moving their entire infrastructure there. The three main platforms are AWS , Google Cloud and Azure . All three have powerful, ever expanding feature sets, including virtual machines, hosted databases, machine learning services and more. Prices are going down rapidly, and the cloud is within reach of small companies and individual developers. For 2017, it would be a good learning experience to deploy a side project to one of these providers. Artificial Intelligence was the buzzword of 2016. Speech recognition and image classification are only two of the user facing applications of the technology, with machines reaching and even surpassing human level performance. There are a lot of startups that apply AI and Machine Learning to new domains. And a lot of open source projects were released like Google's Tensor Flow and Microsoft's Cognitive Toolkit . Machine Learning is a very math-heavy topic, and for those just starting out there are comprehensive online courses available. Virtual Reality (VR) and Augmented Reality (AR) have been around for a while, but finally the technology is mature enough to offer a compelling experience. Facebook ( Oculus Rift ), Google ( Daydream ) and Microsoft ( Windows Holographic ) all have virtual reality platforms that welcome third party developers. VR headsets still face challenges like eliminating nausea and offering compelling use cases outside of gaming, but they are getting there. Learn one of these: Cloud deployment, a Machine Learning library, VR Development. by Martin Angelov Martin is a web developer with an eye for design from Bulgaria. He founded Tutorialzine in 2009 and it still is his favorite side project.",en,95
108,2007,1470659231,CONTENT SHARED,-78066964941874046,7645894863578715801,55662953596428738,,,,HTML,http://testdetective.com/microservices-testing/,microservices testing,"Modern software engineering is all about scalability, product delivery time and cross-platform abilities. These are not just fancy terms - since internet boom, smartphones and all this post-pc era in general, software development turns from monolith, centralised systems into multi-platform applications, with necessity of adjust to constantly changing market requirements. Microservices quickly becames new architecture standard, providing implementation independence and short delivery cycles. New architecture style creates also new challenges in testing and quality assurance. In this article I would like to outline strategies for testing microservices architecture. Most important thing is to understand difference between specific testing layers and their complementarity. Why microservices? What's the architecture of typical, corporate IT system in sectors like financial services, insurances or banking? Typically it's monolith-like architecture, based on single, relational database and SOAP API. One system is responsible for many domains and contexts. Those kind of systems have been working on production for years, and big companies are not willing to risk and change their proven-in-battle architecture. Although, the market requirements are constantly changing, so are technical requirements. Monolith architecture falls short when it comes to scalability or continuous deployment, not to mention technology stack commitment or single point of failure. Here comes microservices ! Are they a silver bullet for all your architectural problems? Well, no - they also have their own issues. What sets them apart though, is ability to quickly adjust your architecture to constantly changing requirements through independent development, deployment and scaling. In simple words, microservices are small, independent applications with single-domain responsibility. Those services communicate between each other through HTTP, usually with use of REST protocol. Microservice architecture is not just a new buzz word. World leading tech companies, like Amazon, Netflix or Twitter base their key system architecture on microservices. Even more significant is that not only new systems are build with use of microservices, but companies also rewrite their old, proven solutions into new architecture. Testing strategies New approach to architecture requires new approach to testing and quality assurance. Focusing mainly on automated testing, we can divide specific layers of tests. Important thing to notice here is that working with single layer is insufficient, and due to their complementarity, you should provide all of them to your project. unit tests - those are nothing new, and nowadays almost everyone understands advantages of TDD approach. In general, unit tests are automatic checks that provide us a feedback whether our implementation complies with the requirements. Unit tests check only system components in the smallest pieces, like single methods, with other dependencies mocked. Unit tests are great when it comes to fast, nearly continuous feedback of our implementation correctness in smallest granulation - methods or single system components. integration tests - as we said, unit tests check single components in isolation. When method A is dependent on method B , we would mock method B in unit test of method A , since we want our test to be deterministic and single-responsible. Although it's desirable at this level, we can have a situation where two system components, working correct in isolation, don't meet functional requirements when integrated together. There we introduce integration tests . Those are ones that check a number of system components or methods working together. For example, if we want to test an endpoint at integration level, we'd send a request, see if service layer done its job and validate a response. Remember that we are in microservices world and our system functionalities depends on few applications communication. So, if the endpoint that we're testing triggers client call to external service, should we let our tests be dependent of application owned by someone else? No - at the level of integration tests, we want to test only one application's components, with external services mocked out. contract testing - microservices architecture depends on communication between services. Although internal implementation of services is independent, interface and API must remain consistent. When we design our service and expose it to the world, we define a contract for our API. Let's imagine hypothetical situation: we're building a service for users registration. Before we store new user in our database, we check if user's ID number does't appear on fraud list. We integrate for this with external service, that we do not owned. Now, if someone - intentionally or not - breaks the contract in validation service, our registration functionality stops working correctly, although we haven't done any changes. Our integration tests where green obviously, since they work on external validation service mocks. Contract tests are automatic checks to assure, that the contract we've agreed on is still preserved. Idea is simple, but how to do that in practice? We depend here largely on the tool that we're using. Worth to mention are tools like Pact or Spring-Cloud Contract - they provide DSL for mocking service on the client side, and interaction playback and verification on server side. end-to-end tests - also known as functional tests, e2e tests are known and used probably even longer than unit tests. In world of microservices, they are much more complex subject though, since our functionalities are based on integration between many application and services. We do not want to mock anything at this level, so our tests are dependent on the state of specific applications. Furthermore, in distributed architecture we change system calls, known from monolithic applications, to network calls. This comes with all the network issues, like timeouts or packets loss. Last but not least, debugging tests with lots of dependencies to external services can be challenging. Nevertheless, end-to-end tests are crucial for our projects quality. To minimize our e2e testing efforts in microservices architecture, we should follow well know, yet rarely kept rules: First of all, functional tests are ones that gives you feedback of key functionalities from your user perspective. That kind of tests tend to be difficult to maintain and have long execution time. Therefore, the number of them should be limited. If some functionality can be tested on lower level - in unit or integration tests - it should be moved there. Keeping user perspective doesn't mean you should build your e2e framework with WebDriver-based libraries. Selenium has many advantages, but it falls short in terms of maintenance cost or execution time. Since you do microservices, majority of user actions can be simulated with service requests, which are not only faster but also more stable. Your test environments should be defined as a code. We don't want to be in situation where test results are dependent to test data or environment state. If you run your functional test suite once a day, good practice would be to build environment from scratch before test execution starts. Obviously, that should be done automatically, with use of tools like , or . Going beyond Test layers I've mentioned, are just core aspects of automated testing. Having vast of our test scope automated, leaves us with time to perform complex manual testing. You can do either exploration or context-driven testing. Another key aspect of quality assurance in microservices are performance tests. Your system architecture depends on distributed network calls, so you should put a great attention to performance of your application. There is a common question, whether you should load-test single endpoints in isolation, or whole functionalities, simulating chain of network calls. In my experience, both of approaches are important. Testing single endpoints gives you knowledge of some technical details of your implementation and lets you to profile service in isolation. Testing chain of service calls draws your attention to performance-bottlenecks of your system and lets you know where scalability effort should be put. Summary Testing in microservices architecture can be more challenging than in traditional, monolithic architecture. In combination with continuous integration and deployment, it's even more complex. It's important to understand layers of tests and how they differ from each other. Putting effort on automation aspect of your tests, doesn't mean you shouldn't drop on manual testing. Only combination of various test approaches gives you confidence in product quality.",en,95
109,1945,1470051678,CONTENT SHARED,2708089973817733462,7645894863578715801,8873526859008375851,,,,HTML,https://blog.frankel.ch/dont-talk-about-refactoring-club/,you don't talk about refactoring club,"The first rule of Fight Club is: You do not talk about Fight Club. The second rule of Fight Club is: You do not talk about Fight Club. I guess the same could be said about refactoring. That would first requires to define what I mean by refactoring in the context of this post : Refactoring is any action on the codebase that improves quality. Which in turn requires to define what is quality. Everyone I've talked with agrees on this: it's quite hard to do. Let's settle for now for the following tentative explanation: Quality is a feature of the codebase (including and not limited to architecture, design, etc.) which the lack of stalls further meaningful changes in the codebase. At the limits: 100% quality means changing the codebase to develop a new feature would require the smallest possible time; 0% quality means the time to do it would be infinite. Given this definition, refactoring includes: Now back to the subject of this post. Should we ask the customer/manager if a refactoring is necessary? Should we put a refactoring sprint in the backlog? I've witnessed first hand many cases where it was asked. As expected, in nearly all cases, the decision was not to perform the refactoring. Taking ages to implement some feature? No design change. Not enough test harness? No tests added. Why? Because the customer/manager has no clue what refactoring and quality means. Let's use a simple analogy: when I take my car to the mechanic, do I get to choose whether he'll check if the repairs have been correctly executed? Not at all. Checks are part of the overall package I get when I choose a professional mechanic. If choice was made possible, some people who probably opt not to do the checks - to pay less. So far, so good. But then if trouble happened, and probability is in favor of that, the mechanic would be in deep trouble. Because he's the professional and didn't do his job well. Developers would also get into trouble if they delivered applications with no tests or with a messy codebase; not their customer nor their manager - especially not their managing (depending on their kind of manager if you catch my drift). So I wonder why developers have to let people that don't know about code taking such important decisions. As a professional developer, you and no one else are responsible for the quality of the application you deliver. Your name is in the source code and the commit history, not your manager's. Stop searching for excuses not to refactor: don't ask, do it. Refactoring is part of the software development package, period. Please enable JavaScript to view the",en,95
110,672,1461893850,CONTENT SHARED,2280365999288629014,3891637997717104548,-7920053441420858215,,,,HTML,https://www.linux.com/news/best-linux-distros-2016,the best linux distros of 2016,"2015 was a very important year for Linux, both in the enterprise as well as in the consumer space. As a Linux user since 2005, I can see that the operating system has come a long way in the past 10 years. And, 2016 is going to be even more exciting . In this article, I have picked some of the best distros that will shine in 2016. Best Comeback Distro: openSUSE SUSE, the company behind openSUSE, is the oldest Linux company; it was formed just a year after Linus Torvalds announced Linux. The company actually predates Linux king Red Hat. SUSE is also the sponsor of the community-based distro openSUSE . In 2015, openSUSE teams decided to come closer to SUSE Linux Enterprise (SLE) so that users could have a distribution that shares its DNA with the enterprise server -- similar to CentOS and Ubuntu. Thus, openSUSE became openSUSE Leap , a distribution that's directly based on SLE SP (service pack) 1. The two distros will share the code base to benefit each other -- SUSE will take what's good in openSUSE and vice versa. With this move, openSUSE is also ditching the regular release cycle, and a new version will be released in sync with SLE. That means each version will have a much longer life cycle. As a result of this move, openSUSE has become a very important distribution because potential SLE users can now use openSUSE Leap. That's not all, however; openSUSE also announced the release of Tumbleweed, a pure rolling-release version. So, now, users can use either the super-stable openSUSE Leap or the always up-to-date openSUSE Tumbleweed. No other distro has made such an impressive comeback in my memory. Most Customizable Distro: Arch Linux Arch Linux is the best rolling-release distribution out there. Period. Ok, I could be biased because I am an Arch Linux user. However, the reason behind my claim is that Arch excels in many other areas, too, and that's why I use it as my main operating system. Arch Linux is a great distro for those who want to learn everything about Linux. Because you have to install everything manually, you learn all the bits and pieces of a Linux-based operating system. Arch is the most customizable distribution. There is no ""Arch"" flavor of any DE. All you get is a foundation and you can build whatever distro want, on top of it. For good or for worse, unlike openSUSE or Ubuntu there is no extra patching or integration. You get what upstream developers created. Period. Arch Linux is also one of the best rolling releases. It's always updated. Users always run the latest packages, and they can also run pre-released software through unstable repositories. Arch is also known for having excellent documentation. Arch Wiki is my to-go resource for everything Linux related. What I like the most about Arch is that is offers almost every package and software that's available for ""any"" Linux distribution, thanks to the Arch User Repository, aka AUR. Best-Looking Distro: elementary OS Different Linux distributions have different focus areas -- in most cases, these are technical differences. In many Linux distributions. the look and feel is an afterthought -- a side project at the mercy of the specific desktop environment. elementary OS is trying to change all that. Here, design is at the forefront, and the reason is quite obvious. The distro is being developed by designers who have made their name in the Linux world by creating beautiful icons. elementary OS is quite strict about the holistic look and feel. The developers have created their own components, including the desktop environment. Additionally, they choose only those applications that fit into the design paradigm. One can find heavy influence of Mac OS X on elementary OS. Best Newcomer: Solus Solus operating system has garnered quite a lot of attention lately. It's a decent-looking operating system that has been created from scratch. It's not a derivative of Debian or Ubuntu. It comes with the Budgie desktop environment, which was built from scratch but aims to integrate with Gnome. Solus has the same minimalistic approach as Google's Chrome OS. I have not played with Solus much, but it does look promising. Solus is actually not a ""new"" OS. It has been around for a while in different forms and names. But the entire project was revived back in 2015 under this new name. Best Cloud OS: Chrome OS Chrome OS may not be your typical Linux-based distribution because it's a browser-based operating system for online activities. However, because it's based on Linux and its source code is available for anyone to compile, it's an attractive OS. I use Chrome OS on a daily basis. It's an excellent, maintenance-free, always updated OS for anyone using a computer purely for web-related activities. Chrome OS, along with Android, deserves all the credit for making Linux popular in the PC and mobile space. Best Laptop OS: Ubuntu MATE Most laptops don't have very high-end hardware, and if you are running a really resource-intensive desktop environment then you won't have much system resources or battery life at your disposal -- they will be used by the OS itself. That's where I found Ubuntu MATE to be an excellent operating system. It's lightweight, yet has all the bells and whistles needed for a pleasant experience. Thanks to its lightweight design, the majority of system resources are free for applications so you can still do some heavy work on it. I also found it to be a great distro on really low-end systems. Best Distro for Old Hardware: Lubuntu If you have an old laptop or PC sitting around, breathe new life into it with Lubuntu . Lubuntu uses LXDE, but the project has merged with Razor Qt to create LXQt. Although the latest release 15.04 is still using LXDE, the future versions will be using LXQt. Lubuntu is a decent operating system for old hardware. Best Distro for IoT: Snappy Ubuntu Core Snappy Ubuntu Core is the best Linux-based operating system out there for Internet of Things (IoT) and other such devices. The operating system holds great potential to turn almost everything around us into smart devices -- such as routers, coffeemakers, drones, etc. What makes it even more interesting is the way the software manages updates and offers containerization for added security. Best Distro for Desktops: Linux Mint Cinnamon Linux Mint Cinnamon is the best operating system for desktops and powerful laptops. I will go as far as calling it the Mac OS X of the Linux world. Honestly, I had not been a huge fan of Linux Mint for a long time because of unstable Cinnamon. But, as soon as the developers chose to use LTS as the base, the distro has become incredibly stable. Because the developers don't have to spend much time worrying about keeping up with Ubuntu, they are now investing all of their time in making Cinnamon better. Best Distro for Games: Steam OS Gaming has been a weakness of desktop Linux. Many users dual-boot with Windows just to be able to play games. Valve Software is trying to change that. Valve is a game distributor that offers a client to run games on different platforms. And, Valve has now created their open operating system -- Steam OS -- to create a Linux-based gaming platform. By the end of 2015, partners started shipping Steam machines to the market. Best Distro for Privacy: Tails In this age of mass surveillance and tracking by marketers (anonymous tracking for targeted content is acceptable), privacy has become a major issue. If you are someone who needs to keep the government and marketing agencies out of your business, you need an operating system that's created -- from the ground up -- with privacy in mind. And, nothing beats Tails for this purpose. It's a Debian-based distribution that offers privacy and anonymity by design. Tails is so good that, according to reports, the NSA considers it a major threat to their mission. Best Distro for Multimedia Production: Ubuntu Studio Multimedia production is one of the major weaknesses of Linux-based operating systems. All the professional-grade applications are available for either Windows or Mac OS X. There is no dearth of decent audio/video production software for Linux, but a multimedia production system needs more than just decent applications. It should use a lightweight desktop environment so that precious system resources -- such as CPU and RAM -- are used sparingly by the system itself, leaving them for the multimedia applications. And, the best Linux distribution for multimedia production is Ubuntu Studio . It uses Xfce and comes with a broad range of audio, video, and image editing applications. Best Enterprise Distro: SLE/RHEL Enterprise customers don't look for articles like these to choose a distribution to run on their servers. They already know where to go: It's either Red Hat Enterprise Linux or SUSE Linux Enterprise . These two names have become synonymous with enterprise servers. These companies are also pushing boundaries by innovating in this changing landscape where everything is containerized and becoming software defined. Best Server OS: Debian/CentOS If you are looking at running a server, but you can't afford or don't want to pay a subscription fee for RHEL or SLE, then there is nothing better than Debian or CentOS . These distributions are the gold standard when it comes to community-based servers. And, they are supported for a very long time, so you won't have to worry about upgrading your system so often. Best Mobile OS: Plasma Mobile Although the Linux-based distribution Android is ruling the roost, many in the open source community, including me, still desire a distribution that offers traditional Linux desktop apps on mobile devices. At the same time, it's better if the distro is run by a community instead of a company so that a user remains in the focus and not the company's financial goals. And that's where KDE's Plasma Mobile brings some hope. This Kubuntu-based distribution was launched in 2015. Because the KDE community is known for their adherence to standards and developing stuff in public, I am quite excited about the future of Plasma Mobile. Best Distro for ARM Devices: Arch Linux ARM With the success of Android, we are now surrounded by ARM-powered devices -- from Raspberry Pi to Chromebook and Nvidia Shield. The traditional distros written for Intel/AMD processors won't run on these systems. Some distributions are aimed at ARM, but they are mostly for specific hardware only, such as Raspbian for Raspberry Pi. That's where Arch Linux ARM (ALARM) shines. It's a purely community-based distribution that's based on Arch Linux. You can run it on Raspberry Pi, Chromebooks, Android devices, Nvidia Shield, and what not. What makes this distribution even more interesting is that, thanks to the Arch User Repository (AUR), you can install many applications than you may not get on other distributions. Conclusion I was astonished and amazed when I worked on this story. It's very exciting to see that there is something for everyone in the Linux world. It doesn't matter if the year of the desktop Linux never arrives. We are happy with our Linux moments!",en,94
111,1435,1466187150,CONTENT SHARED,-447851796385928420,1895326251577378793,-4339659529018274221,,,,HTML,http://www.mobiletime.com.br/17/06/2016/flyhelo-app-permite-compartilhamento-de-jatos-e-helicopteros-em-sao-paulo/442262/news.aspx,mobile time - flyhelo: app permite compartilhamento de jatos e helicópteros em são paulo,"FlyHelo: app permite compartilhamento de jatos e helicópteros em São Paulo O conceito de economia compartilhada chegou à classe alta. Nasceu este ano em São Paulo a FlyHelo, empresa que propõe o compartilhamento de voos de helicóptero e de jato, conceito conhecido como ""coflying"", e que já faz sucesso na Europa e nos EUA. Através de um app ( Android , iOS ), o passageiro pode criar um voo e disponibilizar assentos para que outras pessoas dividam o custo da viagem com ele, ou comprar um assento em um voo já planejado. O pagamento é feito in-app, via cartão de crédito. ""Nosso público-alvo são executivos de 25 a 55 anos que trabalham muito e para quem tempo é dinheiro. São pessoas que não querem gastar cinco ou seis horas para chegar em Ilhabela para passar o fim de semana. De helicóptero demora apenas 30 minutos. E com a venda de assentos fica muito mais atrativo. Queremos conquistar um público desse segmento que nunca voou de helicóptero ou jato"", explica o CEO da FlyHelo, Hadrien Royal. O executivo relata que a maior parte da demanda se concentra nos finais de semana. Os passageiros costumam viajar na sexta no fim do dia ou sábado de manhã, e retornam no domingo à tarde ou segunda de manhã. Os destinos mais comuns partindo de São Paulo são Juqueí, Ilhabela, Angra dos Reis e Campos do Jordão. A criação de um voo pode ser feita pelo app com até duas horas de antecedência. ""Para trechos de curta distância ou para onde não há aeroporto, usamos helicópteros. Quando for acima de 400 Km, recomendamos jatos"", diz Royal. A FlyHelo não tem uma frota própria, mas parcerias com empresas de táxi aéreo com as quais provê opções de voos a partir de Congonhas e do Campo de Marte, no caso dos jatos, ou de vários pontos da capital paulista, no caso de helicópteros. O serviço começa agora a ser testado também no Rio de Janeiro, onde vai operar a partir do Santos Dumont e do aeroporto de Jacarepaguá. A FlyHelo trabalha apenas com empresas homologadas pela Anac, informa o executivo. A criação de um voo São Paulo-Juqueí custa R$ 4.990. E um assento para esse mesmo trecho, R$ 1.490. Dependendo do modelo, o helicóptero pode ter de três a seis lugares. Os jatos, de quatro a sete assentos. O serviço está disponível há um mês. Mais de 50 voos já foram realizados. A meta é chegar a mais de 500 voos mensais após 12 meses de operação.",pt,94
112,1231,1464897148,CONTENT SHARED,7414483722019578252,7645894863578715801,5662638437112457499,,,,HTML,http://greglturnquist.com/2016/05/good-developers-take-breaks.html,good developers take breaks * greetings programs,"Something that has become crystal clear since I joined the Spring team is how important it is to take a break. Good code happens when developers take breaks. Of course there are times when I find myself working solid until 8:00 pm or later. But I try to make that the exception rather than the norm. Most of the time, I'm pulled away by my kids coming home from school. In fact, I'm often annoyed at stopping a minute early. And yet I'm astonished at how many perplexing problems I've solved by simply stopping, doing something else , and **BAM**, the answer pops into my head. Sometimes within minutes, sometimes the next morning while putting shoes on one of my kids. Those are the moments when I remind myself that developers take breaks. I consider that payoff for having stopped early. This phenomena is well know. In fact, you've probably heard of it. I also so it addressed keenly at a writer's conference last year . Your subconscious is still working on the problem whether you are or not. Sometimes, when you aren't actively trying to fix it, your noodle is freed up to look back at what you did, what you've read, and other things. While chatting with a teammate of mine at DevNexus , he expressed that if he hadn't taken various breaks, gone for walks and thought about the architecture he was designing, the project he was striving to build would never have happened. Reflection is a critical component. My martial instructor often taught ""visualize, visualize, visualize."" It's a mechanism to put your mind on the subject, even when you're not actively pursuing it. The key thing is telling yourself to take that break. To pause. To stop and not burn the candle at both ends. If you work that hard, your subconscious will be on the ropes. Those extra ideas that can often succeed when you may have failed all day, may not come at all. It's a big leap I know, but give it a try.",en,94
113,1423,1466112806,CONTENT SHARED,-9019582414165805420,-1799631734242668035,-3130580864866225113,,,,HTML,http://exame.abril.com.br/tecnologia/noticias/bradesco-e-visa-anunciam-pulseira-que-substitui-cartao,bradesco e visa anunciam pulseira que substitui cartão | exame.com,"São Paulo - Você não vai mais precisar do seu cartão bancário para realizar pagamentos no débito no futuro. Ao menos, é isso que a Visa e o Bradesco querem e, por isso, anunciam hoje uma pulseira de pagamentos que funciona em diversos terminais comuns em pontos de vendas. O aparelho é chamado Pulseira Bradesco Visa e realiza a transferência monetária via NFC, uma tecnologia de comunicação por proximidade que troca dados criptografados com a maquininha de débito. O novo gadget de pagamentos será testado no Brasil durante as Olimpíadas do Rio. No país, mais de um milhão e meio de pontos de vendas têm suporte ao NFC, incluindo todos os 4 mil terminais nos Parques Olímpicos. ""O cartão vai migrar para uma série de outros fatores, para o e-commerce, o mobile commerce, o contactless e para os wearables"", afirmou Percival Jatobá, vice-presidente de produtos da Visa. O processo de transferência com a pulseira em nada se difere de um realizado com um cartão de plástico com chip, segundo as empresas. Na hora de pagar, o consumidor encosta o gadget na maquininha e digita sua senha, caso sua compra exceda o valor de 50 reais. Em compras de menos de 50 reais, não é preciso informar a senha do seu cartão. A pulseira é uma solução útil para quem quer sair para correr e não quer levar o smartphone, cartões ou dinheiro. Maleável e com encaixe de dois pontos, o acessório tem design confortável para o uso diário, apesar de apenas uma cor, a azul, estar disponível nessa fase inicial. A recarga será feita por meio de um aplicativo para smartphones Android e iOS, que é chamado Pulseira Bradesco Visa. É possível transferir o dinheiro via boleto ou qualquer cartão bancário. O consumidor tem um número de ID e cartão viculados à sua conta na pulseira, mas não é preciso ter conta no banco Bradesco para usar o acessório. Neste primeiro momento, cerca de 3 mil pessoas selecionadas irão testar a Pulseira Bradesco Visa. O intuito é que elas ofereçam informações de usabilidade que serão analisadas para um lançamento mais amplo do produto. A pulseira de pagamentos ainda não tem previsão de ser oferecida a todos. Confira abaixo uma breve demosntração de um pagamento realizado com a Pulseira Bradesco Visa.",pt,92
114,2310,1473628002,CONTENT SHARED,-7518373517401484139,-4465926797008424436,-4331427112707643376,,,,HTML,https://m.signalvnoise.com/my-favorite-people-and-resources-to-learn-android-programming-from-293f249e2b4e?gi=58d339366f0d,my favorite people and resources to learn android programming from,"���� Twitter I've really enjoyed following these Android community members on Twitter. These folks aren't just knowledgeable teachers and key open-source contributors. They're also positive-minded, hopeful, and friendly. Those qualities are just as important to me as being an expert in the area. Chiu-Ki Chan  - A devoted learner and teacher, Chiu-Ki does it all. She interviews folks , runs 360|AnDev , teaches on Caster , speaks , draws , writes , and probably does 100 other things I don't know about. �� Donn Felker   - Not only an Android GDE, Donn's got a great blog full of helpful posts. He's also half of the Fragmented Podcast along with Kaushik Gopal (who's pretty sharp in his own right). And if that weren't enough, Donn's also the head honcho at Caster.io , a fantastic site for video tutorials. Jake Wharton   - Honestly, if you don't know who Jake is, you might be in the wrong place. Just go here now. �� Kristin Marsicano   -   An instructor at Big Nerd Ranch, Kristin has a wonderful down-to-earth vibe and is clearly a great teacher. Her recent talk at 360|AnDev on the activity lifecycle is a great refresher for something you probably don't think about enough. Ryan Harter   - Ryan's a GDE who's been teaching a lot lately about how to reduce boilerplate code . He also helps run GDG Chicago West and is an instructor at Caster . The Practical Dev -OK, this isn't technically Android specific. But it's such an informative and entertaining commentary on programming, I had to include it. Sometimes reading general programming posts can be really enlightening (and hilarious). (Note: It'd be impossible to write about every single person who's a great Android teacher, but you can find more on this extended Twitter list that I'll keep adding to.)",en,92
115,1532,1466955705,CONTENT SHARED,-5625593730080264433,-1032019229384696495,-2318276786697086259,,,,HTML,https://techcrunch.com/2016/06/19/the-next-wave-in-software-is-open-adoption-software/,the next wave in software is open adoption software,"There's a big shift happening in how enterprises buy and deploy software. In the last few years, open technology - software that is open to change and free to adopt - has gone from the exception to the rule for most enterprises. We've seen the IT stack redrawn atop powerful open-source projects, with developers opting for an ""open-first"" approach to building solutions. More than 78 percent of enterprises run on open source and fewer than 3 percent indicate they don't rely on open software in any way, according to a recent market survey by Black Duck Software. Openness is a near truism in the Valley, but today projects like Hadoop, Cassandra, Docker and Mule are infiltrating even the most conservative and dogmatic organizations. As such, startups like Cloudera , DataStax and MuleSoft are generating hundreds of millions of dollars in revenue each year from real enterprise customers by selling proprietary, value-added products around their open projects. This is a new wave in software - one that's not only displacing incumbent markets, but creating entirely new ones. We call these Open Adoption Software (OAS) companies, and we believe they're primed to build meaningful businesses - and drive large economic outcomes. We're witnessing a big shift in how software is consumed. OAS companies are constructed differently. They go through three phases of company building: Project, Product and Profit . Companies built atop this ""3Ps"" model need to look largely the same and be held to similar financial standards as traditional enterprise software businesses by the time they make it into the ""Profit"" phase. We discussed this a few weeks back in a panel conversation with startups and financial analysts - a timely conversation, as some of these companies reach the scale to potentially IPO. This feels an awful lot like 2003, just before the first SaaS companies started going public. OAS is a customer-driven phenomenon Open software has already rooted itself deep within today's Fortune 500, with many contributing back to the projects they adopt. We're not just talking stalwarts like Google and Facebook; big companies like Walmart, GE, Merck, Goldman Sachs - even the federal government - are fleeing the safety of established tech vendors for the promises of greater control and capability with open software. These are real customers with real budgets demanding a new model of software. And the drumbeat is only getting louder. Each year we host 15 Fortune 500 CIOs as part of Accel's Tech Council , and we continue to hear criticism about proprietary software (""expensive, slow to change""). Here are a few trends we identified that are driving customers toward this new model: The Need for Speed and Control: The demand for innovation and rapid delivery means enterprises need agility from the software they adopt. Nothing is worse than waiting for a vendor to update a library when you're trying to stick to your release schedule. Open platforms allow companies to move faster and integrate at a deeper level without fear of lock-in by removing the dependency on proprietary vendors. Enterprises are no longer beholden to a vendor's product roadmap - they can innovate to their own requirements at any time. Everything is Web Scale: Enterprises are delivering solutions to a global, ever-connected base of users. Consider banks that support tens of millions of end users logging into their banking apps and hundreds of thousands of employees worldwide. Traditional, proprietary vendors are unable to deal with this onslaught of data and user scale. Fortunate for them, many early web 2.0 leaders (Google, Facebook, Linkedin, Yahoo) dealt with these problems and more, contributing much of their learnings to the open community. Developer Power and Network Effects: CIOs are empowering frontline developers to download and adopt the projects they need to drive innovation. Developers are looking to community-led technologies where they adopt, deploy and meaningfully participate . OAS extends beyond Moore's Law by also benefitting from something akin to Metcalfe's Law: its energy and rate of innovation grows exponentially with the developer networks around it. Open software can absorb learnings and requirements far faster than a proprietary vendor, while simultaneously hardening security and stability. Open software is in many respects, much safer. Hadoop and Docker are constantly stretched, pushed, molded and smoothed by their developer communities - they're far more mature than their age would suggest. All of this is to say: We're witnessing a big shift in how software is consumed. OAS is openly adopted and openly developed, and is quickly becoming a dominant model for how enterprises build and deliver IT. While most OAS companies have at least some amount of freely available or open-source components, open source and OAS should not be conflated. Open source describes a software development methodology, whereas OAS pertains more to a go-to-market and company-building philosophy. OAS is not about cheaper alternatives to proprietary on-premise software. It about creating new markets more so than displacing incumbents. It's innovative, it's developer-driven and it's the next wave of software adoption. The next wave of software With each successive wave of technology - from mainframe to client-server to 'X'aaS (IaaS, SaaS, etc.) to OAS - software has gotten progressively easier to adopt. Therefore, adoption has happened faster and has reached a broader audience than the wave before it. Each wave is driven by the democratization of some facet of technology. In the shift from mainframe to client-server, computers became accessible. In the shift to 'X'aaS, hosting and WAN connectivity became accessible. Now, with the shift to OAS, developer community innovation has become accessible. OAS not only represents a new way to provide innovative functionality, but is a delivery model innovation for developers. Through it all, the customer desire for bigger, faster and cheaper offerings remains constant. The technological innovations that each wave brings facilitate change in how software is packaged and delivered so that customers can gain some form of efficiency or cost savings. Being openly adopted is not a panacea. With all of these shifts, industry pundits predicted that the new wave will commoditize existing categories. While some layers of the stack do get cheaper, consumption on the layer above consequently expands dramatically as more applications are developed and new use cases emerge. This new usage outpaces any commoditization. Thus, the value of the market opportunity expands rather than contracts. Salesforce and Amazon Web Services (AWS) exemplify this. Literally thousands of new businesses exist as a result of these platforms than ever could have in the past. OAS is not an answer to all problems While OAS companies drive adoption much faster than their fully proprietary counterparts, being openly adopted is not a panacea. Particularly as public cloud vendors begin hosting open-source projects as a service, it's tremendously important that these companies thoughtfully decide which parts of the product will be open and which parts won't. There is definitely a unique failure mode in which OAS companies go too open and fail to monetize sufficiently. While we certainly believe in OAS, not all open projects are the basis for OAS companies, and not all of these companies are going to be publicly traded - some will be niches, some will struggle, some will be M&A opportunities. It's hard to predict the winners out of the gate. While OAS companies will likely have the same success rate as traditional software companies, there is reason to believe that the winners will be bigger than their predecessors. Featured Image: 31moonlight31/Getty Images",en,91
116,1318,1465488259,CONTENT SHARED,-5380376324140474506,881856221521045800,6178486427970494602,,,,HTML,http://exame.abril.com.br/tecnologia/noticias/mudancas-na-app-store-podem-nao-conquistar-desenvolvedores,mudanças na app store podem não conquistar desenvolvedores | exame.com,"San Francisco - A Apple anunciou uma série de aguardados melhoramentos na App Store, mas os novos recursos podem não reduzir preocupações de desenvolvedores e analistas de mercado que afirmam que o modelo da loja de aplicativos pode estar ultrapassado. A loja reformulada vai permitir que os desenvolvedores divulguem seus aplicativos em resultados de busca e dará a eles uma parcela maior das receitas com assinaturas. A Apple ainda afirmou que acelerou muito a velocidade de aprovação dos aplicativos vendidos na loja. O objetivo é sustentar um ciclo virtuoso no centro do lucrativo negócio criado pelos iPhones. Produtores de software desenvolvem aplicativos para o celular da Apple porque seus clientes têm interesse em pagar por eles e estes consumidores, por sua vez, pagam um prêmio pelo aparelho porque consideram que tem os melhores aplicativos. A loja é mais importante estrategicamente que nunca para a Apple, em um momento em que as vendas do iPhone começam a dar sinais de desaceleração e a companhia busca softwares e serviços para preencher esse espaço. O presidente-executivo da Apple, Tim Cook, afirmou em conferência recente que as receitas da App Store subiram 35% no ano passado. Mas a loja também é vítima de seu próprio sucesso. Oito anos depois de seu lançamento, a App Store tem mais de 1,9 milhão de aplicativos, segundo uma análise da empresa App Annie, o que torna praticamente impossível para desenvolvedores conseguirem um espaço de usuários. Além disso, fica cada vez mais difícil para os usuários encontrarem o que precisam, já que cerca de 14 mil novos aplicativos chegam à loja toda semana. ""O espaço para aplicativos ficou fora de controle"", disse Vint Cerf, um dos inventores da Internet e atualmente vice-presidente do Google durante uma conferência em San Francisco sobre o futuro da web. ""Precisamos deixar esta ideia de ter um aplicativo individual para qualquer coisa individual que você queira fazer."" Alguns usuários estão abandonando aplicativos para adotarem serviços de mensagens como o Slack ou o Messenger, do Facebook, que estão avançando sobre áreas como shopping e armazenagem de documentos. Enquanto isso, rápidos avanços em inteligência artificial podem levar a um mundo em que as pessoas utilizam assistentes digitais controlados por voz como o Siri, da Apple, em vez de abrirem aplicativos individuais. Se estas tecnologias decolarem, podem eliminar a vantagem desfrutada pela Apple por meio de seu forte ecossistema de aplicativos e dar mais força ao Google, que é amplamente considerado como líder na inteligência artificial. ""A atual dinâmica é muito favorável à Apple e isto é uma indicação de que podemos ter uma mudança para uma dinâmica diferente em que o Google terá uma forte vantagem"", disse Benedict Evans, sócio da empresa de investimentos de risco Andreessen Horowitz. ""Não importa o que você faça a uma loja de aplicativos, você sempre vai ter o mesmo problema: É uma lista de milhões de coisas"", acrescentou.",pt,91
117,2707,1478181100,CONTENT SHARED,-790959521412948853,-1578287561410088674,-8549049404053740422,"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) SmartCanvas/0.1.0 Chrome/49.0.2623.75 Electron/0.37.2 Safari/537.36",SP,BR,HTML,https://macmagazine.com.br/2016/11/03/microsoft-lanca-teams-nova-plataforma-de-chat-concorrente-do-slack/,"microsoft lança teams, nova plataforma de chat concorrente do slack | macmagazine.com.br","A Microsoft tem investido cada vez mais em ferramentas para o ambiente corporativo. A novidade desta vez é o mais novo serviço de chat para interação em grupos, o Microsoft Teams . Sendo um competidor direto de serviços como o Slack e o HipChat , o Microsoft Teams é mais uma maneira de manter o trabalho de equipes em um só lugar, para não haver preocupação de perderem-se em uma montanha de aplicativos e serviços diversos. Com certeza, o trunfo de ferramentas como essas é a integração com serviços de terceiros ou da própria empresa, e o Teams parece fazer isso muito bem. Além de, é claro, funcionar com o pacote Office e Skype, sua API aberta possibilita que outros serviços de terceiros possam ser integrados a ele, como o Zendesk, Asana, Twitter, GitHub, etc. Outros recursos também funcionam normalmente com o app, como bots , emojis, stickers , GIFs, extensões e mais. O Microsoft Teams é multiplataforma e está disponível para Mac, Windows, iOS, Android e também para a web. Entretanto, ele faz parte do pacote de assinatura do Office 365, ou seja, é uma ferramenta paga. Por este motivo, talvez seja difícil mensurar o quanto as pessoas vão aderir ao aplicativo; mesmo assim, o potencial dele é grande e pode até significar uma ameaça aos serviços competidores. O que não quer dizer que eles vão se render facilmente; muito pelo contrário: o Slack inclusive já publicou uma carta aberta no New York Times para dar as boas-vindas à Microsoft, bem no estilo Apple em 1981: That feeling when you think ""we should buy a full page in the Times and publish an open letter,"" and then you do. �� pic.twitter.com/BQiEawRA6d - Stewart Butterfield (@stewart) 2 de novembro de 2016 Aquele momento em que você pensa ""nós deveríamos comprar uma página inteira no Times e publicar uma carta aberta"", e aí você faz isso. �� Com ""alguns conselhos de amigo"", o Slack lembra à Microsoft que ""o que mais importa não são os recursos"", que ""uma plataforma aberta é essencial"", que ""é preciso fazer isso com amor"" e, por fim, que ""o Slack está aqui para ficar"". Por último: Slack está aqui para ficar. Nós vamos aonde o trabalho acontece por milhões de pessoas ao redor do mundo. Então, bem-vinda, Microsoft, à revolução. Estamos felizes por você estar nos ajudando a definir essa nova categoria de produtos. Nós admiramos muitas de suas realizações e sabemos que você será um concorrente digno. Temos certeza de que você vai apresentar algumas novas idéias por conta própria também. E nós estaremos bem ali, prontos. A carta - que pode ser lida por completo em inglês aqui - muito se assemelha à que a Apple publicou no Wall Street Journal direcionada à IBM quando - na década de 1980 - esta decidiu entrar no ramo de computadores pessoais. As palavras não são exatamente as mesmas, mas a intenção com certeza foi. O Microsoft Teams está disponível para testes em 170 países e 18 idiomas, para os clientes do pacote Office 365 dos planos Enterprise ou Business . Seu lançamento oficial será no primeiro trimestre de 2017. Microsoft Teams de Microsoft Corporation [via MacRumors ] Se houver algum erro no post acima, selecione-o e pressione Shift + Enter ou clique aqui para nos notificar. Obrigado! chat colaboração conversa equipes HipChat Integração Microsoft Microsoft Teams Office 365 produtividade skype Slack times Sobre o Autor Aviso: nossos editores/colunistas estão expressando suas opiniões sobre o tema proposto e esperamos que as conversas nos comentários sejam respeitosas e construtivas. O espaço acima é destinado a discussões, debates sobre o tema e críticas de ideias, não às pessoas por trás delas. Ataques pessoais não serão tolerados de maneira nenhuma e nos damos ao direito de ocultar/excluir qualquer comentário ofensivo, difamatório, preconceituoso, calunioso ou de alguma forma prejudicial a terceiros, assim como textos de caráter promocional e comentários anônimos (sem nome completo e/ou email válido). Em caso de insistência, o usuário poderá ser banido.",pt,91
118,1370,1465842144,CONTENT SHARED,-4228415104574264137,3302556033962996625,-4935617751218062693,,,,HTML,https://medium.com/google-developers/up-your-app-s-sharing-game-with-direct-share-2a2bc0a9ad36,up your app's sharing game with directshare - google developers,"Up your app's sharing game with Direct Share As you send pictures, messages, goofy drawings, and humble-brags about your sweet life to the people you love, sharing suddenly rises in importance. You may even find yourself daydreaming about how sharing should work in your app. (Because we all know you didn't spend too long on it the first time around.) Thankfully, Marshmallow's Direct Share feature makes it easy to create a custom Share experience without too much hassle. Your app can specify direct share targets that will be displayed in the Share dialog presented to the user. To be clear, this means that instead of launching your app, you can now launch a specific conversation in response to a Share intent. Context can be critical, and your app has a chance to shine here. So, to spruce up your app and amp up your Share game, you'll need a ChooserTargetService . This service needs to implement onGetChooserTargets() to provide the direct share targets to the system. And this method is where you can get fancy . Maybe it makes the most sense to offer up the ten most recent conversations. Or perhaps your app offers a way for users to track who they care the most about, and you can provide a list of those BFFs instead. Or maybe you know which conversations (like group messages, for example) are most prone to using attachments, and those are your likely share targets. This magic is up to you. We just made the framework, so that you could shine. Then, in your manifest, you'll obvi declare the service. But when you do, don't forget to specify the BIND_CHOOSER_TARGET_SERVICE permission. Then, for each activity that you'll be using with the new service, you'll need to attach some new metadata to your intent filter . This is what will launch your wonderful service to provide the extra Share targets. Otherwise, you're stuck with the old-school approach. If you'd like to see this in action, we have a sample for you. Check it out and then update your own app to #BuildBetterApps. Join the discussion on the Google+ post and follow the Android Development Patterns Collection for more!",en,89
119,335,1460407450,CONTENT SHARED,-7681408188643141872,-3390049372067052505,2596549390664327450,,,,HTML,http://blog.intercom.stfi.re/the-end-of-apps-as-we-know-them/?sf=xzypkn,the end of apps as we know them - inside intercom,"The experience of our primary mobile screen being a bank of app icons that lead to independent destinations is dying. And that changes what we need to design and build. How we experience content via connected devices - laptops, phones, tablets, wearables - is undergoing a dramatic change. The idea of an app as an independent destination is becoming less important, and the idea of an app as a publishing tool, with related notifications that contain content and actions, is becoming more important. This will change what we design, and change our product strategy. No more screens full of app icons This is such a paradigm shift it requires plenty of explaining. Whilst it may not transpire exactly as I'm about to describe, there is no doubt what we have today - screens of apps - is going to dramatically change. Bear with me as I run through the context. The idea of having a screen full of icons, representing independent apps, that need to be opened to experience them, is making less and less sense. The idea that these apps sit in the background, pushing content into a central experience, is making more and more sense. That central experience may be something that looks like a notification centre today, or something similar to Google Now, or something entirely new. The primary design pattern here is cards. Critically it's not cards as a simple interaction design pattern for an apps content, but as containers for content that can come from any app. This distinction may appear subtle at first glance, but it's far from it. To understand it, and chart the trajectory, we need to quickly run through two things. Designing systems not destinations I covered this topic in detail in a previous post , so I'll quickly summarise here. Most of us building software are no longer designing destinations to drive people to. That was the dominant pattern for a version of the Internet that is disappearing fast. In a world of many different screens and devices, content needs to be broken down into atomic units so that it can work agnostic of the screen size or technology platform. For example, Facebook is not a website or an app. It is an eco-system of objects (people, photos, videos, comments, businesses, brands, etc.) that are aggregated in many different ways through people's newsfeeds, timelines and pages, and delivered to a range of devices, some of which haven't even been invented yet. So Facebook is not a set of webpages, or screens in an app. It's a system of objects, and relationships between them. Recent changes to iOS and Android notifications Things changed with iOS 8 and Android KitKat. Notifications used to be signposts to go to other places. A notification to tell you to open an app. To open a destination. But that is changing fast. For a while now, you can take action directly in Android notifications. Sometimes that takes you to that action in the app itself, but sometimes you can do the action directly, meaning that you don't need to open the app at all. iOS is following suit here and raising the bar. Interactive notifications. No need to open the app. The notification is the full experience. The next version of Android takes this even further, breaking notifications into independent cards. You can see that cards stack below each other. We've moved pretty quickly from notifications as signposts, to containers (cards) that include content , and actions on that content. Next up: cards housing full product experiences The next iteration is obvious. Lots and lots of notification cards that enable full product experiences and independent workflows right inside the card. Comment on the Facebook post. Retweet the tweet. Buy the item on Amazon. Check in for the flight. Share the news story. Add the reminder to your to-do list. Book the restaurant. Swap the fantasy football player. Annotate the run you just finished. Pay the bill. And on and on. Towards apps as services Breaking things right down into the individual atomic unit, including the content and actions. The atomic unit separate from the container of the app itself, so that it can show up anywhere, on any device. The atomic units are then reassembled based on context. Aggregated in a centralised stream. Or pushed to you on your watch. The content may be reformatted to enable more natural user input, optimized for your situation. Des sent me a text based message, but I'm driving so my watch reads it out to me. I speak my reply to Siri/Google and Des receives it as a text based message, because he's in work at his desk. The actions available change. All this and more is just about to happen. It may be very likely that the primary interface for interacting with apps will not be the app itself. The app is primarily a publishing tool. The number one way people use your app is through this notification layer, or aggregated card stream. Not by opening the app itself. In a world where notifications are full experiences in and of themselves, the screen of app icons makes less and less sense. Apps as destinations makes less and less sense. Why open the Facebook app when you can get the content as a notification and take action - like something, comment on something - right there at the notification or OS level. I really believe screens of apps won't exist in a few years, other than buried deep in the device UI as a secondary navigation. A concept design to make this concrete This is such a fundamental shift that to highlight where it may go, I'll start with a rough system design for how one might interact with a connected device in this world. - Imagine a vertical stream of cards, individually personalised and ranked based on who and what you care about, your current context (location, availability, etc.) and your likelihood to care about things based on historical data when you were in a similar context. - The cards can come from any source that you care about or have given permission to. - This looks a lot like Google Now, but on steroids. You will have almost as many unique sources in your stream as you have apps on your phone. - This also looks a lot like your notifications centre on your phone, but rather than merely signposts to open apps, these cards are notifying you, presenting you with the content to decide what to do next, and with the ability to interact with the content right there and then. So a card from Facebook has all the actions you would have for that content if you viewed it in the Facebook app. Like, comment, share, save, etc. all inline, with no need to open the Facebook app. Cards from travel apps allow you to book, cards from commerce apps allow you to buy, the list is endless. This is the beginning of the end for apps as destinations. Why open the app when you don't need to? Let's take this a step further. Imagine that you can scroll horizontally, and that shows you more content from the same source. So on a Facebook post, that is effectively your newsfeed presented horizontally rather than vertically. This would be the same for all sources, Twitter, Instagram, WhatsApp, news apps, etc. And of course on all devices. OK now let's go a step further again. Imagine that a parent card can support a child card, so for example a Facebook card can support (embed) a card from the BBC. Indeed something similar already exists with Twitter. This is also a little similar to the recently launched Apple Extensions, and is already happening in app development in China with Baidu and WeChat, where smaller apps are being bundled within bigger apps, only surfacing when some interaction in the UI invokes the smaller app. For example, in Baidu Maps you can find a hotel, check room availability, and make a booking, all inside the app. But again the apparent subtlety masks something much more profound. Embedded cards (child cards) within cards (parent cards) also mean you don't need to install the app to experience the content from the child card. You just need the parent card app on your device. Again, this is already happening, Twitter cards currently support Stripe payments inside the card. You don't need the New York Times app to see their content on Twitter. But imagine this pattern was widespread. Suddenly app developers have a powerful discovery channel. And some businesses may be comfortable always appearing as a child card, without ever having an app at all. One final step further. What if the cards came from other ? Like vending machines that you walk up to and pay through the card? Hotels you walk into and order your breakfast or pay for the wifi? The ramifications for websites might also be huge. If a publishing company, for example the New York Times, can push content to cards, and those cards can be seen in many different third party places (with revenue sharing agreements) why bother having a website at all? It's just a huge overhead. We will still open apps. Sometimes In this world, it feels dumb to open apps just to see what lies behind the red counter, or to have to switch between apps. Opening apps is still necessary and great for many contexts, especially composition of new content and dedicated deep workflows, and maybe changing preferences. But not for seeing what's new and interesting. A bank of app icons as a dominant design pattern feels old and inefficient now, and I think it'll disappear within a couple of years, correctly relegated behind a ""show me my apps"" action. The system will learn, creating new competitors As people interact or don't interact with cards presented to them, the system will learn when to show more or less from a specific source (app). As content from different apps will be presented side by side, this changes who you might think you are competing with. Competition is between products that do the same job, not products that are in the same category. This is already the case today; when faced with multiple notifications on a phone screen, they all compete with each other for your attention. Here at Intercom, we're big proponents of the Jobs To Be Done (JTBD) framework, which asks what Job people need to get done that your product fulfills. If you focus purely on the job, and not the industry, you realise airlines selling business class seats are competing with Skype for customers, as they address the same job: the need to have clear communication with colleagues. Similarly, apps will realise they are competing on Jobs they may not have realised their product addresses. Twitter for example, may be competing much more with apps addressing the Job of 'entertain me while I have a short amount of free time' e.g. Games and News apps, than with other social products. This intense competition means businesses will have to spend time designing great notifications/cards, because they will potentially be competing with cards from Facebook, or Amazon, or Google. The days of sending lots and lots of notifications to bring people back to an app are going away, with a much better focus on designing notifications that people engage with there and then, independent of opening the app. Three critical questions There are many signs pointing towards a near future that looks something like this. But many questions remain - these are three that I have no answers for: Will this happen at the app, notification, or OS level? One of the biggest challenges will be whether these experiences will occur: at an app level (like an evolution of Google Now), at a notification level (an evolution of the Android or iOS notification centre), or at the root OS level (a redesigned iOS for example that removes the sea of app icons). Will this be one consolidated stream, or multiple streams? Maybe we will have a friends stream, a news stream, a work stream. Will this be owned at a company level? Maybe there will be a Google version, an Apple version, etc. Or more open systems that are interoperable across platforms (like the web itself), the first of which (like Wildcard and Citia ) are being built now, could come to dominate. Towards better businesses and products This is just a sketch but at a conceptual level I think it's largely where we are headed. Large parts of this are built already; things like Google Now, Android notifications, iOS8 interactive notifications, iOS8 extensions, Twitter cards. Emerging platforms like Android Wear and Apple Watch are confirming these trends towards cards that work as notifications, content and actions. There are also a multitude of user benefits: This new paradigm matches much more closely with how real life works. We don't live our lives in silos, like the app silos that exist today. People start to forget about ""apps"" and just think about businesses and products and services. This is a great thing, the container for content should be invisible to users. This new paradigm also solves a critical problem around volume of incoming content. Navigating to lots of apps is so inefficient. A new problem emerging is an overwhelming volume of notifications. Things will need to be ranked, which will make them more manageable. It's also a better experience, apps maximising their usefulness in a quick lightweight fashion rather than dominating your attention in a slow heavyweight app-oriented experience. The constraint of an individual card also makes you think about the most important thing you could show, and only the most important actions relating to that. That constraint is very powerful. For businesses, it also starts to solve the app discoverability problem. Rather than relying on App Store promotion, advertising, or new deep in app linking to get discovered, an apps content can appear as a card in our stream, particularly when embedded in a parent card. Indeed there may not be a child app, the content and actions in that child card may come from the web. This paradigm shift also starts to ask questions of the bundle or unbundle dilemma (btw both are happening now, it's not just unbundling). Maybe in this world you can have your cake (unbundled simple focused one task experiences) and eat it too (bundled into a coherent individual stream). A deliberately designed eco system of cards where cards are simple, but can carry information and context from other cards you build. 5 key take aways These patterns reinforce two things we wrote about here on Inside Intercom early in their development. That cards are the future of the web , and designers need to design systems not destinations . Cards are happening. Systems are happening. Get fully up to speed on both of these things. Responsive design is a nice thing, but we're heading way beyond that. We're talking about designing content that may appear on an incomprehensible number of devices and in an incomprehensible number of situations. This will need new design principles, new ways of thinking about researching context. Push forward with this yourself, don't wait for it to happen. Designing the notifications, and the actions within them, will become an increasingly important part of product design. We will need to spend as much of our time on this aspect of the experience, as on the experiences within the app. Change how you think and work now, rather than when it is too late. Sketch systems, not screens. Think about who you might integrate with. Integrations as part of a product strategy are increasing, witness the explosion in available APIs, Webhooks and the emergence of services like Zapier and IFTTT. Integrations make things possible that you could never do alone. They give you access to new audiences. Make integrations part of your business plan, product strategy, and product design. I carry around both an iPhone and an Android phone. I often also have my iPad Mini. I wear a Nike Fuelband. I've tried Google Glass whenever I can (in private ;-)). When I can I'll buy an Apple Watch. We all need to dive headfirst into this, eyes open, trying to see what works and fails, trial and error. If you made it this far, thanks for reading. Whilst the trajectories seem clear, much is unknown, we'll all figure this out together. So please let us know your feedback, thoughts, ideas below, and we'll do our best to respond and keep the conversation moving forward. Update: Read our follow up piece: It's not the end of apps . There are many other people writing about these things and I'm indebted to them for sharing their thinking. Here are some key articles if you want to read more: Thanks also to my colleagues at Intercom for contributing to this and making it much much better. Like what you read here? Why not come and work with us? We're hiring for roles in Dublin and San Francisco . Want to read our product management best practices? Download our free book , Intercom on Product Management . It's recommended by folks like Ryan Singer, Hunter Walk, and Dharmesh Shah.",en,89
120,2507,1475614359,CONTENT SHARED,-4996336942690402156,-709287718034731589,-6980904286083816062,"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_1) AppleWebKit/602.2.11 (KHTML, like Gecko) Version/10.0.1 Safari/602.2.11",SP,BR,HTML,https://medium.freecodecamp.com/live-asynchronously-c8e7172fe7ea?gi=5c95ee418f3e,live asynchronously.,"We'll start by talking about how interruptions - even scheduled ones - can destroy your productivity. But first, a web comic: Did you see what happened? The boss came by and totally wrecked the developer's train of thought. And for what? To notify him that the next time he checked his email, a message would be there for him? When you interrupt a developer, it takes an average of 10 minutes before they can get back to productively coding. So why do 70% of offices these days look like this? Most studies conclude the same thing that this widely-cited paper does: people hate open plan offices . Employers have chosen to sacrifice job satisfaction and productivity, all so they can pack a few extra sardines into $72-per-square-foot San Francisco office spaces. They do this under the guise of lowering barriers to communication. But if you think about it, they should be raising them instead. Because here's what happens when you need to get some real work done in an open plan office: Reaching that flow state Let's talk a bit about how us humans get work done. Is it four hours of crushing it, a lunch break, then four more hours of crushing it? No. It's more like coffee, email, coffee, meeting, coffee, lunch with coworkers, coffee - OK finally time to get some work done! Did you know the average developer only get two hours of uninterrupted work done a day? They spend the other 6 hours in varying states of distraction. But here's what happens during the two hours they have to themselves. They warm up. They check logs, issues, and wrap their heads around what needs to be done. They dive into the code. Their pupils dilate. They enter what psychologist Mihaly Csikszentmihalyi calls a ""flow state."" If you've ever been ""in the groove"" or ""in the zone,"" that's what this is. A happy state of energized focus. Flow. If your job requires even an ounce of creativity, you'll do your best work in one of these flow states. And yes, you can reach a flow state in an open plan office, with noise cancelling headphones cranked up to 11. But it's a lot easier when you're in a quiet, comfortable room by yourself. So Mihaly figures out that task switching utterly devastates your productivity. Something as mundane as getting a text message about dinner will completely wipe out all those things you're juggling in your working memory. It will knock you out of your flow state. Most of Mihaly's fellow researchers agree . There's a serious cost to switching between tasks. Mihaly spends the next 30 years researching flow states. He publishes a ton of papers. He does some consulting. He teaches at Berkeley. He records a TED talk: He writes some books. Here's a good one: But employers, for the most part, don't listen. They continue to cram their teams together into noisy open plan offices. They continue to pepper their teams' days with meetings. They expect their teams to be responsive to emails or Slack, further dashing hopes of ever reaching a flow state and getting some real work done. Do you think Tolstoy could have written War and Peace in an open plan office? Do you think Mozart could have composed The Marriage of Figaro in between stand-ups and one-on-ones? Do you think Torvalds could have designed the Linux Kernel with Slack notifications popping up every 15 seconds? How to live asynchronously Here are three things you can do reclaim your flow state. I'll assume that, like most people, you're crazy busy. You're skeptical of the gazillions of productivity tips you see here on Medium. And you probably don't greet major lifestyle changes with open arms. So I'll introduce these in increasing order of commitment required. One Dalí Clock means you can do it immediately after finishing this article. Four Dalí Clocks means you'll need a full-blown action plan to make the leap. Tip #1: Turn off as many notifications as you can (difficulty: 1 Dalí Clock) Remember when Microsoft Outlook added that feature where it showed you a notification every time you received an email? You shut that off, right? Good. Now shut off pretty much every other notification on your phone and computer. Do you really need to be notified right this instant that a new podcast will be available during your commute home tonight? Or that your aunt liked the photo you took of your lunch two days ago? The only notification you really need is that old standby from 100 years ago: the phone ring. Because if it's really important, people will call. I can't find you in the food court. Call. Your kid threw up at school. Call. The servers are melting down. Call. Some of my friends will still leave their text message notifications on (or their WhatsApp notifications when they're overseas). I did this for a few years, too. But I turned these off a few months ago. So far, planes have not fallen from the sky. As a bonus, when people expect to hear back from you within 24 hours - rather than 10 minutes - they take the time to actually think about what they want to say to you. No more ""hey there"" texts. Or that old favorite: ""can I ask you a question?"" By the way, just for fun, here's the extreme opposite of no notifications: Tip #2: Defend your time by dodging meetings (difficulty: 2 Dalí Clocks) Next time you sit in a meeting, do a quick experiment. Write down all the important things discussed that couldn't have just been mentioned in an email thread. There may be a few. But were those things really worth the 30 to 60 minutes you just spent away from your work? Email brings out the Hemingway in all of us. When someone can't get their point across in an email, it just means they're going to have an even harder time explaining it in person. The next time someone writes you asking if you have time to meet, try responding: ""What do you want to talk about?"" They'll write you back with an answer. Then respond: ""OK - what are your thoughts on that?"" They'll write you back with an answer. Then respond with your own thoughts on the matter if necessary. Or just say: ""OK - sounds good."" Phew. Meeting dodged. Sometimes you get that message from a cheerful stranger on LinkedIn: ""Let's grab coffee and catch up!"" Or that message from a coworker: ""Can I swing by your desk and pick your brain?"" If these people valued your time, they would just tell you what they wanted right up front. But in many cases, they don't know exactly what they want. Don't commit your scarce time to meet with them unless it's clear that they know what they want, and they're willing to tell you before hand. A lot of people may bristle at you not just accepting their meeting request. Be polite and patient, and tell them you're happy to answer their questions right here - in the email chain, or instant message client - wherever here may be. Your time is your most valuable asset. Be protective of it. Don't let others thoughtlessly waste it. Tip #3: Ask for a private office, or to work from home (difficulty: 4 Dalí Clocks) There are tremendous benefits to working remotely. When you have 4 minutes, read this article: A lot of the 13% productivity gain you get when you work remotely is because you're not in a noisy office. If you can't work remotely - or if you are one of the many people who enjoys the energy of the workplace - ask for a private office. If you value your productivity (and your sanity), consider prioritizing this during your job offer negotiation , or the next time you discuss a raise with your boss. Time to think. Time to create. Time enough at last. In closing, I strongly recommend this book by one of the founders of Pixar. You can learn about their experiments with improving their teams' productivity and creativity. Among other things, it explores the evolution of their meetings, their use of open plans, and their transition back to mostly private offices. So what are some ways you keep things asynchronous? Do you have any tips for us? Share them with everyone in the comments section below. Oh, and if you liked this, click the �� below. Follow me for more articles on technology.",en,88
121,730,1462275589,CONTENT SHARED,-730957269757756529,-331066625167168067,-6677202055740773123,,,,HTML,https://medium.com/@vilucas/por-que-a-limita%C3%A7%C3%A3o-da-banda-larga-%C3%A9-uma-forma-de-ignorar-o-futuro-12069206541,por que a limitação da banda larga é uma forma de ignorar o futuro,"Por que a limitação da banda larga é uma forma de ignorar o futuro Com um olhar direcionado para os excessos dos consumidores e com a tentativa das grandes empresas de telefonia e internet de barrarem a expansão de serviços de streaming, a Anatel fecha os olhos para as tendências do mundo da tecnologia. Apesar de o assunto ainda estar aguardando decisões, um ponto ainda foi muito pouco explorado quando falamos da limitação da banda larga no Brasil: os impactos para o uso da tecnologia em um horizonte de médio a longo prazo. Mas antes de entrar a fundo no tema, é preciso entender o que aconteceu. Hoje quando se contrata um plano de banda larga residencial, contrata-se um modelo de acesso aos dados baseado na velocidade do canal. Uma internet de 15 MEGA permite que seu usuário acesse dados da internet em forma de download a uma velocidade de no mínimo 1.5 MB por segundo. Neste modelo ainda, define uma franquia de consumo, por exemplo, 80GB de downloads mensais. As empresas em geral, apesar de definirem estes limites, durante longo tempo permitiam que o usuário consumisse livremente sua banda. Recentemente começou a reduzir a velocidade do download quando este limite era atingido. O usuário não era comunicado e somente saberia da redução através da percepção de má qualidade do link de internet. Toda a discussão existe porque hoje as empresas de telefonia e internet não são capazes de dizer ao usuário que sua franquia se aproxima do fim. Quando a Anatel resolve dar um parecer de que, ainda que temporariamente, as empresas não poderão suspender o serviço quando tal franquia for atingida, até que se permita um controle claro pelo usuário de seu volume consumido, deixa explícito que concorda com a política do bloqueio, se posicionando a favor das empresas provedoras de internet banda larga. Seu presidente, João Rezende, inclusive cita que é inevitável que se realize o limite da franquia, usando como exemplo quem joga online como o grande ""culpado"". O serviço, que já é alvo de incontáveis críticas pela comunidade usuária pelas constantes falhas e variações na velocidade, agora também se vê criticado por querer barrar as evoluções de serviços que se demonstram cada vez mais presentes na vida das pessoas que usam a internet, como Netflix, vídeos em alta resolução, e os próprios jogos online. Pois bem, chegamos ao problema. A tecnologia vem evoluindo constantemente, e a inovação chega cada vez mais rápido. Quando chegam nas lojas, câmeras que fazem vídeos em alta-resolução, e se permite que eles sejam levados à internet, imaginamos que pessoas terão interesse em assistí-los. As televisões, Full-HD, e agora 4K permitem a exibição de vídeos em altíssima qualidade. Ao mesmo tempo, quando as televisões smart nos são oferecidas, com uma série de aplicativos de streaming de vídeo, com conteúdo personalizado, imaginamos que haverá consumidores que pagarão por estes serviços. Quando nos prometem, que a cada dia mais que nossos aplicativos e dispositivos sejam conectados, seja uma geladeira, uma impressora, uma casa inteligente, e que desta forma poderemos realizar nossas atividades de qualquer lugar, focamos no serviço oferecido, e consequentemente entendemos que a tecnologia por trás da transferência de dados esteja disponível e funcione adequadamente. Vamos além: Hoje o mundo tem sido inundado por dados, uma análise constante do comportamento de pessoas e consumidores através de Big Data . Bom, se os dados não podem ser usados em favor dos usuários, não deveriam ser gerados. Dispositivos de Internet das Coisas (IoT - Internet of Things) capturam dados em tempo real que permitem uma série de ações (algumas que ainda nem imaginamos). A limitação da banda larga novamente influenciará para que evoluções nestes dispositivos parem no tempo, afinal, não haverá infra-estrutura por trás que permitam sua expansão. Relógios smart, TVs smart, notebooks, celulares, tablets, video-games das mais novas gerações. Todos eles consideram que a troca de informações e dados com a rede de internet esteja disponível, e em grandes volumes, afinal, deseja-se explorar a interface e usabilidade de seu usuário, sem limites à criatividade. Logo se percebe que as decisões e regulações que se encaminham buscam que o usuário evite a alta transferência de dados, e priorize toda aquela tecnologia que faça menor uso da banda. A exploração da tecnologia, o incentivo à inovação, a experiência do usuário ficam evidentemente em segundo plano. As pessoas à frente destes órgãos focam no problema imediato, sem avaliar todos os arredores que o cercam. É uma visão limitada que causa hoje uma luta da comunidade contra a imposição de regras retrógradas que nem sequer pensam na qualidade do serviço demandada pelas pessoas e nem mesmo na exploração da tecnologia em favor de um futuro que já está mais do que presente. A Internet passou a ser uma necessidade para todo este mundo a ser explorado. E sua limitação fará novamente que o Brasil pare no tempo em termos de inovação, enquanto o mundo se beneficia dela. E os únicos beneficiados serão os provedores do serviço de internet, que poderão aplicar preços abusivos frente à demanda crescente. A regulação dos limites de uso da internet é como fechar os olhos às tendências, e impedir que utilizemos o potencial e benefícios de outras tecnologias em favor de empresas que só querem garantir uma exploração financeira de seus consumidores.",pt,87
122,3023,1485456802,CONTENT SHARED,1348739322889189648,3609194402293569455,-296407187773930883,"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36",SP,BR,HTML,http://epocanegocios.globo.com/Empresa/noticia/2017/01/johnson-johnson-comprara-grupo-suico-por-us-30-bi.html,johnson & johnson comprará grupo suíço por us$ 30 bi,"Produtos da Johnson & Johnson (Foto: Chris Hondros/ Getty Images) A gigante do setor de saúde Johnson & Johnson vai comprar a empresa suíça de biotecnologia Actelion em uma transação em dinheiro de US$ 30 bilhões, que inclui a cisão da unidade de pesquisa e desenvolvimento da Actelion , informaram as empresas nesta quinta-feira. A aquisição dá à J&J acesso aos medicamentos de preço alto e margem elevada para as doenças raras do grupo suíço, ajudando-a a diversificar o portfólio de produtos. A oferta de pagar US$ 280 por ação, após semanas de negociações exclusivas, foi aprovada por unanimidade pelos conselhos de administração de ambas as empresas. O acordo compreende um prêmio de 23% em relação ao preço de fechamento da Actelion na quarta-feira, de 227,4 francos suíços, supera em mais de 80% o valor da ação em 23 de novembro, antes das primeiras notícias de que a maior empresa de biotecnologia da Europa tinha atraído o interesse de compradores. As ações da Actelion saltavam quase 20%, para 274,10 francos por volta das 10:53 (horário de Brasília). (Reportagem de John Miller e John Revill)",pt,87
123,1198,1464866267,CONTENT SHARED,7943088471380012839,-8020832670974472349,2348433893919184737,,,,HTML,https://coreos.com/blog/torus-distributed-storage-by-coreos.html,torus,"Persistent storage in container cluster infrastructure is one of the most interesting current problems in computing. Where do we store the voluminous stream of data that microservices produce and consume, especially when immutable, discrete application deployments are such a powerful pattern? As containers gain critical mass in enterprise deployments, how do we store all of this information in a way developers can depend on in any environment? How is the consistency and durability of that data assured in a world of dynamic, rapidly iterated application containers? Today CoreOS introduces Torus, a new open source distributed storage system designed to provide reliable, scalable storage to container clusters orchestrated by Kubernetes, the open source container management system. Because we believe open source software must be released early and often to elicit the expertise of a community of developers, testers, and contributors, a prototype version of Torus is now available on GitHub , and we encourage everyone to test it with their data sets and cluster deployments, and help develop the next generation of distributed storage. Distributed systems: Past, present, and future At CoreOS we believe distributed systems provide the foundation for a more secure and reliable Internet. Building modular foundations that expand to handle growing workloads, yet remain easy to use and to assemble with other components, is essential for tackling the challenges of computing at web scale. We know this from three years of experience building etcd to solve the problem of distributed consensus - how small but critical pieces of information are democratically agreed upon and kept consistent as a group of machines rapidly and asynchronously updates and accesses them. Today etcd is the fastest and most stable open source distributed key-value store available. It is used by hundreds of leading distributed systems software projects, including Kubernetes, to coordinate configuration among massive groups of nodes and the applications they execute. The problem of reliable distributed storage is arguably even more historically challenging than distributed consensus. In the algorithms required to implement distributed storage correctly, mistakes can have serious consequences. Data sets in distributed storage systems are often extremely large, and storage errors may propagate alarmingly while remaining difficult to detect. The burgeoning size of this data is also changing the way we create backups, archives, and other fail-safe measures to protect against application errors higher up the stack. Why we built Torus Torus provides storage primitives that are extremely reliable, distributed, and simple. It's designed to solve some major problems common for teams running distributed applications today. While it is possible to connect legacy storage to container infrastructure, the mismatch between these two models convinced us that the new problems of providing storage to container clusters warranted a new solution. Consensus algorithms are notoriously hard. Torus uses etcd, proven in thousands of production deployments, to shepherd metadata and maintain consensus. This frees Torus itself to focus on novel solutions to the storage part of the equation. Existing storage solutions weren't designed to be cloud-native Deploying, managing, and operating existing storage solutions while trying to shoehorn them into a modern container cluster infrastructure is difficult and expensive. These distributed storage systems were mostly designed for a regime of small clusters of large machines, rather than the GIFEE approach that focuses on large clusters of inexpensive, ""small"" machines. Worse, commercial distributed storage often involves pricey and even custom hardware and software that is not only expensive to acquire, but difficult to integrate with emerging tools and patterns, and costly to upgrade, license, and maintain over time. Containers need persistent storage Container cluster infrastructure is more dynamic than ever before, changing quickly in the face of automatic scaling, continuous delivery, and as components fail and are replaced. Ensuring persistent storage for these container microservices as they are started, stopped, upgraded, and migrated between nodes in the cluster is not as simple as providing a backing store for a single server running a group of monolithic applications, or even a number of virtual machines. Storage for modern clusters must be uniformly available network-wide, and must govern access and consistency as data processing shifts from container to container, even within one application as it increments through versions. Torus exists to address these cases by applying these principles to its architecture: Extensibility : Like etcd, Torus is a building block, and it enables various types of storage including distributed block devices, or large object storage. Torus is written in Go, and speaks the gRPC protocol to make it easy to create Torus clients in any language. Ease of use : Designed for containers and cluster orchestration platforms such as Kubernetes, Torus is simple to deploy and operate, and ready to scale. Correctness : Torus uses the etcd distributed key-value database to store and retrieve file or object metadata. etcd provides a solid, battle-tested base for core distributed systems operations that must execute rapidly and reliably. Scalability : Torus can currently scale to hundreds of nodes while treating disks collectively as a single storage pool. ""We have seen a clear need from the market for a storage solution that addresses the dynamic nature of containerized applications and can take advantage of the rapidly evolving storage hardware landscape,"" said Zachary Smith, CEO of Packet, a New York-based bare metal cloud provider. ""We're excited to see CoreOS lead the community in releasing Torus as the first truly distributed storage solution for cloud-native applications."" How Torus works At its core, Torus is a library with an interface that appears as a traditional file, allowing for storage manipulation through well-understood basic file operations. Coordinated and checkpointed through etcd's consensus process, this distributed file can be exposed to user applications in multiple ways. Today, Torus supports exposing this file as block-oriented storage via a Network Block Device (NBD). We also expect that in the future other storage systems, such as object storage, will be built on top of Torus as collections of these distributed files, coordinated by etcd. Torus includes support for consistent hashing, replication, garbage collection, and pool rebalancing through the internal peer-to-peer API. The design includes the ability to support both encryption and efficient Reed-Solomon error correction in the near future, providing greater assurance of data validity and confidentiality throughout the system. Deploying Torus Torus can be easily deployed and managed with Kubernetes. This initial release includes Kubernetes manifests to configure and run Torus as an application on any Kubernetes cluster. This makes installing, managing, and upgrading Torus a simple and cloud-native affair. Once spun up as a cluster application, Torus combines with the flex volume plugin in Kubernetes to dynamically attach volumes to pods as they are deployed. To an app running in a pod, Torus appears as a traditional filesystem. Today's Torus release includes manifests using this feature to demonstrate running the PostgreSQL database server atop Kubernetes flex volumes, backed by Torus storage. Today's release also documents a simple standalone deployment of Torus with etcd, outside of a Kubernetes cluster, for other testing and development. What's next for Torus? Community feedback Releasing today's initial version of Torus is just the beginning of our effort to build a world-class cloud-native distributed storage system, and we need your help. Guide and contribute to the project at the Torus repo on GitHub by testing the software, filing issues, and joining our discussions. If you're in the San Francisco area, join us for the next CoreOS meetup on June 16 at 6 p.m. PT for a deep dive into the implementation and operational details of Torus. ""Distributed storage has historically been an elusive problem for cloud-native applications,"" said Peter Bourgon, distributed systems engineer and creator of Go kit. ""I'm really happy with what I've seen so far from Torus, and quite excited to see where CoreOS and the community take it from here!"" Torus is simple, reliable, distributed storage for modern application containers, and a keystone for wider enterprise Kubernetes adoption. CoreOS is hiring If you're interested in helping develop Torus, or solving other difficult and rewarding problems in distributed systems at CoreOS, join us! We're hiring distributed storage engineers .",en,87
124,3093,1487157951,CONTENT SHARED,-6623581327558800021,599868086167624974,-6471874325422590486,"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36",MG,BR,HTML,https://www.wired.com/2017/02/spanner-google-database-harnessed-time-now-open-everyone/,"spanner, the google database that mastered time, is now open to everyone","About a decade ago, a handful of Google's most talented engineers started building a system that seems to defy logic. Called Spanner, it was the first global database, a way of storing information across millions of machines in dozens of data centers spanning multiple continents, and it now underpins everything from Gmail to AdWords, the company's primary moneymaker. But it's not just the size of this creation that boggles the mind. The real trick is that, even though Spanner stretches across the globe, it behaves as if it's in one place. Google can change company data in one part of this database-running an ad, say, or debiting an advertiser's account-without contradicting changes made on the other side of the planet. What's more, it can readily and reliably replicate data across multiple data centers in multiple parts of the world-and seamlessly retrieve these copies if any one data center goes down. For a truly global business like Google, such transcontinental consistency is enormously powerful. Before Spanner, this didn't seem possible. Machines couldn't keep databases consistent without constant and heavy communication, and communication across the globe took much too long. You know, the speed of light and all that. Google's engineers needed something like the the ansible , a fictional device that first appeared in Ursula Le Guin's 1966 novel Rocannon's World and became a sci-fi trope. The ansible can instantly send information across any distance, defying both time and space. Spanner isn't the ansible. It can't shrink space. But it works because those engineers found a way to harness time. No one else has ever built a system like this. No one else has taken hold of time in the same way. And now Google is offering this technology to the rest of the world as a cloud computing service . Google believes this can provide some added leverage in its battle with Microsoft and Amazon for supremacy in the increasingly important cloud computing market, just because Spanner is unique. And some agree. ""If they offer it, people will want it, and people will use it,"" says Peter Bailis, an assistant professor of computer science at Stanford University who specializes in massively distributed software systems. But as others point out: Few businesses have the same needs as Google. Trusting Time In the past, if you built a system that spanned hundreds of machines and multiple data centers, you followed an important rule: Don't trust time. If a system involved communication between many machines in many different places, time would vary from machine to machine, just because time-precise time-is a hard thing to keep. Services like the Network Time Protocol aimed to provide machines with a common reference point. But this worked only so well, mainly because networks are slow. It takes time to send the time. For Google, this was a problem. If a database spanned multiple regions, it couldn't ensure that transactions in one part of the world lined up with transactions in another. It couldn't get a truly global picture of its operations. It couldn't seamlessly replicate data cross regions or quickly retrieve replicated data when it was needed. So Google's top engineers found a way to trust time. Part of the trick is that they equipped Google's data centers with a series of GPS receivers and atomic clocks . The GPS receivers, much like the one in your cell phone, grab the time from various satellites orbiting the globe, while the atomic clocks keep their own time. Then they shuttle their time readings to master servers in each data center. These masters constantly trade readings in an effort to settle on a common time. A margin of error still exists, but thanks to so many readings, the masters can bootstrap a far more reliable timekeeping service. ""This gives you faster-than-light coordination between two places,"" says Peter Mattis, a former Google engineer who founded CockroachDB, a startup working to build an open source version of Spanner . Google calls this timekeeping technology TrueTime, and only Google has it. Drawing on a celebrated research paper Google released in 2012, Mattis and CockroachDB have duplicated many other parts of Spanner-but not TrueTime. Google can pull this off only because of its massive global infrastructure. A Changing World To be sure, a few others could build a similar service, namely Amazon and Microsoft. But they haven't yet. With help from TrueTime, Spanner has provided Google with a competitive advantage in so many different markets. It underpins not only AdWords and Gmail but more than 2,000 other Google services, including Google Photos and the Google Play store. Google gained the ability to juggle online transactions at an unprecedented scale, and thanks to Spanner's extreme form of data replication, it was able to keep its services up and running with unprecedented consistency. Now Google wants a different kind of competitive advantage in the cloud computing market. It hopes to convince customers that Spanner provides an easier way of running a global business, a easier way of replicating their data across multiple regions and, thus, guard against outages. The rub is that few businesses are truly global. But Google is betting its new service will give customers the freedom to expand as time goes on. Among them is JDA, a company that helps businesses oversee their supply chains, which is now testing Spanner. ""The volume of data-and velocity with which that data is coming at us-is amplifying significantly,"" says JDA global vice president John Sarvari. Spanner could also be useful in the financial markets, allowing big banks to more efficiently track and synchronize trades happening across the planet. And Google says it's already in talks with large financial institutions about this kind of thing. Traditionally, many banks were wary of handling trades in the cloud for reasons of security and privacy. But those attitudes are softening. A few years ago, Spanner was something only Google needed. Now, Google is banking on change.",en,87
125,1983,1470339055,CONTENT SHARED,-8243488279185272615,-1602833675167376798,-8527471549744389578,,,,HTML,https://pagamento.me/elopar-lanca-o-digio-cartao-para-brigar-com-nubank/,"elopar lança o ""digio"", cartão para brigar com nubank","Elopar lança o Digio , o concorrente do Nubank. Criada através da holding do Bradesco e BB, o grupo Elopar é um dos maiores conglomerados do Brasil. Dentro dele, empresas como Alelo (líder de mercado), Stelo (subadquirente) e Livelo (fidelidade), são os motores do grupo para mover boa parte do mercado financeiro no país. Lá dentro, eles também têm um banco, o CBSS , que acabou de ""parir"" seu novo filho, o Digio . O Digio vai conseguir seguir os passos do Nubank? Alguns palpites enxergam que o Nubank está perto de 1 milhão de cartões emitidos. E ainda tem a fila de espera, que chega às dezenas de milhares. Não há dúvida sobre a escala do ""roxinho"". Ele provou algo que bancos até então, não olhavam como um negócio 100% digital. Há quem diga: ""ah mas os bancos já tinham cartões sem anuidade. O que ele fizeram foi somente empacotar isso num aplicativo bonitinho."" Se a gente olhar os números captados pela empresa e a velocidade de crescimento da carteira de clientes, a combinação parece um tanto complexa de se criar uma fórmula. Não há dúvida que a combinação de conceitos do Vale do Silício (David Vélez, co-fundador da empresa, era responsável pelo fundo Sequoia no Brasil) mais uma potente estratégia de marketing, pode sim criar um pequeno gigante e o Nubank, já é um grande case de aquisição de cliente. Além de serem a fintech mais expressiva da América Latina, o Nubank ainda prepara novas viradas de chave, que vão mudar o jogo completamente, pelo menos para o modelo digital financeiro. Eles conseguiram provar que a emissão massiva de cartões, pode ser feita através de tecnologia e não somente de distribuição de pontos de venda. Isso os bancos tradicionais terão que aprender. O Digio, que tem um respaldo financeiro potente por trás e chega no mercado brasileiro com um grande caminho pela frente, num modelo já testado e validado pelo Nubank, pela startup ContaUm (que divulga ter milhares de cartões emitidos) e também pela chegada de novos bancos como Neon e o próprio Original. O Digio, que foi anunciado ontem, chega com esse respaldo econômico e com o suporte da Visa, exatamente no contraponto do Nubank (que tem a Mastercard do lado). Apesar da Elopar ser formada por Bradesco e BB, a empresa seguirá independente, funcionando dentro da holding assim como as outras empresas do grupo. Como uma divisão do banco CBSS. Diferente do Nubank, o Digio não precisa de convite para aprovar usuários. Assim como os aplicativos já disponíveis do mercado, o Digio faz todo processo de cadastro e gestão financeira dos gastos através de um aplicativo. E não vai cobrar tarifas. Esse é um teste (que o Nubank terá) e uma briga que vamos gostar de assistir. De fato, em termos de comunicação, pode embolar um pouco o jogo (no público alvo) de ambos os cartões. Mas é muito importante o Digio ter na cabeça, que esse é um jogo de produto e marketing.",pt,86
126,1316,1465443590,CONTENT SHARED,2687654465640040976,-1032019229384696495,-67483630648809830,,,,HTML,http://www.theverge.com/2016/6/8/11881786/google-search-material-design-layout-test,google is testing a new material design layout for desktop searches,"Material Design is supposed to be the visual template that unifies Google's products, but it's taken a while to reach every part of the tech company's kingdom - especially the larger products. We saw a Material Design overhaul of YouTube last month , and it seems the company is also testing the updated look on its desktop search. Engadget first spotted the change , and various users on Twitter and Reddit have also reported glimpsing the provisional update. It's not a massive change from the current look of Google's desktop searches, and the main visual difference is segmented search results into separate white cards floating on a gray background. There are a few other minor tweaks, including changing the settings icon from a cog to a three vertical dots, and updating the search button from a white-on-blue magnifying glass to a gray-on-white one. The Google homepage also gets a refresh in line with these changes. Okay, I am seeing @google search result page with Material Design. Looks good. @GoogleDesign #design #Google pic.twitter.com/SiLxOhA5aK - Jay Tyagi (@jay7yagi) May 21, 2016 One of the bigger tweaks is to Google's Knowledge Graph results - those information-dense cards that pop up alongside certain search results with basic facts like biographical details. These usually appear to the side of search results, though sometimes at the top of the page too. In the Material Design update Google is testing, it appears they're being shunted into the main column of search results, along with adverts and image searches. Of course, this is all just provisional at the moment. Google often tests various tweaks to its design with various users (e.g. turning search results from blue to black ), analyzing how the changes affect interaction with its sites. Let us know if you've seen the Material Design update yourself and what you think of it.",en,86
127,2005,1470657286,CONTENT SHARED,-8159730897893673528,1895326251577378793,8303966574908950301,,,,HTML,http://www.techrepublic.com/article/amazon-still-crushing-cloud-competition-says-gartner-magic-quadrant-for-iaas/,"amazon still crushing cloud competition, says gartner magic quadrant for iaas - techrepublic","""Up and to the right"" has become shorthand for ""the winning position."" Usually, a number of companies wrangle for position in the upper-right quadrant of Gartner's Magic Quadrant, a market research report that looks at leaders in an industry. But, in the newest Magic Quadrant for cloud infrastructure services , Amazon sits virtually alone, with only Microsoft Azure to keep it company. But, that's not really news. Amazon Web Services (AWS), after all, has completely dominated cloud infrastructure services for so long that Gartner crowns it the ""safe choice."" What is interesting, however, is just how far everyone else keeps falling behind. Looking at successive years of Gartner's IaaS Magic Quadrant reveals a pack of also-rans retreating into the distance. Years and years of winning In 2014, Gartner discovered that AWS had 5X the utilized capacity of its next 14 nearest competitors combined. By 2015, that lead had jumped to 10X . While we'd expect to see an industry pioneer dominate the early days of that industry, AWS is unique in its ability to continue to overshadow its ""peers"" for years on end. SEE Amazon spills the secrets of its success: Impossible goals and repeated failure Part of this derives from AWS' ability to appeal to developers, its original audience, while also being ""frequently chosen"" by traditional IT, Gartner noted. The analyst firm also declared, ""AWS is the provider most commonly chosen for strategic, organization-wide adoption."" Not surprisingly, then, ""AWS has a diverse customer base and the broadest range of use cases, including enterprise and mission-critical applications,"" with ""an ecosystem of open-source tools, along with more than a thousand technology partners that have licensed and packaged their software to run on AWS, have integrated their software with AWS capabilities, or deliver add-on services."" This jibes well with Amazon CFO Brian Olsavsky's contention that AWS keeps winning because of its superior speed of innovation, depth of functionality, and breadth of ecosystem. But, what about everyone else? Meet the rearview mirror Of course, Microsoft isn't far behind, with the enterprise giant integrating on-prem and in-cloud workloads in ways no one else can. According to Gartner, ""Microsoft's brand, existing customer relationships, history of running global-class consumer internet properties, deep investments in engineering and innovative roadmap have enabled it to rapidly attain the status of strategic cloud IaaS provider."" SEE Why Kubernetes may be a bigger threat to Amazon than Google's cloud Microsoft, in short, is a credible competitor to AWS. Google isn't yet in the same class, but it at least attains ""Visionary"" status with Gartner. Everyone else, however, is a rounding error. For some time, other cloud providers seemed to be closing the gap with AWS. No more. Take a look at these three consecutive Magic Quadrants. Here's 2014: And here's 2015: Finally, here's the cloud infrastructure market in 2016: See what happened? AWS looks like it's treading water, but really it's simply defining true north for everyone else. Microsoft Azure has been closing the gap. Google, for its part, really does appear to be running in place relative to its bigger cloud competitors. And CenturyLink, IBM, Rackspace, etc.? They're all just etcetera, and getting more so with each passing day. Also see",en,86
128,2580,1476787589,CONTENT SHARED,-3058031327323357308,7645894863578715801,7779918589008538147,Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:49.0) Gecko/20100101 Firefox/49.0,SP,BR,HTML,https://dzone.com/articles/the-java-8-api-design-principles,the java 8 api design principles,"This article is featured in the new DZone Guide to Modern Java, Volume II. Get your free copy for more insightful articles, industry statistics, and more. Anyone that writes Java code is an API designer! It does not matter if the coders share their code with others or not, the code is still used; either by others, by themselves or both. Thus, it becomes important for all Java developer to know the fundamentals of good API design. A good API design requires careful thinking and a lot of experience. Luckily, we can learn from other clever people like Ference Mihaly, whose blog post inspired me to write this Java 8 API addendum. We relied heavily on his checklist when we designed the Speedment API. (I encourage you all to read his guide.) Getting it right from the start is important because once an API is published, a firm commitment is made to the people who are supposed to use it. As Joshua Block once said: ""Public APIs, like diamonds, are forever. You have one chance to get it right, so give it your best."" A well-designed API combines the best of two worlds, a firm and precise commitment combined with a high degree of implementation flexibility, eventually benefiting both the API designers and the API users. Why use a checklist? Getting the API right (i.e. defining the visible parts of a collection of Java classes) can be much harder than writing the implementation classes that makes up the actual work behind the API. It is really an art that few people master. Using a checklist allows the reader to avoid the most obvious mistakes, become a better programmer and save a lot of time. API designers are strongly encouraged to put themselves in the client code perspective and to optimize that view in terms of simplicity, ease-of-use, and consistency - rather than thinking about the actual API implementation. At the same time, they should try to hide as many implementation details as possible. Do not Return Null to Indicate the Absence of a Value Arguably, inconsistent null handling (resulting in the ubiquitous NullPointerException) is the single largest source of Java applications' errors historically. Some developers regard the introduction of the null concept as one of the worst mistakes ever made in the computer science domain. Luckily, the first step of alleviating Java's null handling problem was introduced in Java 8 with the advent of the Optional class. Make sure a method that can return a no-value returns an Optional instead of null. This clearly signals to the API users that the method may or may not return a value. Do not fall for the temptation to use null over Optional for performance reasons. Java 8's escape analysis will optimize away most Optional objects anyway. Avoid using Optionals in parameters and fields. Do This: Don't Do This: Do not Use Arrays to Pass Values to and From the API A significant API mistake was made when the Enum concept was introduced in Java 5. We all know that an Enum class has a method called values() that returns an array of all the Enum's distinct values. Now, because the Java framework must ensure that the client code cannot change the Enum's values (for example, by directly writing to the array), a copy of the internal array must be produced for each call to the value() method. This results in poor performance and also poor client code usability. If the Enum would have returned an unmodifiable List, that List could be reused for each call and the client code would have had access to a better and more useful model of the Enum's values. In the general case, consider exposing a Stream, if the API is to return a collection of elements. This clearly states that the result is read-only (as opposed to a List which has a set() method). It also allows the client code to easily collect the elements in another data structure or act on them on-the-fly. Furthermore, the API can lazily produce the elements as they become available (e.g. are pulled in from a file, a socket, or from a database). Again, Java 8's improved escape analysis will make sure that a minimum of objects are actually created on the Java heap. Do not use arrays as input parameters for methods either, since this - unless a defensive copy of the array is made - makes it possible for another thread to modify the content of the array during method execution. Do This: Don't Do This: Consider Adding Static Interface Methods to Provide a Single Entry Point for Object Creation Avoid allowing the client code to directly select an implementation class of an interface. Allowing client code to create implementation classes directly creates a much more direct coupling of the API and the client code. It also makes the API commitment much larger, since now we have to maintain all the implementation classes exactly as they can be observed from outside instead of just committing to the interface as such. Consider adding static interface methods, to allow the client code to create (potentially specialized) objects that implement the interface. For example, if we have an interface Point with two methods int x() and int y(), then we can expose a static method Point.of(int x, int y) that produces a (hidden) implementation of the interface. So, if x and y are both zero, we can return a special implementation class PointOrigoImpl (with no x or y fields), or else we return another class PointImpl that holds the given x and y values. Ensure that the implementation classes are in another package that are clearly not a part of the API (e.g. put the Point interface in com.company. product.shape and the implementations in com.company.product.internal.shape). Do This: Don't Do This: Favor Composition With Functional Interfaces and Lambdas Over Inheritence For good reasons, there can only be one super class for any given Java class. Furthermore, exposing abstract or base classes in your API that are supposed to be inherited by client code is a very big and problematic API commitment. Avoid API inheritance altogether, and instead consider providing static interface methods that take one or several lambda parameters and apply those given lambdas to a default internal API implementation class. This also creates a much clearer separation of concerns. For example, instead of inheriting from a public API class AbstractReader and overriding abstract void handleError(IOException ioe), it is better to expose a static method or a builder in the Reader interface that takes a Consumer<IOException> and applies it to an internal generic ReaderImpl. Do This: Don't Do This: Ensure That You Add the @FunctionalInterface Annotation to Functional Interfaces Tagging an interface with the @FunctionalInterface annotation signals that API users may use lambdas to implement the interface, and it also makes sure the interface remains usable for lambdas over time by preventing abstract methods from accidently being added to the API later on. Do This: Don't Do This: Avoid Overloading Methods With Functional Interfaces as Parameters If there are two or more functions with the same name that take functional interfaces as parameters, then this would likely create a lambda ambiguity on the client side. For example, if there are two Point methods add(Function<Point, String> renderer) and add(Predicate<Point> logCondition) and we try to call point.add(p -> p + "" lambda"") from the client code, the compiler is unable to determine which method to use and will produce an error. Instead, consider naming methods according to their specific use. Do This: Don't Do This: Avoid Overusing Default Methods in Interfaces Default methods can easily be added to interfaces and sometimes it makes sense to do that. For example, a method that is expected to be the same for any implementing class and that is short and ""fundamental"" in its functionality, is a viable candidate for a default implementation. Also, when an API is expanded, it sometimes makes sense to provide a default interface method for backward compatibility reasons. As we all know, functional interfaces contain exactly one abstract method, so default methods provide an escape hatch when additional methods must be added. However, avoid having the API interface evolve to an implementation class by polluting it with unnecessary implementation concerns. If in doubt, consider moving the method logic to a separate utility class and/or place it in the implementing classes. Do This: Don't Do This: Ensure That the API Methods Check the Parameter Invariants Before They Are Acted Upon Historically, people have been sloppy in making sure to validate method input parameters. So, when a resulting error occurs later on, the real reason becomes obscured and hidden deep down the stack trace. Ensure that parameters are checked for nulls and any valid range constrains or preconditions before the parameters are ever used in the implementing classes. Do not fall for the temptation to skip parameter checks for performance reasons. The JVM will be able to optimize away redundant checking and produce efficient code. Make use of the Objects.requireNonNull() method. Parameter checking is also an important way to enforce the API's contract. If the API was not supposed to accept nulls but did anyhow, users will become confused. Do This: DON'T DO THIS: Do not Simply Call Optional.get() The API designers of Java 8 made a mistake when they selected the name Optional.get() when it should really have been named Optional.getOrThrow() or something similar instead. Calling get() without checking if a value is present with the Optional.isPresent() method is a very common mistake which fully negates the null elimination features Optional originally promised. Consider using any of the Optional's other methods such as map(), flatMap() or ifPresent() instead in the API's implementing classes or ensure that isPresent() is called before any get() is called. Do This: Don't Do This: Consider Separating Your Stream Pipeline on Distinct Lines in Implementing API Classes Eventually, all APIs will contain errors. When receiving stack traces from API users, it is often much easier to determine the actual cause of the error if a Stream pipeline is split into distinct lines compared to a Stream pipeline that is expressed on a single line. Also, code readability will improve. Do This: Don't Do This: For more insights on Jigsaw, reactive microservices, and more get your free copy of the new DZone Guide to Modern Java, Volume II!",en,86
129,2496,1475518627,CONTENT SHARED,-6654470039478316910,-4465926797008424436,6720261163003363785,"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.116 Safari/537.36",SP,BR,HTML,https://realm.io/news/chiu-ki-chan-advanced-android-espresso-testing/,advanced android espresso,"Espresso is a very powerful UI testing framework for Android. Chiu-Ki Chan describes several techniques to make the most of it, running through: combining matchers ( withParent , isAssignableFrom ) to pinpoint your view; onData and RecyclerViewAction s to test ListView and RecyclerView ; Custom ViewAction and ViewAssertion ; and using Dagger and Mockito to write repeatable tests. What is Espresso? (00:28) Espresso is automatic UI testing ( ""no hands testing!"" ). Once you write the code, Espresso will execute using events: do various things that as a normal user you will do, then verify the test (different from JUnit testing where there is no UI element). In Espresso you can see what is happening. It is a first-party library (Google wrote it under the Android testing support library ). It simulates User interactions (clicking, typing), and it does automatic synchronization of test action with app UI . Espresso will watch the UI thread (the thread that renders all the pixels on screen). Before Espresso, I used ActivityInstrumentationTestCase2 : my test would fail because the button was not drawn yet and I was trying to click on it. You put in some Sleep statements hoping that after Sleeping the pixels will be drawn and then you can click on it. With the automatic synchronization Espresso will loop the Sleep until the UI thread is idle then it will verify and click things. You do not need to Sleep. Espresso has a fluent API (can tack things one after another). The first thing: locate the view that you want to act on. Espresso has the concept of ViewMatchers. Once you locate the view then you can perform actions to it (typing, clicking, swiping). You can check using ViewAssertions. Formula Basic Espresso test: The Hello World app. It has a greet button - when you click on it it will disable it. Using this formula we first need to locate this view: ViewMatcher Espresso comes with a ViewMatcher - withId. In our case we want to look for the view with the R.id.greet_button . We will use the ViewAction click() to click on it. Then, we want to assert that the greet button is no longer enabled. ViewAction In this ViewAssertion (see below), instead of it is not enabled, we are doing it is enabled and then negating it. You can combine different Matchers and Assertions to do exactly what you need. ViewAssertion More info: Espresso library (05:03) Highlighted in yellow (withId, click, matches, isEnabled): comes within Espresso. They are Android-specific: Hamcrest library (06:00) There is also not , from the Hamcrest library (Java library). It has different logic startWith , endWith , if we are dealing with strings and not and allOf . Together with the Espresso patch (Android-specific) and the Java pod you can do very powerful expressions. More Info: Combining Matchers (06:17) Let's say that you wrote this app and you want to verify the Toolbar title is displaying the correct string. Your layout does not contain that view (you do not have the ID). To find this view, we will use Hierarchy Viewer (examines the hierarchy of your app). You have a root node: all the different views hang off it and it shows you the structure (on the screen, not just the path that you set with setContentView ). And the TextView is the one that has the ""My Awesome Title"" string (the one that you want to verify). We have located the view that we want to match. Bad news: There is no ID. But using all the different Matchers, I know that this TextView has a parent (Toolbar). It can do a combination - I will find a view that has two conditions isAssignableFrom : 1) TextView (I am a TextView, I want to find a view that is a TextView), 2) Toolbar (find that one TextView that is under a Toolbar). The reason why we want to do that with two conditions (instead of just the TextView one) is that there can be multiple TextViews. To pinpoint that one particular view I will use allOf Matcher. Then I wrap it into a helper function; every time I want to match a Toolbar I can just call Match Toolbar and then pass a string. Custom Matchers (09:07) You can write a custom Matcher. The previous example works, but it is very fragile (Toolbar class: today, it contains the TextView as the direct child of a Toolbar; tomorrow, they may want to wrap it inside CoordinatorLayout because is not a part of public API). The Toolbar class has a function toolbar.getTitle() which, since it is in the public API, it is going to be stable. Instead of trying to pinpoint the TextView, the ViewMatcher takes on a view that is assignable from the Toolbar class. We are matching on the Toolbar class and we are going to verify that it has the Toolbar title (which is a custom Matcher). With Toolbar title generates a custom Matcher. The BoundedMatcher only works on the Toolbar. We have access to the Toolbar functions. In Match Safely function we wanted to use the getTitle out of the Toolbar and matches the text that we give it. Instead of passing a string you want to match, you are passing a TextMatcher . This way you can use all the Hamcrest test Matchers (such as equalTo ), which is your plain old comparison. Y You can do Prefix Matching, Suffix Matching or Contain String (if you just want the string to be anywhere). If you want to use a function that is not a part of Espresso you can write your own Matcher. More info: ListView operates on a different way. Instead of looking for a view, you are looking for a particular object in the Adapter that is supporting the ListView. That is why your onData takes an object Matcher. You have the same Perform, View Action, Check, View Assertion. And on top, Data options. You can have multiple ListViews. You can pinpoint it in with inAdapterview or you can look inside the item and match on the particular child view. I have a ListView and it has a list of numbers. I want to run a test to verify that item 27 stays 27. I am looking for the item that stays 27, it will scroll and look for it. The basic setup of the app: I have an Adapter (backed by an array of integers), and I will set it as the data for this ListView . The item is a wrapper around a simple integer that has a two string function (the ArrayAdapter knows what to display). Also, when you click on an item the footer TextView it will display that value. We also want to verify that. We are going to verify that the bottom TextView is not displayed. I said, withID this text and I want to match it as not displayed. I am going to use the onData function to look for the item with value 27 inside the AdapterView that has the ID list. Once I find it I will click on it. After I click on it, I will then verify that the bottom TextView is containing the text 27 and also it is also displayed. You have the onView with text ID. And I have performed two checks. Rather than looking for the view again, I can chain them. It will check for the first condition (it has the text 27), and then it will check that it is displayed. You can keep stacking on things, you can perform actions and check, and then perform action. It does not need to go look for the view again. You already have the view in your hand. withValue is a custom Matcher that I wrote. BoundedMatcher takes in an integer and in the Match Safely function it compares to the value of the item using strings. It is a string comparison. We cannot use onData: RecyclerView is a ViewGroup (not AdapterView ). The onData operator only works on AdapterView - it is not just ListView , you can use it with GridView as well. We are going back to using the onView View Matchers. We will use that onView and perform check formula. To click on item is different. We will do the same pre-condition checking that the text is not displayed, but to click on that particular list item we are going to say that onView with ID RecyclerView . We are operating on that RecyclerView level, perform, and then we will use RecyclerViewActions (from the Espresso library). We will have a specific action ( actionOnItemAtPosition(27) ). You look for position item 27 and perform the action click on it. actionOnItemAtPosition is very different (see code below): we are no longer doing Data Matching and we need to look for the position and then perform it all at once in this one giant View Action in it. The rest of the code is the same: we will continue to verify that it displays 27 and it is not hidden. This replaced the middle line (onView). Two ways of thinking, one the AdapterView of looking for things focuses on the data, but if you are doing RecyclerView then you have to go back and focus on the view again. You find the RecyclerView and act on it with the RecyclerViewActions . There are a couple of other ones. If you do not want to look for an at position you can also look for the View Holder which then you have a Matcher that is looking for a specific ViewHolder or you can also do the action item with a ViewMatcher . Make sure that you know they are different. More information: Idling Resource (19:43) Espresso has this notion of the UI queue being idle - no more UI events handled, are queued. That includes clicks, rendering. It also has the notion of idling: the AsyncTask pool is empty. But beyond that it does not know how to wait for your app to be ready to go. Espresso provides a nice framework for you to write your own idling conditions (Idling Resources). Example : Wait until an IntentService is not running ( IntentServiceIdlingResource ). Whenever you write an Item Resource you need to Override three functions: getName , registerIdleTransitionCallback , and isIdleNow . Make sure that you have a unique name (e.g. class name) because you are going to be registering this Idling Resource later, and you use the name as the key. Then register the IdleTransitionCallback you stash that callback into a particular number variable so that in isIdleNow now you can call it when you determine that whatever that you are waiting for is done. You can tell Espresso, ""I am idle, go ahead"". In our particular case we are going to define idle as the IntentService is not running ( isIntentServiceRunning() ). Then I have a callback, I will call it and it will return the boolean. I am going to query the ActivityManager for the specific name (IntentService). If it is there then it is running, I will return true; if I could not find it then I return false. After you have defined your custom Idling resource, you need to register it (most of the time this is JUnit4 syntax). I have an annotation that is before and after - you can register it before your tests run. And then after your test is run unregister it. More info: Dagger and Mockito (31:03) Dagger is a framework for Dependency injection: you have a central repository of classes. We are trying to make tests work: we are doing to use Dagger to provide different objects for app and test; we will use Mockito to mock objects in test. In Dagger2 you are going to define a Component (a collection of different modules that can provide classes to your app and your test). In our case we are going to do something with the clock. It is a classic example because if your app depends on the current time then you cannot verify anything because the current time changes. In the ApplicationComponent we are going to have a real clock module (provides the actual current time) and in the test we are going to have a module that is mocked (using annotation). You have the app's Component and then you have the list of Modules. Dagger is going to then go ahead and do co-generation for you at compile time. You can then call these functions in your app. In sum, you have an ApplicationComponent that provides a clock module and then you also have a TestComponent that provides a mock clock module. The ApplicationComponent only injects into the main activity, it is not aware of the existence of tests. The TestComponent need to inject to both and they are singletons. If you are providing this particular object and then you are changing it in your test the app is going to use the exact same object so that it gets the same value. In your application, when you are onCreate you are creating the application. DaggerDemoApplication_ApplicationComponent is the auto-generated class. It will go through all your annotations, which will allow you to create a component (collection of modules). In your test you are going to be able to use the setComponent function which is defined on the activity, which will provide a different set of modules. With Mockito you can control time. In Joba-Time (a pretty popular dateTime libray for Java) dateTime will give you the current day and time. In your test Mockito allows you to say when someone calls the function getNow on my mock clock return this particular point in time. When you run your test you will be able to do something like this. Maybe you have a TextView that displays today's date. You can verify that it will display the string 2008-09-23 because that is the time the clock is going to provide. Without this mocking, you run the test today you return this value, you run the test tomorrow you return this other value. This is the power of combining Mockito and Dagger and Espresso so that they all work together. You can write test that pretends to be a human, but also in a predictable environment so that you can keep running the test and it will pass unless you wrote a bug in your future code. More info - Blog post abotu Mockito - Example repo - Example using the Shared Preferences Running through the basics (ViewMatcher, ViewAction, ViewAssertion) I showed you how to match the Toolbar using two different ways (combining Matchers, writing your own custom Matcher). I went through ListView and RecyclerView, and how they differ. Spent some time on IdlingResource, and Dagger and Mockito to get you to the next level. I have open souced a complete app . It uses: the Google Plus API, the Nearby API and the Database. At first glance it is really hard to test. It asks you to log into Google Plus. I have to set up a device that has a certain Google Plus account and then I have to log in; next thing it is going to use the Nearby API. It will spell a word depending on the name of the person that you are Google Plus profile logged in as. I used the same technique that I described, Dagger, Mockito to mock stuff. Mockito is more advanced because I need to capture the Google sign in with the listener and then replay it right back. It also has other testing. I have some models, POJO's, I set some function then I verify the conditions. I also have UI-less Instrumentation (to test the Database). I am not executing the UI, but I also need context because it is still going through Android classes. And, of course, it also has Espresso. Thank you! Resources & Q&A (32:28) More links: Q: For the combined Joda-Time and, Mockito and Espresso example, where did you initialize Joda-Time? Did you rely on initializing it with the application? Chiu-Ki: I just call new dateTime. Q: You have to initialize it with a context at some point early in your application lifecycle, and it is suggested that you do it in application onCreate. But I know that we have also been talking about separating test environments so that your, your context is perhaps different than it would be in the normal run time. I am wondering do you have to do anything special to initialize Joda-Time in the test context? Chiu-Ki: I do not and so far nothing blew up, but maybe some other library needs the context. It is not specific to Joda-Time. Currently this set-up only has the clock module component. You will have an additional module that is an Android module which is capable of providing a context. Then in your clock module in the constructor it will take in a context which will be provided by the Android module. And Dagger is smart enough to just do essentially Class Matching. And then the Android module raises its hand, I know how to give you a context. And then it will provide that. And then within the clock module, you will be able to use that. Once you have that context in the constructor of your clock module then you can come back here to the getNow function inside your clock module and then use the context there. Q: Question specific to the Dagger2 example that you showed. The difficulty in Dagger2 is really, there is no module override available in Dagger2. I wanted to know what would be your recommend workaround for being able to override specific dependencies inside of your mock modules without having to completely rewrite, the entire module. You want to have code duplication on the test classes and also on the application. Your example only has the clock module which provides only one dependency, but if you have, 50 dependencies and you only need to override just the one. You would not want to create a test module with 49 real classes and only one mock. What would be the best workaround? Chiu-Ki: It has been a pain point for Dagger2, mostly because it is available in Dagger1! Why did you take that away from us? I have not personally encountered that just because the way I set up my modules, but I have heard people, what they have done is that, you have your app source and then your test source. you can have another source that is the common source which contains the modules that are shared between the test and the app and then when you make your component that is for the app you import those, you include those. And then the test also includes the same ones. And then you only implement each thing that is different once. I heard people do that. Q: I ran into one issue I exactly used the same set-up where I had one class, I had a simple UI which was just fetching a list of GitHub issues at the click of a button and the dependency, the API dependency, was being injected by Dagger and I replaced it by a mock inside one of my modules. I only wanted to verify interaction with that mock once the button's clicked. But because that mock did not return any action that would affect the UI thread, then it was hanging forever. I want to verify interaction with a mock, but I do not want to have to code up an action which is going to affect the UI thread. I just want to verify interaction with a mock. Is there a workaround for things like this? Chiu-Ki: It is a network call, except that you are mocking the result; Is that triggered by a button click or any user action? When does the fetching start? Q: You click the button, it talks to that injected mock. Then that injected mock, you are passing a callback method which will just return a list of issues which will populate an adapter, for example. Chiu-Ki: in that case I would expect it to, instead of hang, go too fast. It did not wait until the mock to come back and was starting to verify things. I was not sure why it was behaving that way. I am not able to diagnose why that happened because that is not the expected behavior that I would be. Q: The fix for me was to take that mock and just use Mockito to manually call the callback that the activity was passing. Chiu-Ki: Espresso testing at that point. You do not really need to, because then nobody is clicking anything. Q: The UI's wired up properly with the back end. You click a button and you want to make sure that when Espresso clicks this button then there is interaction with a mock. Meaning that clicking this button effectively tried to do the API call, you do not care about what the API call returned, but you want to verify that there is some interaction with that mock. Chiu-Ki: Write a blog post, Twitter to me and I will take a look because I feel like there is more to what we can describe. Q: I find one of my major pain points is coming up with complex Macthers for hierarchies that are much more complex than what you showed. I was wondering if you have any general suggestions in finding those complex Matchers perhaps with multiple parents and multiple siblings in a way that will not be fragile. Chiu-Ki: My recommendation would be to simplify your View Hierarchy. Wrap things in the CustomView so that semantically they make more sense. The reason why you have all these things laid out is because probably when you are looking at the UI they are not individual buttons and individual tacks. They are probably logical units. I think it will help not just testing, but your actual app and development if you can group them in logical units. Then at that point then you can hide the internal logic and say, this manual item thing has six things inside, but then I can get them with programmatic calls like the getTitle one. Q: One hardest problems is when elements repeat themselves, such as if you are using a ListView and each item of the ListView has several buttons. It can be difficult to find a container maybe with the text you are looking for and then go back down the chain after that to get to the buttons within the hierarchy. Is there anything that would help sort of expose the chain to get to a path? Chiu-Ki: You can see who is a child of whom; that helps you to expose the structure. One thing that you could do is a helper function. You encapsulate that, every time you are trying to mesh this particular way of setting up you can at least repeat it. I basically smell bad code when you tell me that your view explodes with too many children and too many level deep and it is really difficult to find things. Without going through your specific example it is just really hard to give general advice.",en,86
130,2787,1480422617,CONTENT SHARED,3739926497176994524,8676130229735483748,-2273278933867542008,"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36",SP,BR,HTML,http://meiobit.com/355936/a-chegada-arrival-review-sem-spoilers/,resenha - a chegada,"Supondo que haja vida inteligente no Universo além da civilização humana, ela ainda não fez ou não conseguiu fazer contato. Talvez essa vida inteligente alienígena ainda não esteja em um nível tecnológico que possibilite que a detectemos ou simplesmente os ETs estejam com vergonha de dividir o Universo com a gente. Ou também há a triste possibilidade de os humanos serem a forma vida mais inteligente no Universo conhecido : formas de vida extra-terrestre ainda estariam a evoluir. Supondo que haja sim vida inteligente alienígena e desconsiderando a recomendação de Stephen Hawking , como seria um primeiro contato dos ETs com os seres humanos? Essa é a premissa de um bocado de filmes, séries e tantas outras obras de ficção científica: imaginar como seria um primeiro contato com uma civilização alienígena bem desenvolvida que acabou de chegar ao nosso pequeno planeta azul. A maioria dessas histórias, quando estas não tentam aniquilar a raça humana, não foca tanto na dificuldade que é lidar com uma linguagem totalmente desconhecida: em muitas delas os alienígenas são tão avançados que já sabem inglês. Assim fica fácil, mas e quando os ETs falam de um jeito, escrevem de outro e nada parece ser compreensível por humanos? Como saber o propósito dos visitantes em nosso planeta? Essa é a premissa de . Sony Pictures Brasil - A Chegada | Amy Adams | Hoje nos cinemas Baseado no conto (que Story of Your Life de Ted Chiang, o recente filme da Paramount graças a Odin foi distribuído pela Sony Pictures fora da América do Norte) retrata a dificuldade da lingüista Louise Banks (Amy Adams) e do físico Ian Donnelly (Jeremy Renner) em estabelecer um primeiro contato com seres alienígenas. Seres esses que chegaram ao planeta à bordo de uma dúzia de enormes naves em forma de concha. As naves pairam sobre 12 diversos locais de forma aleatória. Vários países mandam diferentes equipes de militares e cientistas para um primeiro contato. O foco de A Chegada é todo sobre o ponto de vista do imenso trabalho lingüístico de Jó da personagem Louise (Adams) em interpretar os símbolos escritos pelos alienígenas, supostamente pacíficos. E a árdua tarefa de convencer os militares a não matar logo de cara os simpáticos visitantes extra-terrestres com 7 pernas sem antes conhecer o propósito da visita. É um trabalho tão grande, a personagem fica tão imersa no idioma dos heptapodes que ele afeta sua percepção de tempo. E infelizmente uma interpretação bem errada acaba por fazer com que os militares de outra região declarem guerra aos ETs. O filme tem claramente um vilão e ele é a estupidez humana, que desconhece limites. A Chegada foi dirigido pelo mesmo Denis Villeneuve do tenso e isso foi excelente: enquanto em Sicario o espectador não tem descanso algum, em A Chegada quem não parece ter descanso é a protagonista. Seja pela curiosidade, seja pela fascinação pelos visitantes. Ou seja pelo medo... dos humanos. A produção escolheu uma excelente locação para a ""concha"" extra-terrestre pairar e a fotografia é simplesmente linda. O filme se passa quase todo em Montana mas escolheram um belo lugar no Canadá. Se bem que uma das naves paira sobre a Venezuela e a fotografia dos telejornais nos mostra o nível de barbárie que é aquela região. Melhor sorte teve Cuba. Enfim, sobre a trilha sonora original, ela muitas vezes nos faz temer os visitantes. E nos envolve quanto ao drama temporal da Louise. O roteiro é assinado pelo Eric Heisserer ( ele é mais conhecido nos filmes de terror ) e só tem um ou outro problema de ritmo. Tal detalhe não chega a comprometer a obra. Considerações A Chegada é basicamente (1997) feito direito. E sem as aloprações do longo Interestelar . O filme vale cada centavo do ingresso e dura o que tem de durar sem desrespeitar a inteligência do espectador. Não é um filme para todos, assim como Sicario também não era. 4,5 de 5 Lois Lanes. Relacionados: A Chegada , Abbott and Costello , Amy Adams , arrival , Blade Runner 2049 , Denis Villeneuve , Eric Heisserer , Ficção Científica , Forest Whitaker , Jeremy Renner , Sapir-Whorf , Sicario , Sony , Sony Pictures , Story of Your Life , Ted Chiang , The Arrival",pt,85
131,1438,1466220945,CONTENT SHARED,5302085907556673015,-1032019229384696495,7678165322893628235,,,,HTML,http://www.theverge.com/2016/6/17/11964786/google-lady-cfo-protest-response,google employees are adding 'lady' to their job titles to fight sexism,"More than 800 members of Google's staff are standing together in a showing against sexism today by appending a single word to their job titles: ""Lady."" Business Insider has details on the protest , which is happening in response to a ludicrous comment made during Alphabet's shareholder meeting last week, when someone referred to company CFO Ruth Porat as the organization's "" lady CFO ."" The internet immediately exploded in outrage over the sexist remark, and now, one week later, Google's staff has found its own way of responding. According to Business Insider , someone made the title change suggestion to an email group, and it quickly caught on. They launched an internal site to promote the protest, which was planned for this Thursday and Friday. It even got this awesome GIF featuring some of Google's newly proposed emoji : Google In a statement to USA Today , Meg Mason, Google's partner operations manager for shopping - or, for today, its lady partner operations manager for shopping - says of the protest, ""I wanted to do something fun and 'Googley' that allowed us all to stand together, and to show that someone's gender is entirely irrelevant to how they do their job."" Women and men within the company are adopting the lady prefix, making the change over these two days in their email signatures and within Google's internal directory, according to Business Insider . It's a smart and simple response that, like Mason says, effectively conveys just how irrelevant gender is to being able to do your job.",en,85
132,1408,1466012967,CONTENT SHARED,-9019233957195913605,-9016528795238256703,8012540432966887758,,,,HTML,http://observatoriodaimprensa.com.br/curadoria-de-noticias/berners-lee-quer-criar-outra-web/,berners-lee quer criar outra web,"Tim Berners-Lee, o cientista que há 27 anos criou a World Wide Web, mais conhecida como apenas Web, pretende desenvolver um novo sistema de interação entre computadores para neutralizar o controle governamental e privado sobre as comunicações entre indivíduos. Berners-Lee reuniu em San Francisco, na Califórnia, um grupo de especialistas em computação para tentar criar uma rede menos sujeita a violações da privacidade e dê aos usuários maior controle sobre seus dados pessoais. Os detalhes sobre o que o grupo de cientistas está pesquisando ainda são pouco conhecidos, mas Lee afirmou que o trabalho parte dos estudos sobre a infraestrutura de programação digital da moeda eletrônica Bitcoin, do site Wikileaks e os programas de troca gratuita de musicas. O projeto foi batizado de Web Descentralizada e foi discutido no Decentralized Web Summit , na segunda semana de junho de 2016. A grande preocupação do grupo de cientistas da Web Descentralizada é o aumento do controle de redes sociais como o Facebook , do sistema de buscas Google e o de comércio eletrônico da Amazon sobre o uso da Web. Mais detalhes na reportagem The Web's Creator Looks to Reinvent It (acesso por meio de cadastramento grátis) Todos os comentários",pt,85
133,1894,1469624606,CONTENT SHARED,8749720044741011597,-2525380383541287600,-7324931632676877948,,,,HTML,https://br.udacity.com/course/how-to-use-git-and-github--ud775/,como usar o git e o github,"Lição 1: Navegando em um Histórico de Commits Nesta lição, você aprenderá sobre alguns tipos diferentes de sistemas de controle de versão e descobrir o que torna Git um grande sistema de controle de versão para programadores. Você também vai começar a praticar usando Git para ver o histórico de um projeto existente. Você vai aprender a ver todas as versões que foram salvas, fazer o checkout de uma versão anterior e comparar duas versões diferentes Lição 2: Criar e Modificar um Repositório Nesta lição, você aprenderá como criar um repositório e salvar versões de seu projeto. Você vai aprender sobre a área de teste, fazendo commit do seu código, branches e mesclagem, e como você pode usá-los para torná-lo mais eficiente e eficaz Lição 3: Usando GitHub para Colaborar Nesta lição, você vai começar a prática usando GitHub ou outros repositórios remotos para compartilhar suas alterações com os outros e colaborar em projetos multi-desenvolvedor. Você vai aprender como fazer e rever uma solicitação de recebimento no GitHub. Finalmente, você vai começar a prática através da colaboração com outros estudantes Udacity para escrever uma história ""criando sua própria aventura"". Projeto: Contribuir com um projeto ao vivo Os alunos irão publicar um repositório que contém suas reflexões do curso e enviar uma solicitação de recebimento a uma História colaborativa ""criando sua própria aventura"".",pt,85
134,1357,1465667023,CONTENT SHARED,-3027055440570405664,7774613525190730745,-5387740831049193382,,,,HTML,http://joelonsoftware.com/items/2016/05/30.html?utm_source=clouddevweekly&utm_medium=email,joel on software,"Introducing HyperDev One more thing... It's been awhile since we launched a whole new product at Fog Creek Software (the last one was Trello , and that's doing pretty well ). Today we're announcing the public beta of HyperDev , a developer playground for building full-stack web-apps fast. HyperDev is going to be the fastest way to bang out code and get it running on the internet. We want to eliminate 100% of the complicated administrative details around getting code up and running on a website. The best way to explain that is with a little tour. Step one. You go to hyperdev.com. Boom. Your new website is already running. You have your own private virtual machine (well, really it's a container but you don't have to care about that or know what that means) running on the internet at its own, custom URL which you can already give people and they can already go to it and see the simple code we started you out with. All that happened just because you went to hyperdev.com. Notice what you DIDN'T do. You didn't make an account. You didn't use Git. Or any version control, really. You didn't deal with name servers. You didn't sign up with a hosting provider. You didn't provision a server. You didn't install an operating system or a LAMP stack or Node or operating systems or anything. You didn't configure the server. You didn't figure out how to integrate and deploy your code. You just went to hyperdev.com. Try it now! What do you see in your browser? Well, you're seeing a basic IDE. There's a little button that says SHOW and when you click on that, another browser window opens up showing you your website as it appears to the world. Notice that we invented a unique name for you. Over there in the IDE, in the bottom left, you see some client side files. One of them is called index.html. You know what to do, right? Click on index.html and make a couple of changes to the text. Now here's something that is already a little bit magic... As you type changes into the IDE, without saving, those changes are deploying to your new web server and we're refreshing the web browser for you, so those changes are appearing almost instantly, both in your browser and for anyone else on the internet visiting your URL. Again, notice what you DIDN'T do: You didn't hit a ""save"" button. You didn't commit to Git. You didn't push. You didn't run a deployment script. You didn't restart the web server. You didn't refresh the page on your web browser. You just typed some changes and BOOM they appeared. OK, so far so good. That's a little bit like jsFiddle or Stack Overflow snippets, right? NBD. But let's look around the IDE some more. In the top left, you see some server side files. These are actual code that actually runs on the actual (virtual) server that we're running for you. It's running node. If you go into the server.js file you see a bunch of JavaScript. Now change something there, and watch your window over on the right. Magic again... the changes you are making to the server-side Javascript code are already deployed and they're already showing up live in the web browser you're pointing at your URL. Literally every change you make is instantly saved, uploaded to the server, the server is restarted with the new code, and your browser is refreshed, all within half a second. So now your server-side code changes are instantly deployed, and once again, notice that you didn't: Save Do Git incantations Deploy Buy and configure a continuous integration solution Restart anything Send any SIGHUPs You just changed the code and it was already reflected on the live server. Now you're starting to get the idea of HyperDev. It's just a SUPER FAST way to get running code up on the internet without dealing with any administrative headaches that are not related to your code. Ok, now I think I know the next question you're going to ask me. ""Wait a minute,"" you're going to ask. ""If I'm not using Git, is this a single-developer solution?"" No. There's an Invite button in the top left. You can use that to get a link that you give your friends. When they go to that link, they'll be editing, live, with you, in the same documents. It's a magical kind of team programming where everything shows up instantly, like Trello, or Google Docs. It is a magical thing to collaborate with a team of two or three or four people banging away on different parts of the code at the same time without a source control system. It's remarkably productive; you can dive in and help each other or you can each work on different parts of the code. ""This doesn't make sense. How is the code not permanently broken? You can't just sync all our changes continuously!"" You'd be surprised just how well it does work, for most small teams and most simple programming projects. Listen, this is not the future of all software development. Professional software development teams will continue to use professional, robust tools like Git and that's great. But it's surprising how just having continuous merging and reliable Undo solves the ""version control"" problem for all kinds of simple coding problems. And it really does create an insanely addictive form of collaboration that supercharges your team productivity. ""What if I literally type 'DELETE * FROM USERS' on my way to typing 'WHERE id=9283', do I lose all my user data?"" Erm... yes. Don't do that. This doesn't come up that often, to be honest, and we're going to add the world's simplest ""branch"" feature so that optionally you can have a ""dev"" and ""live"" branch, but for now, yeah, you'd be surprised at how well this works in practice even though in theory it sounds terrifying. ""Does it have to be JavaScript?"" Right now the server we gave you is running Node so today it has to be JavaScript. We'll add other languages soon. ""What can I do with my server?"" Anything you can do in Node. You can add any package you want just by editing package.json. So literally any working JavaScript you want to cut and paste from Stack Overflow is going to work fine. ""Is my server always up?"" If you don't use it for a while, we'll put your server to sleep, but it will never take more than a few seconds to restart. But yes for all intents and purposes, you can treat it like a reasonably reliably, 24/7 web server. This is still a beta so don't ask me how many 9's. You can have all the 8's you want. ""Why would I trust my website to you? What if you go out of business?"" There's nothing special about the container we gave you; it's a generic VM running Node. There's nothing special about the way we told you to write code; we do not give you special frameworks or libraries that will lock you in. Download your source code and host it anywhere and you're back in business. ""How are you going to make money off of this?"" Aaaaaah! why do you care! But seriously, the current plan is to have a free version for public / open source code you don't mind sharing with the world. If you want private code, much like private repos, there will eventually be paid plans, and we'll have corporate and enterprise versions. For now it's all just a beta so don't worry too much about that! ""What is the point of this Joel?"" As developers we have fantastic sets of amazing tools for building, creating, managing, testing, and deploying our source code. They're powerful and can do anything you might need. But they're usually too complex and too complicated for very simple projects. Useful little bits of code never get written because you dread the administration of setting up a new dev environment, source code repo, and server. New programmers and students are overwhelmed by the complexity of distributed version control when they're still learning to write a while loop. Apps that might solve real problems never get written because of the friction of getting started. Our theory here is that HyperDev can remove all the barriers to getting started and building useful things, and more great things will get built. ""What now?"" Really? Just go to HyperDev and start playing! HyperDev is the developer playground for building full-stack web apps, fast. Want to know more? You're reading Joel on Software , stuffed with years and years of completely raving mad articles about software development, managing software teams, designing user interfaces, running successful software companies, and rubber duckies. About the author. I'm Joel Spolsky, co-founder of Trello and Fog Creek Software , and CEO of Stack Overflow . More about me.",en,84
135,2722,1478623412,CONTENT SHARED,-7423191370472335463,-4465926797008424436,-4234938118093547320,"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.71 Safari/537.36",SP,BR,HTML,https://medium.com/android-dev-br/espresso-intents-n%C3%A3o-%C3%A9-m%C3%A1gica-%C3%A9-tecnologia-1fcfc8f21d3b,"espresso intents: não é magia, é tecnologia! - android dev br","Se você leu meu último artigo sobre Testes unitários vs aceitação , já ficou bem claro como é difícil controlar o ambiente de teste. Existem diversas API's do próprio Espresso como ActivityMonitor e IdlingResources que nos ajudam nessa empreitada. Utilizando esses dois recursos, já conseguimos criar excelentes testes de aceitação que nos salvam a vida. Mas, nem tudo é mar de rosas. Imagine um cenário como esse: Como iremos voltar para nosso app? A não ser que criamos uma implementação da câmera com a "" casca "" do seu device, fica bem chato testar esse cenário. E agora, Sica, quem poderá nos ajudar? É exatamente sobre isso que esse artigo fala: o herói Espresso Intents . O problema Um dos principais pilares do nosso querido Android são as . Como o nome diz, uma Intent é quando você tem a intenção (olha só) de fazer algo: um objeto de mensagem que pode ser usado para solicitar uma ação de outro componente de uma aplicação. Por exemplo, para abrir a câmera, precisamos enviar uma Intent solicitando o app de câmera que o usuário tem no aparelho: Há três casos de uso fundamentais na utilização das Intent's : para iniciar sua Activity , Service ou enviar um Broadcast . Ou seja, para iniciar (ou utilizar) qualquer coisa, você irá precisar de uma Intent e seus devidos extras ( B ). Precisamos repetir esse processo o tempo todo: seja para utilizar aplicações do próprio device ou até para aplicativos de terceiros. No ambiente de teste Só de ler o parágrafo acima você já deve ter pensado duas vezes em implementar um teste na sua classe que utiliza esses recursos, né? Bom, já que qualquer ação para inicializar algo necessita de uma Intent , fica muito difícil você ter certeza, nos seus testes, que as coisas estão acontecendo bem. E é exatamente isso que o Espresso Intents resolve: você consegue controlar todas as Intents que são disparadas (ou recebidas) pela sua aplicação. Como funciona O Espresso Intents é uma extensão do Espresso que possibilita você validar e fazer stubbings das Intents que são enviadas e recebidas pela sua aplicação dentro do ambiente de teste. Sabe o ? Então, é tipo isso, só que pra Intents . Só de ser possível comparar com o Mockito , já mostra como essa API é valiosa para nossos testes. Vamos entender a sua estrutura: Adeus ActivityTestRule. Olá IntentsTestRule. Para iniciar nossa Activity , dentro de um teste anotado, declaramos nossa ActivityTestRule como uma do e podemos iniciar qualquer Activity que desejamos: Para inicializar o Espresso Intents , você precisa inicializar a API e liberá-la após o uso: Mas, pra mim, é bem chato lembrar de fazer isso em todo teste. Para a nossa alegria, existe a IntentsTestRule que herda da ActivityTestRule . A IntentsTestRule , por sua vez, facilita a API do Espresso Intents funcionar. Ela inicia o Espresso Intents antes de cada teste anotado com nossa famosa anotação e, quando esse teste terminar, ele irá finalizar a execução de toda a API. Tá, mas o que isso faz? Basicamente te torna um mágico das Intents , conseguindo validá-las e controlá-las como um ótimo Merlin . Stubbing de Intent Agora que você é o Merlin, você tem o poder (lê-se algum programador facilitou isso pra você) de gravar todas as Intents que irão iniciar alguma Activity dentro da sua aplicação. Já que você tem todas elas, o Espresso Intents tem dois métodos que fazem toda essa mágica. Utilizando o mesmo exemplo de abrir a câmera, você consegue utilizar o método para retornar um resultado desejado: Validando uma Intent Nesse caso você estava recebendo uma Intent . Agora, avalia essa situação: Se você deixar essa sua Intent vazar, você irá sair do contexto da sua aplicação, onde seus poderes de Merlin surtem efeito, e entrar no ambiente do nosso amiguinho Android, e para interagir com qualquer elemento da tela você irá precisar do UI-Automator . Só de pensar nesse teste eu já abri a boca de sono. Mas, como iremos testar esse cenário? Aí que temos o outro método mágico, o . Com ele, validar alguma Intent, e a combinando com o método , você é capaz de de isolar totalmente seu ambiente de teste: Agora sim, dá pra testar. Limitações No dia a dia, sinto muito a falta de conseguir interceptar uma Intent para abrir outra Activity, e modificar o conteúdo dentro dela, por exemplo: Isso seria ótimo, já que em um ambiente de CI (emulador) não há imagem alguma. Ou seja, você precisa fornecer essas imagens de alguma maneira. Mas, atualmente, isso não é suportado, o que nos obriga a fazer uma implementação menos ""elegante"" . Projeto de exemplo Calma que todo esse código não vai ficar só em um post não. Criei esse projeto de exemplo que utiliza algumas situações interessantes e como você pode viajar no uso dessa API. Conseguiu implementar algo legal? Não hesite em me mandar um pull request ! Sucesso Aposto que seus testes irão fluir muito melhor com essa extensão maravilhosa do Espresso . Mas não pense que seus poderes acabaram por aqui: no próximo irei falar sobre o vasto reino da Web e como interagir com elas. Não esqueça de compartilhar, comentar ou acrescentar em algo nesse post! Valeu! Links",pt,84
136,1832,1469052884,CONTENT SHARED,7734121175534200554,1748162647671155552,-2382936607572694307,,,,HTML,http://www.meioemensagem.com.br/home/comunicacao/2016/07/19/consultorias-promovem-uma-desvalorizacao-de-nosso-negocio.html,"""consultorias promovem a desvalorização do nosso negócio"" - meio & mensagem","Bárbara Sacchitiello19 de julho de 2016 - 13h00 Márcio Santoro, copresidente da Africa sabe que, daqui a alguns anos, a agência continuará cumprindo sua principal função: comercializar ideias criativas para auxiliar as marcas a venderem mais. De que forma e em quais mídias essa atividade será feita são coisas que o executivo não hesita em reconhecer ainda como incógnitas. Crente de que a indústria vivencia a 'era do já era', termo que usa para traduzir as transformações que a tecnologia vêm trazendo ao cenário da publicidade, Santoro ressalta que não apenas as empresas de comunicação, mas as companhias de todos os segmentos estão sofrendo com as mudanças do mercado. E que, para sobreviver a esse furacão, é primordial estar em sintonia com as transformações sociais. Dividindo o comando da agência com Sérgio Gordilho, o executivo vê a união de diferentes visões como o tempero principal da operação - ingrediente este que, segundo ele, foi um dos fatores que chamaram a atenção da holding Omnicom, que em novembro de 2015 acertou a compra do Grupo ABC , do qual a Africa é parte, em uma das maiores transações recentes da indústria global de comunicação. Nessa entrevista, Santoro fala sobre os impactos dessa negociação para a Africa, defende a confiança como base da relação agência-cliente e critica as consultorias que começam a se infiltrar na indústria criativa. ""Elas promovem uma desvalorização de nosso negócio."" Meio & Mensagem - Desde novembro de 2015, quando o grupo ABC foi adquirido pelo Omnicom, a Africa deixou de ser uma agência controlada por uma empresa de capital nacional. Quais são os prós e contras dessa nova realidade? Márcio Santoro - Das negociações estratégicas que poderíamos ter feito, essa foi a melhor possível. Temos uma relação histórica com o Omnicom, desde a época em que o Nizan (Guanaes, chairman do grupo ABC) e o Guga (Valente, CEO do grupo) fizeram o primeiro negócio na DM9, e também quando criamos a Africa, da qual eles foram sócios minoritários. Na prática, muda pouca coisa porque já sabemos bem como eles trabalham e o grupo tem como característica a liberdade que dá aos gestores locais. Quando olhamos para todas as transformações que estão acontecendo no nosso mercado e vemos que temos ao lado uma fonte de informação como o Omnicom, que tem ferramentas para atravessar esse processo, que está mais avançada e bem preparada na digitalização, reconhecemos que estamos em uma situação altamente privilegiada. Vejo o Nizan, o Guga e nós mesmos com as energias renovadas. O Omnicom foi muito inteligente em ter mantido toda a estrutura do ABC. Não consigo nem identificar algo negativo nesse processo. M&M - Há cinco anos você divide o comando da agência com o Sergio Gordilho (que, antes de ser copresidente, era VP de criação da operação). Como avalia esse modelo de gestão compartilhada? Santoro - Em nossa gestão, é como se eu fosse o mâitre, e o Gordilho, o chef de cozinha. Acho que esse modelo acabou inspirando muitas outras agências, que também adotaram o formato de copresidência. Nosso negócio é propaganda. Vivemos de vender criatividade. Esse é o nosso 'prato'e, por isso, é preciso ter um bom chef, que esteja envolvido e que domine esse trabalho. E também tem de ter alguém que faça o papel do mâitre, olhando para o negócio e para os clientes. Vim da parte de atendimento e temos uma estrutura aqui com sócios e vice-presidentes que estão juntos há muito tempo e que respeitam e compreendem o papel de cada um. É uma relação muito saudável e muito boa por isso. M&M - Quais são as principais transformações na relação entre agência e clientes? Santoro - No fundo, o que mantém essa relação estável é a capacidade de entrega criativa. O que mudou é que, antes, essa criatividade era um reclame na televisão ou uma propaganda em revista. Hoje, o que o cliente espera é uma grande ideia, que consiga navegar em todas as mídias que estão disponíveis aos consumidores e que funcione bem no mobile, na TV e em qualquer canal. Evidentemente que, hoje, para chegar a uma solução criativa pertinente, é preciso conhecer muito o negócio do cliente, que também se sofisticou. A ramificação dos canais de distribuição aumentou, seja para a propaganda, seja para uma marca de roupa. Para criar uma ideia relevante, é preciso se envolver mais - e aí acho que o modelo da Africa foi beneficiado. Criamos equipes exclusivas, com tempo para pesquisar os clientes. As pessoas falam muito da mudança da propaganda, mas todas as indústrias estão mudando. Em 1998, a Kodak tinha cem mil funcionários. Isso acabou, mas a essência da foto continua. Tudo está mudando e temos de saber para que lado correr. Todo o mercado está em xeque. Vivemos a era do 'já era' e o que nos salvará, como indústria, é a criatividade. ""A ramificação dos canais de distribuição aumentou, seja para a propaganda, seja para uma marca de roupa. Para criar uma ideia relevante, é preciso se envolver mais."" M&M - Recentemente o Meio & Mensagem trouxe como manchete uma reportagem sobre o avanço das consultorias no território da comunicação. Você vê essas empresas como concorrentes? Santoro - Tenho um super-respeito pela McKinsey, IBM, Accenture, mas não possoacreditar que eles vão trazer ideias criativas e disruptivas. Acredito que elas nem queiram isso, pois estão voltados para a mídia programática e para um segmento bem tecnológico. Agora acho que para nós, agências de criação, sempre haverá espaço porque, efetivamente, temos a missão de nos envolver ativamente com os clientes e fornecer as melhores ideias. Minha questão com essas empresas é quando elas são, ao mesmo tempo, o médico e farmacêutico. Ou seja, quando elas vão ao cliente, fazem o diagnóstico de um problema e oferecem a própria ferramenta como solução. E também quando essas empresas, que nunca criaram uma campanha, palpitam sobre quanto os nossos clientes estão nos pagando. Aí acho que eles fazem uma destruição de valor do nosso negócio e sou totalmente contra. A íntegra desta entrevista está publicada na edição 1719, de 18 de Julho, exclusivamente para assinantes do Meio & Mensagem , disponível nas versões impressa e para tablets iOS e Android .",pt,84
137,791,1462391771,CONTENT SHARED,764116021156146784,7890134385692540512,1034339681607289113,,,,HTML,https://medium.com/tech-diversity-files/if-you-think-women-in-tech-is-just-a-pipeline-problem-you-haven-t-been-paying-attention-cb7a2073b996,"if you think women in tech is just a pipeline problem, you haven't been paying attention - tech diversity files","If you think women in tech is just a pipeline problem, you haven't been paying attention According to the Harvard Business Review , 41% of women working in tech eventually end up leaving the field (compared to just 17% of men), and I can understand why... I first learned to code at age 16, and am now in my 30s. I have a math PhD from Duke. I still remember my pride in a ""knight's tour"" algorithm that I wrote in C++ in high school; the awesome mind warp of an interpreter that can interpret itself (a Scheme course my first semester of college); my fascination with numerous types of matrix factorizations in C in grad school; and my excitement about relational databases and web scrapers in my first real job. Over a decade after I first learned to program, I still loved algorithms, but felt alienated and depressed by tech culture. While at a company that was a particularly poor culture fit, I was so unhappy that I hired a career counselor to discuss alternative career paths. Leaving tech would have been devastating, but staying was tough. I'm not the stereotypical male programmer in his early 20s looking to ""work hard, play hard"". I do work hard, but I'd rather wake up early than stay up late, and I was already thinking ahead to when my husband and I would need to coordinate our schedules with daycare drop-offs and pick-ups. Kegerators and ping pong tables don't appeal to me. I'm not aggressive enough to thrive in a combative work environment. Talking to other female friends working in tech, I know that I'm not alone in my frustrations. When researcher Kieran Snyder interviewed 716 women who left tech after an average tenure of 7 years, almost all of them said they liked the work itself, but cited discriminatory environments as their main reason for leaving. In NSF-funded research, Nadya Fouad surveyed 5,300 women who had earned engineering degrees (of all types) over the last 50 years, and 38% of them were no longer working as engineers. Fouad summarized her findings on why they leave with ""It's the climate, stupid!"" This is a huge, unnecessary, and expensive loss of talent in a field facing a supposed talent shortage. Given that tech is currently one of the major drivers of the US economy, this impacts everyone. Any tech company struggling to hire and retain as many employees as they need should particularly care about addressing this problem. Your company is NOT a meritocracy and you are NOT ""gender-blind"" Nobody wants to think of themselves as being sexist. However, a number of studies have shown that identical job applications or resumes are evaluated differently based on whether they are labeled with a male or female name. When men and women read identical scripts containing entrepreneurial pitches or salary negotiations, they are evaluated differently. Both men and women have been shown to have these biases. These biases occur unconsciously and without intention or malice. Here is a sampling of just a few of the studies on unconscious gender bias: Investors preferred entrepreneurial ventures pitched by a man than an identical pitch from a woman by a rate of 68% to 32% in a study conducted jointly by HBS, Wharton, and MIT Sloan. ""Male-narrated pitches were rated as more persuasive, logical and fact-based than were the same pitches narrated by a female voice."" In a randomized, double-blind study by Yale researchers, science faculty at 6 major institutions evaluated applications for a lab manager position. Applications randomly assigned a male name were rated as significantly more competent and hirable and offered a higher starting salary and more career mentoring, compared to identical applications assigned female names. When men and women negotiated a job offer by reading identical scripts for a Harvard and CMU study, women who asked for a higher salary were rated as being more difficult to work with and less nice , but men were not perceived negatively for negotiating. Psychology faculty were sent CVs for an applicant (randomly assigned male or female name), and both men and women were significantly more likely to hire a male applicant than a female applicant with an identical record. In 248 performance reviews of high-performers in tech , negative personality criticism (such as abrasive, strident, or irrational) showed up in 85% of reviews for women and just 2% of reviews for men. It is ridiculous to assume that 85% of women have personality problems and that only 2% of men do. Most concerningly, a study from Yale researchers shows that perceiving yourself as objective is actually correlated with showing even more bias . The mere desire to not be biased is not enough to overcome decades of cultural conditioning and can even lend more credence to post-hoc justifications. Acknowledging that you have biases that conflict with your values does not make you a bad person. It's a natural result of our culture. The important thing is to find ways to eliminate them. Blindly believing your company is a meritocracy not only does not make it so, but will actually make it even harder to address implicit bias. Bias is typically justified post-hoc. Our initial subconscious impression of the female applicant is negative, and then we find logical reasons to justify it. For instance, in the above study by Yale researchers if the male applicant for police chief had more street smarts and the female applicant had more formal education, evaluators decided that street smarts were the most important trait, and if the names were reversed, evaluators decided that formal education was the most important trait. Good News and Bad News The Bad News... Because of the high attrition rate for women working in tech, teaching more girls and women to code is not enough to solve this problem. Because of the above well-documented differences in how men and women are perceived, training women to negotiate better and be more assertive is also not enough to solve this problem. Female voices are perceived as less logical and less persuasive than male voices . Women are perceived negatively for being too assertive. If tech culture is going to change, everyone needs to change, especially men and most especially leaders. The professional and emotional costs to women for speaking out about discrimination can be large (in terms of retaliation, being perceived as less employable or difficult to work with, or companies then seeking to portray them as poor performers). I know a number of female software engineers who will privately share stories of sexism with trusted friends that we are not willing to share publicly because of the risk. This is why it is important to proactively address this issue. There is more than enough published research and personal stories from those who have chosen to publicly share to confirm that this is a widespread issue in the tech industry. ...and the Good News Change is possible. Although these are schools and not tech companies, Harvey Mudd and Harvard Business School provide inspiring case studies. Strong leaders at both schools enacted sweeping changes to address previously male-centric cultures. Harvey Mudd has raised the percentage of computer science majors that are women to 40% (the national average is 18%). The top 5% of Harvard Business School graduates rose from being approximately 20% women to closer to 40% and the GPA gap between men and women closed, all within one year of making a number of comprehensive, structural changes. So What Can We Do About It? These recommendations on what companies could do to improve their cultures are based on a mix of research and personal experience. My goal is to have a positive focus, and I would love it if you walked away with at least one concrete goal for making constructive change at your company. Train managers It is very common at tech start-ups to promote talented engineers to management without providing them with any management training or oversight, particularly at rapidly growing companies where existing leadership is stretched thin. These new managers are often not aware of any of the research on motivation, human psychology, or bias. Untrained, unsupervised managers cause more harm to women than men , although regardless, all employees would benefit from new managers receiving training, mentorship, and supervision. Formalize hiring and promotion criteria In the Yale study mentioned above regarding applicants for police chief, getting participants to formalize their hiring criteria before they looked at applications (i.e. deciding if formal education or street smarts was more important) reduced bias. I was once on a team where the hiring criteria were amorphous and where the manager frequently overrode majority votes by the team because of ""gut feeling"". It seemed like unconscious bias played a large role in decisions, but because of our haphazard approach to hiring, there was no way of truly knowing. Leaders, speak up and act in concrete ways Leadership sets the values and culture for a company, so the onus is on them to make it clear that they value diversity. Younger engineers and managers will follow their perceptions of what executives value. In the cases of positive change at Harvey Mudd and Harvard Business School , leadership at the top was spearheading these initiatives. Intel is going to begin tying executives' compensation to whether they achieve diversity goals on their teams. As Kelly Shuster, director for the Denver chapter of Women Who Code has pointed out, leaders have to get rid of employees who engage in sexist or racist behavior . Otherwise, the company is at risk of losing talented employees, and is sending a message to all employees that discrimination is okay. Don't rely on self-nominations or self-evaluations There is a well-documented confidence gap between men and women. Don't rely on people nominating themselves for promotions or to get the most interesting projects, since women are less likely to put themselves forward. Google relies on employees nominating themselves for promotions and data revealed that women were much less likely to do so (and thus much less likely to receive promotions). When senior women began hosting workshops encouraging women to nominate themselves, the number of women at Google receiving promotions increased . Groups are more likely to pick male leaders because of their over-confidence , compared to more qualified women who are less confident. Don't rely heavily on self-evaluations in performance scoring. Women perceive their abilities as being worse than they are, whereas men have an inflated sense of their abilities . Formally audit employee data Confirm that men and women with the same qualifications are earning the same amount and that they are receiving promotions and raises at similar rates (and if not, explore why). Make sure that gendered criticism ( such as calling a woman strident or abrasive ) is not used in performance reviews. The trend of tech companies releasing their diversity statistics is a good one, but given the high industry attrition rate for women, they should also start releasing their retention rates broken down by gender. I would like to see companies release statistics on the rates at which women are given promotions or raises compared to men, and how performance evaluation scores compare between men and women. By publicly sharing data, companies can hold themselves accountable and can track changes over time. Don't emphasize face time A culture that rewards facetime and encourages people to regularly stay late or eat dinner at the office puts employees with families at a disadvantage (particularly mothers), and research shows that working excess hours does not actually improve productivity in the long-term since workers begin to experience burn out after just a few weeks. Furthermore, when employees burn out and quit, the cost of recruiting and hiring a new employee is typically 20% of the annual salary for that position . Create a collaborative environment Stanford research studies document that women are more likely to dislike competitive environments compared to men and are more likely to select out of them, regardless of their ability. Given that women are perceived negatively for being too assertive, it is tougher for women to succeed in a highly aggressive environment as well. Men who speak up more than their peers are rewarded with 10% higher ratings, whereas women who speak up more are punished with 14% lower ratings . Creating a competitive culture where people must fight for their ideas makes it much tougher for women to succeed. Offer maternity leave Over 10% of the 716 women who left tech in Kieran Snyder's research left because of inadequate maternity leave. Several were pressured to return from leave early or to be on call while on leave. These women did not want to be stay-at-home-parents, they just wanted to recover after giving birth. Just as you would not pressure someone to return to work without recovery time after a major surgery, women need time to physically heal after delivering a baby. When Google increased paid maternity leave from 12 weeks to 18 weeks, the number of new moms who quit Google dropped by 50% . Some final thoughts... A note on racial bias There is a huge amount of research on unconscious racial bias , and tech companies need to address this issue. As Nichole Sanchez, VP of Social Impact at GitHub, describes, calls for diversity are often solely about adding more white women , which is deeply problematic. Racial bias adds another intersectional dimension to the discrimination that women of color experience. In interviews with 60 women of color who work in STEM research , 100% of them had experienced discrimination, and the particular negative stereotypes they faced differed depending on their race. A resume with a traditionally African-American sounding name is less likely to be called for an interview than the same resume with a traditionally white sounding name. I do not have the personal experience to speak about this topic and instead encourage you to read these blog posts and articles by and about tech workers of color on the challenges they've faced: Erica Joy (Slack engineer, former Google engineer), Justin Edmund (designer, Pinterest's 7th employee), Aston Motes (Engineer, Dropbox's 1st employee), and Angelica Coleman (developer advocate at Zendesk, formerly at Dropbox). Now I'm currently teaching software development at all-women Hackbright Academy , a job that I love and that suits me perfectly. I want all women to have the opportunity (and I mean truly have the opportunity, without implicit or explicit discrimination) to learn how to program - knowing software development provides so many career and financial possibilities; it's intellectually rewarding and fun; and being a creator is deeply satisfying. Although I know many women with frustrating experiences of sexism, I also know women who have found companies where they're happily thriving. I'm glad for the attention tech's diversity problem has been receiving and I am hopeful about continued change. Thanks for review, edits, and discussion to: Jeremy Howard and Angie Chang.",en,83
138,2574,1476722765,CONTENT SHARED,1992928170409443117,-4028919343899978105,6702709297756973694,"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.143 Safari/537.36",SP,BR,HTML,http://googlediscovery.com/2016/10/11/google-vai-reduzir-em-50-consumo-de-memoria-do-chrome/,google vai reduzir em 50% consumo de memória do chrome | google discovery,"O Google anunciou que a versão 55 do Chrome vai incluir um novo motor JavaScript que irá reduz significativamente o consumo de memória no navegador. A empresa afirma que seus testes internos, que incluem páginas populares como o New York Times, Twitter, Reddit e YouTube, irão utilizar 50% menos RAM, em média, do que a atual versão 53 do Chrome. O upgrade está programado para acontecer no dia 06 de dezembro para a versão estável do Chrome, mas os usuários de versões Beta, Canary e DEV poderão desfrutar em primeira mão. A equipe do Chromium espera tornar o Chrome mais funcional em dispositivos menos sofisticados, principalmente aqueles com menos de 1 GB de memória.",pt,83
139,1099,1464181070,CONTENT SHARED,6437568358552101410,-1032019229384696495,3498647633704503701,,,,HTML,http://techcrunch.com/2016/05/25/tech-in-brazil-is-booming-despite-the-countrys-political-troubles/,tech in brazil is booming despite the country's political troubles,"It was a balmy night in Rio de Janeiro and a pretty J.P. Morgan Private Wealth representative was buying drinks for newly paper-rich entrepreneurs. Nearby at the pool of the city's only Relais et Chateaux property mingled investors who had flown in from New York, San Francisco, London and Berlin. The next morning the Hotel Santa Teresa's projector beamed countless up-and-to-the-right charts as entrepreneurs presented their well-funded ideas to the participants of Founders Forum Brazil 2012. Few of those companies still exist today. The hype of that era is long gone, overwhelmed by a political and economic crisis that regularly fills the front pages of global newspapers. However, unfazed by the high-profile failures of many foreign-backed startups, technology in Brazil continued its forward march. While Brazil's overall GDP fell 4 percent in 2015, the tech industry has been largely immune to the slowdown, growing 20 percent from 2014-2015. Contrary to conventional wisdom, venture investments in Latin America have also consistently grown from those heady days, reaching US$594 million in 2015, up from US$387 million in 2012, according to the Latin America Venture Capital Association. Surprisingly, recession-racked Brazil consumed 63 percent of the region's total investment, with the city of Sao Paulo receiving the lion's share of VC dollars. The reality is that the recession and political crisis created a clear divergence of fortunes among Brazil's technology companies. Many of the companies that presented at Founders Forum Brazil 2012 were plays on the emergence of Brazil's middle class. The entrepreneurs and investors were wagering that these newly empowered consumers would snap up furniture, shoes and baby supplies at record rates. Unfortunately, an economy fueled by ever-looser credit and constantly increasing government spending could not last. As in most countries, recessions lead to belt-tightening among companies and consumers. Just as expected, the technology companies that thrived are those that offer consumers and businesses a way to improve efficiency and sustainably reduce their costs. Examples include small business SaaS ERPs like Conta Azul and Omie , consumer productivity apps like Gympass and GuiaBolso , and companies that facilitate B2B transactions like Intelipost and Loggi . The relative unconcern about the recession among Brazilian technology entrepreneurs was evident last week at the award gala hosted by Latam Founders Network , a group made up of founders, executives and investors from across the region. Camera crews from all of Brazil's major television stations were there to cover what Brazil's leading business magazine, EXAME, dubbed the ""Oscar for Startups."" Companies competed in eight categories, including B2C, Best Investor, and Most Innovative. (Full disclosure: My company, Gaveteiro.com.br, was nominated in the B2B category). Past winners have included Printi , Nubank and iFood , all of which went on to raise large rounds. Taking home the prizes were companies that fit the trend of efficiency-driving technology. Contabilizei , SaaS accounting software, won in the B2B category. Dr. Consulta , a chain of low-cost, technology-enabled health clinics serving poor areas, won Most Impactful. Finally, Pipefy , a SaaS workflow tool with global ambitions, won Most Innovative. Entrepreneur of the Year went to Bruno Pierobon , CEO of Zup , a company focused on helping companies deal with all the complexities of systems integration and outsourcing services. Investors, too, have recognized this trend. Kaszek Ventures , one of the region's largest funds, has invested heavily in B2B and consumer finance (credit card interest rates in Brazil can pass 200 percent a year). Eight of their last 10 investments listed in CrunchBase have been in these two categories. Ditto for Redpoint eventures , another large fund focused on Brazil that took home this years' Investor of the Year award. No one knows the final outcome of Brazil's political crisis - until now it has been an unpredictable cross of Gabriel Garcia Marquez's surrealism and Netflix's House of Cards. What we do know, however, is that efficiency-driving innovation will have its place, come whatever market environment. Those entrepreneurs and investors who can thrive in a tough macroeconomic scenario are surely well-positioned for long-term success in the world's seventh-largest economy.",en,83
140,1158,1464693171,CONTENT SHARED,1441248639512899483,3609194402293569455,4752652671493723871,,,,HTML,http://www.diolinux.com.br/2016/05/one-dollar-board-um-projeto-brasileiro.html,one dollar board: um projeto brasileiro para revolucionar a educação,"Com o objetivo de popularizar o acesso inicial a Internet das coisas (IoT) e robótica para crianças e adultos de países em desenvolvimento evitando o analfabetismo tecnológico, surgiu a One Dollar Board. Uma placa para entrar no mundo da eletrônica e da programação, compatível com o popular software livre e gratuito IDE Arduino, acompanhada de um manual de instruções impresso nela mesma (on-board). Criada para fazer parte da lista básica de materiais escolares de crianças, a One Dollar Board tem o propósito de fazer a criança sair da escola já sabendo programar, assim como aprende os conteúdos básicos, matemática, gramatica, geografia, etc Com o objetivo de incentivar a programação de hardware, você que é palestrante, programador, professor, gosta de robótica e quer fazer seu amigo interessar-se por eletrônica e iniciar-se na Internet das Coisas (IoT). Agora você pode presenteá-lo sem precisar gastar muito e iniciá-lo facilmente neste universo. Se você quer gerar um impacto na vida das pessoas presenteando-as ou quer realizar uma doação, o investimento será menor comparado com os resultados efetivos. O One Dollar Board será comercializada com um preço acessível para qualquer pessoa do mundo, tendo como foco principal os países em desenvolvimento. Ela contém o básico para iniciar projetos e ser funcional. O que você faria com um Dólar? O que você faria com 1 Dólar? Quer uma dica? Por quê não se aventurar e descobrir a programação de uma maneira mais lúdica, tendo possibilidades de trabalhar com robótica e Internet das coisas? A One Dollar Board nasceu para ser livre, o que permite aumentar seu impacto na sociedade possibilitando à qualquer pessoa modificar a versão original ou melhorar para outras aplicações comerciais ou não comercias. Ela possui licença Open Source Hardware O projeto ainda está buscando apoio financeiro para realizar seus objetivos, você pode colaborar e ver mais detalhes da campanha na página da One Dollar Board no IndieGOGO. O projeto nasceu nas Campus Party 2015 de São Paulo através das mãos de Claudio Olmedo. Curtiu a ideia? Então apoie o projeto e compartilhe as informações para que mais pessoas possam saber desta iniciativa. Viu algum erro ou gostaria de adicionar alguma sugestão a essa matéria? Colabore, clique aqui.",pt,82
141,816,1462470341,CONTENT SHARED,-5570129644089964821,-48161796606086482,3791045604731925300,,,,HTML,https://novoed.com/prototyping-2016-1,design kit: prototyping,"The Course Course Description: Human-centered design is a process that starts with the people you're designing for and ends with new solutions that are tailor-made to suit their needs. Prototyping is a crucial part of this process. Building prototypes gives you a chance to get your ideas out of your head and into the hands of the people you're designing for. By getting feedback early and often, and continuing to improve your idea, you'll be well on your way to getting useful ideas and solutions out into the world. This four-week course builds on Design Kit: The Course for Human-Centered Design , a class that has been taken by thousands of teams around the world. The curriculum will strengthen your existing knowledge of the human-centered design process and help you to become a more innovative problem solver by building up your prototyping skills. You'll walk away knowing how to prototype products, services, and environments, and get constructive feedback along the way. What You'll Learn: After completing this course, you will be able to: Practice the mindsets and methods of human-centered design so that you can become a more effective, innovative problem solver, specifically through prototyping Produce tangible results quickly and bring your ideas to life through early, rough prototypes Experiment with the power of prototyping in multiple forms-from products to services to environments Gain valuable feedback from the people you're designing for, and how that feedback can inform and be integrated into new iterations Demonstrate how the cycle of prototyping, feedback, and iteration leads to concept refinement How the Course Works: You will work through the course with a group of 2-6 people that we will refer to as your ""design team."" It's strongly recommended you form a team before the start of the course with people located nearby, with whom you can meet in-person, but you will also have the opportunity to find a team using the course platform once the course begins. You will learn the prototyping process by applying it to a real-world design challenge that we will provide for you. Each week you will explore the main concepts of prototyping through readings, case studies, and short videos. Then you'll be expected to meet in-person with your design team to get your hands dirty building prototypes and practicing other relevant human-centered design methods. Throughout the course, you'll have the opportunity to interact and gain inspiration from design teams around the world also taking the course. Due to the size of this course, instructors will be available to answer questions, but cannot provide feedback on individual assignments. The course occurs entirely online and there are no set meeting times that you are required to log in, but you will set meeting times to meet with your own team. You can log in and access the course materials at any time between the course opening and closing dates. Building Your Design Team: Many of the workshop activities will work best if your group has 2-6 members (ideally at least 4). Recruit your friends, colleagues, or family members Find a group through the course platform once it starts More Information Approximately 4 hours per week:",en,81
142,2776,1479901922,CONTENT SHARED,8428597553954921991,268671367195911338,-4581094555157170117,"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.98 Safari/537.36",SP,BR,HTML,http://www.huffingtonpost.com/advertising-week/time-to-re-think-design-t_b_12455924.html,time to re-think design thinking,"Olof Schybergson, CEO and Shelley Evenson, Head Of Organizational Evolution At Fjord, Design and Innovation From Accenture Interactive Faced by growing competition and nimbler start-ups , many organizations are struggling. They suffer from a crisis of innovation. Unable to differentiate their brands, their products and their services in a digitally disruptive world, organizations' future success depends on better managing and responding to change. Their very existence hinges on their ability to continuously and rapidly innovate. In order to do so successfully, they must place people at the heart of everything they do. They must harness the power of design. Business leaders once distinguished business strategy from customer experience but, today, that mindset is changing: business strategy has become experience strategy. In fact, 89% of companies recently surveyed by Gartner claim that experience will be their primary basis for competitive advantage this year. These shifts, along with growing evidence that design-centric companies are outperforming the market average, are fueling private and public sector interest in design - an agile and collaborative discipline that enables human-centered innovation to be brought to market, fast. The appetite for design thinking to reframe experience has never been greater. More corporations are opening their eyes to the power of design thinking as a way to solve the crisis of innovation. They see disruptive companies like AirBnB get this. But misguided efforts -- however well-intentioned -- may do more harm than good. The truth is, design thinking has become broken in today's digital age. The current interpretation of design thinking is often shallow and, as widely understood, not the answer. Simply put, design thinking is not enough. True success comes from building a complete design system, and no organization can build such a system on design thinking alone. Here's how to do it. The fact is, design thinking only has value when combined with design doing and supported by a strong design culture. You can't be good at only one or two. The Design Rule of 3 constitutes the three fundamental rules that underpin every successful design system employed by leading organizations, across sectors. When optimized and deployed in unison, organizations can effectively unlock the full potential of design to transform not only their own value and performance but peoples' experiences of the products or services they provide. Re-Thinking Design Thinking Design thinking should bring a quest for truth, empathy with people, and a systematic reframing of the business challenge-zooming in and out of the opportunity space, and providing a strategic compass to help executives understand how to reorient their businesses. Co-creation has to be integral. An organization must be willing and able to break down organizational silos to enable it. Fundamentally, design thinking must align a design perspective with business realities and technical possibilities. Key Tenants of Modern-Day Design Thinking: Co-Creation Top Down, Bottom Up Design Prototyping Continuous Feedback & Testing Measuring customer delight is essential, by employing Net Promoter Score ( NPS ) or an equivalent. At Fjord, we use our research-based Love Index to quantify and understand people's engagement. The Love Index is an actionable tool that allows you to understand what people feel about your offering, and why.Mastery requires both C-suite and grassroots support; this simultaneous top-down and bottom-up commitment is essential for design thinking to become broadly embedded across an organization. Staff training and ""learning by doing"" project-based experiences will help get you there. Crucially, successful design thinking must also include an element of making - early experience prototypes are important to validate thinking and align teams. Proponents of design thinking often get caught up in the methodologies (""how to get there"") versus the actual destination. Hands-on creation is often forgotten in today's rush to apply design thinking organization-wide. Financial software giant Intuit is a powerful example of an organization that has optimized design thinking and the context in which it lives. VP and Executive Creative Director Suzanne Pellican oversees the company's Design for Delight program that aims to inject design thinking into the company's DNA. She's trained and cultivated a community of 200 innovators who've run more than 1,000 workshops over five years to change the way people work across every function. By becoming design-driven, Intuit shifted from what Pellican has described as ""the best run, no growth company in the Valley"" to ""a 30-year-old startup."" Make no mistake, design thinking is crucial to improving everything from the value of a company's offering to reimagining the employee experience. But misunderstanding what it is and how best to apply it risks sidelining design thinking into just another passing management fad. The reason is simple: design thinking is just the beginning -- a catalyst. What's critical is to convert theory into reality to catalyze change. This is where design doing comes in. Not Just Design Thinking, but Doing Design doing is where design thinking meets the real world. While design thinking should be embedded across the organization, design doing must involve design experts. It must be driven by people passionate about the craft of design across its every application, powered by design practices such as rapid iteration and real-world testing. As the digitization of everything takes hold, the medium for design doing constantly evolves. Instead of designing for print or TV, today we design for mobile consumption and voice interaction. Instead of creating static products and websites, we design living services tailored for each individual, and powered by data. We are designing for experience in an ever-broadening context. This requires interdisciplinary teams of designers collaborating with experts as diverse as data scientists and developers. "" Design thinking is nothing without design doing. "" This broader context brings unprecedented complexity. Great design cuts through the clutter and helps prioritize and progressively reveal -- without dumbing down. The ability to simplify and make complex systems easy, engaging and intuitive for people is one of the critical contributions of good design (and great designers) at a time when the strategic business value of simplicity has never been greater. An emotional connection to brands, products, and services is also crucial for success. Rather than focus narrowly on Minimum Viable Product, the aim should be to imagine and shape a Minimum Lovable Product. Great design did not always come easy in the engineering-obsessed culture of Google. But after Larry Page took reigns as CEO in 2011, the company started crafting a common design language for experience that, for the first time, unified a vast collection of offerings into one coherent family. C-suite support, willingness to invest and strategic commitment created a program led by a core team of designers that enabled and applied great design doing across Google's diverse initiatives. In this way, an everyone-for-themselves approach to design was replaced by design becoming a central guiding force for the organization. The results speak for themselves. Google products are now perceived by many as having made the most strides at improving design, according to KPCB's 2016 Design In Tech report. Some 64% of those surveyed rated Google as ""most improved"" when asked which tech companies were best improving their design. Only 33% said so of Apple. Design doing can generate powerful results. But it can only do so within a considered and optimized organizational culture conducive to innovating and doing design well. This brings us to the last rule in the Design Rule of 3. Why Design Culture is All Design culture is equally important as design thinking and design doing because it enables the others. Creating a design culture is no trivial undertaking. It requires organizational commitment and patience. This is where most organizations stumble. Brilliant people will fail if the environment in which they work doesn't foster creativity, collaboration and innovation. Fostering a Design Culture: Diverse teams including change agents Learning & evolution of individuals Flexible physical spaces Given that great ideas emerge from diverse teams, change agents should be recruited as ambassadors and implementers of cultural transformation. Care must be taken to ensure they are set up for success. Money alone will no longer secure the best designers. People want environments in which they are challenged, continuously learn and make impact. Our own Fjord Evolution team helps our clients create a learning design environment, a culture that will encourage the best design talent to come, learn, thrive -- and stay. Flexible and open workspace is important to facilitate the best design work; and visual representations of ideas and their impact are a powerful tool. Design and innovation requires full mind and body engagement. Photo: Werner Huthmacher, Fjord Berlin It's relatively easy to copy a good business idea today, and technology solutions are cheaper and more flexible than ever. Differentiation through a clever business model or a novel technology is challenging. Culture, however, is hard to emulate. A vibrant design culture can be the best and most sustainable differentiator for an organization. Commerzbank, the global banking and financial services company, is one organization working to build such a culture through a considered deployment of design. Having initially considered buying a design consultancy to import a fully-fledged design function, it decided to change its culture from within. Work is now underway to create an internal design function that will embody and enact the company's new vision. The design space and design team is being built and managed by Fjord, as their strategic partner, until the bank's management team takes over day-to-day operations. As they're discovering, fostering a culture of design is an organizational and mindset shift that does not happen overnight. It requires commitment to a multi-year process, one that constantly evolves. Benefits of an Effective Design System In today's world, digital may look like the driving force of change but, in reality, people are at the heart of digital. Design, by its very nature, creates a culture obsessed with people willing to listen and learn from multiple vantage points, ignore hierarchies and experiment until finding the best solution. This explains why human-centricity is what powers successful organizations, across sectors, and why design is a fundamental part of their DNA. Harnessed properly, design can boost an organization's performance and value because enduring customer relationships are a natural outcome. Other evidence points to a happier workforce too. Bottom line -- mindset matters. Acquiring design thinking methods is a great first step, but must be followed through with changing how products and services are conceived and delivered, every day in every way. Effecting that transformation means simultaneously creating the right culture. Good intentions can only become reality if underpinned by a considered, effective design system built on solid foundations: the Design Rule of 3.",en,81
143,1910,1469719009,CONTENT SHARED,1881534532776527237,-1032019229384696495,3028199142413699225,,,,HTML,http://thecooperreview.com/non-threatening-leadership-strategies-for-women/,9 non-threatening leadership strategies for women,"In this fast-paced business world, female leaders need to make sure they're not perceived as pushy, aggressive or competent. One way to do that is to alter your leadership style to account for the (sometimes) fragile male ego. Should men accept powerful women and not feel threatened by them? Yes. Is that asking too much? IS IT? Sorry I didn't mean to get aggressive there. Anyhoo, here are 9 non-threatening leadership strategies for women. #1 When setting a deadline, ask your coworker what he thinks of doing something, instead of just asking him to get it done. This makes him feel less like you're telling them what to do and more like you care about his opinions. #2 When sharing your ideas, overconfidence is a killer. You don't want your male coworkers to think you're getting all uppity. Instead, downplay your ideas as just ""thinking out loud,"" ""throwing something out there,"" or sharing something ""dumb,"" ""random,"" or ""crazy."" #3 Pepper your emails with exclamation marks and emojis so you don't come across as too clear or direct. Your lack of efficient communication will make you seem more approachable. #4 If a male coworker steals your idea in a meeting, thank him for it. Give him kudos for how he explained your idea so clearly. And let's face it, no one might've ever heard it if he hadn't repeated it. #5 When you hear a sexist comment, the awkward laugh is key. Practice your awkward laugh at home, with your friends and family, and in the mirror. Make sure you sound truly delighted even as your soul is dying inside. #6 Men love explaining things. But when he's explaining something and you already know that, it might be tempting to say ""I already know that."" Instead, have him explain it to you over and over again. It will make him feel useful and will give you some time to think about out how to avoid him in the future. #7 Pointing out a mistake is always risky so it's important to always apologize for noticing the mistake and then make sure that no one thinks you're too sure about it. People will appreciate your ""hey what do I know?!"" sensibilities. #8 When collaborating with a man, type using only one finger. Skill and speed are very off-putting. #9 When all else fails, wear a mustache so everyone sees you as more man-like. This will cancel out any need to change your leadership style. In fact, you may even get a quick promotion!",en,81
144,1324,1465499895,CONTENT SHARED,7400903238402587728,-1387464358334758758,2157275533176536828,,,,HTML,http://googlediscovery.com/2016/06/06/inbox-by-gmail-ganha-formatacao-de-texto/,inbox by gmail ganha formatação de texto | google discovery,"Em resposta as solicitações dos usuários, o Google anunciou hoje a disponibilidade de um novo recurso de formatação de texto no Inbox by Gmail para desktop. Os usuários poderão escolher entre 11 tipos de letras, 4 tamanhos pré-definidos e uma grande variedade de cores para marcar o texto. Além das clássicas opções de negrito, itálico e sublinhado. é bacharel em administração de empresas e fundador da FragaNet Networks - empresa especializada em comunicação digital e mídias sociais. Em seu portfólio estão projetos como: Google Discovery, TechCult, AutoBlog e Arquivo UFO. Também foi colunista de tecnologia no TechTudo, da Globo.com.",pt,81
145,1739,1468340950,CONTENT SHARED,-8043889175123948305,-4465926797008424436,3133301208369799012,,,,HTML,http://keeptesting.com.br/2014/11/04/dicas-avancadas-de-ruby-capybaracucumber/,dicas avançadas de ruby (capybara/cucumber),"Esse post foi escrito originalmente em inglês ( ) e achei que seria bom ter a versão na minha lingua nativa :) Como de constume, continuo vendo um pessoal dizendo que automação não é desenvolvimento, logo não usam técnicas como DRY (dont repeat yourself) ou coisas bacanas da linguaguem que escolheram usar para automatizar seus testes. Para ajudar essas pobres almas, eu juntei mais algumas dicas avançadas para que você deixe seu código mais bonito, semântico e o tempo que você vai economizar para estender seus testes, você poderá gastar ficando mais tempo no break do café :) 1 - Você deveria usar o Bundler Uma das coisas legais do ruby é seu gerenciador de depenência chamado bundler . Com ele você tem o controle de quais gems o seu projeto de teste depende. No diretório raiz do seu projeto, crie um arquivo chamado Gemfile (sem extensão) . Uma cópia do conteúdo pode ser vista abaixo (adicione outras gems que você usa em seus scripts) salve o arquivo e chame o comando abaixo no mesmo diretório. O bundler vai pegar a última versão de todas as gem's do arquivo Gemfile e irá instalar para você. Caso ela já esteja instalada, vai mostrar na tela a seguinte mensagem: ""Using gem XXXX"". Se você quer uma versão específica de uma gem, pode usar a sintaxe ao lado Quer saber mais sobre o bundler ? . 2 - Se possível, evite usar IF's O condicional if é o método mais comum utilizado em todas as linguagens e é facil de usar, porém com o tempo e o aumento de complexidade do seu script, fica fácil se perder em tantos if's jogados por ai. Muitos ifs tornam o código difícil de manter, ler e estender. Se você tem uma condicional simples, não tem problema usar if mas se ela é um pouco maior, tente usar case . Veja o exemplo abaixo: Usando case : Não é nem pela quantidade de linhas, mas de como o código é exibido. Vamos concordar que é bem mais semântico e fácil de entender 3- Não declare variáveis vazias( x = nill).. Muitas pessoas que vieram do mundo JAVA , constumam declarar variáveis vazias para poder popular mais tarde dentro de um loop ou dentro de uma condicionais ( if ou case ) . Pelo amor de deus não faça mais isso.. Se você está tentando preencher uma variável dentro de uma condicional, tente usar a sintaxe abaixo: if : case : Reparou que eu não faço myVariable = ""true""? Pois cada condicional tem um valor de retorno implícito e ele preenche a variável com esse valor de retorno. 3.1 Você pode expandir essa dica para métodos também: Viu? sem usar o método return ..e funciona tranquilamente :) 3.2 Tente usar inline / if's tenários É o mesmo que: Ou você pode usar a forma tenária de se escrever: 4 - Coloque seus processos comuns em métodos de suporte. Com cucumber você já consegue reutilizar STEP's (isso é nativo da biblioteca e é genial) porém algumas vezes existem processos que não entram nesses steps e você pode utilizar eles frequentemente. Exemplo: Você precisa fazer logoff da aplicação para testar se o processo de teste trocar senha está funcionando. . O Step disso poderia ser: ""E o usuário loga novamente para confirmar a alteração de senha"" . O step não é um processo específico de logoff porém utiliza os passos para fazer um logoff. Então por que não criar um método específico para ele? Ou exemplo poderia ser um método que faz um calculo que é geralmente utilizado em grande parte dos testes? Dentro da pasta support , qualquer arquivo .rb que você colocar lá dentro, os métodos poderão ser acessados de qualquer arquivo de teste dentro do seu projeto Mais fácil que fritar ovo! 5 - Utilize valores padrão para argumentos dos seus métodos Em tempos sombrios onde o selenium 1 era beta e eu estava aprendendo a programar em java. Meus métodos orientados a objetos utilizados para dar suporte aos meus testes tinham 3..5.. 10 argumentos EU pensava que era normal por causa da complexidade dos testes... o restante da história eu vou deixar de lado para poupar vocês da dor e agonia. Primeiro de tudo, tente evitar fazer métodos que use mais que 4 argumentos. Também verifique quais argumentos são passíveis de um valor padrão. Por exemplo, vamos imaginar que você precise chamar um método para salvar um log no banco de dados e algumas vezes você precisa jogar exibir na tela alguma mensagem no console. Na maior parte do tempo, você não irá exibir nada no console. Em tempos passados eu faria desta forma: Como falado acima, grande parte do tempo não será exibido nada no console, então por que temos que chamar o método savelog sempre com o argumento ""console"" false?Por que não deixar esse argumento com falor padrão = false? Vamos refatorar! Repare que só deixei explícito o argumento console igual a ""true"" e quando eu não mando nada, ele usa o padrão do método 5.2 Se você precisar mandar mais que 4 argumentos, procure enviar um hash. Imagine que você tem um método assim: Se você precisa o método, mas não precisa passar todos os argumentos, mesmo com valor padrão você precisaria colocar algo para identificar a ordem certa dos paramêtros. (arg3 e arg4 tem que ser nill para identificar o arg5) Feio né? Sabia que podemos chamar um método com argumentos em hash? Bem mais bonito e semântico! E perceba que não está na ordem de antigamente, pois ele identifica o nome do parametro pela chave do hash! mas espera um pouco... o arg3 e arg4 vai exibir nill? vai dar pau? Vai exibir ""nill"" e isso nos leva a última dica :) 5.3 - Tratando variáveis nulas (nill) Lembra do exemplo anterior? puts options[:arg3] e puts options[:arg4] vai ser exibido nill. Mas e se eu não quero exibir nill ? É possível setar um valor default para caso o argumento não venha OU venha _nill Agora caso não venha o parâmetro requisitado, ele irá preencher com uma mensagem genérica ""não veio o argumento""! Por enquanto é só. Espero que tenham gostado! Dúvidas, críticas, café, sugestão.. sinta-se a vontade para comentar! Sobre o Autor: Leonardo Galani é Agile Tester, dev, gamer, dj and etc. Mantém o fórum | (profile)| (blog em inglês) Please enable JavaScript to view the comentários por Disqus. comments powered by",pt,80
146,2962,1484567664,CONTENT SHARED,-7120369133608664808,3915038251784681624,2839973453847029371,"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.95 Safari/537.36",SP,BR,HTML,http://cio.com.br/gestao/2017/01/16/e-chegada-a-hora-do-desenvolvimento-de-software-orientado-por-hipoteses/,é chegada a hora do desenvolvimento de software orientado por hipóteses - cio,"Embora pouco conhecido, o conceito de desenvolvimento de software e negócios Hypothesis-Driven Development (ou ""Desenvolvimento Orientado por Hipóteses"") não é estranho à maioria dos profissionais. Basta recordar as aulas de metodologia científica. Recordando: observa-se o mundo ao redor, formula-se uma explicação prévia, ou hipótese, para explicar determinado fenômeno. Então, testamos essa hipótese em um teste controlado. Caso o resultado esperado seja obtido, nossa teoria é confirmada, servindo de base para novas explorações. Caso não, a hipótese inicial é abandonada e novas alternativas são buscadas. A experimentação é a base do método científico, uma forma sistemática de explorar o mundo. Embora os experimentos científicos sejam associados a laboratórios, eles podem ocorrer em todo o lugar, a qualquer tempo, inclusive no desenvolvimento de software e novas propostas de negócio. Praticar a metodologia de ""Desenvolvimento Orientado por Hipóteses"" é pensar sobre o desenvolvimento de novas ideias, produtos e serviços - incluindo mudanças organizacionais - como uma série de experimentos para determinar se um resultado esperado foi alcançado. O processo é realizado até que o resultado esperado seja obtido ou a ideia se mostre inválida. Precisamos passar a entender uma solução proposta para determinado problema como uma hipótese, especialmente no desenvolvimento de um novo produto ou serviço - que envolva o mercado que se está mirando, o modelo de negócio, como os códigos irão se comportar e até como o consumidor irá usá-lo. Não fazemos mais projetos, mas sim experimentos. As estratégias de Customer Discovery (Descoberta do Consumidor) e Lean Startup são empregadas para testar suposições acerca dos consumidores. O princípio da experimentação também se aplica ao desenvolvimento orientado a testes - nós criamos o teste antes, depois usamos o teste para validar se a programação está correta. Em última instância, o desenvolvimento de um produto ou serviço é um processo de teste de uma hipótese de um comportamento em um ambiente ou mercado almejado. Os principas benefícios da abordagem de experimentação são as evidências mensuráveis e o contínuo aprendizado. Apreender é o que ganhamos ao conduzir um experimento. O que esperávamos realmente aconteceu? Se não, quais inputs podem nos ajudar a direcionar o que devemos fazer a seguir? Ou seja, para aprender, temos que utilizar a metodologia científica para investigar um fenômeno, adquirindo novos conhecimentos e corrigindo e integrando os conhecimentos prévios à nossa visão geral. Com o amadurecimento da indústria de software, temos agora novas oportunidades de alavancar capacidades fundamentais como entrega e design contínuo para maximizar nosso potencial de aprender rápido o que funciona ou não. Ao empregarmos a abordagem experimental para descobrir informações, nós podemos testar mais rapidamente as soluções propostas versus os problemas identificados nos produtos e serviços que queremos desenvolver. Com o objetivo de otimizar a efetividade da solução dos problemas certos, ao invés de se tornar uma fábrica de recursos que continuamente constrói soluções. Um exemplo de empresa para qual trabalhamos que utiliza a metodologia de Desenvolvimento Orientado a Hipóteses é a Lastminute.com. A equipe do projeto formulou uma hipótese que previa que consumidores apenas tinham predisposição a pagar um preço premium para um hotel de acordo com o período do dia em que eles fazem a reserva. Tom Klein, CEO e presidente da Sabre Holdings, compartilhou essa história de como eles aumentaram a taxa de conversão em 400% em uma semana. Combinar práticas como o ""Desenvolvimento Orientado a Hipóteses"" e a ""Entrega Contínua"" acelera a experimentação e amplifica o conhecimento validado. Isso permite que aceleremos a taxa de inovação enquanto reduzimos sensivelmente os custos, deixando os concorrentes para trás. Idealmente, visamos atingir o seguinte processo: mudanças radicais, que nos permitem identificar relações de casualidade entre mudanças em nossos produtos e serviços, com seus respectivos impactos nas métricas. ""Desenvolvimento Orientado a Hipóteses"" é uma grande oportunidade para testar o que você pensa ser um problema, antes de começar a trabalhar na solução. (*) Barry O'Reilly é co-autor do livro ""Lean Enterprise""",pt,80
147,144,1459524591,CONTENT SHARED,2709926970543371965,-1130272294246983140,-5732531255984918367,,,,HTML,http://www.revistacapitolina.com.br/a-importancia-dos-filmes-de-mulherzinha/,a importância dos filmes de mulherzinha - capitolina,"Outro dia conversava com amigos - todos homens - sobre dois filmes que estavam concorrendo a diversas premiações. Um era O Regresso ( The Revenant, 2015 ) e o outro Brooklyn (2015). O primeiro conta a história de um homem que após ser atacado por um urso e traído pelos seus companheiros de viagem, precisa lutar para sobreviver na floresta no meio de um inverno rígido. O segundo conta a história de uma menina irlandesa que migra sozinha para os EUA no final dos anos quarenta. A grande discussão era que eu não senti absolutamente nada além de tédio vendo o primeiro filme. Não tiro o mérito técnico do filme, realmente é muito bem feito, mas pra mim, é mais um homem branco tentando sobreviver, e já vi tantos. Homem branco tentando sobreviver no mar, no fogo, em Marte, já deu pra mim. Enquanto o outro eu amei, chorei do início ao fim. Falou demais comigo esse filme. Meus colegas não aceitavam isso, Regresso era o melhor filme do ano e pronto, não deveria nem ter sido indicado. Depois de quebrar diversos argumentos deles, finalmente me falaram por que Brooklyn era tão inferior: é por ser filme de mulherzinha. Essa discussão me fez pensar muito sobre esse tipo de filme. Dramas centrados em mulheres ou até mesmo as famosas comédias românticas, obras que visam o público feminino, assim como tudo que é destinado à gente ou faz parte do nosso universo é considerado inferior. Parei para pensar nos grandes filmes que são considerados clássicos do cinema ou até nos filmes recentes que ganharam premiações e são considerados ""grandes filmes"" e a maioria esmagadora, passa longe de ser destinado a mulheres. A maioria esmagadora é centrada em homens e suas histórias, nós mulheres estamos ali só para acompanhar e no máximo ajudar eles a alcançarem seus objetivos. Esses são filmes ""sérios"". Nossas histórias são besteiras, são filmes de mulherzinha. Pegue um diretor como Woody Allen, por exemplo. Seus filmes são bem concentrados no diálogo e ele fala até bastante de amor, fez filmes como Noivo neurótico, noiva nervosa ( Annie Hall, 1977) , Manhattan (1979), Tudo pode dar certo (Whatever Works, 2009) , mas na maioria das vezes o ponto de vista é masculino, com um personagem principal homem. Ele (Allen) é considerado um gênio, um mestre do cinema. Agora uma diretora como Nora Ephron, que também fazia filmes muito concentrados em diálogos, que também falava de amor, como Sintonia do Amor (Sleepsless in Seattle, 1993) e Harry e Sally - Feitos um para o outro (When Harry Met Sally, 1989) , esses são filmes de mulherzinha, esses não são tão grandiosos quanto os do Woody Allen, por quê? Voltando ao Brooklyn : li uma crítica feita por um jornalista homem que detonava o filme e basicamente resumia o filme a uma garotinha que quando acha um namorado fica tudo bem e que seu maior desafio é escolher entre dois amores e como isso era pequeno e não valia a pena ser visto no cinema. Primeiro que Brooklyn é muito mais que isso, o filme é sobre amadurecer, sobre quando crescemos precisamos fazer escolhas e lidar com o fato de quando escolhemos algo, inevitavelmente, estamos abrindo mão de outra coisa e que isso causa um sofrimento e uma angústia muito forte. Será que se fosse um homem passando por tudo isso que ela passa, o jornalista conseguira enxergar essa complexidade? Tendo a achar que sim. Segundo que, mesmo se o filme fosse só ela sofrendo as idas e vindas do amor, por que assistir a mulheres lidando com amores, felicidades e sofrimento relacionados a isso é tão menos importante do que ver homens sobrevivendo na selva? Quando eu vi o filme 500 dias com ela ( 500 Days of Summer, 2009 ), fiquei bem irritada e não sabia exatamente o porquê. O filme é uma comédia romântica que conta a história de um rapaz que se apaixona por uma menina que não acredita em amor e como os dois lidam com essa situação. Temos aqui uma troca de papel de gêneros que estamos acostumados a ver no cinema, que é até interessante. Aqui temos o homem super apaixonado e a mulher não; a mulher não curtindo ter relacionamentos, não sabendo lidar com isso direito. Além da personagem feminina ser uma clássica manic-pixie-dream-girl , descobri o que me irritava: esse filme é considerado ""cool"", não é como uma comédia romântica comum, ele é interessante, porque é o homem que está sofrendo por amor e, de repente, quando o homem tem que lidar com isso o filme fica interessante e merece ser comentado. Esse não é o único filme faz isso. Posso citar vários, a maioria são considerados filmes ""indie"", sensíveis e estão em festivais como o Sundance e outros. Não tiro a validade deles e até gosto da maioria, só questiono por que as comédias românticas clássicas com mulheres sofrendo e vivendo por amor não estão nesses festivais. Isso me faz pensar também como é um reflexo da vida real. Quando vemos um homem passando por um término difícil, sofrendo ou tendo que lidar com uma rejeição como ele é sensível e como ela foi uma idiota por ter dispensado ele, já quando é uma mulher tendo que lidar com a mesma coisa, ela é uma maluca que está com raiva, ou coitada, ela é uma desesperada que não para de correr atrás de homem, tem que superar logo. E como esse julgamento diferenciado às vezes faz com que nós mulheres escondamos nossos sentimentos, algo que só nos faz mal . Não quero com esse texto fazer todo mundo amar comédias românticas e filmes de mulherzinha. Entendo também como todos esses filmes podem ser problemáticos, começando pela imensa falta de representatividade, porque só falamos aqui de casais héteros e mulheres brancas, magras e cis. O mundo das mulheres não gira somente em torno dos homens e de amores, tem muito mais a ser explorado e grande parte desses filmes deixam de ir mais a fundo e ficam na superfície. O que eu quero é não cair mais nesse falso julgamento de que para ser um grande filme sério e com méritos a história tem que ser centrada em um homem. Precisamos olhar para os filmes que tratam do mundo feminino com outros olhos. Dando a mesma importância e valor que damos aos outros. Precisamos principalmente incentivar mulheres a entrarem nesse clube do Bolinha que é o cinema e deixar elas se tornarem roteiristas e diretoras, para elas poderem contar as nossas histórias.",pt,80
148,1596,1467296491,CONTENT SHARED,2267321579404324900,881856221521045800,-1609125166981192463,,,,HTML,https://developer.ibm.com/bluemix/2016/02/01/migrating-from-parse-to-bluemix/,migrating from parse to bluemix,"This week Parse.com , Facebook's Mobile Backend as a Service offering, shocked both their customers and the development community by announcing that they are sunsetting the service to ""focus resources elsewhere."" Luckily, they're not leaving current customers high and dry - they are giving developers a year's notice before the service is shut down, providing data migration tools to MongoDB , and open-sourcing parse-server , a Parse.com API-compatible router package for Express.js on top of Node.js. If you're a Parse customer looking to move your app infrastructure to someplace secure, then you're in luck. Bluemix is the place for you. In this post, I spell out the process of migrating an application from Parse.com to Bluemix. Let's get started! Create your Parse-on-Bluemix application The first thing you need to do is create an app on Bluemix . If you don't already have a Bluemix account, you can sign up here (it's free to try!). Once you're logged in, it's time to create a new Node.js app on Bluemix to host the Parse server. Click the CREATE APP button on your dashboard. Then, when prompted, select the WEB application option. This doesn't mean that you can't connect a mobile app to it; this option just sets up the backend application server, without provisioning any Bluemix mobile services like Mobile Client Access . The web option is used to deliver REST API calls, but does not support any enhanced mobile security or analytics features. Next you need to specify the type of application you want to create. Bluemix supports a variety of server-side processing options; in this case you want to select ""SDK for Node.js"" and then CONTINUE. Next you are prompted to enter an application name. Go ahead and enter one. (Application names must be unique across Bluemix.) When you click FINISH, the app is staged, and you are presented with instructions and next steps. At this point, click ""Overview"" to go back to the app's dashboard. Add the MongoDB service to your Bluemix application The next step is to create a MongoDB instance to migrate the data from Parse.com. From the app's dashboard select ADD A SERVICE OR API. Scroll down to the ""Data and Analytics"" section and select ""MongoDB by Compose"" to add a MongoDB instance to your application. You are then presented with details for configuring a MongoDB instance. Open up a new browser window for the next step - leave Bluemix open, because you're going to need to come back here to finalize the MongoDB configuration. Create and configure the MongoDB instance on Compose.io The MongoDB instance on Bluemix is available through Compose.io, a recent IBM acquisition. To create and configure the MongoDB instance, click on this Compose link and navigate to compose.io. If you don't already have a Compose account, you need to create one (they're free for 30 days!). Once you're logged in, click on the deployments icon (top icon on the left side) and select the MongoDB option to create a new MongoDB deployment. You need to specify a deployment name and region, then click ""Add deployment"" to create the MongoDB instance. For this example I created a deployment called ""parse-migration"". Next you need to create a database instance. Once the deployment is created, click on ""Add Database"" and create a new database. In this sample I created a database called ""myDatabase"". Once the database is created, you need to add a user account to access the database. On the database screen, select the ""Users"" menu option on the left side, and then click ""Add User"" to create a new user. Next click on the ""Admin"" (gear) link on the left side to view the connection info for your database instance - you're going to need this in the next step. Go back to the Bluemix MongoDB screen and specify the MongoDB host string, port, and login credentials, then click CREATE to finalize the MongoDB configuration within your Bluemix app. The host string is in this format: For this sample app, it is: The port is 10373 , and the username and password are the ones for the account that was just created. Migrating data from Parse.com to your MongoDB instance At this point you're ready to migrate data out of Parse.com, and into the MongoDB instance you just created. Detailed instructions are available in the Parse.com migration guide . Log into Parse.com and select the app that you want to migrate. Go into ""App Settings"", then select ""General"", and click the ""Migrate"" button to begin the process. Note: You must be in the new/beta view to see this option. To begin the migration process, a modal dialog asks you to specify your database connection string. The connection string is in the format: So, in the example app, it is: Click ""Begin the migration"". In a few minutes the migration will complete, and you can return to your MongoDB database on Compose.io to see the migrated data. Note: Migration times vary depending upon the amount of data you need to migrate. Configuring your Bluemix application Next, let's configure some environment variables to use in the Node.js application. On the Bluemix app's dashboard select the ""Environment Variables"" option on the left menu, then select USER-DEFINED. Add environment variables for APP_ID , MASTER_KEY , and REST_KEY that correspond with the App Keys from Parse.com. Also add a PARSE_MOUNT key that contains the path where the services will be exposed on Node.js. For this last one, just use the value /parse . On Parse.com, you can access the ID and Key values by going to the app, selecting ""App Settings"", and then selecting ""Security & Keys"". Save these environment variables. Now we're ready to deploy the Node.js application. Deploy your Bluemix application The parse-server API is a router for Express.js (an application framework for Node.js). It can be easily leveraged in a new Node.js/Express.js app, or dropped into an existing Express.js app to expose the Parse.com API. There is a sample application in the parse-server repository that you can copy to get started. Or, you can take the easy route. IBM Cloud Data Services Developer Advocate Mike Elsmore has put together a base Node.js application that can be quickly deployed that already supports the Bluemix environment configuration. You can click the ""Deploy to Bluemix"" button below to deploy this project directly to Bluemix in a single click, or head over to github.com/ibm-cds-labs/parse-on-bluemix to view the source code. If you want to deploy the application manually, then clone it to your local machine and use the Cloud Foundry API cf push command to push it to Bluemix. You application will be deployed and staged, and it's now ready for consumption. Testing the deployment You should now test the deployment using curl commands from the command line to verify that the migration and deployment were successful. Note: the actual URL depends on your data model. If things were successful, you should see data returned from your parse server instance: Or you can post data to the server to test write ability: What Next? If you are using Parse Cloud Code, then you can copy your Cloud Code files into the Node.js project. (See the ""cloud"" directory in the git project.) For additional details, be sure to check Parse.com's migration guide. Configure the client apps Once you've stood up the new back-end system for your app(s), you're ready to point the mobile client applications to the new back-end infrastructure. The most recent version of the Parse.com SDK introduced the ability to change the server URL (at least version 1.12 for iOS, 1.13.0 for Android, 1.6.14 for JS, 1.7.0 for .NET). Limitations and warnings Parse.com has done a great job providing you with tools to migrate your apps off their platform, but be warned - not all features of the Parse.com platform are available in the open-source parse-server. Push notifications are not implemented. Luckily, the push notification implementation can be modified to support IBM Bluemix's Push notification service , so your app does not have to suffer loss of functionality. In addition, Parse.com's App Links do not have a direct replacement in the parse-server. Many of these offerings have separate Node.js npm modules that can be leveraged, but each requires additional development effort; they are not drop-in ready. App settings are impacted by this transition. This includes social logins, user authentication and sessions. App analytics, the client dashboard, in-app purchases, and jobs are also not supported. Luckily, the Bluemix catalog has many of these features covered, plus a whole lot more. If you run into any cases where the parse-server seems to just ""do nothing"", meaning services don't seem to exist or they fail silently, then double-check your PARSE_MOUNT environment variable and MongoDB connection/host string to make sure they are in the proper formats mentioned above. For more details on transitioning away from Parse.com, be sure to read the sunset announcement , the database migration tool , and the parse-server migration guide .",en,80
149,1768,1468496385,CONTENT SHARED,2727743992157210358,268671367195911338,876332810118518702,,,,HTML,http://thefinancialbrand.com/59403/digital-banking-transformation-future/,the future of digital banking is now,"Banking executives convened to discuss the opportunities and challenges of digitization in banking. Their consensus was that becoming a 'digital bank' is no longer just an option... it's a necessity. Digital transformation isn't new - it's been happening in different industries over the past 20 or 30 years - with different waves occurring across various industry segments. In the 1990s, music, retailing, photography and video were all impacted by new entrants who used digital capabilities to change the way services were delivered and consumed. In the 2000s, TV, travel and recruitment were impacted, with the advent of YouTube, online travel sites and job posting boards. The 2010s find industries like retailing experiencing their second wave of digitization (first seen in the 1990's) while financial services finally begins to discover the opportunities and challenges of digitization. Beyond simply making current processes digital, today's transformation is dramatically impacting the way in which customers interact with brands. Instead of visiting physical facilities, consumers in all industries are beginning their shopping and buying experiences online or with a smart phone, altering all phases of the traditional customer journey. A new report, produced by and Oracle Financial Services Global Business Unit , entitled ' Digital Transformation - The Challenges and Opportunities Facing Banks ' looks at the current and future impacts of digitization within the banking industry. This report is the culmination of a series of three 'Think Tank' sessions hosted by these organizations. Impact of Digital Transformation While a great deal of attention is given to the individual players that are impacted by digital transformation (Kodak, Blockbuster, Borders, etc.),there is often quite a large financial impact on overall industry segments. As stated in the Efma/Oracle report, ""A clear example can be seen in the music industry, where the global market now is only worth about half of its value in 2000. Another example is with the newspaper print advertising market, which is about a third of what it used to be."" Not only is there a major financial impact to digital transformation. The report also found that digital transformation occurs significantly faster at a much lower cost than transformations of industries in the past. In addition, because of the lower cost of entry, more players can be informed in the process (enter fintech start-ups). The consequence is a more rapid pace of change. For example, as mentioned in the report, in Brett King's book, 'Banking 3.0', he chooses a level of 50 million users as the definition of a target figure for a market. For planes and cars to reach this level took over 60 years. Credit cards took 28 years but more recently, contactless credit cards took only four years to reach 50 million users, while Facebook and Twitter took only 3 and 2 years respectively. ""So, the pace of change is getting much faster and the financial services industry has to adapt if banks are to succeed and survive,"" states the research. Role of Fintechs It is well documented that there is a mass influx of new competitors in the financial services space globally, with massive funding of both new start-ups and innovations by large, established technology companies like Google, PayPal, Facebook and Amazon. According to the research study, fintechs have primarily focused on three segments of the financial services industry: payments, lending and personal finance. The two main reasons why these are the primary segments that the fintechs are pursuing include: They are areas with significant fees and that also have a strong push towards a digital interaction. Fintechs want to be able to work cheaper, faster and clearer and to provide a better transparency of what's happening. Despite the aggressive competitive environment, the vast majority of fintech firms lack scale and market awareness. While part of this can be attributable to a lack of successful marketing by all but a few of the new entrants, the major hurdles for fintech firms include a lack of capital, no legacy customer base, the trust level afforded legacy financial organizations and the understanding of regulatory and compliance issues by traditional banking firms. As a result, many fintech firms (and legacy banking organizations) are pursuing partnerships. ""The fintech companies have the advantage in terms of speed, agility, and the capacity to understand and quickly build a very good user experience. However, they don't have the legacies that banks have and they have a completely different mindset - and with the lack of scale and trust, it's not as easy as it might seem for fintechs to move forward without banks,"" says the report. Ultimately, some fintechs will obtain a banking license, others will partner with larger banks or perhaps later will be acquired by larger financial institutions, while others may do a bit of both. Key Digital Strategies for Banking Oracle has observed different approaches that banks have been using to help to drive a digital strategy as well as to react and deal with some of the fintech companies. The report explores four key digital strategies that banks have been using with varying success. These are: Launching a digital brand - This might involve positioning a new brand differently from the existing one, or developing a set of processes that enable the new digital brand to compete in a different way. Many different banks have done this across a lot of different markets. Examples include Fidor Bank in Germany, UBank in Australia and mBank in Poland. Digital brands focus on simplicity of design and ease of use through digitization. They also compete on price because as digital-only players, they can become more aggressive in this area, as they have much lower costs than traditional banks. Many of these brands have achieved relatively modest scale. Digitizing processes - This is a key area where traditional banking organizations can compete with fintech firms. To do so, legacy banks must digitize both front-office and back-office processes based on the expectations that have been set by all of the other digital brands (not just financial services). The key digital processes from the consumer's perspective include customer onboarding, originations and relationship pricing. Modernizing the digital experience - The development of a digital enterprise is based around the four 'Ps' - Product, Price, People and Place. Place (which can be physical or virtual) refers to the need to enhance the digital experience at the point where the customer is. Unfortunately, the digital experience of many financial services companies tend to be rather dated. Key customer-facing digitization includes use of HTML5, responsive design, the ability to fully support all mobile devices, the integration of the Internet of Things, and even open APIs. Launching new digital capability - When looking at a new capability, organizations may want to deliver something completely new outside their overarching mobile app, such as money movement apps, mobile wallets or the use of data as 'currency'. Oracle also believes banks need to start thinking more seriously about how they can take advantage of innovations such as virtual reality, FitBit or the Internet of Things, as all of these will play an integral role in the future lives of consumers. They believe banking organizations also need to reexamine and transform the role of the branch, so that it becomes more focused on customer service and advice rather than transactions. The Time to Take Action It is clear that differentiation - and competitive advantage - is occurring based on the ability of financial institutions to embrace and implement digitization. The firms that aggressively pursue the digitization of both back and front-offices will be in a better position to compete for the increasingly digital consumer and will be able to reduce costs (and increase revenue) based on this transition. As stated in the Efma/Oracle report, ""Overall, although the financial services sector isn't unique in terms of suffering from the effects of disruption, banks can no longer afford to sit back and do nothing. There are numerous new technologies that they should be taking advantage of - and they could learn a great deal from other industries who have already had to face disruption. Despite the problem of regulation, there are also still countless valuable insights that they could can gain from different sources of customer information that would enable them to sell more and also to provide better advice."" It is becoming abundantly clear that becoming a 'digital bank' is quickly becoming 'table stakes' as opposed to a nice benefit or feature of today's banking experience. Access the Report Oracle teamed up with the Efma to learn about the digital strategies that banks are implementing to tackle these challenges in a series of virtual Think Tanks that were attended by 40+ banks from across EMEA and JAPAC. EFMA is a global not-for-profit organisation that brings together more than 3,300 retail financial services companies from over 130 countries - including almost a third of all large retail banks worldwide. Download and read the 32-page report, ""Digital Transformation"" here . Jim Marous is co-publisher of The Financial Brand and publisher of the Digital Banking Report , a subscription-based publication that provides deep insights into the digitization of banking, with over 150 reports in the digital archive available to subscribers. You can follow Jim on Twitter and LinkedIn . All content © 2016 by The Financial Brand and may not be reproduced by any means without permission.",en,80
150,2647,1477438797,CONTENT SHARED,8769121796537771237,2542290381109225938,3896024018019760972,Android - Native Mobile App,FL,US,HTML,http://www.revistaapolice.com.br/2016/10/susep-divulga-nota-de-esclarecimento/,susep divulga nota de esclarecimento sobre a youse seguros,"Em atenção à propaganda e à forma de comunicação empregadas pela Youse, em especial no seu sítio eletrônico na Internet, a Susep - autarquia federal responsável pela supervisão dos mercados de seguros, resseguros, previdência complementar e capitalização, esclarece: 1. A Youse não é sociedade seguradora autorizada a funcionar pela Susep, consequentemente, não possui produtos aprovados pela Susep, nem tampouco autorização para comercializar produtos de seguro, nessa condição; 2. A referida empresa já foi notificada pela autarquia quanto à propaganda e à comunicação empregadas, por induzirem a erro potenciais consumidores, afrontando direito básico do consumidor, conforme prevê o art. 6°, inciso III, da Lei n° 8.078/90 (Código de Defesa do Consumidor); 3. A Caixa Seguros Holding S.A ingressou com pedido de autorização de constituição para seguradora denominada Youse, sob o processo Susep n° 15414.001677/2016-46, cuja análise ainda se encontra em andamento, portanto, pendente de aprovação; 4. As reclamações e denúncias apresentadas à Susep a respeito da comercialização de produtos pela empresa estão sob análise da Diretoria de Supervisão de Conduta. Com o objetivo de continuar protegendo os interesses dos cidadãos e demais agentes envolvidos com os mercados de seguros, resseguros, previdência complementar e capitalização, a Susep coloca-se à disposição para prestar esclarecimentos adicionais. Joaquim Mendanha de Ataídes Superintendente da Susep Fonte: Susep L.S. Revista Apólice",pt,80
151,3104,1487597538,CONTENT SHARED,-6872546942144599345,-1393866732742189886,-6350745898785551312,"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36",MG,BR,HTML,https://medium.com/@husayn.hakeem/my-experience-with-googles-associate-android-developer-certification-40b34b27ebc7,my experience with google's associate android developer certification,"In this article I talk about my personal experience through the Google Android Certification exams. I'm expressing my opinion, don't take it for facts. Also, any information given here about the Google Android Certification is also available in more details on its official website . On the new year's eve, I decided to take the Google Android Certification. It was an abrupt decision, and frankly speaking I didn't know what to expect, even though my interest for android development had began 2 years prior to this, I had only just started working professionaly as an android developer 4 months prior to making this decision. Before doing so, I did a bit of research online, read about the certification on its officiel web site and forums and discussed it a bit with my team lead at work. About the Google Android Certification The Google Android Certification covers the following sections: Testing and debugging Application User Interface (UI) and User Experience (UX) Fundamental Application Components Persistent Data Storage Enhanced System Integration So basically a candidate taking this certification is expected to have an understanding of the full lifecycle of Android applications development, from clearly defining and understanding the app's main functionalities, to designing and developing UX friendly interfaces, passing by being able to debug the app during its development process, and finally to testing the app. The applicant is also expected to have a minimum knowledge of data persistance, Android's 4 main application components (Activity, Service, etc) and when & how to use them, and finally how to manage app notifications and widgets. The certification exam is divided into two main parts: The programming phase and the interview phase . The programming phase is to be completed and submitted within 48 hours after starting it. I had read on a couple of forums that it shouldn't take more than 1 day to complete it, personally I was able to complete it in about 4 hours, while taking my time and taking one pause. The interview phase was also fun, I was asked a couple of questions to which I felt I answered quite good. The durations between the phases of the certification, their results and receiving the digital badge aren't long at all, from what I had read on a couple of forums I got the impression that the process would take forever, but to my surprise it was quite rapid and smooth, and the response from the support team was quite fast, they got back to me a couple of hours after I had sent them some remarks regarding the first exam. So all in all, was it worth it? I'd definitely yes, for the following reasons: As a fresh graduate and working as a junior Android developer, the certification definitely made me look more ""mature"" to recruiters, especially that it is provided by Google, which gives it, if anything, more authenticity and credibility. Being certified guarantees and testifies that I have -at the least- moderate skills in the basics of Android development. The exams aren't difficult at all (they weren't for me at least), but it's a great way to expose your android coding skills and put them to the test. The experience was fun, I didn't know what to expect before taking the certification, so I was expecting the process to be a bit stressful, but it wasn't at all, I really enjoyed both phases of the certification. During the first phase of programming, I encountered a malfunction in the provided starter code. I'm not sure whether this was done intentionally or not, but I was able to find it, fix it and report it to the appropriate team. Proving again that your skills are put to the test during the exam. The certification isn't expensive at all, at the time I took it it was $100. What I didn't appreciate? I wouldn't go as far as to say that there are things I didn't like in the certification process, but I think a couple of things can definitely be improved. I wish there were different Android certifications with different levels of difficulty. This would be more interesting as it would put one's skills to the test in a more challenging way. It would be better if the Android certification test (at least the first phase of coding) was done in designated exam centers preferably without internet access, making it impossible for candidates to copy code off the internet and forcing them to rely solely on their skills. So, to sum up? To sum up, I'd say that my experience taking the Google Android Certification was quite a good one, if you're thinking of taking it I'd say go for it! Especially if you're a novice to android development. If on the other hand you're been into Android app development for a while (a couple of years for example), I think your experience and apps you've worked on would speak louder for your skills than the certification would.",en,80
152,2576,1476729411,CONTENT SHARED,-1787737696395220629,-4028919343899978105,6702709297756973694,"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.143 Safari/537.36",SP,BR,HTML,http://www.b9.com.br/67542/tech/laboratorio-da-disney-cria-robo-que-pula/,laboratório da disney cria robô que pula,"Como se não bastasse o robô capaz de subir pelas paredes , a Disney inventou um novo robô. Dessa vez os laboratórios da empresa criaram um robô que pula. Veja no vídeo acima. No topo da sua perna única, ficam o computador, os sensores e uma bateria, enquanto que a perna é feita de um mecanismo de molas paralelas. O computador é capaz de calcular e executar pelo menos 19 pulos antes dele perder o equilíbrio e cair, mas ainda assim é um progresso interessante para uma companhia investindo na área. A sugestão do The Verge é que ele seja transformado em um brinquedo o mais rápido possível - e se não for incomodar muito, que seja um robô do Tigrão, do Ursinho Puff.",pt,80
153,1609,1467340692,CONTENT SHARED,7851022564818849560,6644119361202586331,-2763188495283035571,,,,HTML,https://www.linkedin.com/pulse/workplace-diversity-brazil-theres-still-long-way-go-ruy-shiozawa,workplace diversity in brazil: there's still a long way to go,"Portuguese: Here, at Great Place to Work Brazil, we're always encouraging companies to promote diversity in the workplace. A great example of that was our latest GPTW Women Event , which took place on March 8th. Although we were able to identify important advances related to diversity then, there's still a lot of work to do as well . We are used to commend Brazil for its diverse cultural background. We like to believe that people from all tribes, colors and groups can live together around here. That's all very nice in theory but, as usual, the reality is uglier: Yes, Brazil is a big cultural cauldron, but unfortunately it's one still boiling with prejudice - gender, ethnicity, sexual orientation, age, disability... they all suffer from some sort of bias or stigma. And those are amplified, especially in the workplace . To keep it shorter, we'll talk now about the first three groups and compare theory and practice. Let's start with gender inequality . Equal job opportunities for men and women are one of the many fronts in this battle. Looking at GPTW's Best Workplaces in Brazil, we can say we're really close to equality - in a superficial level. However... We already talked about this in our blog a couple of times. The higher we go in the companies' hierarchies, less women we are able to find. And the more excuses we hear: women aren't as willing to relocate because of family - wouldn't you too if society had historically told you that you're the one supposed to take care of the household? They could be on leave for months due to pregnancy (a decision that tends to involve a man too). And even absurdities like women not being able to convey credibility to investors who are, in their large majority, (guess who?) men. If we are tired of hearing this nonsense, imagine how women feel about it. And we still haven't even touched salary inequality. Even among the companies recognized in our GPTW Women Event, a woman is paid, on average, 35% less than a man with the same position . And there are excuses for this too, but frankly they don't even deserve to be written here. Enough to say it's another byproduct of a biased society that allows ignorance to perpetuate ad infinitum . Talking about ignorance, let's demystify one of the main excuses for ethnic inequality in the workplace , shall we? It goes somehow like this: ""there are less black people than white people in leadership positions because they tend to be part of an underprivileged population with less access to education and, therefore, not qualified enough to be an executive leader in today's competitive market."" First of all, using this is an excuse makes it seem like a fact of life; one we should simply accept and move on. No! It's actually something rotten in our society that needs to be fixed right away . But for the reality deniers out there, let's pretend this is an acceptable argument for a second , just so we can smash it into oblivion once and for all. According to DIEESE (Inter-Union Department of Statistics and Socio-Economic Studies), 11,8% of the black population in Brazil have a college degree. Among the non-black population, this number jumps to 23,4% (almost twice as much). At a first glance, it looks like the argument makes sense. But once again, that's true only in a superficial level. Borrowing some data from Previ ( Banco do Brasil 's pension fund), we find out that only 2% of the top and middle management positions in the 114 Brazilian biggest companies are filled by darker-skinned professionals. Therefore, there are around twice as many white people than black people in our population with a college degree, but that distribution (2:1) jumps to 49:1 in companies' leadership positions . Weird, huh? Also weird is the fact that most companies argue they have practices to eliminate prejudice related to race/ethnicity and to develop non-Caucasian leadership. Looking at those numbers, there's a simple message I can see: It's time for a change! Those practices are clearly ineffective . Besides, that same DIEESE research points that white professionals in the industry and the trade sectors earn around 20% more than black professionals among people without complete elementary school or lesser education. Among professionals with a college degree, the difference grows to nearly 40% . It makes me wonder how much less a black woman is paid when compared to a white man. And now, to end this horror show: a research conducted by the recruiting and hiring company Elancers revealed that 7% of the interviewed companies have declared they would not hire a homosexual candidate under no circumstances whatsoever , and 11% of them would only hire that candidate if they knew there was no chance that employee could reach a position of influence/visibility (such an executive). And remember: those are the companies that declared so. How many more felt the same way but didn't have the guts to admit it? In other words, if you're a homosexual man or woman in Brazil and you want a job where you'll be treated fairly, at least next to one in five companies in the country are not for you . But that doesn't happen only around here! In an experiment conducted by the Harvard University, nearly identical résumés were sent to around 1,700 recruiters in the USA. The only difference among the documents: in half of them, it was also added that the candidate had previously worked as treasurer in the college's LGBT group. The result: the résumé without that extra piece of information got four times more calls to a job interview . And the problems do not end in the hiring process: according to the LGBT Out Now research, only 3 in 10 gay executives in Brazil speak openly about their sexual orientation at work . They're afraid for their jobs if they come out. And companies are losing money over it: the same research also points that among openly gay employees, 75% consider themselves to be productive. Among the ""closeted"" ones, only 46% felt the same way. It's hard to focus on work if you're spending all your energy to pretend being someone different than you are . And we haven't even talked about transgenders , who can barely hope to find any job in a company, let alone a decent one with perspective to grow. And this leads to very serious consequences: according to ANTRA (National Association of Transgenders and Cross-Dressers), 90% of the transgender and cross-dresser population in Brazil need to prostitute themselves as a mean to survive. So enough saying Brazil isn't a sexist country. Or racist. Or homophobic. It's all of that (and alarmingly so) inside and outside of the workplace. Enough with all the excuses! In order to solve any problem, it's necessary to admit its existence in the first place. And that's what Brazilian companies (and society as a whole) need to do right now. Whatever it is we're doing, it is not enough . We have to do more. Much more. From our side, we'll be watching more and more attentively. And we'll be more and more demanding of the companies - which is why you can expect more actions from Great Place to Work Brazil to support diversity, such as GPTW Women. Together we can work to eliminate prejudice from the workplaces and, hopefully, our society.",en,79
154,1180,1464790328,CONTENT SHARED,-8813724423497152538,-1602833675167376798,-3784159144403077709,,,,HTML,http://www.baguete.com.br/noticias/25/04/2016/itau-e-pioneiro-do-blockchain,itaú é pioneiro do blockchain,"O Itaú é o primeiro banco da América Latina a participar do consórcio R3, uma iniciativa que reúne 42 duas instituições financeiras de todo o mundo para desenvolver projetos relacionados a tecnologias de registros compartilhados baseados em Blockchain. Banco é pioneiro na América Latina. Foto: divulgação. A iniciativa,comandada pela startup americana R3CEV, começou em setembro do ano passado com nove bancos: Barclays, BBVA, Commonwealth Bank of Australia, Credit Suisse, Goldman Sachs, J.P. Morgan,Royal Bank of Scotland, State Street e UBS. Um blockchain é um banco de dados distribuído, no qual novos registros de transação estão linkados entre si por marcadores de tempo compartilhados. Cada bloco, acessível por todos os participantes, contém o registro de uma série de transações. A tecnologia ficou conhecida por ser a base do bitcoin, criptomoeda eletrônica que tem sido uma febre no setor financeiro nos últimos anos, mas o princípio pode ser utilizado numa série de sistemas financeiros ou qualquer outra situação que demande registros públicos confiáveis. ""A tecnologia tem outras aplicações que possibilitam novos modelos de negócios, agilidade e eficiência em um ambiente seguro, de forma descentralizada, garantindo rastreabilidade das transações e autenticidade das partes envolvidas"", aponta o Itaú em nota. No que vai de ano, o R3 já anunciou três testes bem sucedidos envolvendo bancos participantes da iniciativa. O último deles contou com tecnologias da Eris Industries, IBM, Intel e Chain para facilitar a negociação de instrumentos de débito. Esse experimento foi a continuação de um primeiro feito com 11 bancos, usando Ethereum hospedado na nuvem Azure da Microsoft. ""Estamos certos de que essas inovações trarão benefícios para nossos clientes e ganhos reais de eficiência para o setor como um todo"", afirma Márcio Schettini, diretor-geral de Tecnologia e Operações do Itaú. O Itaú é o maior banco privado do Brasil, com 90 mil colaboradores, mais de 5 mil agências e PABs, e quase 26 mil caixas eletrônicos.",pt,79
155,2363,1474048569,CONTENT SHARED,-4655195825208522542,-3595444231792050977,-5218411221113627439,,,,HTML,http://blog.caelum.com.br/java-9-na-pratica-jigsaw/,java 9 na prática: jigsaw,"Há muito tempo se diz sobre modularizar a plataforma Java . É um plano que começou desde antes do Java 7, foi uma possibilidade no Java 8 e por fim, para permitir mais tempo de desenvolvimento, revisão e testes, foi movido para o Java 9. O projeto Jigsaw, como foi chamado, é composto por uma série de JEPs . Algumas delas inclusive já disponíveis no Java 8, como os conhecidos Compact Profiles . A idéia por trás do projeto não é só criar um sistema de módulos, que poderemos usar em nossos projetos, mas também aplicá-lo em toda a plataforma e JDK em busca de melhor organização e desempenho. we propose to design and implement a standard module system for the Java SE Platform and to apply that system to the Platform itself, and to the JDK. The module system should be powerful enough to modularize the JDK and other large legacy code bases, yet still be approachable by all developers. Neste novo post da serie Java 9 prático não só veremos que o projeto já existe e está integrado aos últimos builds, como também vamos explorar e implementar nosso próprio projeto modular. Tudo ao estilo hands on, claro - preparem seus compiladores! Preparando o seu ambiente para o Java 9 modular O Jigsaw foi integrado ao build do JDK 9 desde a versão ea+111, portanto tudo que você precisa para usá-lo é baixar uma versão igual ou superior a ela. Eu recomendo bastante que, mesmo que você já tenha uma versão relativamente atual, baixe sempre a última antes de qualquer teste. Você pode fazer o download aqui . A versão que estou usando neste post é do build 134 (mais atual de agora). Criando um hello world tradicional No lugar de já começar criando o projeto modular com Jigsaw, vamos criar um projeto da forma tradicional e depois migrá-lo, assim a diferença entre as duas abordagens deve ficar bastante clara. Nos exemplos do post vou fazer tudo pela linha de comando e ir compartilhando os snippets, mas para acompanhar, você pode e deve usar o gerenciador de arquivos do seu sistema operacional e qualquer editor de sua preferência. Os passos são simples, começando pela criação dos diretórios do projeto e de seus pacotes. O projeto se chamará hello-jigsaw e as classes Java devem ficar no pacote br.com.caelum.jigsaw . Podemos fazer tudo com o comando: Claro que, se você preferir, pode criar usando o gerenciador de arquivos do seu sistema operacional favorito. O importante é que, no final, a estrutura de diretórios fique assim: Tudo ok. Então agora podemos criar a classe JigSaw.java , dentro de src/br/com/caelum/jigsaw : Aqui tem um vídeo com todos os passos desse processo, caso queira dar uma olhada. E também vou deixar um link para fazer download ou navegar pelos arquivos com esse estado do post. Compilando e executando, ainda da forma atual Para compilar o projeto podemos usar o comando javac e pronto. Nada de especial aqui. Feito isso, caso não exista nenhum erro, podemos executar com: Repare que, como já criamos dentro de um pacote e da pasta source , foi preciso passar o parâmetro -cp (classpath) apontando para ela e também usar o nome completo da classe, que inclui o pacote. Talvez você não esteja tão acostumado com isso, afinal a maioria de nós não compila classes na linha de comando no dia a dia, mas não tem nenhuma novidade por enquanto. Ao executar, o output, conforme esperado, será a mensagem: Olá Java 9 modular! . Você pode ver aqui o vídeo do processo de compilação e execução. Agora finalmente vamos para a parte divertida, que será migrar o projeto para que seja modular! Migrando para um projeto modular A migração não é nada complicada. Você precisa basicamente criar uma nova pasta com o nome do seu módulo, que por convenção, é o pacote base do seu projeto. Você pode pensar no nome do módulo como algo parecido com o groupId de um projeto do Maven. É aberto, você pode escrever qualquer coisa, mas idealmente deve seguir uma convenção. Em nosso caso o módulo será chamado de br.com.caelum.jigsaw : Ok. O próximo passo será mover todos os arquivos do projeto, de dentro da pasta src, para dentro deste novo diretório do módulo. Você pode fazer isso com o comando mv do terminal ou simplesmente mover a pasta br pelo seu gerenciador de arquivos com tudo que tem dentro para o diretório do módulo. Na linha de comando seria assim: Agora falta criar um arquivo chamado module-info.java , também dentro deste diretório do módulo - tudo ficará nele a partir de agora. Esse arquivo será o responsável pela declaração do módulo e futuramente as suas dependências. A configuração é mínima: Tudo pronto, no final de todos os passos nosso projeto deve ficar assim: Vou deixar aqui os links do post nesta fase também, caso queira acompanhar: [ video , download do código , navegar pelo projeto ] Nosso projeto agora é modular! Nada complicado, não é mesmo? Vamos ver agora como compilar e executar um projeto com esse novo formato. Compilando um projeto modular O comando javac para um projeto modular é um pouco diferente. Ele terá a seguinte estrutura: Onde <1> é o diretório onde ficará o código compilado, <2> é o caminho para o seu arquivo module-info.java , e <3> é o caminho completo para o seu arquivo .java. O comando completo para compilar esse nosso projeto ficará assim: Execute e, se nenhum erro de compilação for exibido, você verá que a nova pasta mods foi criada com todo o código compilado. Inclusive os metadados do module-info. O projeto ficou assim: Agora temos o diretório src, com o código fonte, e o diretório mods , com o código compilado. Executando um projeto modular Para executar você usará o comando java , como sempre, mas agora com esse formato: Onde <1> é o nome do diretório onde está o código compilado do nosso módulo, ou seja, a pasta /mods . E <2> o nome completo ( full qualified name ) da classe que será executada. Um detalhe importante é que, agora que estamos trabalhando de forma modular, o nome da classe recebe também o nome do módulo como prefixo, logo antes do nome do pacote. O comando para executar a classe JigSaw ficará assim: E a saída, assim como esperado, será o print: Olá Java 9 modular! Aqui vão os links de [ video , download do código , navegar pelo projeto ] Adicionando um alert do Swing Por enquanto usamos apenas um System.out no projeto, apenas recursos do pacote padrão java.lang . Vamos experimentar usar um importe de alguma outra API do próprio Java pra ver o que acontece? Uma forma simples seria usando um alert do Swing para imprimir a mensagem: Maravilha, agora basta compilar utilizando o mesmo comando de antes e ops... Você pode ver aqui um video com esses passos e o erro gerado. O que houve? A mensagem diz que o pacote javax.swing não existe! Claro que o Swing não foi removido no Java 9... mas a partir do momento em que compilamos nosso projeto modular, o compilador só usará os módulos que declararmos como dependência no arquivo module-info.java . Até então não tinhamos nenhuma. O JRE modular A primeira coisa que eu fiz ao baixar o build do Java 9 com Jigsaw foi abrir o diretório onde ele está instalado e ver o que mudou. Se você fizer isso aí na sua casa, vai perceber que no lugar da pasta jre/lib , onde ficavam todos os .jar s da JRE, você encontrará um diretório jmods com todos os módulos. E não são poucos. The original goal of this Project was to design and implement a module system focused narrowly upon the goal of modularizing the JDK, and to apply that system to the JDK itself. We expected the resulting module system to be useful to developers for their own code, and it would have been fully supported for that purpose, but it was not at the outset intended to be an official part of the Java SE Platform Specification. O pensamento foi: se vamos criar um sistema de módulos na plataforma Java, por que não modularizar o próprio JDK? E assim foi feito, todo o código foi reorganizado em módulos. Você pode ver todos executando o seguinte comando: Você pode ver todos na imagem deste link . Um detalhe que você deve ter percebido é que, assim como não era preciso importar o java.lang para usar as classes System , String e outras, você também não precisa declarar nenhum módulo. Essas classes estão presentes no módulo padrão, java.base . Outro detalhe importante é que você não é obrigado a usar Java modular, claro. Você só precisa declarar de quais módulos a sua aplicação depende se ela for modular... caso exista o arquivo module-info . Declarando a dependência entre módulos As classes do Swing , como o JOptionPane que usamos, estão em um módulo chamado java.desktop . Para ensinar ao compilador que precisamos desse módulo em nosso projeto, basta declará-lo no arquivo module-info.java . A mudança é simples, o código ficará assim: E pronto! Você não precisa de nenhuma configuração adicional, basta usar o mesmo comando para compilar e agora ele saberá de onde vem o pacote javax.swing . Zero erros. Agora executamos o código e o alert será exibido: Aqui vão os links de [ video , download do código , navegar pelo projeto ] É claro que, assim como você não precisa executar os comandos javac e java manualmente na linha de comando hoje, passando o classpath e caminho para as dependências do projeto, você também não vai precisar fazer isso com o projeto modular. As IDEs vão adicionar suporte para que isso fique transparente e para que você não se preocupe tanto com os detalhes. O post foi um overview rápido sobre um assunto bastante abrangente, então, sem dúvida alguma novos posts entrando em detalhes que não foram discutidos aqui virão em breve. Não deixe de deixar suas perguntas e sugestões de assuntos aqui como comentário, são elas que estão guiando os posts da serie Java 9. E aí, o que achou do Jigsaw? Tem outros assuntos que gostaria de ler aqui no blog sobre Java 9?",pt,78
156,3108,1487608319,CONTENT SHARED,-2402288292108892893,-4465926797008424436,-8412615285329455721,"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.54 Safari/537.36",SP,BR,HTML,http://www.codeatest.com/como-testar-excecoes-em-java-junit/,como testar exceções em java com o junit - code a test,"Introdução Neste post vou mostrar de forma sucinta e prática como testar exceções em Java, utilizando o framework JUnit 4. Vou demonstrar 3 formas de escrever testes unitários que verificam o comportamento de exceções. Nossa classe para ser testada As 3 formas de se testar exceções em Java serão feitas em cima da classe mostrada abaixo. O código é simples. Faz a transferência de um valor entre uma conta origem e uma conta destino. Se o saldo da conta origem não for suficiente para a realização da transferência, uma exceção será lançada. Abaixo o código da exceção. Vamos aos testes. Como testar exceções em Java #1: @Test(expected=...) O JUnit 4 permite definirmos, opcionalmente, como argumento da anotação @Test o expected . O expected nos permite especificar uma exceção que esperamos que seja lançada pelo código sendo testado. O teste só tem sucesso se a exceção for lançada, caso contrário temos uma falha. O exemplo abaixo mostra o código de teste com o expected . O código de teste acima deve lançar uma exceção, pois o valor que desejamos transferir - 2000 - é maior que o saldo da conta origem: 1500. O argumento expected=SaldoInsuficienteException.class diz ao JUnit que esperamos que o teste ao executar lance uma exceção do tipo SaldoInsuficienteException . Veja que não precisamos fazer asserções nesse código; o argumento expected de @Test faz esse papel. Embora o teste anterior seja legal e bastante conciso, ele não atende a um ponto: e se quisermos verificar a mensagem que foi lançada? Nesse caso, vamos ter que usar uma abordagem diferente. Vamos a ela. Como testar exceções em Java #2: blocos try {} catch() Uma outra forma de como testar exceções em Java é utilizando um bloco try{} catch . Essa forma era bastante utilizada no JUnit 3, antes de aparecer a funcionalidade @Test(expected=...) . Embora essa forma de se testar exceções não seja mais tão utilizada, ela ainda é útil quando queremos verificar a mensagem de detalhe da exceção disparada. Isso ocorre, por exemplo, se a mensagem da exceção vai ser exibida na camada de apresentação para o usuário. Nesse caso, é legal validarmos se a mensagem é aquela que esperamos. Vamos ao código de teste com try{} catch() . A preparação e a condição de falha é a mesma do teste anterior. A diferença está na forma que verificamos a ocorrência da exceção. Veja as duas linhas em destaque: A primeira contém o Assert.fail() ; se o código de teste executar essa linha, indica que ele falhou, pois, o teste deveria lançar uma exceção; É no catch() que esperamos que a execução do teste vá. Na segunda linha em destaque, utilizamos o assertEquals para verificar se a mensagem da exceção ( ex.getMessage() ) é a que esperamos. Esse código atende muito bem nossas expectativas. Mas não sei o que vocês acham, mas na minha opinião não é muito elegante utilizar try {} catch() para testar a ocorrência de uma exceção e validar sua mensagem. Pois bem. O JUnit 4 tem uma forma mais elegante de resolver isso. Como testar exceções em Java #3: @Rule e ExpectedException O Junit 4.7 introduziu o conceito de Rules . As Rules , de maneira geral, permitem adicionar comportamentos que serão executados antes e depois de cada método de teste. O JUnit já vem com algumas test rules predefinidas e permite, também, criarmos as nossas próprias Rules . Uma das test rules oferecidas pelo JUnit é a ExpectedException . É ela que vamos usar em nosso próximo exemplo. Vamos mostrar o código. Uma ExpectedException é uma rule que nos permite verificar se o nosso código lança uma determinada exceção. As linhas em destaque no trecho de código anterior fazem uso dessa rule : As duas primeiras linhas destacadas mostram a declaração da rule . Para isso, utilizamos a anotação @Rule e a classe ExpectedException . Como se pode ver, a varíavel excecaoEsperada é inicializada com o o valor ExpectedException.none() ; essa inicialização é para informar que, por padrão, nenhuma exceção é esperada. A próxima linha em destaque - excecaoEsperada.expect() - modifica o comportamento padrão definido anteriormente, informando qual o tipo de exceção esperamos: SaldoInsuficienteException ; A última linha destacada nos permite verificar a mensagem da exceção ( excecaoEsperada.expectMessage ). Cabe ressaltar algumas coisas sobre os test rules (variáveis anotadas com @Rule ) para que elas funcionem adequadamente: A variável deve ser pública; A variável não pode ser estática ( static ); A variável deve ser um subtipo de TestRule . Se quiser saber um pouco mais sobre as test rules recomendo a leitura do excelente artigo Testes isolados com jUnit Rules do Rafael Ponte. Conclusão Apresentei, neste post, 3 formas diferentes de como testar exceções em Java utilizando o framework JUnit. Eu, pessoalmente gosto da primeira e terceira formas, pois o código fica mais limpo com as mesmas. Utilizo a primeira quando não preciso verificar a mensagem da exceção. A última, por sua vez, utilizo quando preciso garantir que a mensagem de exceção é a esperada. Todo o código desse post pode ser encontrado nesse link do meu GitHub . Abraços e até a próxima.",pt,78
157,668,1461879742,CONTENT SHARED,-6467708104873171151,-1032019229384696495,-1941773591979139720,,,,HTML,http://justcuriousblog.com/2016/04/5-reasons-your-employees-arent-sharing-their-knowledge/,5 reasons your employees aren't sharing their knowledge,"I rarely hear someone say ... WOW! We have so many people sharing on [insert name of enterprise platform]! Everyone's problems are getting solved so quickly nowadays! Instead, I hear plenty of ... We have [insert name of enterprise platform] at work, but no one really uses it ... What's going on? It's 2016! People like sharing online, right? I mean, look what happens in an internet minute nowadays ... 347,222 tweets are sent. 293,000 Facebook statuses are updated. 527,760 Snapchat photos are shared. 2,780,000 YouTube videos are viewed. If sharing as enabled by social technology has become a standard component of everyday life, why can't organizations take advantage of these behaviors and scale knowledge sharing in the workplace? For me, it comes down to a simple answer: its not the same . The technology may look familiar, and the desired behaviors may be similar. But, there are a few key considerations that most organizations ignore when attempting to generate shared organizational knowledge. It's by no means impossible ... It's just a lot more difficult than most people think. Here are 5 reasons your employees aren't sharing their workplace knowledge ... and a few things you can do about it ... #1 - Sharing tech isn't work tech ... When and where are you asking employees to share their knowledge? By WHEN, I'm referring to the expectation that sharing is actually part of the job - not an extra, nice-to-have task. Few organizations hold employees accountable for sharing their knowledge as a core responsibility and are therefore left with only the information from designated subject matter experts, communications teams and a few ""go-getters"" who derive personal value from such sharing. By WHERE, I'm referring to the technology component, often embodied by an intranet or enterprise social network (ESN). Yes, knowledge sharing is more about organizational culture than it is about technology, but right-fit technology must be applied to enable sharing at the speed and scale needed in the modern workplace. So, what tools do your employees need to use to do their jobs every day? Email? POS? SRM? Are your intranet or ESN ever part of that list? Employees may go to your SharePoint or Google Drive now and then to download a file, but these tools typically aren't getting their eyes every day like email and your SRM. Therefore, employees will be less inclined to contribute given minimal traffic time and value-add as part of their regular tasks. #2 - Networks require scale ... I'm admittedly not a Facebook fan. I'm more of a Twitter guy. So why am I still on Facebook? Well, that's where the crowd is. I have a feeling there are plenty of Facebook users with that same predicament. I've tried plenty of other social networks with similar capabilities, but I keep coming back to Facebook because that's where the people with whom I want to engage are sharing. The same is true at work. ESNs and intranets require a flow of constant user activity to deliver value to a single user. Would you stay on Facebook if your newsfeed was empty for a month? A week? A day? People share and consume content where they can engage with the desired audience at the moment of need. To solve problems quickly by leveraging the community, the community has to be there - all the time. Enterprise knowledge sharing rarely hits this necessary level of scale and utility and therefore cannot sustain value. #3 - Most sharing tech is transient ... What was that thing that person said on Facebook the other day? What group did they upload that file to? If you can't remember who said it or when they shared it, good luck finding it on a traditional social network. Newsfeed-oriented tools aren't built for long-term knowledge retention. They're inherently transient, showing users what's going on NOW within the network. This translates into the workplace with popular tools like Yammer. It's also the reason I'm not super-enthused by Facebook at Work. Most organizations introduce separate tools like SharePoint for long-term documentation. This establishes a disconnected knowledge experience for employees. They can share what they know over HERE, but the info they need on the job is over THERE. This model puts employees at a sharing disadvantage, as they are usually required to pass new knowledge through hierarchical channels to get it added to a formal document repository for long-term use. Too often, this privilege is limited to designated SMEs or senior managers - who are several steps removed from the employee/customer experience. Shared knowledge can't attain the scale and longevity necessary to become valuable to the community, and formal documentation processes cannot keep up with the pace of the modern workplace. #4 - Knowledge sharing requires just enough structure ... Have you tried to search your organization's intranet? Can you actually find what you need - quickly? Or are you stuck with PDFs and PowerPoint presentations from 7 years ago? The internet doesn't just organize itself. We have the teams from Google, YouTube and Wikipedia to thank for making the world's shared knowledge searchable. Unfortunately, most organizations don't apply a similar structured approach to internal knowledge sharing. Either people have no ability to share, or the gates are flung open and everyone can pile on however they'd like. This is why your SharePoint instance has become a jumbled mess of locked-down team sites and confusing file directories. Employees shouldn't just be expected to know how to effectively contribute their knowledge to the larger community. Every team shouldn't be granted permission to organize their small corner of shared knowledge however they see fit. Regardless of title or desire, most people just don't understand how to best structure knowledge sharing at an enterprise scale to meet the needs of individual employees. Therefore, they do what they know and install hierarchical folder structures organized by department and supported by rigid process with a dose of overarching ownership mentality. #5 - Why should they ...? It all comes down to motivation and culture. Frankly, if they aren't held accountable to it as part of their roles, why should employees share their knowledge? If the sharing experience isn't super simple - like it is in the real world - why should they dedicate the time? If they aren't likely to get any value back from other employees' sharing, why should they make the effort? If no one is going to recognize their contribution, why contribute? And if they aren't TRUSTED to do the right thing, why would they broadcast their work for everyone to pick apart? ""If you build it, he will come"" worked out pretty well for Kevin Costner, but it has never proven to be a solid strategy for knowledge sharing in the workplace. Difficult - but not impossible. Over the past few years, I've witnessed organizations get around these challenges and begin to realize true value from shared knowledge. Not just business value thanks to the improved ability to solve problems via the community but also cultural value derived from increased employee engagement. Remember - SMEs and formal documentation are a great start, but most of your organization's HOWs - the way the work really gets done every day - are locked in the minds of your employees. You NEED them to share so you can keep up with your competition and maximize your company's long-term potential. How have companies overcome these issues and helped their employees share their knowledge? Here are a few practical tips. Start with trust. As I already mentioned, knowledge sharing is more about culture than technology. Before getting started with strategy, assess the role trust plays in your organization and take steps to address any related issues. Move knowledge sharing closer to the workflow. Help employees talk about the work ON the work. Select technology and enable processes that merge sharing with on-the-job information reference in a single seamless experience. Select right-fit technology. Search is the killer app when it comes to shared knowledge. Leverage technology that looks and feels like Wikipedia, YouTube and other prevalent real-world sharing tools. Provide just enough support. Don't just install technology and process. Find the people in your organization who understand enterprise knowledge sharing - regardless of formal role or past experience - and give them the keys (and the accountability). Make it about their peers. I have come to realize that employees are almost always more willing to share when the act is positioned as a way to help their coworkers, not just the organization or customers - who also benefit anyway. Recognize contribution. Not only should sharing be a core component of their roles, but employees should also be continuously recognized for the value they are contributing through shared knowledge. This could be as simple as mentioning key contributors during group meetings or take on a more strategic form with the use of integrated game mechanic, such as leaderboards, badges, and redeemable points. Get the managers to do it too . This isn't just about frontline employees sharing what they know. Managers must leverage the same behaviors for consuming and sharing knowledge for their own benefit and to reinforce the desired behaviors for their teams. Are you seeing similar challenges in your organization? Have you found ways to motivate your employees to share their knowledge? What technology have you found most helpful when trying to generate and organize shared knowledge?",en,78
158,1387,1465950308,CONTENT SHARED,-4205346868684833897,-1443636648652872475,6340331642204025925,,,,HTML,http://techcrunch.com/2016/06/14/google-launches-springboard-an-ai-powered-assistant-for-its-enterprise-customers/,"google launches springboard, an ai-powered assistant for its enterprise customers","Google has unwrapped two significant announcements for its enterprise customers, the most notable of which is the rollout of Springboard, a new digital assistant to help enterprise companies that make use of Google's services for business. Springboard, which has been in testing with ""a small set of customers,"" is a little like Google Now for enterprise workers. That's to say that it offer a single search interface which utilises artificial intelligence to surface information within a user's suite of Google products - such as Google Drive, Gmail, Calendar, Google Docs and more. That's important because, according to Prabhakar Raghavan, VP of Engineering for Google Apps, ""the average knowledge worker [currently] spends the equivalent of one full day a week searching for and gathering information."" Beyond search, Springboard also provides ""useful and actionable information and recommendations"" to users throughout their work day. The second side of today's news is a new design for Google Sites, a product that acts like an information portal for housing internal company information like quarterly reports, or newsletters. Now, when designing a Google Site, users have drag and drop editing and real-time collaboration, creation features that have become standard in other services like Google Docs and Google Sheets. Finally, in terms of presentation, Google Sites has been revamped so that the content fits to any kind of screen, be it a smartphone, laptop or 30-inch monitor. These changes are rolling out to early adopter programs that existing Google Apps for Work customer can join - the Springboard program is here and Google Sites program is here - while the search giant has teased that it has ""a lot more in store"" for both services.",en,78
159,2573,1476722551,CONTENT SHARED,2715453133655798791,-4028919343899978105,6702709297756973694,"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.143 Safari/537.36",SP,BR,HTML,https://www.engadget.com/2016/10/17/the-simpsons-vr-google-cardboard/,'the simpsons' celebrates 600 episodes with a vr couch gag,"For the opening of Treehouse of Horror XXVII , which aired last night, viewers were treated to a Planet of the Apes parody called Planet of the Couches. Fans could (and still can) download the Google Spotlight Stories app (available on both iOS and Android ) to engage with a 360-degree experience that responds to their movement. The couch gag normally runs for 45 seconds, but Planet of the Couches offers an extra two minutes of bonus content. We won't ruin it for you but there are six different scenes that include a Moe's ""Cavern"" and a snarky quip from The Comic Book Guy. Just make sure to explore all 360 degrees to catch all of the clever references and in-jokes. The experience is best viewed with a viewer like Cardboard, but you can still enjoy the VR short by opening the Spotlight Stories app and physically moving your iPhone or Android device around (it just won't be as immersive). Get out your #GoogleCardboard , the 600th episode couch gag VR experience via @Google starts now! pic.twitter.com/ip3jLs0N8D - The Simpsons (@TheSimpsons) October 17, 2016",en,77
160,2039,1470922612,CONTENT SHARED,-5628897645967553681,-5527145562136413747,-145525363878254760,,,,HTML,http://itspronouncedmetrosexual.com/2015/03/the-genderbread-person-v3/,the genderbread person v3 | it's pronounced metrosexual,"Taking the lessons learned from version 2 , and applying them to sexuality in a more meaningful way, here's my best rendition of the genderbread person yet. If you want to see a more in-depth explanation using the first version of this graphic I made, check out my post Breaking through the binary: Gender explained using continuums . Someone pointed out that I hadn't shared this here yet, even though it's been out for awhile. I am in disbelief about how long it's taken me to get this up on the site. I really don't know what happened - time flies? I created the first iteration of my version 3 of the genderbread person when I published my book back in 2013. I've posted it on Facebook , it's been translated a few times, it's even been Santafied , and don't have much of an explanation to share for why I've neglected to share it here. But... here it is! Sorry? Sorry. What's new? There are two big differences from v2 to the most recent v3: separating romantic & sexual attraction, and the labeling of the continua. I'm also sharing it here with a more simplified version that I encourage folks to use for Powerpoints or to supplement any articles they're writing, where the additional verbiage isn't necessary. Separating romantic and sexual attraction is a more accurate way of describing some of the ways we all experience attraction (or don't), and it's also an effort to make the graphic more inclusive of asexual folks. Check out this great article on Asexuality.org about sexual and romantic attraction if you'd like to read more. Labels have always been one of the toughest parts of of the genderbread person , because in my versions I'm trying to simultaneously reinforce folks current understandings (or former, or problematic ) of gender, while expanding and building upon their understanding, enhancing it, and making it a more honest depiction of the ways we all actually experience gender. Using the language that reinforces problematic and limiting understandings of gender as a means to expand understanding feels a lot like using the master's tools to deconstruct the master's house , and in this version I'm trying to get away from as much language as possible. Here's a testament to that ""less language is better"" sentiment, a more Powerpoint-friendly version you're welcome to use: What's the same? It's still totally uncopyrighted and yours to use however you'd like. No need to ask for permission. It's still based on the deconstruction of gender into identity, expression, and sex, building on my second and first versions of the genderbread person, which are my spin on the model that's been around for a couple decades, which was based on the work of feminists from the past century. And it's still, I hope, as adorable as ever. Join the Discussion",en,77
161,2842,1481114167,CONTENT SHARED,8869347744613364434,997469202936578234,2204632498906085879,"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36",MG,BR,HTML,https://www.infoq.com/br/articles/Java_Garbage_Collection_Distilled,java garbage collection essencial,"Serial, Parallel, Concurrent, CMS, G1, Young Gen, New Gen, Old Gen, Perm Gen, Eden, Tenured, Survivor Spaces, Safepoints e as centenas de flags de inicialização da JVM. Deixam tudo confuso quando se está tentando otimizar o garbage collector para obter a taxa de transferência e latência necessária para a aplicação Java? A documentação sobre a coleta de lixo parece um manual para aeronaves. Cada botão e seletor está explicado detalhadamente, mas em nenhum lugar é possível encontrar um guia sobre como voar. Este artigo vai tentar explicar as vantagens e desvantagens na escolha e ajuste dos algoritmos do garbage collector em uma determinada carga de trabalho. O foco será nos coletores da Oracle Hotspot JVM e do OpenJDK que são de uso mais comum. No final, outras JVMs comerciais serão discutidas para mostrar alternativas. Vantagens e desvantagens O sábio povo continua nos dizendo: ""não se consegue nada de graça"". Quando queremos algo geralmente temos que abrir mão de algo em troca. Quando se trata da coleta de lixo trabalhamos com 3 principais variáveis que definem metas para os coletores: Taxa de transferência: O total de trabalho realizado por uma aplicação como uma proporção do tempo gasto no GC. Configurando a taxa de transferência para ‑XX:GCTimeRatio=99, 99 é o padrão que equivale a 1% do tempo do GC. Latência: O tempo gasto pelos sistemas em resposta a eventos que são afetados por pausas introduzidas através da coleta de lixo. Configure a latência para pausas do GC com -XX: MaxGCPauseMillis = <n>. Memória: A quantidade de memória que nossos sistemas usam para guardar o estado, que é frequentemente copiado e movido quando estão sendo geridos. O conjunto de objetos ativos mantidos pela aplicação em qualquer tempo é conhecido como Live Set. Tamanho máximo da heap (Maximum heap size) -Xmx<n> é um parâmetro de ajuste para configurar o heap size disponível para uma aplicação. Observação: Muitas vezes o Hotspot não pode alcançar essas metas e vai silenciosamente continuar sem nenhum aviso, mesmo após ter falhado por uma grande margem. Latência é a distribuição entre eventos. Pode ser aceitável ter um aumento médio de latência para reduzir o pior caso de latência, ou torná-los menos frequentes. Não devemos interpretar o termo ""real-time"" como menor latência possível; mas sim que se refere a ter uma latência determinística, independentemente da taxa de transferência. Para cargas de trabalho de algumas aplicações, a taxa de transferência é a meta mais importante. Um exemplo seria um processamento em lote de longa duração, não importa se o processamento em lote é ocasionalmente interrompido por alguns segundos enquanto a coleta de lixo é executada, desde que o processamento em lote possa ser concluído logo. Para praticamente todas as outras cargas de trabalho, desde aplicações interativas com humanos a sistemas de comércio financeiro, se um sistema não responder após alguns segundos ou milissegundos, isso pode significar um desastre. Em negócios financeiros frequentemente vale a pena negociar alguma taxa de transferência em troca de latência consistente. Podemos também ter aplicações que são limitadas pela quantidade de memória física disponível e ter que se manter presente, neste caso temos que melhorar o desempenho em ambos, latência e taxa de transferência. A seguir algumas vantagens e desvantagens: Em grande parte o custo da coleta de lixo, como um custo amortizado, pode ser reduzido fornecendo ao garbage collector algoritmos com mais memória. Os piores casos observados de pausas induzidas por latência devido à coleta de lixo podem ser reduzidos limitando o live set e mantendo a heap pequena. A frequência com que as pausas ocorrem pode ser reduzida pelo gerenciamento da heap e o tamanho das gerações, e controlando a taxa de alocação de objetos. A grande frequência de pausas pode ser reduzida pela execução do GC concorrentemente com a aplicação, às vezes à custa da taxa de transferência. Ciclo de vida dos Objetos Os algoritmos de coleta de lixo são frequentemente otimizados na expectativa de que a maioria dos objetos viva por um curto período de tempo, enquanto que relativamente poucos vivem por muito tempo. Na maioria das aplicações, os objetos que vivem por um período significativo de tempo tendem a constituir uma porcentagem muito pequena dos objetos alocados ao longo do tempo. Na teoria, esse comportamento observado na coleta de lixo é conhecido como ""mortalidade infantil"" ou ""hipótese geracional fraca"". Por exemplo, loops em Iterators são em sua maioria de curta duração enquanto que Strings são efetivamente imortais. A experiência tem demonstrado que os garbage collectors geracionais normalmente podem suportar uma taxa de transferência maior que os collectors não geracionais, portanto são quase sempre usados em servidores JVMs. Pela separação das gerações de objetos, sabemos que a região na qual os novos objetos são alocados é provavelmente muito escassa para objetos vivos. Portanto, um collector que varre procurando por objetos vivos nessa região e os copia para outra região de objetos mais velhos pode ser muito eficiente. Os coletores de lixo do Hotspot gravam a idade de um objeto de acordo com o número de ciclos do GC que sobreviveu. Observação: Se uma aplicação sempre gera um monte de objetos que vivem por um tempo bastante longo, então é de se esperar que essa aplicação gaste uma porção significativa deste tempo com a coleta de lixo, assim como tenha que se gastar um tempo significativo ajustando o coletor de lixo do Hotspot. Isto é devido à redução da eficiência do GC que acontece quando o ""filtro"" geracional é menos efetivo, e ao custo resultante de coletar gerações vivas há mais tempo com uma maior frequência. Gerações mais velhas são menos escassas e, como resultado, a eficiência dos algoritmos de coleta das gerações mais velhas tendem a ser bem ruim. Os Garbage Collectors geracionais tendem a operar em dois ciclos de coletas distintos: as coletas Menores (Minor collections), em que os objetos de curta duração são coletados, e as menos frequentes coletas Maiores (Major collections), em que as regiões mais velhas são coletadas. Eventos do tipo Para-O-Mundo (Stop-The-World) As pausas sofridas pelas aplicações durante a coleta de lixo são devidas aos, como são conhecidos, ""eventos do tipo stop-the-world"". Por razões práticas de engenharia, para que o garbage collector opere, é necessário parar a aplicação em execução periodicamente para que a memória possa ser gerenciada. Dependendo dos algoritmos, diferentes coletores dispararão o evento stop-the-world em específicos pontos da execução, variando o tempo da parada. Para fazer uma aplicação parar totalmente é necessário pausar todas as threads em execução. Os garbage collectors fazem isso avisando as threads para parar quando elas estiverem em um ""ponto seguro"", que é o ponto durante a execução do programa no qual é conhecido por todos os GC e todos os objetos na heap estão consistentes. Dependendo do que a thread estiver executando, pode demorar algum tempo até ela alcançar um ponto seguro. Pontos seguros são normalmente estabelecidos nos retornos de método e nos finais dos loops, mas podem ser otimizados para alguns pontos diferentes, tornando-os dinamicamente mais raros. Por exemplo, se uma Thread está copiando um grande array, clonando um objeto grande, ou executando um loop, pode demorar muitos milissegundos antes de um ponto seguro ser atingido. O tempo para atingir um ponto seguro é uma preocupação importante em aplicações com baixa latência. Este tempo pode ser visualizado habilitando a flag ‑XX:+PrintGCApplicationStoppedTime junto com as outras flags do GC. Observação: Para aplicações com um número grande de threads em execução, quando um evento stop-the world ocorre, o sistema sofrerá uma pressão enorme assim que as threads estiverem liberadas dos pontos seguros. Por isso que algoritmos meno sependentes de eventos stop-the-world são potencialmente mais eficientes. Organização da Heap no Hotspot Para entender como os diferentes coletores funcionam é melhor explorar como a Heap é organizada para suportar os coletores geracionais. Os objetos que vivem durante um tempo longo o suficiente são eventualmente promovidos para a tenured space . A perm gen é o lugar em que a Runtime (JVM) armazena os objetos ""conhecidos"" por serem efetivamente imortais, tais como Classes e Strings estáticas. Infelizmente o uso comum do carregamento de classes de forma contínua em muitas aplicações motiva a equivocada suposição de que por trás da perm gen as classes são imortais. No Java 7 as Strings internas foram movidas da perm gen para a tenured , e a partir do Java 8 a perm gen não existirá mais, e não será abordada neste artigo. A maioria dos coletores comerciais não usa uma perm gen separada e tende a tratar todos os objetos de longa duração na tenured . Observação: As áreas virtuais permitem que os coletores ajustem o tamanho das regiões para cumprir as metas de taxa de transferência e latência. Os coletores mantêm estatísticas para cada fase da coleta e ajustam o tamanho da região na tentativa de cumprir as metas. Alocação de Objetos Para evitar disputas, cada thread é atribuída a um buffer de alocação de thread local (Thread Local Allocation Buffer - TLAB) no qual os objetos são alocados. O uso de TLABs permite que a alocação de objetos possa escalar de acordo com número de threads, evitando disputas de um único recurso na memória. A alocação de objetos via TLAB é uma operação muito barata, a TLAB simplesmente aponta o ponteiro para o tamanho do objeto que leva cerca de 10 instruções na maioria das plataformas. A alocação da pilha de memória para o Java é ainda mais barato do que usar o malloc do runtime existente no C. Observação: Visto que a alocação individual de objetos é muito barata, a taxa à qual coletas menores devem ocorrer é diretamente proporcional à taxa de alocação do objeto. Quando uma TLAB está exausta uma thread simplesmente socilita uma nova para o Eden . Quando o Eden está cheio uma coleta menor é iniciada. Pode acontecer de não ser possível alocar grandes objetos (-XX:PretenureSizeThreshold=n) na young gen e assim eles terão de ser alocados na old gen , por exemplo um grande array. Se o limite configurado é menor que o tamanho do TLAB, então os objetos que couberem no TLAB não serão criados na old gen . O novo coletor G1 manipula grandes objetos diferentemente e o mesmo será discutido mais adiante. A cópia para o tenured space é conhecida como uma promoção ou amadurecimento. Promoções ocorrem para objetos que são suficientemente velhos (-- XX:maxTenuringThreshold ), ou quando o survivor space estoura. Objetos vivos são objetos que podem ser acessados pela aplicação, quaisquer outros objetos que não possam ser acessados podem portanto ser considerados mortos. Em uma coleta menor, a cópia dos objetos vivos é feita primeiramente pelo que é conhecido como GC Roots, e de forma iterativa copia qualquer objeto acessível para o survivor space. GC Roots normalmente incluem referências da aplicação e campos estáticos internos da JVM, e pilhas de threads, tudo o que efetivamente aponta para gráficos de objetos acessíveis da aplicação. O cartão de mesa do Hotspot é um array de bytes no qual cada byte é usado para marcar a potencial existência de referências entre gerações em uma região correspondente a 512 bytes da old gen . As referências são armazenadas na heap, a ""barreira de armazenamento"" de código marcará os cartões para indicar que uma potencial referência da old gen para a new gen possa existir na região 512 byte associada. No momento da coleta, o cartão de mesa é usado para procurar por referências entre gerações, que efetivamente representam GC Roots adicionais para a new gen . Entretanto um custo fixo significativo nas coletas menores é diretamente proporcional ao tamanho da old gen . Existem duas survivor spaces na new gen da Hotspot, que se alternam em suas regras em seus "" espaço destino "" e "" espaço origem "". No início de uma coleta menor, o "" espaço destino "" survivor space é sempre vazio, e atua como uma área de cópia para a coleta menor. O survivor space da coleta menor anterior é parte do ""espaço origem"", que também inclui o Eden , local que os objetos vivos que precisam ser copiados podem ser encontrados. O custo de um GC de coleta menor é geralmente dominado pelo custo da cópia de objetos para os survivor spaces e tenured spaces . Objetos que não sobrevivem a coleta menor estão efetivamente livres para serem tratados. O trabalho realizado durante a coleta menor é diretamente proporcional ao número de objetos vivos encontrados, e não ao tamanho da new gen . O tempo total gasto executando a coleta menor pode ser praticamente reduzida a metade cada vez que o tamanho do Eden é dobrado. A memória portanto pode ser trocada por taxa de transferência. A duplicação do tamanho Eden resultará no aumento do tempo de coleta por ciclo, mas isso é relativamente pequeno se tanto o número de objetos a serem promovidos como o tamanho de geração mais velha é constante. Observação: Na Hotspot as coletas menores são eventos stop-the-world. Isso está rapidamente se tornando uma questão importante conforme nossas heaps ficam maiores com mais objetos vivos. Já vemos a necessidade de coletas concorrentes da young gen para atingirmos as metas de pausa de tempo. Coletas maiores O coletor da old gen vai tentar adivinhar quando é necessário coletar para evitar um fracasso na promoção da young gen . Os coletores monitoram um limite de preenchimento para a old gen e começam a coletar quando este limite é ultrapassado. Se este limite não é suficiente para atender as necessidades de promoção, então um ""FullGC"" é acionado. O FullGC envolve a promoção de todos os objetos vivos da new gen seguidas por uma coleta e compactação da old gen . A falha de promoção é uma operação muito cara e os objetos promovidos a partir deste ciclo deve ser desfeitos para que o evento FullGC possa ocorrer. Observação: Para evitar a falha na promoção será necessário ajustar o preenchimento que a old gen permite para acomodar promoções (‑XX:PromotedPadding=<n>). Observação: Quando a Heap precisa aumentar um FullGC é acionado. Esse redimensionamento da Heap por FullGCs podem ser evitado ajustando -Xms e -Xmx para o mesmo valor. Além do FullFC, uma compactação da old gen é provávelmente o maior evento stop-the-world que uma aplicação vai experimentar. O tempo para esta compactação tende a crescer linearmente com o número de objetos vivos no tenured space . Coleta em série Coletor paralelo Em sistemas com muitos processadores o Coletor Paralelo Antigo vai dar melhor vazão que qualquer coletor. Ele não impacta em uma aplicação em execução até a coleta acontecer, e então vai coletar em paralelo usando múltiplas threads usando um algoritmo mais eficiente. Isso faz o Coletor Paralelo Antigo muito eficiente para aplicação em batch. O custo de coletar as gerações old é afetado pelo número de objetos mantendo uma maior extensão do que o tamanho da heap. Portanto a eficiência do Coletor Paralelo Antigo pode ser aumentada para alcançar uma maior taxa de transferência, fornecendo mais memória e aceitando maiores, mas em número menor, pausas para coleta. Espere coletas menores mais rápidas com este coletor porque a promoção para o tenured space é um simples operação de cópia e acerto de ponteiros. Para servidores de aplicações o Coletor Paralelo Old deve ser a porta de entrada. Entretanto se as pausas para coletas maiores são maiores que a aplicação pode tolerar então será preciso considerar empregar um coletor concorrente que colete os objetos na tenured concorrentemente enquanto a aplicação está em execução. Observação: Espere pausas em ordem de 1 a 5 segundos por GB de informação viva em máquinas modernas enquanto a old gen é compactada. Coletor Concurrent Mark Sweep (CMS) O coletor Concurrent Mark Sweep (CMS) (-XX:+UseConcMarkSweepGC) é executado na old gen coletando objetos maduros que não são mais acessíveis durante a coleta maior. Ele é executado concorrentemente na aplicação com o objetivo de manter espaço livre o suficiente na old gen , então falhas de promoção na young gen não ocorrem. A falha de promoção irá desencadear um FullGC. O CMS segue um processo de várias etapas: Marcação inicial <stop-the-world>: Procura GC Roots; Marcação concorrente: Marca todos os objetos acessíveis pelo GC Roots; Pré-limpeza concorrente: Verifica referências de objetos que foram atualizadas e objetos que foram promovidos durante a fase de remarcação concorrente; Remarcação <stop-the-world>: Captura referências de objetos que foram atualizados desde a fase de pré-limpeza; Varredura concorrente: Atualiza as listas livre com a recuperação da memória ocupada por objetos mortos; Reinicio concorrente: Reinicia a estrutura de dados para a próxima execução. Assim que os objetos maduros se tornam inacessíveis, o espaço é recuperado pelo CMS e colocado em listas livres. Quando a promoção acontece, deve ser pesquisado um local de tamanho adequado para as listas livres para o objeto promovido. Isso aumenta o custo da promoção e assim aumenta o custo da coleta menor comparado ao Coletor Paralelo. Observação: O CMS não é um coletor de compactação, o que ao longo do tempo pode resultar na fragmentação da old gen. A promoção do objeto pode falhar, pois um objeto grande pode não caber nos locais disponíveis na old gen . Quando isso acontece uma mensagem ""promoção falha"" é registrada e um FullGC é acionado para compactar os objetos maduros vivos. Para tais compactações orientadas ao FullGCs, espere pausas piores que das coletas maiores usando o Coletor Paralelo Antigo porque CMS usa uma simples thread para a compactação. O CMS é na maior parte concorrente com a aplicação, que tem um número de implicações. Primeiro, o tempo da CPU é consumido pelo coletor, deste modo reduzindo a CPU disponível para a aplicação. O total de tempo requerido pelo CMS aumenta linearmente com o montante de objetos promovidos para o tenured space . Segundo, para algumas fases concorrentes do ciclo do GC, todas as threads tem que ser trazidas para um ponto a salvo para o GC Roots marcar e realizar uma remarcação paralela para verificar se há mutações. Observação: Se uma aplicação percebe mutações significantes nos objetos maduros, então uma fase de remarcação pode ser significante, nos extremos pode levar mais tempo do que uma compactação completa com o Coletor Paralelo Antigo. O CMS faz do FullGC um evento menos frequente à custa da redução da taxa de transferência, coletas menores são mais caras, e mais marcantes. A redução na taxa de transferência pode ser qualquer coisa entre 10% a 40% em relação ao Coletor Paralelo, dependendo da taxa de promoção. O CMS também exige 20% mais memória para acomodar estruturas de dados adicionais e ""lixo flutuante"" que pode ser perdido durante a marcação simultânea que será transferida para o próximo ciclo. Altas taxas de promoção e fragmentação resultante podem às vezes ser reduzidas pelo aumento do tamanho de ambos a young gen e old gen . Observação: O CMS pode sofrer ""falhas de modo concorrente"", que pode ser visto nos logs, quando deixa de coletar a uma taxa suficiente para manter-se com a promoção. Isso pode ser causado quando a coleta começa tarde, que podem ser definidos por meio de ajuste. Mas isso também pode acontecer quando a taxa de coleta não consegue acompanhar a alta taxa de promoção ou a alta mutação de objetos de algumas aplicações. Se o taxa de promoção ou mutação da aplicação é tão alta, então sua aplicação pode necessitar de algumas mudanças para reduzir a pressão da promoção. Adicionando mais memória para um sistema deste tipo, por vezes, pode piorar a situação, porque o CMS teria mais memória para examinar. Coletor Garbage First (G1) O Garbage First (G1) (-XX:+UseG1GC) é um novo coletor introduzido no Java 6 e agora oficialmente suportado no Java 7. É um algoritmo parcialmente concorrente que também tenta compactar o tenured space em pequenas e incrementais pausas stop-the-world para tentar minimizar os eventos FullGC que aflige o CMS por causa da fragmentação. G1 é um coletor geracional que organiza a heap diferentemente dos outros coletores dividindo-o em regiões de tamanho fixo de efeito variável, ao invés de regiões contíguas com o mesmo objetivo. O G1 adota a abordagem de concorrentemente marcar regiões para rastrear referências entre regiões, e concentra a coleta nas regiões com mais espaço livre. Estas regiões são então coletadas em uma pausa stop-the-world descarregando os objetos vivos para uma região vazia, assim compactando no processo. Objetos maiores que 50% da região são alocados em uma região monstruosa, que é um múltiplo do tamanho da região. A alocação e coleta de objetos monstruosos podem ser muito caros para o G1, e até agora teve pouco ou nenhum esforço de otimização aplicada. O desafio com qualquer coletor de compactação não é o de mover objetos mas o de atualizar as referências destes objetos. Se um objeto é referenciado em várias regiões, então atualizar estas referências podem demorar significativamente mais do que mover o objeto. O G1 rastreia quais objetos em uma região tem referências de outras regiões via ""Remembered Sets"" (Conjuntos lembrados). Se um Remembered Set tornar-se grande, então o G1 pode desacelerar significativamente. Quando liberando objetos de uma região para outra, o tamanho dos eventos stop-the-world associados tendem a ser proporcionais ao número de regiões com referências que precisam ser escaneadas e potencialmente corrigidas. Manter os Remembered Sets aumenta o custo das coletas menores resultando pausas maiores do que visto com o Coletor Paralelo Antigo ou o CMS para coleções menores. O G1 é configurado baseado na latência -XX:MaxGCPauseMillis=<n>, valor padrão = 200ms. A meta irá influenciar a quantidade de trabalho realizado em cada ciclo somente na base nos melhores esforços. Estabelecer metas em dezenas de milissegundos é mais fútil, e da forma como foi escrito, tentar alvejar dezenas de milissegundos não foi o foco do G1. O G1 é em geral um bom coletor para grandes pilhas que tem a tendência de se tornar fragmentado quando uma aplicação pode tolerar pausas entre 0.5 e 1.0 segundo para compactações incrementais. O G1 tende a reduzir a frequência dos piores casos de pausa visto no CMS devido a fragmentação ao custo das coletas menores e compactação incremental da old gen . Mais pausas acabam sendo obrigatórias para o desenvolvimento regional ao invés de compactações de pilhas cheias. Como o CMS, o G1 pode deixar de manter-se com as taxas de promoção, e voltará para um FullGC stop-the-world. Assim como o CMS tem ""concorrente modo de falha"", G1 pode sofrer uma falha de evacuação, visto nos logs como ""estouro de espaço"". Isso ocorre quando não existem regiões livres na qual os objetos possam ser evacuados, que é similar a uma falha de promoção. Se isso ocorrer, tente usar uma grande pilha e mais threads de marcação, mas em alguns casos mudanças na aplicação são necessárias para reduzir os custos de alocação. Um problema desafiador para o G1 é lidar com objetos populares e regiões. Compactação incremental stop-the-world funciona bem quando regiões tem objetos vivos que não são fortemente referenciados em outras regiões. Se um objeto ou região é popular então o Remembered Set será grande e o G1 evitará coletar estes objetos. Eventualmente ele pode não ter escolha, que resultará em muitas pausas frequentes de média duração até que a heap seja compactada. Coletores Concorrentes Alternativos CMS e G1 são frequentemente chamados de coletores concorrentes. Quando olhamos o trabalho total realizado é claro que a young gen , promoção e até mesmo o trabalho da old gen não é concorrente ao todo. O CMS é mais concorrente para a old gen ; O G1 é muito mais do que um coletor incremental do tipo stop-the-world. Ambos CMS e G1 tem significantes e regulares ocorrências dos eventos stop-the-world, e nos piores cenários que frequentemente tornam-nos impróprios para rigorosas aplicações de baixa latência, como uma negociação financeira or interfaces reativas ao usuário. Coletores alternativos como Oracle JRockit Real Time, IBM Websphere Real Time e Azul Zing estão disponíveis. Os coletores JRockit e Websphere levam vantagem na latência na maioria dos casos sobre o CMS e G1, mas frequentemente veem limitações na faixa de transferência e continuam sofrendo significantes eventos stop-the-world. Zinf é o único coletor Java conhecido por este autor que pode ser verdadeiramente concorrente para coleta e compactação enquanto mantém uma alta taxa de transferência para todas as gerações. Zing tem algumas frações de milisengundos em eventos stop-the-world, mas há trocas de fases no ciclo de coleta que não estão relacionados ao tamanho do conjunto vivo. JRockit RT pode alcançar tempos de pausa típicos em dezenas de milisegundos para altas taxas de alocação contidas no tamanho da pilha, mas as vezes tem que deixar para traz a pausa para a compactação completa. Websphere RT pode alcançar pausas de poucos milisegundos através de taxa de alocação restrita e tamanho de conjuntos vivos. O Zing pode alcançar pausas de frações de milisegundos com altas taxas de alocação por ser concorrente por todas fases, incluindo durante as coletas menores. O Zing tem condições de manter esse comportamento consistente graças ao tamanho da heap, permitindo que o usuário aplique grandes tamanhos de heap conforme preciso, acompanhando a taxa de tranferência do aplicativo ou a necessidade dos estados dos objetos, sem medo de maiores tempos de pausa. Para todos os coletores concorrentes focados na latência é necessário dar um pouco de taxa de transferência e ganho de rastro. Dependendo da eficiência do coletor concorrente pode-se dar um pouco de taxa de transferência, mas está sempre adicionando rastro significativo. Se for realmente concorrente, com poucos eventos stop-the-world, então mais núcleos de CPU serão necessários para habilitar a concorrência e manter a vazão. Observação: Todos coletores concorrentes tendem a funcionar mais eficientemente quando há espaço suficiente para a alocação. Como regra de ponto de partida tente reservar uma heap com pelo menos duas ou três vezes o tamanho dos conjuntos vivos, para a operação ser eficiente. Entretanto, o espaço necessário para manter as operações concorrentes aumentam com a taxa de transferência da aplicação, e está associada a alocação e taxas de promoção. Então para aplicações de maior vazão, uma heap maior para um conjunto vivo proporcional pode ser justificado. Dados os enormes espaços de memória disponíveis para os sistemas de hoje o uso da memória raramente é um problema no lado do servidor. Monitorando e Ajustando a Coleta de Lixo Para entender como a aplicação e o garbage collector estão se comportando, inicie a JVM com as seguintes configurações: Então carregue os logs dentro de uma ferramenta como Chewiebug para realizar as analises. Para ver a natureza dinâmica do GC, execute o JVisualVM e instale o plugin Visual GC que permitirá a visualização do GC em ação, como por exemplo na aplicação a seguir. Para entender o que o GC da aplicação precisa é necessário carregar testes representativos que possam ser rodados repetidamente. À medida que se familiariza com a forma como cada coletor trabalha, então execute os teste de carga com diferentes configurações até chegar na taxa de transferência e latência desejadas. É importante medir a latência da perspectiva do usuário final. Isso pode ser alcançado capturando o tempo de resposta de cada requisição do teste em um historograma, e é possível ler mais sobre isso aqui . Se houver picos de latência que estão fora do range aceitável, então tente relacioná-las com os logs do GC para determinar se o problema é o GC. É possível que outras questões possam causar os picos de latência. Outra ferramenta útil a considerar é o jHiccup que pode ser usado para acompanhar as pausas dentro da JVM e através do sistema como um todo. Se os picos de latência são devidos ao GC, então invista em ajustar o CMS ou G1 para ver se suas métricas de latência podem ser cumpridas. As vezes isso não é possível devido a alta alocação e taxas de promoção combinadas com os requisitos de baixa latência. Os ajustes do GC podem se tornar um exercício altamente necessário e que muitas vezes requer mudanças de aplicativos para reduzir as taxas de alocação ou ciclo de vida dos objetos. Se for o caso, então avalie a economia de tempo e recursos gastos no ajustes do GC e mudanças na aplicação com a compra de um dos concorrentes comerciais de compactação de JVMs como JRockit Real Time Azul ou Zing.",pt,77
162,1249,1464967629,CONTENT SHARED,-4798431998955209742,-709287718034731589,-2752546383439495719,,,,HTML,http://journal.stuffwithstuff.com/2015/02/01/what-color-is-your-function/,what color is your function?,"I don't know about you, but nothing gets me going in the morning quite like a good old fashioned programming language rant. It stirs the blood to see someone skewer one of those ""blub"" languages the plebians use, muddling through their day with it between furtive visits to StackOverflow. (Meanwhile, you and I, only use the most enlightened of languages. Chisel-sharp tools designed for the manicured hands of expert craftspersons such as ourselves.) Of course, as the author of said screed, I run a risk. The language I mock could be one you like! Without realizing it, I could let have the rabble into my blog, pitchforks and torches at the ready, and my fool-hardy pamphlet could draw their ire! To protect myself from the heat of those flames, and to avoid offending your possibly delicate sensibilities, instead, I'll rant about a language I just made up. A strawman whose sole purpose is to be set aflame. I know, this seems pointless right? Trust me, by the end, we'll see whose face (or faces!) have been painted on his straw noggin. A new language Learning an entire new (crappy) language just for a blog post is a tall order, so let's say it's mostly similar to one you and I already know. We'll say it has syntax sorta like JS. Curly braces and semicolons. if , while , etc. The lingua franca of the programming grotto. I'm picking JS not because that's what this post is about. It's just that it's the language you, statistical representation of the average reader, are most likely to be able grok. Voilà: Because our strawman is a modern (shitty) language, we also have first-class functions. So you can make something like like: This is one of those higher-order functions, and, like the name implies, they are classy as all get out and super useful. You're probably used to them for mucking around with collections, but once you internalize the concept, you start using them damn near everywhere. Maybe in your testing framework: Or when you need to parse some data: So you go to town and write all sorts of awesome reusable libraries and applications passing around functions, calling functions, returning functions. Functapalooza. What color is your function? Except wait. Here's where our language gets screwy. It has this one peculiar feature: 1. Every function has a color. Each function-anonymous callback or regular named one-is either red or blue. Since my blog's code highlighter can't handle actual color, we'll say the syntax is like: There are no colorless functions in the language. Want to make a function? Gotta pick a color. Them's the rules. And, actually, there are a couple more rules you have to follow too: 2. The way you call a function depends on its color. Imagine a ""blue call"" syntax and a ""red call"" syntax. Something like: If you get it wrong-call a red function with *blue after the parentheses or vice versa-it does something bad. Dredge up some long-forgotten nightmare from your childhood like a clown with snakes for arms under your bed. That jumps out of your monitor and sucks out your vitreous humour. Annoying rule, right? Oh, and one more: 3. You can only call a red function from within another red function. You can call a blue function from with a red one. This is kosher: But you can't go the other way. If you try to do this: Well, you're gonna get a visit from old Spidermouth the Night Clown. This makes writing higher-order functions like our filter() example trickier. We have to pick a color for it and that affects the colors of the functions we're allowed to pass to it. The obvious solution is to make filter() red. That way, it can take either red or blue functions and call them. But then we run into the next itchy spot in the hairshirt that is this language: 4. Red functions are more painful to call. For now, I won't precisely define ""painful"", but just imagine that the programmer has to jump through some kind of annoying hoops every time they call a red function. Maybe it's really verbose, or maybe you can't do it inside certain kinds of statements. Maybe you can only call them on line numbers that are prime. What matters is that, if you decide to make a function red, everyone using your API will want to spit in your coffee and/or deposit some even less savory fluids in it. The obvious solution then is to never use red functions. Just make everything blue and you're back to the sane world where all functions have the same color, which is equivalent to them all having no color, which is equivalent to our language not being entirely stupid. Alas, the sadistic language designers-and we all know all programming language designers are sadists, don't we?-jabbed one final thorn in our side: 5. Some core library functions are red. There are some functions built in to the platform, functions that we need to use, that we are unable to write ourselves, that only come in red. At this point, a reasonable person might think the language hates us. It's functional programming's fault! You might be thinking that the problem here is we're trying to use higher-order functions. If we just stop flouncing around in all of that functional frippery and write normal blue collar first-order functions like God intended, we'd spare ourselves all the heartache. If we only call blue functions, make our function blue. Otherwise, make it red. As long as we never make functions that accept functions, we don't have to worry about trying to be ""polymorphic over function color"" (polychromatic?) or any nonsense like that. But, alas, higher order functions are just one example. This problem is pervasive any time we want to break our program down into separate functions that get reused. For example, let's say we have a nice little blob of code that, I don't know, implements Dijkstra's algorithm over a graph representing how much your social network are crushing on each other. (I spent way too long trying to decide what such a result would even represent. Transitive undesirability?) Later, you end up needing to use this same blob of code somewhere else. You do the natural thing and hoist it out into a separate function. You call it from the old place and your new code that uses it. But what color should it be? Obviously, you'll make it blue if you can, but what if it uses one of those nasty red-only core library functions? What if the new place you want to call it is blue? You'll have to turn it red. Then you'll have to turn the function that calls it red. Ugh. No matter what, you'll have to think about color constantly. It will be the sand in your swimsuit on the beach vacation of development. A colorful allegory Of course, I'm not really talking about color here, am I? It's an allegory, a literary trick. The Sneetches isn't about stars on bellies, it's about race. By now, you may have an inkling of what color actually represents. If not, here's the big reveal: Red functions are asynchronous ones. If you're programming in JavaScript on Node.js, everytime you define a function that ""returns"" a value by invoking a callback, you just made a red function. Look back at that list of rules and see how my metaphor stacks up: Synchronous functions return values, async ones do not and instead invoke callbacks. Synchronous functions give their result as a return value, async functions give it by invoking a callback you pass to it. You can't call an async function from a synchronous one because you won't be able to determine the result until the async one completes later. Async functions don't compose in expressions because of the callbacks, have different error-handling, and can't be used with try/catch or inside a lot of other control flow statements. Node's whole shtick is that the core libs are all asynchronous. (Though they did dial that back and start adding ___Sync() versions of a lot of things.) When people talk about ""callback hell"" they're talking about how annoying it is to have red functions in their language. When they create 4089 libraries for doing asynchronous programming , they're trying to cope at the library level with a problem that the language foisted onto them. I promise the future is better People in the Node community have realized that callbacks are a pain for a long time, and have looked around for solutions. One technique that gets a bunch of people excited is , which you may also know by their rapper name ""futures"". These are sort of a jacked up wrapper around a callback and an error handler. If you think of passing a callback and errorback to a function as a concept , a promise is basically a reification of that idea. It's a first-class object that represents an asynchronous operation. I just jammed a bunch of fancy PL language in that paragraph so it probably sounds like a sweet deal, but it's basically snake oil. Promises do make async code a little easier to write. They compose a bit better, so rule #4 isn't quite so onerous. But, honestly, it's like the difference between being punched in the gut versus punched in the privates. Less painful, yes, but I don't think anyone should really get thrilled about the value proposition. You still can't use them with exception handling or other control flow statements. You still can't call a function that returns a future from synchronous code. (Well, you can , but if you do, the person who later maintains your code will invent a time machine, travel back in time to the moment that you did this and stab you in the face with a #2 pencil.) You've still divided your entire world into asynchronous and synchronous halves and all of the misery that entails. So, even if your language features promises or futures, its face looks an awful lot like the one on my strawman. (Yes, that means even Dart , the language I work on. That's why I'm so excited some of the team are experimenting with other concurrency models .) I'm awaiting a solution C# programmers are probably feeling pretty smug right now (a condition they've increasingly fallen prey to as Hejlsberg and company have piled sweet feature after sweet feature into the language). In C#, you can use the await keyword to invoke an asynchronous function. This lets you make asynchronous calls just as easily as you can synchronous ones, with the tiny addition of a cute little keyword. You can nest await calls in expressions, use them in exception handling code, stuff them inside control flow. Go nuts. Make it rain await calls like a they're dollars in the advance you got for your new rap album. Async-await is nice, which is why we're adding it to Dart. It makes it a lot easier to write asynchronous code. You know a ""but"" is coming. It is. But... you still have divided the world in two. Those async functions are easier to write, but they're still async functions . You've still got two colors. Async-await solves annoying rule #4: they make red functions not much worse to call than blue ones. But all of the other rules are still there: Synchronous functions return values, async ones return Task<T> (or Future<T> in Dart) wrappers around the value. Sync functions are just called, async ones need an await . If you call an async function you've got this wrapper object when you actually want the T . You can't unwrap it unless you make your function async and await it. (But see below.) Aside from a liberal garnish of await , we did at least fix this. C#'s core library is actually older than async so I guess they never had this problem. It is better. I will take async-await over bare callbacks or futures any day of the week. But we're lying to ourselves if we think all of our troubles are gone. As soon as you start trying to write higher-order functions, or reuse code, you're right back to realizing color is still there, bleeding all over your codebase. What language isn't colored? So JS, Dart, C#, and Python have this problem. CoffeeScript and most other languages that compile to JS do too (which is why Dart inherited it). I think even ClojureScript has this issue even though they've tried really hard to push against it with their core.async stuff. Wanna know one that doesn't? Java. I know right? How often do you get to say, ""Yeah, Java is the one that really does this right.""? But there you go. In their defense, they are actively trying to correct this oversight by moving to futures and async IO. It's like a race to the bottom. C# also actually can avoid this problem too. They opted in to having color. Before they added async-await and all of the Task<T> stuff, you just used regular sync API calls. Three more languages that don't have this problem: Go, Lua, and Ruby. Any guess what they have in common? Threads. Or, more precisely: multiple independent callstacks that can be switched between . It isn't strictly necessary for them to be operating system threads. Goroutines in Go, coroutines in Lua, and fibers in Ruby are perfectly adequate. (That's why C# has that little caveat. You can avoid the pain of async in C# by using threads.) Remembrance of operations past The fundamental problem is ""How do you pick up where you left off when an operation completes""? You've built up some big callstack and then you call some IO operation. For performance, that operation uses the operating system's underlying asynchronous API. You cannot wait for it to complete because it won't. You have to return all the way back to your language's event loop and give the OS some time to spin before it will be done. Once it is, you need to resume what you were doing. The usual way a language ""remembers where it is"" is the callstack . That tracks all of the functions that are currently being invoked and where the instruction pointer is in each one. But to do async IO, you have to unwind discard the entire C callstack. Kind of a Catch-22. You can do super fast IO, you just can't do anything with the result! Every language that has async IO in its bowels-or in the case of JS, the browser's event loop-copes with this in some way. Node with its ever-marching-to-the-right callbacks stuffs all of those callframes in closures. When you do: Each of those function expressions closes over all of its surrounding context. That moves parameters like iceCream and caramel off the callstack and onto the heap. When the outer function returns and the callstack is trashed, it's cool. That data is still floating around the heap. The problem is you have to manually reify every damn one of these steps. There's actually a name for this transformation: continuation-passing style . It was invented by language hackers in the 70s as an intermediate representation to use in the guts of their compilers. It's a really bizarro way to represent code that happens to make some compiler optimizations easier to do. No one ever for a second thought that a programmer would write actual code like that . And then Node came along and all of the sudden here we are pretending to be compiler back-ends. Where did we go wrong? Note that promises and futures don't actually buy you anything, either. If you've used them, you know you're still hand-creating giant piles of function literals. You're just passing them to .then() instead of to the asynchronous function itself. Awaiting a generated solution Async-await does help. If you peel back your compiler's skull and see what it's doing when it hits an await call you'd see it actually doing the CPS-transform. That's why you need to use await in C#: it's a clue to the compiler to say, ""break the function in half here"". Everything after the await gets hoisted into a new function that it synthesizes on your behalf. This is why async-await didn't need any runtime support in the .NET framework. The compiler compiles it away to a series of chained closures that it can already handle. (Interestingly, closures themselves also don't need runtime support. They get compiled to anonymous classes. In C#, closures really are a poor man's objects .) You might be wondering when I'm going to bring up generators. Does your language have a yield keyword? Then it can do something very similar. (In fact, I believe generators and async-await are isomorphic. I've got a bit of code floating around in some dark corner of my hard disc that implements a generator-style game loop using only async-await.) Where was I? Oh, right. So with callbacks, promises, async-await, and generators, you ultimately end up taking your asynchronous function and smearing it out into a bunch of closures that live over in the heap. Your function passes the outermost one into the runtime. When the event loop or IO operation is done, it invokes that function and you pick up where you left off. But that means everything above you also has to return. You still have to unwind the whole stack. This is where the ""red functions can only be called by red functions"" rule comes from. You have to closurify the entire callstack all the way back to main() or the event handler. Reified callstacks But if you have threads (green- or OS-level), you don't need to do that. You can just suspend the entire thread and hop straight back to the OS or event loop without having to return from all of those functions . Go is the language that does this most beautifully in my opinion. As soon as you do any IO operation, it just parks that goroutine and resumes any other ones that aren't blocked on IO. If you look at the IO operations in the standard library, they seem synchronous. In other words, they just do work and then return a result when they are done. But it's not that they're synchronous in the sense that it would mean in JavaScript. Other Go code can run while one of these operations is pending. It's that Go has eliminated the distinction between synchronous and asynchronous code . Concurrency in Go is a facet of how you choose to model your program, and not a color seared into each function in the standard library. This means all of the pain of the five rules I mentioned above is completely and totally eliminated. So, the next time you start telling me about some new hot language and how awesome its concurrency story is because it has asynchronous APIs, now you'll know why I start grinding my teeth. Because it means you're right back to red functions and blue ones.",en,77
163,1149,1464631078,CONTENT SHARED,5270696484536580646,3609194402293569455,3334185486919070217,,,,HTML,http://www.inc.com/brian-de-haaff/8-insanely-simple-productivity-hacks-.html,8 insanely simple productivity hacks,"You earnestly set goals and try to meet them each day . But something always seems to get in the way and upset your progress. I know because it happens to me too. It could be your colleague's weekend recap that throws off your Monday morning, or the distraction of calls and texts throughout the day. When your best-laid plans face off with the reality of your workday, reality often wins. That disconnect between your goals and your productivity is likely cause for major frustration. You're not alone -- studies show many people are overworked and underperforming. A 2015 survey of 2,000 workers conducted by Staples Business Advantage revealed that 37 percent of employees believe more workplace flexibility would help them be productive. And about 50 percent point to email as the main culprit for reduced productivity. While there are no shortcuts to hard work, there are practical ways to get more done in the time you have. But first, you must take back control of your workday and start setting yourself up for success. These eight hacks can help you accomplish more meaningful work in half the time: 1. Rise and shine. You are most productive in the first two hours after you get up, so plan your most challenging and meaningful work for that time of day. Arrive before the rest of the team and dive into your work -- you will be ready to take a quick coffee break and catch up with everyone once they arrive. 2. Make mini-goals. Far-reaching goals are admirable, but they can seem so distant they appear unreachable. Help yourself realize goals by breaking them down into smaller, manageable mini-goals. These smaller ""wins"" will help you make progress and boost your confidence. 3. Stop multitasking. Multitasking is not only ineffective, but it reduces the quality of your work, causes more stress, and can even reduce your IQ. Stop trying to do everything at once. Prioritize essential work, and schedule time each day to deal with non-urgent matters like answering routine emails. 4. Tell others. Clearly communicate to your co-workers when you are heads-down on a task and need to work undisturbed. Block time in your calendar, put up a do-not-disturb sign, or otherwise set your status to ""busy"" to avoid social interruptions. 5. Seek quiet. An open floor plan is a noisy work environment, making it challenging for anyone to perform even simple tasks. Purchase noise-canceling headphones, find a quiet corner or an empty boardroom, or work remotely . Remember: The ding of an incoming text message is enough to disrupt your focus. Silence your phone during critical work, and put it out of sight. 6. Schedule breaks. If you work nonstop, your work will suffer and so will you. The ideal work-to-break ratio? One study says 52-minute work sprints followed by 17-minute breaks. Make your break-time meaningful -- stretch or go for a walk outside if you can. The important thing is to take time away from your screen and think about something else. 7. Measure progress. At the end of the workday, step back and take stock. What did you accomplish, and how do these accomplishments line up with your goals? What were your low points? What got in your way, and what can you do better tomorrow? By measuring your progress, you can spot patterns and opportunities to revamp how you work. 8. Prep for tomorrow. Devote the last 15 minutes of your day (no matter when that is) to creating a to-do list for the next day. That way, you can jump right into the work the next morning. This is especially important before you are going to take a bit of time off, like over the weekend -- and you will be happy to discover your list on Monday morning. It takes discipline and hard work to unlearn bad habits, reestablish boundaries, and regain control over your productivity. But the effort will be worth it. You will be proud of how much more you can accomplish daily, as well as the meaningful progress you will make toward those long-range goals . So when the next day rolls around and you're faced with a chatty co-worker or a buzzing phone, you now have a plan to silence them both -- and get more done.",en,77
164,1898,1469649566,CONTENT SHARED,1005751836898964351,3429602690322213789,6505036434267960593,,,,HTML,https://www.linkedin.com/pulse/seria-stranger-things-uma-obra-de-arte-do-algoritmo-da-gustavo-miller,seria stranger things uma obra de arte do algoritmo da netflix?,"Matei Stranger Things no último final de semana, sete dias após o seu lançamento na Netflix, tomado pelos milhares posts nostálgicos que floodaram minhas redes sociais sobre a série que emula os filmes de aventura infantil dos anos 80. Antes de ser mais um a dizer que os criadores foram geniais ao colocarem em 8 episódios todos os nossos filmes favoritos de infância, ninguém chegou a pensar que essa série talvez seja a maior obra de arte do algoritmo da Netflix? Pensem aqui: com base nos hábitos de seus assinantes, há cinco anos o algoritmo deles mostrou que os filmes do Kevin Spacey eram muito vistos, assim como os dirigidos por David Fincher, e que uma série britânica dos anos 90 sobre os bastidores sujos do Parlamento tinha uma interessante legião de seguidores. Daí saiu a adaptação americana para House of Cards. Desta vez temos uma série que costurou ET com Conta Comigo, Alien com Carrie, Contatos Imediatos do Terceiro Grau com Evil Dead, Goonies com Poltergeist, Além da Imaginação com Chamas da Vingança... Tudo isso estrelado por dois dos atores mais populares da década perdida: Winona Ryder e Matthew Modine. A Netflix costuma se fechar quando o assunto é a maneira como eles exploram dados e criam produtos a partir dos insights que o big data sugere. O algoritmo deles, que já ajudou (mas não é crucial, deixemos claro) a ressuscitar seriados como Arrested Development e Full House, além de estreitar cada vez mais a parceria com a Marvel Studios ( vide a penca de novas séries anunciadas na Comic-Con ), é algo tão valioso dentro da empresa que ela criou internamente o Netflix Prize , que justamente premia quem melhorar a capacidade daqueles números darem previsões de consumo. Em 2013, 75% dos assinantes escolhiam o que assistir ali dentro por base das recomendações da empresa. E hoje? A Netflix não divulga. Uma de minhas frases favoritas sobre a empresa, por sinal, é a de que há "" 33 milhões de versões diferentes da Netflix"" . Conhecendo um pouco como a Netflix analisa todos os nossos hábitos ali, aposto um quindim que tem muito big data por trás de Stranger Things. E que nenhuma das 99973 referências da série estão ali por acaso. Basta ver a reação apaixonada de todo mundo nas redes sociais na semana passada.",pt,76
165,379,1460554787,CONTENT SHARED,4761910285123871012,-1032019229384696495,-6165125069895538182,,,,HTML,https://wit.ai/blog/2016/04/12/bot-engine?hn,bot engine,"Today we are releasing the first step of our vision for conversational bots: an early beta of Bot Engine. Since we joined Facebook 15 months ago, the Wit community grew from 6,000 to 21,500 developers. Many of you are already making bots, leveraging Wit to parse messages into structured, actionable data. We are also building Facebook M , your personal assistant in Messenger. We've learned a lot in the process. Making a bot seems easy until you try. And then, the curse of combinatorics bites you. Understanding the user intent is a necessary step, but that's not enough. How should the bot respond? What actions should it perform? What question should it ask? This is the problem we are trying to help solve with Bot Engine. Here is how: Key concept 1: Stories To make a bot, there are two schools: rules or machine learning. (Everybody claims rules are bad and their bot is powered by AI, but when you really look under the hood, the core is often imperative.) Machine learning is of course more desirable, but the problem is the training dataset. Training a Wit intent with a dozen examples works well, and it's easy to leverage the community to get more examples. But in order to entirely learn the business logic of a bot of medium complexity, we would need many, many thousands of example conversations. Rules (or any kind of imperative approach, including plain script/program) are kind of the opposite. The good thing with rules is, you can have a demo working after you write two rules. As long as you follow the script carefully, your bot will work and your audience will be impressed. But as you discover new ""paths"" in the dialog, you'll add more and more rules, until one day everything collapses. You're doomed by the curse of combinatorics. Any new rule conflicts with old rules that you totally forgot the reason for. Your bot cannot improve anymore. Bot Engine is trained with Stories. Stories are example conversations. On top of user's and bot's messages, Stories also contain the bot actions: When you create your bot, you just start with a few stories that describe the most probable conversations paths. At this stage, Bot Engine will build a machine learning model that deliberately overfits the stories dataset. Practically, it means that stories will behave almost like rules. It enables you to start beta testing your bot and collect conversations. You will then turn these conversations into new stories (exactly like you turn logs into new expressions in the original Wit). The more stories you have, the better the model becomes. Unlike rules, Stories can ignore each other; when you discover a new use case, you can add a new story without the need to take into account all previous stories. With this approach, we are trying to get the best of both worlds: rules when you get started and don't have much data, and AI once your dataset grows. Key concept 2: Action Trained by Stories, Bot Engine predicts the next action your bot may execute at each step of the conversation. This action is executed on your side, with your code. We've made this decision because we believe that developers need total freedom of platform and execution scope for their bot's actions. You should be able to use any programming language of your choice and call any API you need. That wouldn't be practical if actions were executed on our side. An action typically modifies the context of the conversation. You pass the new context to Bot Engine when you need to predict the next action. Actions also help relieve the pressure on the machine learning model: they encapsulate some business logic that would require a huge dataset to learn end-to-end. It enables Wit to train efficient models before you have hundreds of Stories. Key concept 3: Inbox In line with the original Wit philosophy , we think that you really discover your users' expectations and logic only after you release a first version of your bot. As a result, the initial build should be as lightweight as possible, and the platform optimized around continuous learning and improvement based on actual usage. In Bot Engine, we are extending the concept of the Wit Inbox to the conversation logs: we will provide an easy way to turn logs into new Stories. This is the learning loop. Each time you add a new Story, your bot's models are rebuilt in real time. We are trying to solve a very hard problem, and we are not pretending that we have the definitive solution. There are many kinds of bots, and many different ways to address the problem. This is an early beta: our hope is that you won't hesitate to contact us when you are trying to do something that doesn't seem to be well handled. We are committed to spend a significant amount of time supporting the community and learning more about the problem. While at first glance it may look easier to just use scripts, AIML, or slot-based workflow templates to build bots, we believe that these approaches soon lead to bottlenecks. Bot Engine might not be as easy as putting a few rules together in a basic bot, but it's designed to handle scaling complexity with simplicity ( special dedication to our hero Rich Hickey ), as additional stories don't need to comply with a complex net of existing rules. Some of us at Wit have been building bots for 15 years. We've tried many things that don't work, and we're still looking for what could work. Please give Bot Engine a try , and give us your feedback! Alex Lebrun & Team Wit Note: If you already have a Wit app - First of all, thanks! We are thrilled to continue to serve you. - Your existing app will continue to work as before. The /message API is unchanged. - You won't have the Stories tab in your existing app. We'll add a migration process very soon. - If you create a new app, you'll get the Stories tab. You will also notice a change in the Console: we've merged the concepts of intents and entities under an Understanding tab, and slightly updated the training UX . Follow @WitNL",en,76
166,1749,1468376407,CONTENT SHARED,4375556914674736641,-1836083230511905974,3650195981289591968,,,,HTML,http://codigo-google.blogspot.com.br/2016/07/firebase-tag-manager-360.html?m=1&_utm_source=1-2-2,código google: introdução da próxima geração do google tag manager e do tag manager 360 para mobile apps,"O Google Tag Manager é conhecido por facilitar a implementação e o gerenciamento de analítica, remarketing, acompanhamento de conversão e outros tipos de tags de sites e aplicativos. E com a introdução do Firebase - a nova plataforma da Google para desenvolvedores para dispositivos móveis Android e iOS -, usar o Tag Manager ou o Tag Manager 360 para configurar a medição interna no aplicativo nunca foi tão fácil nem tão poderoso! No Google I/O , anunciamos que estamos expandindo o Firebase para torná-lo uma plataforma unificada para desenvolvedores para dispositivos móveis projetada para facilitar mais do que nunca a criação de aplicativos usando produtos e serviços da Google - e o Google Tag Manager é um destes serviços! A versão mais recente do Tag Manager e do Tag Manager 360 para aplicativos móveis foi projetada para funcionar com o Firebase e estender seus recursos para desenvolvedores e profissionais de marketing. Instrumentação interna do aplicativo unificada No coração do Firebase está o Firebase Analytics - um produto de analítica ilimitado e gratuito projetado especificamente para aplicativos móveis. Mas o Firebase Analytics é mais do que um produto de analítica, é uma forma unificada para os desenvolvedores medirem tudo o que estiver acontecendo em um aplicativo, desde os principais direcionadores da empresa até a interação detalhada do usuário. Isto cria uma fonte única confiável para atividades internas do aplicativo, que você pode compartilhar com outros recursos do Firebase e produtos da Google. Para o Tag Manager, isto torna o Firebase Analytics a nova camada de dados, o que significa que qualquer um que use o Firebase Analytics está pronto para começar a usar o Tag Manager sem precisar reprogramar. Para começar com o Google Tag Manager e o Tag Manager 360, basta se inscrever no Firebase, acessar o Tag Manager para configurar um novo contêiner do Firebase e, depois, adicionar o Firebase Analytics e o Google Tag Manager ao seu aplicativo. Tudo que você estiver medindo com o Firebase Analytics estará pronto para usar em tags, gatilhos e variáveis no Tag Manager. Medição dinâmica de aplicativos O Firebase Analytics facilita a medição do que está acontecendo no seu aplicativo. Mas o que acontece se você identificar errado um evento ou esquecer de adicionar um parâmetro fundamental? Ao adicionar o Tag Manager ou o Tag Manager 360 ao aplicativo, você pode fazer alterações na configuração de medição sem precisar perder tempo no processo de atualização do aplicativo. Como os profissionais de marketing experientes sabem, sem o gerenciamento de tags, mesmo as mudanças mais básicas em tags podem exigir muito tempo e esforço, além de coordenação entre as equipes de marketing e desenvolvimento e de retirar recursos de outros projetos. Com o Tag Manager e o Firebase, você poderá desacoplar as mudanças de medição dos ciclos de desenvolvimento - e, ao fazê-lo, simplificará o modo com que as equipes de marketing e desenvolvimento trabalham juntas. Um SDK, muitas opções Apesar de o Firebase visar facilitar ao máximo a criação de aplicativos e medir o comportamento do usuário, ele não se destina a ser uma solução para todos os problemas. Os desenvolvedores e profissionais de marketing muitas vezes escolhem usar soluções diversas de vários provedores nos aplicativos. O Google Tag Manager e o Tag Manager 360 podem ajudar a fazer com que estas ferramentas distintas façam sentido. Com o Firebase Analytics, fica fácil medir o que está acontecendo no aplicativo - sem limitar você a um único conjunto de ferramentas. O Google Tag Manager e o Tag Manager 360 permitem que você envie os dados para diversas outras ferramentas de analítica, tanto da Google - como o Google Analytics - quanto de outros parceiros. Estamos muito felizes em anunciar parcerias de fornecimento de tags com diversas das soluções de atribuição do aplicativo, incluindo Kochava , Tune , adjust , AppsFlyer , Apsalar e muito mais. O Google Tag Manager sempre foi respeitado por seu compromisso com ser agnóstico em relação aos provedores para medição web e está animado em estender o mesmo compromisso para os aplicativos móveis. E nossos parceiros também estão! ""Sempre fomos apaixonados por apoiar os desenvolvedores, garantindo que o Kochava sempre estivesse integrado às melhores ferramentas. Por isso estamos tão felizes em fornecer suporte pronto para uso para o Google Tag Manager pelo Firebase."" - Charles Manning, Presidente da Kochava Se você não está vendo o parceiro que procura, não se desespere. Estamos adicionando mais parceiros continuamente por meio do Programa de modelos de tag de provedores . Comece quando quiser Se já estiver usando o Google Tag Manager ou o Tag Manager 360 para aplicativos móveis, não se preocupe, seus contêineres existentes e os SDKs atuais continuarão trabalhando da mesma maneira de antes. Mas, como com qualquer versão importante de recurso, recomendamos atualizar para a versão mais recente do Google Tag Manager para aplicativos móveis - e com o Firebase junto - o mais cedo possível. Assim, você pode ter certeza de que extrairá o máximo da experiência de gerenciamento de tags móveis. Pronto para o Google Tag Manager? Saiba mais e comece hoje!",pt,76
167,2230,1472566708,CONTENT SHARED,-5044204125574973395,3829784524040647339,-5600479164389645980,,,,HTML,"http://link.estadao.com.br/noticias/gadget,samsung-lanca-dispositivo-para-deixar-carro-conectado-a-partir-de-chip-4g,10000071168",samsung lança dispositivo para deixar carro conectado a partir de chip 4g - link - estadão,"A Samsung lançou, durante a última semana, uma série de acessórios para acompanhar a chegada ao mercado norte-americano de seu mais recente smartphone, o Galaxy Note 7. Entre fones de ouvido e uma versão melhorada dos óculos de realidade virtual Gear VR, um dispositivo se destaca: o Connect Auto. Parecido com um pendrive ou um adaptador de tomada, o dispositivo poderá ser utilizado em qualquer carro para conectar o veículo, a partir de um chip habilitado para transmitir dados via redes 4G. Para funcionar, o dispositivo deve ser conectado à porta OBD II - utilizada nos carros dos últimos 20 anos para fornecer dados sobre o automóvel de forma eletrônica. Com isso, o Connect Auto consegue receber informações do veículo, e enviá-las para o usuário. Anunciado em fevereiro, o Connect Auto consegue disponbilizar sinal de Wi-Fi para até 10 dispositivos, desde que tenha um chip 4G em seu interior. Por enquanto, o aparelho está sendo vendido nos EUA exclusivamente para usuários da operadora AT&T - o preço varia de acordo com o plano contratado, mas o preço cheio do dispositivo é de US$ 109. Ainda não há previsão para o lançamento no Brasil.",pt,75
168,996,1463494497,CONTENT SHARED,4369833742675497700,268671367195911338,-9042980566649783979,,,,HTML,http://www.mckinsey.com/industries/financial-services/our-insights/transforming-life-insurance-with-design-thinking,transforming life insurance with design thinking,"Better addressing the evolving needs of consumers can help incumbents win their loyalty-and protect against new competitors. For generations, the life insurance industry delivered its promise of financial security with the help of strong actuarial functions, working through intermediated distribution channels. Complex products, limited services, rigid processes, and cumbersome consumer interactions did not necessarily hamper business success. In the past decade, however, the rules of the game have changed. Today's consumers reward transparency, speed, and flexibility, new competitors are looming on the horizon, and the low-interest-rate environment makes the traditional business model a thing of the past. This challenge is reflected in the sector's financial performance: the life industry has grown only 3.1% p.a. globally in the past decade (and only 2% p.a. in Europe), significantly lagging behind other mature industries such as banking or manufacturing, which have achieved a 5 to 6% p.a. growth rate. European life insurance delivered an after-tax ROE of around 8.6% between 2010 and 2015E -in line with the performance of banks (8.6%) and slightly above that of asset management (7.6%)-however still below other mature industries such as retailing (13.1%). To some fintechs, noninsurance incumbents, and venture capitalists, the industry's challenges suggest opportunity. The life insurance value chain is increasingly losing share to these players, who are chipping away at the profit pool of incumbents (Exhibit 1). How can incumbent life insurers keep pace in today's fast-moving competitive environment and meet customers' changing needs? There are three major ingredients of the ""winning recipe"": simplified, compelling product design, a streamlined cost base, and delightful customer journeys. This article focuses on the third ingredient, and describes a new approach called ""design thinking (and doing)"" that connects every aspect of the business, from marketing to distribution, underwriting, and claims. It is a method that goes beyond the traditional mantra of customer centricity and aims to change not only a company's processes but also its people. Incumbent life insurers that don't keep pace will falter; some might disappear. As Klaus Schwab, chairman of the World Economic Forum, famously put it: ""In the new world, it is not the big fish which eats the small fish, it's the fast fish which eats the slow fish."" Size or complexity cannot be an excuse for sluggishness: the top global insurers typically have a market cap of around USD 60 to 80 billion, while Alphabet's (Google) is well over USD 500 billion. The disconnect between life insurers' value proposition and today's customers Imagine Susan, a 34-year-old high school teacher who is expecting her first child and hence decides to buy a life insurance policy. First, it is very unlikely that a provider will proactively reach out to her. If she tries to shop for a policy online, she may be intimidated by complex products and technical terminology that is confusing and makes her feel incompetent. If she looks for an agent and is lucky enough to find one that she feels she can trust, she is likely to have concerns about how much the policy will really cost, the meaning of all the fine print, and whether the agent is more interested in her needs or a quick commission. If Susan overcomes these doubts and decides to buy a policy, she will begin what may be an arduous application process: lengthy forms with sensitive medical questions, weeks of waiting, and little transparency on where things stand. She may wonder why the process is so complicated and time-consuming when many companies in other industries offer convenient, fast (and mostly digital) services. If she has questions on her coverage after buying the product or wants to change her policy, she will likely struggle with the limited self-service options offered by the insurers and may find that her agent has moved on. Such pain points may make Susan abandon the process or take her business elsewhere, as illustrated by the most common business leakage drivers in Exhibit 2. Susan's journey illustrates some of the big challenges facing life insurance: Low engagement . Life insurers have long struggled to engage prospective clients and nurture relationships with existing ones. The product spurs high customer interest but low engagement, leading to significant untapped demand. Distribution is often intermediated through brokers, independent financial advisors, or banks, putting distance between insurers and their customers. And typical customers have very few interactions with their provider compared to other industries, such as banking (Exhibit 3). This is a major barrier to reducing attrition and enabling cross- and upselling. Limited ability to meet the preferences of Generation Y (and the new needs of Generation X) . Generation Y, the millennials now coming of age, will comprise close to half of the insurance customer pool within the next ten years. They expect highly interactive digital experiences, complete price transparency as well as fast and even instant delivery. When seeking information, they rely less on friends and family, looking instead to social communities and online reviews. Thanks to the ""equalizing"" effect of smartphones, the members of Generation X are adopting many of these behaviors rapidly. Few life insurers have the management mindset, functional firepower, or digital talent to develop propositions that meet these preferences. They also have difficulty adjusting to their fast-paced changes. These gaps frustrate consumers-and open the door to agile innovators. Legacy cost structures and IT systems . Life insurance carries a large, inflexible stock of customers/policies, and it is hard to change products or systems when some of the policies on the books were sold 30 years ago. At many incumbents, this means a clunky IT landscape that is difficult and costly to transform, especially given an expense ratio of 9 to 10%. The rigidity of the in-force book creates an environment where old processes never die and costs keep building over time. As a result, industry expense ratios have fallen only slightly in the past ten years; for key European markets, the total expense ratio declined by only 0.5 percentage points between 2000 and 2013. The dilemma of the slow fish . Insurers have historically been slow to change since their business is built on stability and risk aversion. Product development cycles often stretch to a year, and most IT upgrades follow a sequential ""waterfall"" approach. Accountability is diluted, which leads to management by committee and slows delivery. In our recent proprietary Digital Quotient™ assessment (a comprehensive tool that rates an organization's digital maturity across roughly 20 management practices), insurers lag behind digital leaders especially in terms of performance steering and measurement (Exhibit 4). As a consequence, speed of decision making and agility suffer. The life industry has not stood still in the face of these challenges, of course. Many incumbents have launched efforts to transform themselves. Most have experimented with digital initiatives, agile work modes, customer immersion, and so on. Some have set up incubators or accelerators to explore new, customer-centric solutions outside their core operations. Only a few are making the more painful change of transforming their IT platforms to enable digital delivery. And hardly any have truly transformed their basic value propositions or created genuinely customer-centric cultures. Initiatives in this direction often remain cosmetic or produce artifacts that never see the light of day. Applying design thinking to systematically rework customer journeys How can incumbent life insurers reorient themselves around the customer? One powerful approach is to apply the principles of customer-centric ""design thinking,"" which can deliver a compelling end product and be disruptive enough to transform a company's culture along the way. The methodology is based on the insight that optimizing individual touch points is insufficient to deliver a truly satisfactory overall journey-what is required is an end-to-end redesign. It is also informed by the understanding that consumers today do not separate products or services from the experience of buying and owning them. As a result, the entire ""package"" needs to be carefully designed. A few common misconceptions need correcting up front. Design thinking is not just about creating pretty interfaces or replacing paper with digital records. What it really means is applying creative, nonlinear approaches to reinvent how customers interact with the business. It touches on all functions, from product development through underwriting to IT. The essence of this new perspective is to view the customer experience as a source of competitive advantage and taking action-and making investments-accordingly. Applying design thinking to the customer journey in life insurance requires a fundamental shift along four dimensions: Instilling customer empathy . Good design requires real empathy that goes beyond what customers want: it reflects why they want it. In life insurance, fintechs are successfully attacking incumbents in this arena. The sleek experience provided by PolicyGenius, Knip, Acorns, and other start-ups shows that they are anchored in customer preferences. Like with Uber, Airbnb, and many others, the heart of their success is not the digital tool, but the experience it enables. Real empathy allows designers to respond to true underlying needs, not superficial, stated interests. By doing this, it spurs breakthrough innovation. In fact, we believe it is the only way an incumbent insurer can be sure of delivering more than a ""me-too"" customer experience. This kind of empathy requires specific research tools that typically have an ethnographic flavor, such as ""shadowing,"" ""follow me home,"" or ""shopalongs""-but empathy is about more than just collecting ""soft"" insights. A critical part of instilling empathy across the organization is translating it into hard operational metrics that can be tracked and managed (e.g., response time to under-writingrelated inquiries, number of broker or agent calls that get resolved without a call transfer, share of rejections where no alternative product was offered). In this sense, improving customer experience metrics has to become a ""contact sport"" where the entire organization works together to move the same set of numbers. Getting comfortable with an iterative approach . To use the design methodology, insurers have to feel comfortable releasing ""imperfect"" products that offer basic functionalities and services with the goal of obtaining immediate customer feedback. This approach helps avoid costly mistakes down the road and aims to bring new solutions to the market in less than 16 weeks rather than 9 to 12 months. It also means that the product is never really complete: it undergoes constant iteration. In VC and start-up jargon, it is the ""minimum viable product"" refined in multiple consecutive user tests. As an example, in testing a digital application tool, designers found that brokers were most excited about the ""accelerator"" button that allowed them to fast track three policies per month for their high-potential clients. By contrast, bank-based advisors liked a feature that prompted them to hand over the tablet to the customer to answer the sensitive medical questions themselves. To drive this kind of highly responsive innovation, many insurers need to develop a two-speed IT architecture that permits experimentation without disrupting the architecture that supports a massive in-force book. Replacing functional siloes with agile cross-functional teams . Life insurers are conservative organizations built around functions, such as Risk, Underwriting, and Sales. These offer deep expertise, but are too rigid to respond to a rapidly changing environment. In a functional setup, no one owns the full customer experience. In fact, it typically takes one to two weeks (and many work sessions) to create a complete view of the end-to-end customer experience and pain points. What is the solution? Agile, cross-functional teams that are self-motivating and entrepreneurial. They follow a new cadence, including structured, transparent, weekly sprints with daily stand-ups, review meetings, and retrospectives, to learn and take on more accountability. The cultural gap between the traditional and new setup is immense, and incumbents find it hard to navigate the transition. What is important is to anchor this thinking at the top. Having one person monitor every aspect of design thinking could work in the transition phase. It need not be a stand-alone position-one of the existing managers, such as the Chief Marketing Officer, can take on the role. The key is to have a senior leader oversee the end-to-end customer journey. ""Braiding"" business and IT . Traditionally, the business has led IT. Sales and product managers defined their requirements, and IT (and operations) executed. Today's environment has propelled Chief Operating and Chief Technology Officers to a more critical role. Since they are typically the ones who get firsthand exposure to what fintechs and digital providers are experimenting with, they have become the door openers to new possibilities. Operational and IT leaders are often tempted to take over the company's digital agenda, but this approach can easily backfire. Digital and customer transformations stall if leaders cannot show how they deliver value. IT needs the business leaders to link the investment to a clear customer need (and business case). On the other hand, business leaders trying to bypass IT out of frustration with its speed or capabilities doesn't work either. This creates isolated, greenfield solutions that are hard to integrate back into the IT landscape. Common examples are product navigators, insurance coverage calculators and advice tools that are not linked into a single vision or IT infrastructure. The pragmatic middle way is a ""braided"" approach where operations, IT, and business leaders work in close partnership to define and execute a customer-centric strategy. Hallmarks of this approach include an honest dialog about the starting position as well as real-time investment in defining a joint digital ambition. Turning design thinking into results Life insurers can adopt design thinking with an agile, sprint-based approach that allows for a full experience redesign in 8 to 12 weeks (for one product proposition). A design wave of this kind has three critical components: Immersion . As described earlier, customer immersion involves a rich set of interviews, shadowing, and desk observations of end customers and distribution partners. It is critical to engage a broad set of stakeholders in this phase: customer immersion should be mandatory for everyone who works on the design. It is also important to differentiate between traditional customer research and immersion, coach the team on the difference between both, and show how to uncover and document fresh insights. A sign of success is if the team members start quoting the customers they have spoken to and tackle service issues through the eyes of customer ""personas"" they have created. The outcome of the immersion is a detailed depiction of the customer journey that highlights pain points and links them to hard operational metrics, such as policy processing time, number of rejections without an alternative offer, or average number of interactions with clients per year. Ideation . Ideation starts with brainstorming around the target journey, devoid of any internal or external constraints. The goals are to resolve basic pain points and servicing issues as well as to develop ""signature moments"" that can drive customer loyalty and cross-selling. Examples could be emotionally charged moments, such as year-end thank-you cards to the policy beneficiaries to remind them that they are protected. Other features like a simplified 2-minute upfront risk check to provide immediate price guidance, or a real-time application status tracker could be very compelling as well. Surprise and delight elements, such as accelerated paths for simple applications, can also be powerful. The vision then needs to be translated into pragmatic actions, many of which will not be digital in nature, such as underwriting services or turnaround times. A systematic leakage analysis along the customer journey provides the basis for prioritizing across these initiatives. In a next step, the initiatives are translated into mock-ups of processes that allow visualization of the new customer experience. At the end of the ideation phase, the crude mock-ups become detailed prototypes. Iteration . Iterations are built on user tests and are ideally done in short, weekly sprints. The willingness to let go of ""pet"" features and ideas if they fail the customer test is a critical prerequisite in this phase. While the designs are being iterated, the technical implementation can begin in an agile way to deliver value to the customer (and road test the new design) in the shortest time possible. Importantly, iteration continues after the launch of the new proposition or service. As the organization moves towards a test-and-learn approach, it needs to use market feedback to spur the next iteration. Since today's consumers expect frequent updates and new features in nearly every product and service, the insurer's innovation work is never done. Transforming into an analytics-driven insurance carrier Read the article Hallmarks (and impact) of a revamped customer journey The end product of the redesign process is a new customer experience that can combine radical changes with surprise elements and simple operational improvements. It is an experience that our protagonist Susan can enjoy and even recommend to friends. What could this look like? Susan might be proactively contacted by an insurer with a short message that explains the value of buying a life insurance policy given her family situation, thus saving her extensive research. The outreach could include a link to tool that maps her needs (and demographics) in a quick, ""human"" and fun way-and delivers an instant offer with a buying rationale that feels personal to her (even if it is ""just"" a smart algorythm). When visiting the insurer's Web site, she might be able to select her agent based on shared interests or background (which is proven to build trust-and hence sales successs). If her policy is not issued instantly, she can track her application status online, and see the estimated day when her policy will be (e)mailed to her. When the policy arrives, it comes with an emotional gesture that ""rewards"" her for taking care of an important financial to do for her family. Examples could be a token gift or a branded card with the product name and coverage amount that she can share with her beneficiary. Obscure jargon is banned from all communications. Once Susan becomes a customer, she gets quarterly account updates with just the two to three headline data she cares about. What is more, she can adapt the policy on a self-service site without having to download an app, and she can choose to buy additional products without having to reenter all her data. These elements are just examples and focused purely on the end customer. Our work with different insurance clients has shown that there is typically a portfolio of 40 to 60 experience elements along the journey that can be built in to create a delightful experience-not only for end customers, but also for agents and brokers. The financial impact is significant. Depending on the starting situation, an overhaul of the journey can generate a 50 to 100% increase in new premium. Life insurers in Europe and across the globe need a jolt to keep pace with a new breed of customers and attackers. We believe that design thinking can be a fresh approach that provides this, because it allows transformation that spans functional lines and changes the way people think and act. It is also a proven tool that has already delivered results in many other industries. Incumbent life insurers are now called upon to take action and make the much-needed change happen. About the author(s) Jochen Kühn and Ildiko Ring are partners in McKinsey's Zurich office, where Maximilian Straub is a consultant. Markus Berger-de Leon, digital partner, is head of McKinsey Digital Labs Studio Germany. The authors wish to thank Mark Bothorn for his contributions to this article.",en,75
169,768,1462319974,CONTENT SHARED,-6603351162010419903,-3500661007957156229,-6855475010330807605,,,,HTML,http://info.versionone.com/webinar-safe-4-value-streams-email.html?mkt_tok=eyJpIjoiTXpjM05HVXhNRGc0TkdOayIsInQiOiJGWk90angzRjl4V0VLXC9iaW9KQzcrMm8yaXk4TlwvSW1SdkFEWTdBeG9oT3pBVDZBNjVNaWV1Zlo5YzR5UFwvTGdtQVRqTTR5VTdUWE8rZExkb0U3YTN0ZE80c2VvM2E4XC9HZUdpRm9ZenRCN1k9In0%3D,versionone agilelive™ webinar series| safe® expert webinar,"Alex Yakyma SAFe Fellow & Principal Consultant Scaled Agile, Inc. Alex Yakyma is a methodologist, trainer, and consultant who has been applying lean and agile practices in the software industry for over a decade. Alex joined Dean Leffingwell back in 2006 and since then has been actively working on the Scaled Agile Framework and its numerous implementations in the field. Alex's broad prior experience as a program manager in highly distributed multi-cultural environments allows him to perfect his clients' operational capabilities at the program level, their cross-program coordination, and their portfolio strategy.",en,74
170,2315,1473684520,CONTENT SHARED,-8511291357261863413,6960073744377754728,-8019369593564010761,,,,HTML,http://ronjeffries.com/articles/016-09ff/defense/,dark scrum,"Let's talk about ""Dark Scrum"". Too often, at least in software, Scrum seems to oppress people. Too often, Scrum does not deliver as rapidly, as reliably, as steadily as it should. As a result, everyone suffers. Most often, the developers suffer more than anyone. The theme behind a lot of my thinking lately is this: Kent Beck, my original ""Agile"" mentor, once said that he invented Extreme Programming to make the world safe for programmers. It turns out that the world is not yet safe for programmers. Scrum can be very unsafe for programmers. To paraphrase Ken Schwaber, one of the co-creators of Scrum, in another context: ""That makes me sad"". Scrum, done even fairly well, is a good framework. I'm quite sincere about that. It's good to have a strong connection to the business deciding what needs to be done, and what needs to be deferred. It's good to have all the skills you need to build the product right on the team. It's good to build a tested tangible product every couple of weeks, and it's good to show it to stakeholders, and it's good to review how things went and how to improve them. I've studied, worked with, and thought about Scrum for years, and everything about it is really quite good. Not perfect, but really quite good. Unfortunately, that's Scrum as a concept, Scrum as an ideal, Scrum done as intended. Like every good idea, the reality sometimes falls short. Sometimes it falls far short. I call the result ""Dark Scrum"". Dark Scrum: Some Typical Abuses of Scrum Let's look at how things can go wrong in Dark Scrum, in just a few steps. In this section we observe a team experiencing Dark Scrum. The Dark Scrum leaders mean well: they're just doing it wrong. Self-organization is slow to happen. Clearly it takes some time to get good at Scrum. It has new roles and activities. Even more difficult, it asks that we take on new values. We're supposed to let our developers self-organize to do the work. It's easy to call the Scrum meetings and call ourselves by Scrum roles. It's not at all easy to really walk the walk. Scrum generally starts with very few people trained, even fewer understanding it, and a lot of people who think they know what they're supposed to do. It should be no surprise if they do it wrong, and very often they do. Often, today's power holders believe that their job is to decide what their people should do, tell them to do it, and see that they do. Scrum teaches otherwise: explain what needs to be done, then stand back and let the people self-organize to do it. People can't just click over into Scrum's new mode of operation. It takes time to unlearn the old ways. It takes time to learn new habits, time to learn to trust the team. It takes time for the team to learn how to be trusted: in a way that's the underlying message of this article. Scrum's training begins this learning process for the people who get the training. Dark Scrum begins when people who know their old job, but not their new Scrum job, begin to do the Scrum activities. It goes like this: Awesome, we can help the team every day! Every day, the team is supposed to get together and organize the day's work. This practice, the ""Daily Scrum"", is imposed on the typical team. There might be one person in the room, the ScrumMaster, who has been told how it should be done. The programmers haven't been told. Quite often, even the Product Owner hasn't been told. Almost certainly other power holders haven't been told. But the power holder already knows his job. His job is to stay on top of what everyone is doing, make sure they're doing the right things, and redirect them if they're not. How convenient that there's a mandatory meeting where he can do that, every single day! The result: instead of the team rallying around their joint mission and sorting out a good approach for the day, someone else drags information of of them, processes it in their head, and then tells everyone what to do. Since nothing ever goes quite as we expected yesterday morning, this improper activity often comes with a lot of blame-casting and tension. Dark Scrum oppresses the team every day. Self-organization cannot emerge. We also have convenient frequent planning! Every Sprint, the Scrum Product Owner is supposed to meet with the team, and describe what's needed. The team then figures out what they can do to meet the needs, and begins to build it. Well, that's the theory. In practice, there may not even be a business-side Product Owner. Even if there is, odds are they are not trained in how to be a Product Owner. Well, the power holders may be new to Scrum, but they know a lot about how to handle this problem. So every two weeks they show up and tell these programmers what they have to build. Oh, those programmers will push back. They are lazy, and recalcitrant. But the power holders keep the pressure on, because that's how you manage people. So they tell the team what to do and they'd better do it. Of course, power holders have little or no idea how to program, and the programmers usually at least have some clue. They have no idea what the state of the code is, and the programmers usually do know. The programmers are supposed to decide how much work to take on in Scrum, but in Dark Scrum, power holders know better. They pile it on. They see that as their job: drive the team. The results never vary. The team sincerely tries to do what is demanded. They know there's no point in saying it's impossible, even though it is. They'll just get berated for being lazy, stupid, trouble-makers. So they do their best and it isn't good enough. At the end of the Sprint, results are not what was demanded. Magic has once again not happened. The developers have failed again. Fortunately we have just the chance to set them straight: the Sprint Review! We get to critique what's done - and what's not. Every Sprint, stakeholders look at what the team has accomplished and provides guidance on how to go forward. Great theory, rarely done in practice when organizations are not expert in Scrum. In practice the Dark Sprint Review begins by someone reminding everyone what the team ""promised"" to do. (That is, what was demanded right before the team said ""We'll try"". That's a promise, isn't it?) Then we look at the pitiful failure the team brings us. Guess what. The organization demanded more than could be done, and they didn't get it. The team tried, and in trying, they took every shortcut they could think of to get all those unreasonable requests finished. They skimped on testing. They skimped on design. They even worked when they were too tired to think straight. They fell short and what they did accomplish wasn't very good. The team failed. Again. Fortunately, Dark Scrum has power holders, Product Owners, and stakeholders for this effort. They make sure the programmers are made fully aware of how badly they've done. That will surely inspire everyone to do better next time. To help with that, we have the Sprint Retrospective! We get to tell them how to improve ... In the Sprint Retrospective, the team looks back at the preceding Sprint, and at prior history. They observe what has gone well and what has not gone well. They figure out how to improve their process for next time. In practice, Dark Scrum leaders help them with that. We remind the team that they fell short, despite how closely we were managing them. We make it clear to these developers that their failure - and it is a failure - was surely due to a combination of laziness and incompetence. To inspire the team to do better, we point out the consequences of not improving, which will almost certainly include the rolling of heads. That always gets their attention. Sometimes the team will offer suggestions. Let me prepare you right now for that. Here are some things they might say, and how you might answer them: Developers : We need to do more testing to reduce the bug count. Power Holder : No. You're already falling behind on features. You need to code smarter. Stop putting in bugs and you won't have to remove them. We need features, not tests! Developers : The design is getting crufty. We need to refactor to improve it. Power Holder : No. Why did you write a crappy design in the first place? No one told you to write a crappy design. Stop doing that and fix what's wrong. There's a weekend coming up. Do it then! Developers : The requirements are unclear, they don't get clarified, and then what we build gets rejected at the last minute. Power Holder : We hired you to be smart and figure things out. You're supposed to self-organize to solve these problems. Quit sitting on your asses and build what we want! You get the idea. Keep these people's nose to the grindstone, and their feet to the fire. That has always been what it takes to get software done. Scrum doesn't change that. Wow, that was depressing. Well, of course, Scrum is actually trying to change all that. But until the hearts and minds of the organization actually change, there'll be too much of the old-style management happening ... and now it happens every couple of weeks ... often every single day! What can we developers do? The things that happen in Dark Scrum are abuses. They really are in opposition to what Scrum tries to teach us. No real Scrum expert would recommend any of these things. People who do these things are not ""doing it right"". Everyone agrees about that. Still, Dark Scrum does happen, and it happens far too often. In my view, abuses are almost inevitable until people really learn Scrum's principles and values. It might seem that there's nothing a development team can do but accept oppression. Fortunately, that's not the case. The good news is that there's something very powerful that we can do. The bad news is that it isn't easy. The other good news, though, is that it's mostly about programming and we're usually pretty good at that. Much of the pressure from Product Owner and/or other power holders comes because they can't see clearly what we're working on, what we're actually getting done. If we can make progress crystal clear, we can change how we're treated. If the product has a lot of known defects, our leadership will assume that there are even more, and they'll be afraid. In their fear, they'll put us under even more pressure, and since fear often manifests as anger, they'll be angry with us. Often, very angry. If we're working on design or infrastructure before features, features seem to come out slowly. This makes our leaders afraid that we'll be late or fail to deliver important features. Again, we are subjected to fear and anger. If we slow down because our design is falling behind, we produce fewer features. Fewer features, more fear, more anger, more pressure. When leadership cannot see working software, they become afraid about the future - and rightly so. They show that fear, invariably, in ways that are not helpful, and that are generally painful. Developers can stop that from happening. The only way I know is to deliver software. Real, live, working software. With visible features! The Power of the Increment Scrum asks us to deliver a tested, integrated, working, shippable increment of the product, at the end of every Sprint. We'll talk shortly about how to do that. For now, let's imagine that we can do it: we have the ability to deliver a working product increment every couple of weeks. Much of the fear will go away by itself, because leadership can see real results. Some will remain, however, because quite likely real results lag behind their hopes and dreams. Everyone hopes for a sports car, but sometimes you only get a bike. Maybe you wanted a pony, but you only get a dog, not even a cat, which a man could at least respect. (But I digress.) Still, with a solid increment in hand, tested, integrated, containing the most important features they've given us to do, we can change the conversation: Power Holders : You're not getting enough done: you have to do more. Developers : We're doing all we can, and we'll keep trying to improve. Meanwhile, all this works and is usable. It might be wise to use our real rate of delivery to predict how fast we'll go. It might be best to have us work on the most important backlog items first. It might be best to trim each item down to be as lean as possible. This won't just work magically right away. But it will begin to help, and the more true and real our product increment is, the more it will influence people to look at reality and base their management on it. Contrary to what we often think, our leaders are not stupid. They're doing the best they can with the information they have. If we can give them better information, in the form of working software, they'll begin to use that information. Using that information, they'll resort to less pressure and less abuse. Again, this isn't magic and it won't happen overnight. However, I've seen it happen time and again. Other than in the most dysfunctional organization on earth - and only a few of you readers actually work there - it will work, if we keep at it. There's a catch. There's always a catch. The catch here is that in order to change the conversation, in order to reduce pressure and abuse, the increment must actually work. It really has to be tested, integrated, ready to ship, containing all the backlog items we've undertaken to do. That isn't easy. Worse yet, we didn't learn in Computer Science how to do it. We didn't learn in Software Engineering how to do it. We can't learn how to do it at IT Tech Institutes, and we won't find it in Your Favorite Language for Dummies . There are practices that are specially formed to allow teams to build software incrementally, keeping it well tested, well designed, focused on features, and ready to go at all times. If we're going to change Dark Scrum into Scrum as She Should Be Played , we need to learn these practices. Fortunately, they're not that hard to pick up, although like all programming, they do require practice and hard work if we want to be good at them. Later in this series we'll look at them in more detail, but let's look at some of them briefly here: Acceptance Testing It is quite common for there to be disputes, at the end of a Sprint, about whether the team did or did not build what the Product Owner ""wanted"". These disputes are wasteful and they tend to drive Product Owner and developers apart. We want them working closely together, not fighting with each other. One very strong practice is for the PO and the developers to agree on concrete, executable acceptance tests for each story. It can be productive to think of these as ""examples"". Team: OK, we want the system to show all the organizations' accounts that are 30 days in arrears, and a total of the arrearage. Here's a table of accounts, and a table showing what the identified accounts would look like, with the sum. PO: No, that's not quite right. We don't want to show them at thirty, only beyond thirty. Each little example becomes an acceptance test for the story or backlog item in question. By convention, if the tests don't run, the team has not finished that story. Conversely, if the tests do run, the Product Owner and team agree that we have done what was asked. It's possible, of course, that the Product Owner won't like what she sees. But now the conversation has changed. She can't legitimately say ""That's not what I wanted"". It is what she wanted when we agreed. She has learned something and now wants something different. That's just fine, it's the PO's job to find her way to the best possible product. Working from concrete acceptance tests helps us understand what's being asked for, helps us know when we've accomplished it, and helps us know when we're refining our understanding and asking for something to be changed. As an important bonus, when we automate these examples, we can run all of them and show with certainty that the product is still doing what we agreed it should do. Incremental Design In a Scrum project, we're supposed to deliver a tested, working, potentially shippable Product Increment after each Sprint. Obviously, we can't have the whole design figured out at the beginning: we only know roughly what the product will even be. Equally obviously, we need to wind up with a good design at the end. Therefore, the design of our software must be created incrementally, a bit at a time. That's OK: when the product is small and incomplete, it doesn't need much of a design. As it grows, it needs a larger, better, more robust design. We just need to grow the design as we grow the product. ""Just"" . The most dangerous word in software development. Building up the design as we go is difficult. It requires skill in design, skill in testing, and skill in refactoring. We'll explore those here below, and in other articles in this series. Over time, we'll point you to some resources on the subject. But the fundamental truth remains: if we are going to deliver a Product Increment every Sprint, we must find a way to do good design, incrementally. Refactoring The best way we know today to do incremental design is called ""Refactoring"". Refactoring is the process of improving the design of existing code - without breaking it ! Refactoring isn't rewriting. Certainly if we have a program with a bad design, we can write a new program, using a better design. Probably I should say here ""just write"", because that trick rarely works. But in principle we could. It would take a long time, however, and in Scrum we don't have a long time: we need a better design by the end of the current Sprint. Refactoring is a process of transforming the code, usually in very small steps, to make it better. Modern IDEs help us with this: most of them offer a number of automated refactorings. They can extract a commonly-used bit of code and turn it into a method or function. They can move code from one class to another. They can rename things, or change the signature of a method. Using refactoring well, we can incrementally improve our design as we go. The operative word is "" well "", however. We do have to build up that skill. Incremental design improvement is necessary. Refactoring is how we do it. Programmer Testing We need programmer tests in addition to acceptance tests. Every feature we build is composed of many programming steps. In any of of those steps, we could make a mistake. We avoid these mistakes using programmer testing. One very good form of programmer testing is Test-Driven Development, which goes like this: Think of some small next step on the way to our feature. Write a test that should run when that step is complete. Run it: make sure it doesn't work now. Build the code to make the step complete. Run the test: make sure it works now. Check the code to see if it looks clear enough. If not, refactor it. Run the test again to be sure everything still works. Repeat until the feature is done. This isn't the only way we can do things, but it's one of the best we know right now. One way or another, it's very helpful to have a lot of little tests supporting each larger acceptance test, because the little tests tell us which bit we got wrong, while the acceptance test just screams ""Something Broke!"" These tests also help us with larger refactorings. I suppose a perfect TDD programmer could do all her design improvement in the TDD cycle, but my experience is that sometimes I don't notice a design problem right away. When that happens, I may need to put in some extra refactoring effort the next time I'm working with that code. Just like when we develop a feature, our tests can help us be sure we haven't broken anything with our design change. Continuous Integration Scrum requires a running, thoroughly-tested, usable increment, containing the value of all the backlog items from previous Sprints: a complete, running, tested, operable program. Obviously, this requires that all the team's work must wind up integrated, tested, and working together by the end of the Sprint. Delayed integration uncovers problems, many of which require substantial debugging. The longer we wait, the worse it gets. Conversely, if we integrate after making small working changes, it becomes much easier. An automated build process, running the system's test suite, turns out to be an essential part of a Scrum team's development process. For most teams, the more frequently they build, the better things go. Building ""all the time"" seems to go best, which is why this practice is called Continuous Integration. Surviving Dark Scrum TL;DR There is a common, perhaps inevitable Dark Scrum stage in many Scrum installations; Dark Scrum situations follow a common pattern; The Dev Team can make things better for themselves, and can move Dark Scrum toward real Scrum, a better place to be. If you'd like to provide input, my email is open at ronjeffries at acm dot org .",en,74
171,1107,1464205754,CONTENT SHARED,-6980020268309524947,-1032019229384696495,3498647633704503701,,,,HTML,https://www.acquia.com/blog/cross-channel-user-experiences-drupal/16/05/2016/3294766,cross-channel user experiences with drupal,"Last year around this time, I wrote that The Big Reverse of Web would force a major re-architecture of the web to bring the right information, to the right person, at the right time, in the right context. I believe that conversational interfaces like Amazon Echo are further proof that the big reverse is happening. New user experience and distribution platforms only come along every 5-10 years, and when they do, they cause massive shifts in the web's underlying technology. The last big one was mobile, and the web industry adapted. Conversational interfaces could be the next user experience and distribution platform - just look at Amazon Echo (aka Alexa), Facebook's messenger or Microsoft's Conversation-as-a-Platform . Today, hardly anyone questions whether to build a mobile-optimized website. A decade from now, we might be saying the same thing about optimizing digital experiences for voice or chat commands. The convenience of a customer experience will be a critical key differentiator. As a result, no one will think twice about optimizing their websites for multiple interaction patterns, including conversational interfaces like voice and chat. Anyone will be able to deliver a continuous user experience across multiple channels, devices and interaction patterns. In some of these cross-channel experiences, users will never even look at a website. Conversational interfaces let users disintermediate the website by asking anything and getting instant, often personalized, results. To prototype this future, my team at Acquia built a fully functional demo based on Drupal 8 and recorded a video of it. In the demo video below , we show a sample supermarket chain called Gourmet Market. Gourmet Market wants their customers to not only shop online using their website, but also use Echo or push notifications to do business with them. We built an Alexa integration module to connect Alexa to the Gourmet Market site and to answer questions about sale items. For example, you can speak the command: ""Alexa, ask Gourmet Market what fruits are on sale today"". From there, Alexa would make a call to the Gourmet Market website, finding what is on sale in the specified category and pull only the needed information related to your ask. On the website's side, a store manager can tag certain items as ""on sale"", and Alexa's voice responses will automatically and instantly reflect those changes. The marketing manager needs no expertise in programming -- Alexa composes its response by talking to Drupal 8 using web service APIs. The demo video also shows how a site could deliver smart notifications. If you ask for an item that is not on sale, the Gourmet Market site can automatically notify you via text once the store manager tags it as ""On Sale"". From a technical point of view, we've had to teach Drupal how to respond to a voice command, otherwise known as a ""Skill"", coming into Alexa. Alexa Skills are fairly straightforward to create. First, you specify a list of ""Intents"", which are basically the commands you want users to run in a way very similar to Drupal's routes. From there, you specify a list of ""Utterances"", or sentences you want Echo to react to that map to the Intents. In the example of Gourmet Market above, the Intents would have a command called GetSaleItems . Once the command is executed, your Drupal site will receive a webhook callback on /alexa/callback with a payload of the command and any arguments. The Alexa module for Drupal 8 will validate that the request really came from Alexa, and fire a Drupal Event that allows any Drupal module to respond. It's exciting to think about how new user experiences and distribution platforms will change the way we build the web in the future . As I referenced in Drupalcon New Orleans keynote , the Drupal community needs to put some thought into how to design and build multichannel customer experiences. Voice assistance, chatbots or notifications are just one part of the greater equation. If you have any further thoughts on this topic, please share them in the comments.",en,74
172,2975,1484837692,CONTENT SHARED,3091351089612339864,3609194402293569455,-8817271067166899413,"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36",SP,BR,HTML,https://www.sitepoint.com/whats-the-best-programming-language-to-learn-in-2017/,what's the best programming language to learn in 2017?,"Many of you will reflect on your skill set and career choices as we embark on the new year. There are numerous sources of ""best language"" statistics, so that's where we'll begin ... Stack Overflow Developer Survey More than 56,000 developers in 173 countries completed the Stack Overflow Developer Survey during 2016. Here are the most-used technologies : The survey also asked what developers loved most : and what developers most dreaded: Perhaps more useful are the technologies developers are interested in learning: Stack Overflow Top Tech Stack Overflow also collated statistics for questions, answers and votes : PYPL Popularity The PYPL Popularity of Programming Languages Index uses data from Google Trends to determine how often language tutorials are searched online: TIOBE Index, January 2017 The TIOBE Programming Community Index rates languages using search engine results to provide a ranking percentage: The biggest riser during 2016 was Go , which leapt from nowhere to 2.3% (#13). Java fell 4.19%, but it remains almost double C's score. What Do Surveys Tell Us? Surprisingly little. Results are interesting but often contradictory, and data collection methods are limited: Search engine results can favor older, more problematic or more widespread languages. Few would expect VisualBasic to appear above JavaScript. Online surveys are limited to a specific audience. Stack Overflow is populated by reasonably knowledgeable developers who have encountered problems in popular languages and frameworks. Historical usage patterns do not necessarily indicate future trends. Node.js did not exist a decade ago. In the mid-1990s, Perl or C were the most viable options for server-side development. For example, all surveys rank Java higher than PHP. Java is often adopted for education and used to develop command line, desktop and native Android applications. Yet WordPress powers 27.3% of the web and is written in PHP. PHP is used on 82.4% of web servers compared to just 2.7% for Java. PHP was developed for the web and has a more widespread adoption on the platform. There's nothing wrong with Java but, if you want a web development career, PHP could serve better. Probably. Depending on where you live and work. And the industry you work in. And what you do. Surveys are flawed, so perhaps we can seek ... Other Developer Opinions I've been writing ""best language"" articles for several years and they attract numerous comments. Everyone has an opinion, and that's great. Yet everyone is wrong. No developer has experience in every language. Some will have good knowledge of several, but no one can offer an unbiased choice. Whatever language a developer chooses and uses daily becomes their preferred option. They will passionately defend that decision because, if they can't, they'd switch to something else. Other developers can offer lessons learned from their experiences. That is useful information, but you're unlikely to have identical aspirations. To flip this on its head, seek opinions from developers who've been forced to use a particular language or framework: the majority will hate that technology. Why trust someone else to make a decision for you? If we can't rely on surveys or the opinions of others, where does it lead? ... There's no ""Best Language"" If you learn to drive a car, that knowledge can be transferred to driving a bus, a truck or a tractor. Similarly, most computer languages implement input, output, variables, loops, conditions and functions. Learn the basics of any language and learning another becomes considerably easier. It's mostly a different syntax . You cannot choose the ""wrong"" language; all development knowledge is good knowledge. Perhaps picking COBOL for an iOS game isn't the best choice, but you'd quickly discover it was impractical and learn something about the language which was useful elsewhere. The hardest part of any learning process is making a start ... Are You Asking the Right Questions? Those with some programming experience know where they've been struggling. The gaps in their knowledge are more obvious: If you're spending too much time manually manipulating spreadsheet data, invest some effort in learning its macro language. If you've been developing a website and are unhappy with the layout, improving your CSS knowledge is an obvious next step. If you're developing a server application and need to store data, learning SQL or a NoSQL alternative is a logical option. Those asking ""what language should I learn?"" are probably new to the software industry. A comparably vague question would be ""what clothes should I wear?"" . No one can answer until they appreciate your age, gender, size, taste, preferences, country, local weather, customs, decency laws, where it will be worn, etc. It's impossible to suggest a language without knowing: whether you're genuinely interested programming what problems you want to solve what hardware and systems are available to you what time and learning opportunities you have, and all the variables associated with the factors above. No one wakes up and decides to embark on a professional development career without any programming experience. If you're genuinely interested in development, pick a small project, choose a language, dig out some tutorials and get going. A few places to start on SitePoint ... Then Keep Learning Despite stating that other developer opinions won't align with your situation, I will offer a morsel of advice to SitePoint's primary web development audience: If you're primarily a front-end developer, attempt back-end coding. Try PHP, Node.js, Ruby or whatever piques your interest, then add SQL to your skill set. If you're primarily a back-end developer, learn HTML, CSS and JavaScript. Browser APIs and data formats such as JSON are also beneficial. Frameworks don't count! Learn the basics of the language first. That knowledge will remain invaluable regardless of the ever-changing whims, opinions and tool sets used by the development community. You may not want to become a full-stack developer but, at the very least, it will help you appreciate the work of others and make a better contribution to your project. Best of luck. Stop procrastinating. Stop reading articles like this. Just start coding!",en,74
173,2888,1482157149,CONTENT SHARED,7065704533945771463,1895326251577378793,-5664675415604649356,"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.98 Safari/537.36",SP,BR,HTML,http://idgnow.com.br/internet/2016/12/19/nova-regra-do-banco-central-pode-significar-o-fim-do-nubank-entenda/,nova regra do banco central pode significar o fim do nubank; entenda - idg now!,"Uma das principais fintechs do Brasil, a Nubank pode fechar as portas caso o Banco Central confirme nesta terça-feira, 20/12, uma nova regra que mudaria de forma drástica o prazo de pagamento das empresas de cartão de crédito para os comerciantes no país. A medida afetaria todas as companhias do segmento, mas seria pior justamente para as menores, que não possuem o mesmo capital para financiamento como bancos gigantes. Em entrevista para a Agência Estado, a cofundadora do Nubank, Cristina Junqueira, criticou duramente a ideia do governo federal de reduzir de 30 dias para 2 dias o prazo para esse pagamento. Atualmente, os lojistas brasileiros levam 30 dias para receber das emissoras de cartão de crédito, prazo bem maior do que nos EUA, onde esse período é de apenas dois dias. ""Atualmente, um cliente que usa o cartão pagará a fatura, em média, 26 dias depois. Assim, o Nubank, como emissor, receberá o dinheiro apenas após este prazo. Com o dinheiro, pagamos o adquirente (operador do cartão), que leva mais dois ou três dias para pagar o varejista. Isso dá o prazo de 30 dias"", afirma Cristina, que descreve a possível mudança como ""apocalíptica"" para a fintech que já emitiu mais de 1 milhão de cartões no Brasil desde 2014. Caso a mudança do prazo, cuja intenção de redução foi oficializada recentemente pelo presidente Michel Temer, seja realmente colocada em prática, o Nubank teria de pagar a operadora do cartão antes mesmo de o cliente pagar a fatura. Segundo Cristina, já foram feitas algumas simulações internas e nem mesmo um prazo de 15 dias seria suficiente para a continuidade do Nubank, que se destaca no mercado por não cobrar taxas e permitir o gerenciamento do uso do cartão por meio de um app para smartphones. ""Nós já fizemos algumas simulações. Com dois dias é apagar a luz e fechar a porta. Com 15 dias, a gente precisaria de quase R$ 1 bilhão de capital adicional do dia para a noite"", decreta.",pt,74
174,2867,1481666562,CONTENT SHARED,6569463984699537820,3609194402293569455,5472228981666337194,Mozilla/5.0 (Windows NT 10.0; WOW64; rv:50.0) Gecko/20100101 Firefox/50.0,SP,BR,HTML,https://googlediscovery.com/2016/12/13/google-anuncia-android-things-plataforma-para-internet-das-coisas/,"google anuncia android things, plataforma para a internet das coisas | google discovery","O Google anunciou hoje o Android Things , sua nova plataforma baseada no Android que vem sendo desenvolvida para trabalhar com dispositivos conectados à internet (aka ""Internet das Coisas""). Além de contar com APIs do Android e serviços do próprio Google, Android Things utiliza grande parte dos esforços do Projeto Brillo , e as ferramentas de desenvolvedor do Android, como o Android Studio, Android SDK, Google Play Services e serviços de computação em nuvem do Google. ""Agora qualquer desenvolvedor Android pode rapidamente construir um dispositivo inteligente usando APIs do Android e serviços do Google, enquanto permanece altamente seguro com as atualizações direto do Google"", publicou o buscador. A gigante de Mountain View também anunciou um update da plataforma Weave que permite aos dispositivos se conectarem à nuvem e interagir com serviços como o Google Assistant. ""Fabricantes de aparelhos como a Philips Hue e Samsung SmartThings já usam Weave, e vários outros como Belkin Wemo, LiFX, Honeywell, Wink, TP-Link e First Alert estão implementando a tecnologia"". publicou Wayne Piekarski , Developer Advocate.",pt,73
175,1593,1467291890,CONTENT SHARED,-2828628091730682303,7645894863578715801,-6062980417313435691,,,,HTML,http://martinfowler.com/articles/serverless.html,serverless architectures,"Mike Roberts Mike is an engineering leader living in New York City. While spending much of his time these days managing people and teams he also still manages to code, especially in Clojure, and has Opinions about software architecture. He is cautiously optimistic that Serverless architectures may be worth some of the hype they are current receiving. Serverless is a hot topic in the software architecture world. We're already seeing books , open source frameworks , plenty of vendor products , and even a conference dedicated to the subject. But what is Serverless and why is (or isn't) it worth considering? Through this evolving publication I hope to enlighten you a little on these questions. A couple of examples UI-driven applications Let's think about a traditional 3-tier client-oriented system with server-side logic. A good example is a typical ecommerce app (dare I say an online pet store?) Traditionally the architecture will look something like this, and let's say it's implemented in Java on the server side, with a HTML / Javascript component as the client: With this architecture the client can be relatively unintelligent, with much of the logic in the system - authentication, page navigation, searching, transactions - implemented by the server application. With a Serverless architecture this may end up looking more like this: This is a massively simplified view, but even with this there are a number of significant changes that have happened here. Please note this is not a recommendation of an architectural migration, I'm merely using this as a tool to expose some Serverless concepts! We've deleted the authentication logic in the original application and have replaced it with a third party BaaS service. Using another example of BaaS, we've allowed the client direct access to a subset of our database (for product listings), which itself is fully 3rd party hosted (e.g. AWS Dynamo.) We likely have a different security profile for the client accessing the database in this way from any server resources that may access the database. These previous two points imply a very important third - some logic that was in the Pet Store server is now within the client, e.g. keeping track of a user session, understanding the UX structure of the application (e.g. page navigation), reading from a database and translating that into a usable view, etc. The client is in fact well on its way to becoming a Single Page Application . Some UX related functionality we may want to keep in the server, e.g. if it's compute intensive or requires access to significant amounts of data. An example here is 'search'. For the search feature instead of having an always-running server we can implement a FaaS function that responds to http requests via an API Gateway (described later.) We can have both the client, and the server function, read from the same database for product data. Since the original server was implemented in Java, and AWS Lambda (our FaaS vendor of choice in this instance) supports functions implemented in Java, we can port the search code from the Pet Store server to the Pet Store Search function without a complete re-write. Finally we may replace our 'purchase' functionality with another FaaS function, choosing to keep it on the the server-side for security reasons, rather than re-implement it in the client. It too is fronted by API Gateway. Message-driven applications A different example is a backend data-processing service. Say you're writing a user-centric application that needs to quickly respond to UI requests, but secondarily you want to capture all the different types of activity that are occurring. Let's think about an online ad system - when a user clicks on an advertisement you want to very quickly redirect them to the target of the ad, but at the same time you need to collect the fact that the click has happened so that you can charge the advertiser. Traditionally, the architecture may look like this. The 'Ad Server' synchronously responds to the user - we don't care about that interaction for the sake of this example - but it also posts a message to a channel that can be asynchronously processed by a 'click processor' application that updates a database, e.g. to decrement the advertiser's budget. In the Serverless world this looks like: There's a much smaller difference to the architecture here compared to our first example. We've replaced a long lived consumer application with a FaaS function that runs within the event driven context the vendor provides us. Note that the vendor supplies both the Message Broker and the FaaS environment - the two systems are closely tied to each other. The FaaS environment may also process several clicks in parallel by instantiating multiple copies of the function code - depending on how we'd written the original process this may be a new concept we need to consider. Unpacking 'Function as a Service' We've mentioned the FaaS idea a lot already but it's time to dig into what it really means. To do this let's look at the opening description for Amazon's Lambda product. I've added some tokens to it, which I then expand upon. AWS Lambda lets you run code without provisioning or managing servers. (1) ... With Lambda, you can run code for virtually any type of application or backend service (2) - all with zero administration. Just upload your code and Lambda takes care of everything required to run (3) and scale (4) your code with high availability. You can set up your code to automatically trigger from other AWS services (5) or call it directly from any web or mobile app (6) . Fundamentally FaaS is about running back end code without managing your own server systems or your own server applications. That second clause - server applications - is a key difference when comparing with other modern architectural trends like containers and PaaS (Platform as a Service.) If we go back to our click processing example from earlier what FaaS does is replace the click processing server (possibly a physical machine, but definitely a specific application) with something that doesn't need a provisioned server, nor an application that is running all the time. FaaS offerings do not require coding to a specific framework or library. FaaS functions are regular applications when it comes to language and environment. For instance AWS Lambda functions can be implemented 'first class' in Javascript, Python and any JVM language (Java, Clojure, Scala, etc.). However your Lambda function can execute another process that is bundled with its deployment artifact, so you can actually use any language that can compile down to a Unix process (see Apex later on.) FaaS functions do have significant architectural restrictions, especially when it comes to state and execution duration, and we'll come to that soon. Let's consider our click processing example again - the only code that needs to change when moving to FaaS is the 'main method / startup' code, in that it is deleted, and likely the specific code that is the top-level message handler (the 'message listener interface' implementation), but this might only be a change in method signature. All of the rest of the code (e.g. the code that writes to the database) is no different in a FaaS world. Since we have no server applications to run deployment is very different to traditional systems - we upload the code to the FaaS provider and it does everything else. Right now that typically means uploading a new definition of the code (e.g. in a zip or JAR file), and then calling a proprietary API to initiate the update. Horizontal scaling is completely automatic, elastic, and managed by the provider. If your system needs to be processing 100 requests in parallel the provider will handle that without any extra configuration on your part. The 'compute containers' executing your functions are ephemeral with the FaaS provider provisioning and destroying them purely driven by runtime need. Let's return to our click processor. Say that we were having a good day and customers were clicking on 10 times as many ads as usual. Would our click processing application be able to handle this? For example did we code to be able to handle multiple messages at a time? Even if we did would one running instance of the application be enough to process the load? If we are able to run multiple processes is auto-scaling automatic or do we need to reconfigure that manually? With FaaS you need to write the function ahead of time to assume parallelism, but from that point on the FaaS provider automatically handles all scaling needs. Functions in FaaS are triggered by event types defined by the provider. With Amazon AWS such stimuli include S3 (file) updates, time (scheduled tasks) and messages added to a message bus (e.g. Kinesis ). Your function will typically have to provide parameters specific to the event source it is tied to. With the click processor we made an assumption that we were already using a FaaS-supported message broker. If not we would have needed to switch to one, and that would have required making changes to the message producer too. Most providers also allow functions to be triggered as a response to inbound http requests, typically in some kind of API gateway. (e.g. AWS API Gateway , Webtask ) . We used this in our Pet Store example for our 'search' and 'purchase' functions. State FaaS functions have significant restrictions when it comes to local (machine / instance bound) state. In short you should assume that for any given invocation of a function none of the in-process or host state that you create will be available to any subsequent invocation. This includes state in RAM and state you may write to local disk. In other words from a deployment-unit point of view FaaS functions are stateless . This has a huge impact on application architecture, albeit not a unique one - the 'Twelve-Factor App' concept has precisely the same restriction . Given this restriction what are alternatives? Typically it means that FaaS functions are either naturally stateless - i.e. they provide pure functional transformations of their input - or that they make use of a database, a cross-application cache (e.g. Redis), or network file store (e.g. S3) to store state across requests or for further input to handle a request. Execution Duration FaaS functions are typically limited in how long each invocation is allowed to run. At present AWS Lambda functions are not allowed to run for longer than 5 minutes and if they do they will be terminated. This means that certain classes of long lived task are not suited to FaaS functions without re-architecture, e.g. you may need to create several different coordinated FaaS functions where in a traditional environment you may have one long duration task performing both coordination and execution. Startup Latency At present how long it takes your FaaS function to respond to a request depends on a large number of factors, and may be anywhere from 10ms to 2 minutes. That sounds bad, but let's get a little more specific, using AWS Lambda as an example. If your function is implemented in Javascript or Python and isn't huge (i.e. less than a thousand lines of code) then the overhead of running it in should never be more than 10 - 100 ms. Bigger functions may occasionally see longer times. If your Lambda function is implemented on the JVM you may occasionally see long response times (e.g. > 10 seconds) while the JVM is spun up. However this only notably happens with either of the following scenarios: Your function processes events infrequently, on the order of longer than 10 minutes between invocations. You have very sudden spikes in traffic, for instance you typically process 10 requests per second but this ramps up to 100 requests per second in less than 10 seconds. The former of these may be avoided in certain situations by the ugly hack of pinging your function every 5 minutes to keep it alive. Are these issues a concern? It depends on the style and traffic shape of your application. My former team has an asynchronous message-processing Lambda app implemented in Java which processes hundreds of millions of messages / day, and they have no concerns with startup latencey. That said if you were writing a low-latency trading application you probably wouldn't want to use FaaS systems at this time, no matter the language you were using for implementation. Whether or not you think your app may have problems like this you should test with production-like load to see what performance you see. If your use case doesn't work now you may want to try again in a few months time since this is a major area of development by FaaS vendors. API Gateway One aspect of FaaS that we brushed upon earlier is an 'API Gateway'. An API Gateway is an HTTP server where routes / endpoints are defined in configuration and each route is associated with a FaaS function. When an API Gateway receives a request it finds the routing configuration matching the request and then calls the relevant FaaS function. Typically the API Gateway will allow mapping from http request parameters to inputs arguments for the FaaS function. The API Gateway transforms the result of the FaaS function call to an http response, and returns this to the original caller. Amazon Web Services have their own API Gateway and other vendors offer similar abilities. Beyond purely routing requests API Gateways may also perform authentication, input validation, response code mapping, etc. Your spidey-sense may be buzzing about whether this is actually such a good idea, if so hold that thought - we'll consider this further later. One use case for API Gateway + FaaS is for creating http-fronted microservices in a Serverless way with all the scaling, management and other benefits that come from FaaS functions. At present tooling for API gateways is achingly immature and so while defining applications with API gateways is possible it's most definitely not for the faint-hearted. Tooling The comment above about API Gateway tooling being immature actually applies, on the whole, to Serverless FaaS in general. There are exceptions however - one example is Auth0 Webtask which places significant priority on Developer UX in its tooling. Tomasz Janczuk gave a very good demonstration of this at the recent Serverless Conference. Debugging and monitoring are tricky in general in Serverless apps - we'll get into this further in subsequent installments of this article. Open Source One of the main benefits of Serverless FaaS applications is transparent production runtime provisioning, and so open source is not currently as relevant in this world as it is for, say, Docker and containers. In future we may see a popular FaaS / API Gateway platform implementation that will run 'on premise' or on a developer workstation. IBM's OpenWhisk is an example of such an implementation and it will be interesting to see whether this, or an alternative implementation, picks up adoption. Apart from runtime implementation though there are already open source tools and frameworks to help with definition, deployment and runtime assistance. For instance the Serverless Framework makes working with API Gateway + Lambda significantly easier than using the first principles provided by AWS. It's Javascript heavy but if you're writing JS API Gateway apps it's definitely worth a look. Another example is Apex - a project to 'Build, deploy, and manage AWS Lambda functions with ease.' One particularly interesting aspect of Apex is that it allows you to develop Lambda functions in languages other than those directly supported by Amazon, e.g. Go. What isn't Serverless? So far in this article I've defined 'Serverless' to mean the union of a couple of other ideas - 'Backend as a Service' and 'Functions as a Service'. I've also dug into the capabilities of the second of these. Before we start looking at the very important area of benefits and drawbacks I'd like to spend one more moment on definition, or at least defining what Serverless isn't. I've seen some people (including me in the recent past) get confused about these things and I think it's worth discussing them for clarity's sake. Comparison with PaaS Given that Serverless FaaS functions are very similar to 12-Factor applications, are they in fact just another form of 'Platform as a Service' (PaaS) like Heroku ? For a brief answer I refer to Adrian Cockcroft If your PaaS can efficiently start instances in 20ms that run for half a second, then call it serverless. -- Adrian Cockcroft In other words most PaaS applications are not geared towards bringing entire applications up and down for every request, whereas FaaS platforms do exactly this. OK, but so what, if I'm being a good 12-Factor App developer there's still no difference to how I code? That's true, but there is a big difference to how you operate your app. Since we're all good DevOps-savvy engineers we're thinking about operations as much as we are about development, right? The key operational difference between FaaS and PaaS is scaling . With most PaaS's you still need to think about scale, e.g. with Heroku how many Dynos you want to run. With a FaaS application this is completely transparent. Even if you setup your PaaS application to auto-scale you won't be doing this to the level of individual requests (unless you have a very specifically shaped traffic profile), and so a FaaS application is much more efficient when it comes to costs. Given this benenfit, why would you still use a PaaS? There are several reasons but tooling, and maturity of API gateways, are probably the biggest. Furthermore 12-Factor Apps implemented in a PaaS may use an in-app readonly cache for optimization, which isn't an option for FaaS functions. #NoOps Serverless doesn't mean 'No Ops'. It might mean 'No internal Sys Admin' depending on how far down the serverless rabbit hole you go. There are 2 important things to consider here. Firstly 'Ops' means a lot more than server administration. It also means at least monitoring, deployment, security, networking and often also means some amount of production debugging and system scaling. These problems all still exist with Serverless apps and you're still going to need a strategy to deal with them. In some ways Ops is harder in a Serverless world because a lot of this is so new. Second even the Sys Admin is still happening - you're just outsourcing it with Serverless. That's not necessarily a bad thing - we outsource a lot. But depending on what precisely you're trying to do this might be a good or a bad thing, and either way at some point the abstraction will likely leak and you'll need to know that human sys admins somewhere are supporting your application. Charity Majors gave a great talk on this subject at the recent Serverless Conference and I recommend checking it out once it's available online. Until then you can read her write-up here and here . Stored Procedures as a Service Another theme I've seen is that Serverless FaaS is 'Stored Procedures as a Service'. I think that's come from the fact that many examples of FaaS functions (including some I've used in this article) are small pieces of code that wrap access to a database. If that's all we could use FaaS for I think the name would be useful but because it is really just a subset of FaaS's capability then thinking of FaaS in such a way is an invalid constraint. That being said it is worth considering whether FaaS comes with some of the same problems of stored procedures, including the technical debt concern Camille mentions in the referenced tweet. There are many lessons that come from using stored procs that are worth reviewing in the context of FaaS and seeing whether they apply. Some of these are that stored procedures: Often require vendor-specific language, or at least vendor-specific frameworks / extensions to a language Are hard to test since they need to be executed in the context of a database Are tricky to version control / treat as a first class application Note that not all of these may apply to all implementations of stored procs, but they're certainly problems that I've come across in my time. Let's see if they might apply to FaaS: (1) is definitely not a concern for the FaaS implementations I've seen so far, so we can scrub that one off the list right away. For (2) since we're dealing with 'just code' unit testing is definitely just as easy as any other code. Integration testing is a different (and legitimate) question though which we'll discuss later. For (3), again since FaaS functions are 'just code' version control is OK. But as to application packaging there are no mature patterns on this yet. The Serverless framework which I mentioned earlier does provide its own form of this, and AWS announced at the recent Serverless Conference in May 2016 that they are working on something for packaging also ('Flourish'), but for now this is another legitimate concern. This is an evolving publication, and I shall be extending it over the coming days and weeks to cover more topics on serverless architecture including the benefits and drawbacks of this approach, and where we might see serverless evolve over the next year or two. To find out when we publish these installments, keep an eye on the site's RSS feed , my twitter feed , or Martin's twitter feed.",en,73
176,2212,1472434664,CONTENT SHARED,1060159005880386235,-8132559109129514792,-8668326016989888923,,,,HTML,http://blogs.wsj.com/cio/2015/05/26/internet-of-things-drives-outcome-based-business-models/,internet of things drives outcome-based business models,"By placing intelligent hardware where digital and physical worlds intersect, business leaders and CIOs can gain quantifiable, end-to-end insights into the outcomes their customers are trying to achieve, and use those insights to differentiate themselves competitively and to enter new markets, Guest Contributor Paul Daugherty writes.",en,73
177,2861,1481626644,CONTENT SHARED,-1878128207048892154,-2979537012405607453,-2564952476915635344,"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.98 Safari/537.36",MG,BR,HTML,https://blog.codinghorror.com/the-ten-commandments-of-egoless-programming/,the ten commandments of egoless programming,"The Ten Commandments of Egoless Programming, as originally established in Jerry Weinberg's book The Psychology of Computer Programming : No matter how much ""karate"" you know, someone else will always know more. Such an individual can teach you some new moves if you ask. Seek and accept input from others, especially when you think it's not needed. Critique code instead of people - be kind to the coder, not to the code. As much as possible, make all of your comments positive and oriented to improving the code. Relate comments to local standards, program specs, increased performance, etc. The human principles of software are truly timeless; The Psychology of Computer Programming was written way back in 1971, a year after I was born!",en,73
178,1192,1464813325,CONTENT SHARED,-3161714324304758767,-1479311724257856983,676862310426775888,,,,HTML,http://news.mit.edu/2015/csail-deep-learning-algorithm-predicts-photo-memorability-near-human-levels-1215,"deep-learning algorithm predicts photos' memorability at ""near-human"" levels","Researchers from MIT's Computer Science and Artificial Intelligence Laboratory (CSAIL) have created an algorithm that can predict how memorable or forgettable an image is almost as accurately as humans - and they plan to turn it into an app that subtly tweaks photos to make them more memorable. For each photo, the ""MemNet"" algorithm - which you can try out online by uploading your own photos - also creates a heat map that identifies exactly which parts of the image are most memorable. ""Understanding memorability can help us make systems to capture the most important information, or, conversely, to store information that humans will most likely forget,"" says CSAIL graduate student Aditya Khosla, who was lead author on a related paper. ""It's like having an instant focus group that tells you how likely it is that someone will remember a visual message."" Team members picture a variety of potential applications, from improving the content of ads and social media posts, to developing more effective teaching resources, to creating your own personal ""health-assistant"" device to help you remember things. Part of the project the team has also published the world's largest image-memorability dataset, LaMem. With 60,000 images, each annotated with detailed metadata about qualities such as popularity and emotional impact, LaMem is the team's effort to spur further research on what they say has often been an under-studied topic in computer vision. The paper was co-written by CSAIL graduate student Akhil Raju, Professor Antonio Torralba, and principal research scientist Aude Oliva, who serves as senior investigator of the work. Khosla will present the paper in Chile this week at the International Conference on Computer Vision. How it works The team previously developed a similar algorithm for facial memorability . What's notable about the new one, besides the fact that it can now perform at near-human levels, is that it uses techniques from ""deep-learning,"" a field of artificial intelligence that use systems called ""neural networks"" to teach computers to sift through massive amounts of data to find patterns all on their own. Such techniques are what drive Apple's Siri, Google's auto-complete, and Facebook's photo-tagging, and what have spurred these tech giants to spend hundreds of millions of dollars on deep-learning startups. ""While deep-learning has propelled much progress in object recognition and scene understanding, predicting human memory has often been viewed as a higher-level cognitive process that computer scientists will never be able to tackle,"" Oliva says. ""Well, we can, and we did!"" Neural networks work to correlate data without any human guidance on what the underlying causes or correlations might be. They are organized in layers of processing units that each perform random computations on the data in succession. As the network receives more data, it readjusts to produce more accurate predictions. The team fed its algorithm tens of thousands of images from several different datasets, including LaMem and the scene-oriented SUN and Places (all of which were developed at CSAIL). The images had each received a ""memorability score"" based on the ability of human subjects to remember them in online experiments. The team then pitted its algorithm against human subjects by having the model predicting how memorable a group of people would find a new never-before-seen image. It performed 30 percent better than existing algorithms and was within a few percentage points of the average human performance. For each image, the algorithm produces a heat map showing which parts of the image are most memorable. By emphasizing different regions, they can potentially increase the image's memorability. ""CSAIL researchers have done such manipulations with faces, but I'm impressed that they have been able to extend it to generic images,"" says Alexei Efros , an associate professor of computer science at the University of California at Berkeley. ""While you can somewhat easily change the appearance of a face by, say, making it more 'smiley,' it is significantly harder to generalize about all image types."" Looking ahead The research also unexpectedly shed light on the nature of human memory. Khosla says he had wondered whether human subjects would remember everything if they were shown only the most memorable images. ""You might expect that people will acclimate and forget as many things as they did before, but our research suggests otherwise,"" he says. ""This means that we could potentially improve people's memory if we present them with memorable images."" The team next plans to try to update the system to be able to predict the memory of a specific person, as well as to better tailor it for individual ""expert industries"" such as retail clothing and logo design. ""This sort of research gives us a better understanding of the visual information that people pay attention to,"" Efros says. ""For marketers, movie-makers and other content creators, being able to model your mental state as you look at something is an exciting new direction to explore."" The work is supported by grants from the National Science Foundation, as well as the McGovern Institute Neurotechnology Program, the MIT Big Data Initiative at CSAIL, research awards from Google and Xerox, and a hardware donation from Nvidia.",en,71
179,2025,1470832760,CONTENT SHARED,-1573329182923097618,7774613525190730745,6403122829531428804,,,,HTML,http://searchsecurity.techtarget.com/feature/DevOps-security-requires-new-mindset-and-tools-for-visibility-automation,"devops security requires new mindset and tools for visibility, automation","In 2011, Intuit Inc. embarked on an effort to completely revamp its QuickBooks Online accounting service for small businesses with better integration of payroll, customer relationship management and third-party services. With its shift toward software as a service, Intuit decided to transform its desktop production model -- development, QA and IT operations -- so developers could quickly prototype incremental functionality to address customers' demands. As part of the effort, which culminated in the announcement of a completely new service in September 2013, the financial software company focused on responsive development and continuous rollout. In other words, they needed DevOps. Consumer awareness of massive data breaches started around the same time period. Online attackers scooped up at least 60 million email addresses belonging to Fortune 100 clients of marketing firm Epsilon Data Management in 2011. The same year, hackers caused disruptions in Sony's PlayStation Network for more than three weeks and made off with credentials and information for 77 million PSN and Qriocity accounts. And the colossal Target breach, in which attackers stole 110 million records, capped off 2013. With the introduction of CRM features and payroll information, it's little wonder, then, that Intuit put a major focus on DevOps security. There's no recipe book for integrating security into DevOps' continuous integration and continuous delivery (CI/CD) cycle, says Shannon Lietz, senior manager for cloud security engineering at Intuit. With 3,000 developers, Intuit faced enormous problems because the security and agile DevOps teams continued to lead separate existences. ""We realized that the DevOps teams were throwing [responsibility] over the wall to security, and we [security] had all the information; we knew all the attacks that were coming in, and the DevOps people...did not have the information to make the decisions,"" she says. ""And we said, 'Alright, we have to figure out how to fix this problem.' It was a head-banging experience."" Fast-tracking DevOps security Lietz is not alone. Other major firms that created cloud services, or moved their services to the cloud, dealt with similar issues. Video-streaming service Netflix and Etsy, a marketplace for crafters, are well-known proponents of DevOps security. Both companies also pioneered many of the efforts to secure Agile software development . ""There were a million lessons to be learned along the way and some pretty spectacular failures,"" says Zane Lackey, a former director of security engineering for Etsy, who left and founded Signal Sciences in 2014 with Etsy colleagues Andrew Peterson and Nick Galbreath. With the online craft marketplace pushing out 50 deploys in a day, Etsy faced several challenges in fast-tracking DevOps security testing and providing visibility to developers. ""How do we get coverage and visibility over the applications and APIs that we are trying to defend?"" Lackey says. ""And how can we couple that up with the ability to really block attacks against apps that can really scale and be used in production?"" Phoenix rises Developers have always pursued faster development methods. With the publication of the Manifesto for Agile Software Development in 2001, and the move from software to cloud services, the practice of developers working more closely with IT operations to automate more of their workflow has taken off. ""DevOps is different,"" says Rich Mogull, CEO of security consultancy Securosis. ""It is as much of a philosophy as an approach. DevOps is about extending the concepts of an assembly line to figure how we get code into the hands of customers."" The DevOps concept -- popularized in The Phoenix Project (IT Revolution Press, 2013), a novel authored by DevOps proponents Gene Kim, George Spafford and Kevin Behr -- has gone from niche to mainstream in the past decade. According to RightScale's ""2016 State of the Cloud Report : DevOps Trends,"" 74% of the 1,060 IT professionals surveyed worldwide indicated their organizations are adopting DevOps for software development, up from 66% in 2015. However, it is not companywide -- 60% of DevOps adoption is by specific business units or project teams. The fundamental problem? While agile DevOps pushes small batches of code out in days -- and in some cases hours -- security teams work on projects that take weeks and months. To make DevOps more secure, security teams need to have ongoing collaboration with DevOps teams, providing coders with visibility and feedback and automating security checks as early in the development process as possible. Can security be automated into a set of actions like DevOps? Whatever you call it -- DevOps security, DevOpsSec or Rugged DevOps , which relies on peer-pressure tactics to make developers feel good or bad about their code -- here are four ways to tightly integrate security throughout the CI/CD cycle: 1. Don't be a gatekeeper When Lackey joined Etsy in 2011, the company already had a strong focus on the continuous deployment model, in which any code that passed the automated testing phase was released into production. The question was how to create a security organization that could really enable the company to quickly push out product features, but in a secure way. ""For security, that feels very scary at first because we have always operated as a gatekeeper,"" Lackey says. ""But the problem is that gatekeeper mentality means, in this modern world, becoming a blocker. Security cannot be the blocker that slows down the business because the developers will route around you."" Training security to not say ""no"" all the time is only part of the answer. Historically, security has focused on longer projects -- static analysis that required days to run, analyze and build a remediation list -- and that slow feedback loop blocks development in its own ways. ""In the old SDLC [secure development lifecycle] model, we built a lot of security controls that were very heavyweight and very top-down,"" Lackey says. ""And they required a lot of investment to tune and work correctly; and in the end, we would get a report once a month or once a week."" Getting beyond that requires a commitment from the security team to work on DevOps security with developers. Etsy, for example, reused the antivirus toolkit in its continuous integration environment to test source code. 2. Give the developer visibility Developers want to quickly push out code. When security teams return scan results that show flaws from code checked weeks ago, it requires developers to slow down or, often, to ignore the results. Instead, security teams should give developers quick visibility into the vulnerabilities in their code. Such visibility helps to tighten the feedback loop, says Joshua Corman, former CTO at Sonatype and co-founder of Rugged Software. (Corman is now the director of the Cyber Statecraft Initiative for the Atlantic Council, which works on international conflict and cooperation.) Twitter, for example, created a tool called Brakeman to quickly scan submitted code for vulnerabilities and immediately notify the developers. A report, originally in email, lets the developers know when they checked in vulnerable code and how to fix it. ""They found that tightening the feedback loop helped the developers not hate security; it was integrated into their development process,"" Corman says. ""One of the big problems with application security is that there is a time lag between when a bug is inserted and when it is detected."" Providing quick feedback at code check-in is extremely important. Intuit, for example, uses a golden master for its Amazon Machine Image (AMI) infrastructure, and whenever a developer pushes code and creates a new image, their scanners return a letter grade for the security of the image. At Etsy, Lackey and his team built tools that would deliver security results directly to the developer's queue of defects. That way, security was not a separate process, but part of the development cycle (See ""The tools they use""). ""We can't just say that security is everyone's responsibility and then not give them visibility or tools for security,"" he says. ""The way in which you actually deliver on that as a security team is by bringing visibility to everyone."" 3. Automate security checks and run them all the time Developing as part of the DevOps CI/CD model means that security has to be as close to continuous as possible. Netflix, one of the pioneers of CI/CD development, created a collection of programs that would frequently test its systems to make sure they had no vulnerabilities, were configured correctly and complied with company policy. Each program was called a monkey , and the automation suite became Netflix's Simian Army . Using monkeys -- or automation in general -- a company can assess that accounts are configured correctly, that the most secure and up-to-date version of third-party software is used and that new code is detected, analyzed and vetted through an automated red team process, says Adrian Cockcroft, former cloud architect and director of web engineering at Netflix. ""You need tooling that will break a build if the developer is violating any of these policies,"" he says. ""Once you have built something that you think is good, then it's posted to the testing environment and is automatically checked."" While basing your infrastructure in the cloud is not mandatory, the service-oriented nature of the cloud -- with its APIs -- makes accessing the code and infrastructure much easier for security teams. A significant part of securing DevOps is using that infrastructure to allow programs to automatically check code and the infrastructure. The service-oriented architecture of the cloud means that applications tend to be less monolithic and, as a result, have smaller codebases. Programs with a million lines of code are not as common anymore. ""Because the size of the codebase has been considerably reduced, the number of bugs has been reduced as well,"" says Meera Subbarao, director of the secure development practice at Cigital. ""However, security has not changed. We still see the bugs in these programs with 200,000 lines of code."" Cigital in May released The Agile Security Manifesto , which suggests four principles to build security into the Agile process. BSIMM 6 -- the Building Security In Maturity Model co-authored by the company's CTO Gary McGraw, a former columnist for this magazine -- was released in October. 4. Don't punish mistakes or blame the developer When developers move quickly, mistakes are made. And often the security team is the group that finds the errors. Blaming developers results in negative feedback and worsens relations between developers and security teams, says Lackey. Moreover, developers are not the only ones who will be making mistakes. DevOps security is such a new discipline that most security teams are learning by doing, he adds. ""I certainly made plenty of mistakes along the way,"" Lackey says. ""There were not a lot of other people doing it at the time, but that is how we learned to experiment."" In the end, security groups need to remember that any strategy to secure the agile DevOps cycle that attempts to change how developers work or interferes with their goals will likely fail, says Securosis' Mogull. ""Security needs to learn the developers' language and understand their requirements,"" he says. ""If they do not ship on time, they are in trouble; and for them, that's what matters."" About the author: Robert Lemos is an award-winning technology journalist who has reported on computer security and cybercrime for 20 years. He currently writes for several publications focused on information security issues.",en,71
180,257,1460038639,CONTENT SHARED,374352050712569304,1895326251577378793,-3394884316493790889,,,,HTML,http://arquiteturadeinformacao.com/user-experience/o-tenue-equilibrio-da-equipe-que-desenha-produtos-digitais/,o tênue equilíbrio da equipe que desenha produtos digitais,"É sempre fascinante escutar os relatos com riqueza de detalhes que as pessoas contam sobre a concepção, desenvolvimento e lançamento de um novo produto ou serviço, seja em uma grande empresa ou pequena de amigos. Todavia, existe uma questão intrínseca na concepção de produtos, que é vital para o sucesso, mas que para algumas pessoas, passa despercebida quando a solução já está no mercado. Podemos chamar esta questão de ""equilíbrio dos elementos"" de uma equipe de produto. Hoje, muitas empresas de diferentes portes trabalham com equipes multidisciplinares e isso é cada vez mais comum em todos os nichos de mercado, principalmente porque diferentes profissionais com experiências distintas, apresentam maior probabilidade de alcançar o máximo de inovação em soluções para os projetos. Entretanto, alinhar uma equipe em prol de um objetivo universal e com responsabilidades claras é um desafio permanente pelo qual todos que trabalham com produtos ou serviços, seja digital ou físico, já passaram ou vão passar algum dia. O que torna crítico o entendimento dos papéis envolvidos nesse processo, suas armadilhas e as idiossincrasias de cada área. Partindo da ótica de equipes de produtos digitais, habitualmente temos três grupos chaves que trabalham em conjunto: negócios, design e desenvolvimento. Se você trabalha no ramo há algum tempo já deve ter visto variações dos termos, ao invés de negócios, product management ou ao invés de design, setor de criação e assim por diante. Esses três elementos formam o núcleo responsável por boa parte dos produtos que existem hoje no mercado de varejo digital, desde um website à uma aplicação multiplataforma. Alguns exemplos de nomes que as empresas costumam utilizar: Em toda equipe, obviamente, existe algum tipo de conflito seja ela multidisciplinar ou não, mas aqui exploraremos os mais comuns em cada um dos núcleos. Vamos notar que boa parte nasce do pouco entrosamento das equipes. Uma raiz comum de conflitos é falta de entendimento do que é ""trabalho em equipe"". Considerando a educação formal o ponto de partida para o trabalho que exercemos hoje, infelizmente boa parte de nós saiu de escolas e/ou faculdades fazendo trabalhos em grupo, sem o foco no que realmente importa, a colaboração e sintonia entre as pessoas, que é a essência do trabalho em equipe. Podemos dizer inclusive, que os trabalhos em equipe mais essenciais são executados por times esportivos ou integrantes de uma força militar, porque em ambos os exemplos, se o time não estiver em sintonia dificilmente ganhará a partida ou a batalha. Pode-se pensar também que a cultura de certas empresas espera que seu comportamento seja ""competitivo"" de acordo com as metas estabelecidas aos colaboradores, mas não se pode esquecer que a cultura é feita de pessoas e essas podem trazer o seu melhor, com o engajamento certo. Listo aqui, as principais situações que pude vivenciar e/ou presenciar em equipes de desenvolvimento de produtos. Esses exemplos apresentam nuances para cada time ou empresa. Infelizmente os desequilíbrios se repetem, contribuindo para a saturação dos três elementos principais e em alguns casos extremos podem até gerar a dissolução de equipes. Desequílibrio em Negócios Negócios ou Gerência é naturalmente a ponta com maior força, não só pelas atribuições de liderança ou planejamento do produto, mas porque é onde são armazenados e interpretados os números mais relevantes do produto para a companhia: vendas! Os números de vendas ou dados relevantes não são comunicados periodicamente - parece óbvio deixar a equipe na mesma sintonia, sempre respeitando o devido sigilo a cada tipo de produto, mas alguns tipos de gerenciamento de produto preferem manter a discrição dos dados, essa preferência é um equívoco, principalmente quando os demais membros da equipe só tomam conhecimento dos números em uma reunião de urgência. Esse tipo de ""informação estratégica"" revelada em cima da hora, pode ser motivo de muito estresse para a equipe. Não são definidos os principais índices de performance do produto (KPIs) - é interessante frisar esse fator, pois os índices devem existir em qualquer planejamento de produto, portanto se ele não existe é preciso rever como o projeto está sendo gerenciado, pois impactará o que se espera que a equipe entregue. O planejamento/roadmap do produto vem pronto - talvez esse seja o fator mais contraditório, nos atuais tempos de implantação em massa de . O ato de conceber ou planejar um produto sem ajuda da equipe que vai desenvolvê-lo é algo muito arriscado, pois a capacidade técnica e o tempo dos envolvidos influencia na qualidade e nos prazos de entrega das soluções prometidas. Desequílibrio em Desenvolvimento A área de Desenvolvimento já foi considerada o coração e a mente de empresas de tecnologia de renome. No entanto, a tecnologia é o elemento que tem mais interdependência dos demais elementos da equipe, pois é uma área que requer um direcionamento ou objetivo ligado às necessidades das pessoas e isso é algo que dificilmente nasce da própria tecnologia em si. O produto é projetado apenas com as tecnologias vigentes - de acordo com o desafio proposto pelo produto, é necessário testar novas soluções tecnológicas ou a mescla de linguagens de programação, mas algumas equipes técnicas dificultam esse processo, pois isso requer mais horas de estudos, mudança de infra-estrutura ou planejamento para implementação em larga escala e diversas correções de bugs. As interfaces são baseadas em máquina-máquina e não homem-máquina - algumas companhias, para agilizar seu processo produtivo, utilizam frameworks ou configurações padrões de software, porém algumas vezes esses ""comportamentos"" não atendem de forma assertiva os usuários daquele produto. Padrões agilizam o desenvolvimento, mas para se ter certeza de como as pessoas interagem não existe atalho, só testando com potenciais ou reais usuários do produto. A tecnologia dita o ritmo de todas as etapas do produto - é sabido que a tecnologia dita o ritmo hoje das inovações, mas nem sempre seus potenciais usuários ou clientes estão dispostos a absorvê-la com toda a sede que você gostariam de vender. Além disso, nem todos os processos do desenvolvimento do produto são quantitativos, passíveis de serem resolvidos em segundos por um algoritmo O fator humano deve ser considerado, mesmo que isso signifique que seus desenvolvedores devam ficar parados em uma sprint. Desequílibrio em Design Apesar da área de gerência/negócios ter noções da importância de prover a melhor experiência para seus clientes, existe um conflito de interesses na medida em que Negócio é cobrado pelo ""o que"" vai ser feito, enquanto Design é cobrado por ""como"" vai chegar ao cliente. Os papéis apresentam algumas sobreposições, porém as demandas e/ou expertises tem pontos de vista distintos sob o produto. As pesquisas não são estratégicas para o produto - algumas técnicas apresentam grande versatilidade de aplicação. Todavia é tentador para os profissionais da área escolherem técnicas por costume ou por confiança na execução, somando pouco ao produto final. Falta de foco no que causa maior impacto - o problema desse desequilíbrio é fazer com o que o produto possa apresentar apenas melhorias estéticas e sem impacto para as demais áreas envolvidas ou inovações. Importante também, correlacionar as metas do produto com as pesquisas ou técnicas aplicadas, para que os demais comecem a entender o valor da melhor experiência do usuário. O designer pensa as soluções sozinho - agindo de forma independente sem a colaboração ou compartilhamento de ideias com os demais elementos, a possibilidade de soluções inatingíveis ou apenas conceituais ocorrem com frequência, além disso podem minar a construção da visão de uma empresa centrada no usuário. Na tentativa de gerar inovação e uma real perspectiva dos clientes, algumas empresas como AirBnb tem colocado a gerência/negócios para cuidar do ponto de vista do cliente , o que talvez ocasione um conflito de interesses, já que esse setor está impregnado com a visão da melhor forma de vender mais, onde não raramente isso pode afetar a privacidade, paciência e a melhor experiência de uso para os clientes. Indo além do desequilíbrio Sabe-se também que viabilizar equipes de produto em equilíbrio é difícil e existem até convenções que promovem debates sobre isso. Portanto, uma grande equipe não é apenas medida pela velocidade de entrega das tarefas ou a qualidade do produto no mercado e sim a harmonia e colaboração dos elementos envolvidos, dirimindo os deslizes mais comuns e tornando os produtos significativos para as pessoas que o desenvolveram. O trabalho em equipe precisa ser discutido mais vezes pelos profissionais de produtos digitais, porque poucas pessoas que trabalham no desenvolvimento de produtos entendem que o equilíbrio desses três elementos é o que torna o produto/serviço algo significativo para os membros da equipe. Portanto, sem o engajamento que surge do ""pertencimento"" a algo que ""vale a pena"", com uma visão concreta de valor, há grandes chances de ocorrerem inúmeros conflitos ou desmotivação da equipe ao longo do processo. Contudo, o equilíbrio desses três elementos é almejado, mas poucas vezes é algo alcançado sem o empenho de todos envolvidos, pois cada uma das pontas tem um ponto de vista que precisa ser complementado com os outros elementos, talvez a empatia seja a melhor representação do centro dos três elementos. E qualquer um pode começar hoje essa mudança no seu local de trabalho, através de uma simples e direta comunicação, indo além dos problemas de transparência ou hierarquia em prol de um melhor dia-dia. Boa sorte na sua jornada! Se interessou pelo assunto?",pt,71
181,2850,1481455576,CONTENT SHARED,7088167897470452815,6013226412048763966,2517113148561143287,"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36",SP,BR,HTML,https://www.tonyrobbins.com/coaching/life-coach-vs-therapist/,"life coach vs. therapist, learn the difference | tony robbins","One of the most common misconceptions about life coaching is that it is therapy in disguise - or, worse yet, therapy from an unlicensed practitioner. In reality, life coaching is truly its own unique service designed to help ambitious achievers meet the outcomes that will bring them success and fulfillment. Here are some of the differences between life coaching and therapy, and a basic guide for when each service is appropriate. Defining terms Therapy, also called counseling or psychotherapy, is a long-term process in which a client works with a healthcare professional to diagnose and resolve problematic beliefs, behaviors, relationship issues, feelings and sometimes physical responses. The idea behind therapy is to focus on past traumas and issues to change self-destructive habits, repair and improve relationships and work through painful feelings. In this sense, therapy focuses on the past and on introspection and analysis. Life coaching is a process which may be long- or short-term. In life coaching, a client works with a coach who is not a healthcare professional in order to clarify goals and identify obstacles to success and problematic behaviors in order to create action plans to achieve desired results. It takes the client's current starting point as an acceptable neutral ground and is more action-based from that point onward. Similarities and differences between therapy and life coaching The fundamentals of life coaching are what distinguish it from therapy. Life coaches do not diagnose, while therapists determine illnesses and pathologies so they can be clinically treated. Therapists analyze their client's past as a tool for understanding present behaviors, whereas life coaches simply identify and describe current problematic behaviors so the client can work to modify them. In other words, therapists focus on ""why"" and coaches work on ""how."" Therapists help clients explore and understand their subconscious and unconscious mind. Their goal in this exploration is deep understanding. Life coaches focus on results and actions. Their goals can be measured with key performance indicators and specific behavioral outcomes and goals. Therapy and life coaching do share certain traits and aims, however. Both therapists and life coaches work to enable clients to make positive changes in their lives and become more productive. While therapists do diagnose and treat from a healthcare perspective, not all therapy clients are ill; many healthy people seek the services of both therapists and life coaches. Therapists may at times work with specific results in mind, such as the cessation of a particular problematic behavior. Despite occasional areas of overlap, however, the work and processes of therapists and life coaches are distinct. Want to learn more? Check out the Life Coach v. Therapy Infographic ! Should I seek out a therapist or a life coach? Naturally, the decision to seek out a therapist or a life coach is a very personal one. It might help to imagine yourself getting ready to climb a mountain. You could either hire an expert sherpa and guide for your expedition or a doctor. Which should you choose? If you are physically unwell, or would be in danger if you even attempted the climb, a sherpa and guide wouldn't do you any good. You need to be at a baseline level of good health before you can make the climb at all, so if you're not, you might need to see the doctor before trying something that challenging. However, if you're healthy and just need someone to help you with climbing strategy, carrying the load of supplies and finding the best path, the sherpa and guide is the best bet. The therapist is like the doctor in this example. He or she gets you well enough to take on major challenges in your life by exploring your mental and emotional well-being. The life coach is like the sherpa and guide. He or she has an expert knowledge of your climb and can help you reach the summit. A life coach would be able to offer guidance by: Clarifying and achieving personal and professional goals Creating business plans Working to improve communication skills Achieving financial independence and security Achieving a work/life balance Starting a new business or growing a current business A therapist, on the other hand, focuses their conversation on ways to: Recover from past traumas Explore why past relationships (business or personal) have been destructive Work through depression or anxiety that affect your ability to function at home or work Survive a divorce or loss of a loved one Although life coaches and therapists occasionally help clients with similar problems, their work is not the same. In order to get the right kind of professional expertise, it is crucial to know which kind of guidance will serve you best. Life coaching isn't simply a watered-down version of therapy. It is a dynamic discipline designed to help motivate and inspire people to achieve more than they believe is possible.",en,71
182,2414,1474647970,CONTENT SHARED,7226503561499839165,7645894863578715801,8514890134781099260,,,,HTML,http://techblog.netflix.com/2016/09/zuul-2-netflix-journey-to-asynchronous.html,"zuul 2 : the netflix journey to asynchronous, non-blocking systems","We recently made a major architectural change to Zuul , our cloud gateway. Did anyone even notice!? Probably not... Zuul 2 does the same thing that its predecessor did -- acting as the front door to Netflix's server infrastructure, handling traffic from all Netflix users around the world. It also routes requests, supports developers' testing and debugging, provides deep insight into our overall service health, protects Netflix from attacks, and channels traffic to other cloud regions when an AWS region is in trouble. The major architectural difference between Zuul 2 and the original is that Zuul 2 is running on an asynchronous and non-blocking framework, using Netty. After running in production for the last several months, the primary advantage (one that we expected when embarking on this work) is that it provides the capability for devices and web browsers to have persistent connections back to Netflix at Netflix scale. With more than 83 million members, each with multiple connected devices, this is a massive scale challenge. By having a persistent connection to our cloud infrastructure, we can enable lots of interesting product features and innovations, reduce overall device requests, improve device performance, and understand and debug the customer experience better. We also hoped the Zuul 2 would offer resiliency benefits and performance improvements, in terms of latencies, throughput, and costs. But as you will learn in this post, our aspirations have differed from the results. Differences Between Blocking vs. Non-Blocking Systems To understand why we built Zuul 2, you must first understand the architectural differences between asynchronous and non-blocking (""async"") systems vs. multithreaded, blocking (""blocking"") systems, both in theory and in practice. Zuul 1 was built on the Servlet framework. Such systems are blocking and multithreaded, which means they process requests by using one thread per connection. I/O operations are done by choosing a worker thread from a thread pool to execute the I/O, and the request thread is blocked until the worker thread completes. The worker thread notifies the request thread when its work is complete. This works well with modern multi-core AWS instances handling 100's of concurrent connections each. But when things go wrong, like backend latency increases or device retries due to errors, the count of active connections and threads increases. When this happens, nodes get into trouble and can go into a death spiral where backed up threads spike server loads and overwhelm the cluster. To offset these risks, we built in throttling mechanisms and libraries (e.g., Hystrix ) to help keep our blocking systems stable during these events. Multithreaded System Architecture Async systems operate differently, with generally one thread per CPU core handling all requests and responses. The lifecycle of the request and response is handled through events and callbacks. Because there is not a thread for each request, the cost of connections is cheap. This is the cost of a file descriptor, and the addition of a listener. Whereas the cost of a connection in the blocking model is a thread and with heavy memory and system overhead. There are some efficiency gains because data stays on the same CPU, making better use of CPU level caches and requiring fewer context switches. The fallout of backend latency and ""retry storms"" (customers and devices retrying requests when problems occur) is also less stressful on the system because connections and increased events in the queue are far less expensive than piling up threads. Asynchronous and Non-blocking System Architecture The advantages of async systems sound glorious, but the above benefits come at a cost to operations. Blocking systems are easy to grok and debug. A thread is always doing a single operation so the thread's stack is an accurate snapshot of the progress of a request or spawned task; and a thread dump can be read to follow a request spanning multiple threads by following locks. An exception thrown just pops up the stack. A ""catch-all"" exception handler can cleanup everything that isn't explicitly caught. Async, by contrast, is callback based and driven by an event loop. The event loop's stack trace is meaningless when trying to follow a request. It is difficult to follow a request as events and callbacks are processed, and the tools to help with debugging this are sorely lacking in this area. Edge cases, unhandled exceptions, and incorrectly handled state changes create dangling resources resulting in ByteBuf leaks, file descriptor leaks, lost responses, etc. These types of issues have proven to be quite difficult to debug because it is difficult to know which event wasn't handled properly or cleaned up appropriately. Building Non-Blocking Zuul Building Zuul 2 within Netflix's infrastructure was more challenging than expected. Many services within the Netflix ecosystem were built with an assumption of blocking. Netflix's core networking libraries are also built with blocking architectural assumptions; many libraries rely on thread local variables to build up and store context about a request. Thread local variables don't work in an async non-blocking world where multiple requests are processed on the same thread. Consequently, much of the complexity of building Zuul 2 was in teasing out dark corners where thread local variables were being used. Other challenges involved converting blocking networking logic into non-blocking networking code, and finding blocking code deep inside libraries, fixing resource leaks, and converting core infrastructure to run asynchronously. There is no one-size-fits-all strategy for converting blocking network logic to async; they must be individually analyzed and refactored. The same applies to core Netflix libraries, where some code was modified and some had to be forked and refactored to work with async. The open source project Reactive-Audit was helpful by instrumenting our servers to discover cases where code blocks and libraries were blocking. We took an interesting approach to building Zuul 2. Because blocking systems can run code asynchronously, we started by first changing our Zuul Filters and filter chaining code to run asynchronously. Zuul Filters contain the specific logic that we create to do our gateway functions (routing, logging, reverse proxying, ddos prevention, etc). We refactored core Zuul, the base Zuul Filter classes, and our Zuul Filters using RxJava to allow them to run asynchronously. We now have two types of filters that are used together: async used for I/O operations, and a sync filter that run logical operations that don't require I/O. Async Zuul Filters allowed us to execute the exact same filter logic in both a blocking system and a non-blocking system. This gave us the ability to work with one filter set so that we could develop gateway features for our partners while also developing the Netty-based architecture in a single codebase. With async Zuul Filters in place, building Zuul 2 was ""just"" a matter of making the rest of our Zuul infrastructure run asynchronously and non-blocking. The same Zuul Filters could just drop into both architectures. Results of Zuul 2 in Production Hypotheses varied greatly on benefits of async architecture with our gateway. Some thought we would see an order of magnitude increase in efficiency due to the reduction of context switching and more efficient use of CPU caches and others expected that we'd see no efficiency gain at all. Opinions also varied on the complexity of the change and development effort. So what did we gain by doing this architectural change? And was it worth it? This topic is hotly debated. The Cloud Gateway team pioneered the effort to create and test async-based services at Netflix. There was a lot of interest in understanding how microservices using async would operate at Netflix, and Zuul looked like an ideal service for seeing benefits. While we did not see a significant efficiency benefit in migrating to async and non-blocking, we did achieve the goals of connection scaling. Zuul does benefit by greatly decreasing the cost of network connections which will enable push and bi-directional communication to and from devices. These features will enable more real-time user experience innovations and will reduce overall cloud costs by replacing ""chatty"" device protocols today (which account for a significant portion of API traffic) with push notifications. There also is some resiliency advantage in handling retry storms and latency from origin systems better than the blocking model. We are continuing to improve on this area; however it should be noted that the resiliency advantages have not been straightforward or without effort and tuning. With the ability to drop Zuul's core business logic into either blocking or async architectures, we have an interesting apples-to-apples comparison of blocking to async. So how do two systems doing the exact same real work, although in very different ways, compare in terms of features, performance and resiliency? After running Zuul 2 in production for the last several months, our evaluation is that the more CPU-bound a system is, the less of an efficiency gain we see. We have several different Zuul clusters that front origin services like API, playback, website, and logging. Each origin service demands that different operations be handled by the corresponding Zuul cluster. The Zuul cluster that fronts our API service, for example, does the most on-box work of all our clusters, including metrics calculations, logging, and decrypting incoming payloads and compressing responses. We see no efficiency gain by swapping an async Zuul 2 for a blocking one for this cluster. From a capacity and CPU point of view they are essentially equivalent, which makes sense given how CPU-intensive the Zuul service fronting API is. They also tend to degrade at about the same throughput per node. The Zuul cluster that fronts our Logging services has a different performance profile. Zuul is generally receiving logging and analytics messages from devices and is write-heavy, so requests are large, but responses are small and not encrypted by Zuul. As a result, Zuul is doing much less work for this cluster. While still CPU-bound, we see about a 25% increase in throughput corresponding with a 25% reduction in CPU utilization by running Netty-based Zuul. We thus observed that the less work a system actually does, the more efficiency we gain from async. Overall, the value we get from this architectural change is high, with connection scaling being the primary benefit, but it does come at a cost. We have a system that is much more complex to debug, code, and test, and we are working within an ecosystem at Netflix that operates on an assumption of blocking systems. It is unlikely that the ecosystem will change anytime soon, so as we add and integrate more features to our gateway it is likely that we will need to continue to tease out thread local variables and other assumptions of blocking in client libraries and other supporting code. We will also need to rewrite blocking calls asynchronously. This is an engineering challenge unique to working with a well established platform and body of code that makes assumptions of blocking. Building and integrating Zuul 2 in a greenfield would have avoided some of these complexities, but we operate in an environment where these libraries and services are essential to the functionality of our gateway and operation within Netflix's ecosystem. We are in the process of releasing Zuul 2 as open source. Once it is released, we'd love to hear from you about your experiences with it and hope you will share your contributions! We plan on adding new features such as http/2 and websocket support to Zuul 2 so that the community can also benefit from these innovations. - The Cloud Gateway Team ( Mikey Cohen , Mike Smith , Susheel Aroskar , Arthur Gonigberg , Gayathri Varadarajan , and Sudheer Vinukonda )",en,71
183,1576,1467205950,CONTENT SHARED,3075564241645350154,-1032019229384696495,6023728345695403953,,,,HTML,https://techcrunch.com/2016/06/28/decentralizing-iot-networks-through-blockchain/,decentralizing iot networks through blockchain,"Imagine a washer that autonomously contacts suppliers and places orders when it's low on detergent, performs self-service and maintenance, downloads new washing programs from outside sources, schedules its cycles to take advantage of electricity prices and negotiates with peer devices to optimize its environment; a connected car, smart enough to find and choose the best deal for parts and services; a manufacturing plant where the machinery knows when to order repairs for some of its parts without the need of human intervention. All these scenarios - and many more - will be realized thanks to the Internet of Things (IoT). Already, many of the industries that historically didn't fit well with computers have been transformed by the billions of IoT devices connected to the internet; other industries will follow suit as billions more enter the fray . The possibilities are virtually countless , especially when the power of IoT is combined with that of other technologies, such as machine learning. But some major hurdles will surface as billions of smart devices will want to interact among themselves and with their owners. While these challenges cannot be met with the current models that are supporting IoT communications, tech firms and researchers are hoping to deal with them through blockchain , the technology that constitutes the backbone of the famous bitcoin. The problem with the centralized model Current IoT ecosystems rely on centralized, brokered communication models, otherwise known as the server/client paradigm. All devices are identified, authenticated and connected through cloud servers that sport huge processing and storage capacities. Connection between devices will have to exclusively go through the internet, even if they happen to be a few feet apart. While this model has connected generic computing devices for decades, and will continue to support small-scale IoT networks as we see them today, it will not be able to respond to the growing needs of the huge IoT ecosystems of tomorrow. Existing IoT solutions are expensive because of the high infrastructure and maintenance cost associated with centralized clouds, large server farms and networking equipment. The sheer amount of communications that will have to be handled when IoT devices grow to the tens of billions will increase those costs substantially. Even if the unprecedented economical and engineering challenges are overcome, cloud servers will remain a bottleneck and point of failure that can disrupt the entire network. This is especially important as more critical tasks such as human health and life will become dependent on IoT . There's no single platform that connects all devices. Moreover, the diversity of ownership between devices and their supporting cloud infrastructure makes machine-to-machine (M2M) communications difficult. There's no single platform that connects all devices and no guarantee that cloud services offered by different manufacturers are interoperable and compatible. Decentralizing IoT networks A decentralized approach to IoT networking would solve many of the questions above. Adopting a standardized peer-to-peer communication model to process the hundreds of billions of transactions between devices will significantly reduce the costs associated with installing and maintaining large centralized data centers and will distribute computation and storage needs across the billions of devices that form IoT networks. This will prevent failure in any single node in a network from bringing the entire network to a halting collapse. However, establishing peer-to-peer communications will present its own set of challenges, chief among them the issue of security. And as we all know, IoT security is much more than just about protecting sensitive data. The proposed solution will have to maintain privacy and security in huge IoT networks and offer some form of validation and consensus for transactions to prevent spoofing and theft. The blockchain approach Blockchain offers an elegant solution to the peer-to-peer communication platform problem. It is a technology that allows the creation of a distributed digital ledger of transactions that is shared among the nodes of a network instead of being stored on a central server. Participants are registered with blockchains to be able to record transactions. The technology uses cryptography to authenticate and identify participating nodes and allow them to securely add transactions to the ledger. Transactions are verified and confirmed by other nodes participating in the network, thus eliminating the need for a central authority. The ledger is tamper-proof and cannot be manipulated by malicious actors because it doesn't exist in any single location, and man-in-the-middle attacks cannot be staged because there is no single thread of communication that can be intercepted. Blockchain makes trustless, peer-to-peer messaging possible and has already proven its worth in the world of financial services through cryptocurrencies such as Bitcoin, providing guaranteed peer-to-peer payment services without the need for third-party brokers. Tech firms are now mulling over porting the usability of blockchain to the realm of IoT. The application of blockchain to IoT isn't without flaws and shortcomings. The concept can directly be ported to IoT networks to deal with the issue of scale, allowing billions of devices to share the same network without the need for additional resources. Blockchain also addresses the issue of conflict of authority between different vendors by providing a standard in which everyone has equal stakes and benefits. This helps unlock M2M communications that were practically impossible under previous models, and allows for the realization of totally new use cases. Concrete uses of blockchain in IoT The IoT and blockchain combination is already gaining momentum, and is being endorsed by both startups and tech giants. IBM and Samsung introduced their proof-of-concept system, ADEPT , which uses blockchain to support next-generation IoT ecosystems that will generate hundreds of billions of transactions per day. In one of the first papers to describe the use of blockchain in IoT , IBM's Paul Brody describes how new devices can be initially registered in a universal blockchain when assembled by the manufacturer, and later transferred to regional blockchains after being sold to dealers or customers, where they can autonomously interact with other devices that share the blockchain. The combination of IoT and blockchain is also creating the possibility of a circular economy and liquefying the capacity of assets, where resources can be shared and reused instead of purchased once and disposed after use. An IoT hackathon hosted by blockchain platform leader Ethereum put the concept of blockchain-powered IoT to test, in which some interesting ideas were presented , including in the domain of energy sharing and electricity and gas billing. Filament is another startup that is investing in IoT and blockchain with a focus on industrial applications such as agriculture, manufacturing and oil and gas. Filament uses wireless sensors, called Taps, to create low-power autonomous mesh networks for data collection and asset monitoring, without requiring a cloud or central network authority. The firm uses blockchain technology to identify and authenticate devices and also to charge for network and data services through bitcoin. Chain of Things is a consortium that is exploring the role of blockchain in dealing with scale and security issues in IoT. In a recent hackathon held in London, the group demonstrated the use of blockchain and IoT in a case study involving a solar energy stack designed to provide reliable and verifiable renewable data, speeding up incentive settlements and reducing opportunities for fraud. The system facilitates the process in which a solar panel connects to a data logger, tracks the amount of solar energy produced, securely delivers that data to a node and records it on a distributed ledger that is synced across a broader global network of nodes. Caveats and challenges The application of blockchain to IoT isn't without flaws and shortcomings, and there are a few hurdles that need to be overcome. For one thing, there's dispute among bitcoin developers over the architecture of the underlying blockchain technology, which has its roots in problems stemming from the growth of the network and the rise in the number of transactions. Some of these issues will inevitably apply to the extension of blockchain to IoT. These challenges have been acknowledged by tech firms , and several solutions, including side-chains, tree-chains and mini-blockchains, are being tested to fix the problem. Processing power and energy consumption is also a point of concern. Encryption and verification of blockchain transactions are computationally intensive operations and require considerable horsepower to carry out, which is lacking in many IoT devices. The same goes for storage, as ledgers start to grow in size and need to be redundantly stored in network nodes. And, as Machina Research analyst Jeremy Green explains , autonomous IoT networks powered by blockchain will pose challenges to the business models that manufacturers are seeking, which includes long-term subscription relationships with continuing revenue streams, and a big shift in business and economic models will be required. It's still too early to say whether blockchain will be the definite answer to the problems of the fast-evolving IoT industry. It's not yet perfect; nonetheless, it's a very promising combination for the future of IoT, where decentralized, autonomous networks will have a decisive role. Featured Image: Morrowind / Shutterstock",en,71
184,932,1463090835,CONTENT SHARED,2916072977192006313,-2626634673110551643,5975453253989656448,,,,HTML,https://bitsonblocks.net/2016/05/09/confused-by-blockchains-revolution-vs-evolution/,confused by blockchains? revolution vs evolution,"This article attempts to explain the difference between the revolutionary disruptive innovation of bitcoin and the evolutionary efficiency innovations of industry workflow tools , and why calling them both ""blockchains"", even as a generic term, is incredibly confusing. For the rest of this post, I will use the phrase ""industry workflow tools"" instead of industry blockchains, as some of the emerging solutions being proposed in this space are not blockchains (eg, R3's Corda is not a blockchain but DAH's solutions are - however, both companies are proposing industry workflow tools). Just as it's not helpful to call Twitter and Microsoft Sharepoint ""database companies"" although they both use variations of databases, it's not helpful to call cryptocurrencies, cryptocurrency companies, blockchain platforms and industry workflow tool companies ""blockchain companies"", although this frequently happens in the popular press. Why? Because you don't want to create misunderstandings like ""But I thought you could only use 140 characters in Sharepoint."". To be clear, both cryptocurrencies and industry workflow tools both have admirable objectives in their own ways for their own purposes, as do both Twitter and Sharepoint. Disruptive innovation: Public Cryptocurrencies The purpose of bitcoin , according to Satoshi Nakamoto's original whitepaper is to create "" A purely peer-to-peer version of electronic cash [which] would allow online payments to be sent directly from one party to another without going through a financial institution "". This is new and radically different to anything that has ever existed before. It is meant to enable: value to be held electronically without any third party being involved, and value to be transmitted without a specific third party being able to censor the transaction at will. The problem statement is: How do we use technology to create a financially inclusive system that anyone can participate in? The proposed solution is: Efficiency innovation: Industry Workflow Tools The purpose of successful incumbent institutions is to maintain and improve on their position by keeping customers happy, increasing revenues, reducing costs, becoming more efficient - ie to maintain a competitive, well-run business. The problem statement is: How do we use technology to improve our business and add shareholder value? The proposed solution is: Use industry workflow tools These are incredibly different, more or less polarised problem statements, requiring incredibly different and targeted solutions. Blockchains have somehow been caught in the middle. So why the conflation? Why are people confused about these entirely separate problems and solutions? Somehow in all the hype and PR, industry workflow tools still seem to have retained some of the connotations of bitcoin, when industry workflow tools and cryptocurrencies have almost the opposite ideology! Industry workflow tools and bitcoin are both separately exciting and innovative, but for very different reasons. How did the confusion happen? 2013 In 2013 The 'thing' was bitcoin. No one really talked about ""blockchain"" apart from talking about bitcoin's blockchain, ie the replicated ledger of all bitcoin transactions. It was called ""the blockchain"" because there was only one. (I am excluding the alt-coins like Litecoin, Dogecoin and other digital tokens as they only take up a tiny fraction of the mind-share of bitcoin). 2014 In 2014, because bitcoins were described as a new currency, the financial industry started to take notice. Bitcoins were pitched to the financial industry as an investment (buy bitcoins because the price will go up) and a trading asset (buy and sell bitcoins because you can make money) and a strategy (integrate bitcoin functionality because your customers will want it). The financial services industry as a whole wasn't interested in an anonymous, open, unregulated self-declared 'currency', backed by no government or central bank. They had no mandate to invest, and no framework to price it or understand it. Associations between bitcoins and scams, drugs and underground markets made the concept even less tasteful for traditional financial service industry participants. Even those who saw some potential were mostly put off by the small size and illiquidity of the market. 2015 In late 2014/15 the narrative moved away from bitcoins, towards blockchains and distributed ledgers (replicated ledgers without necessarily having chains of blocks). Institutions were saying ""We're not interested in bitcoin, but we are interested in the data-sharing technology behind it, the Blockchain"". In summary, "" Bitcoin bad, blockchain good "". The industry responded. In an attempt to garner interest, funding, customers, and higher company valuations, many bitcoin companies started rebranding as blockchain companies using more or less a text find-and-replace strategy (find the word 'bitcoin' and replace it with 'blockchain'). This was also to avoid the negative connotations of the word 'bitcoin', and to participate in the interest in the word 'blockchain'. I have heard the phrase ""we send it on/over/using blockchain"" or ""using blockchain technology"" as a deliberate tactic not to use the word bitcoin, and to hide what is going on. This misleads customers, investors, and regulators. This deliberate misdirection is still currently pursued by some companies who use bitcoins, who assert that they are blockchain-powered when really they mean that they transfer value by buying, transferring, and selling bitcoins. One of the arguments is that by using the word blockchain instead of bitcoin, they attract less regulatory scrutiny and have a higher chance of opening a bank account, needed for fiat deposits. This has hurt the industry by creating confusion, and in retrospect is incredibly short-sighted. This also hurt the companies who were genuinely using blockchain technology (not bitcoins) to attempt to solve other problems. I call this The Blockchain Blunder . Later, journalists, industry leaders, politicians, figureheads, consultants, bloggers and dinner party speakers started waving their hands and talking about ""blockchain"" (without ever specifying which one) being a solution for everything from disrupting banks to saving banks, from replacing 3rd parties to creating more efficient 3rd parties, and of course enabling financial inclusion. More pundits jumped on the bandwagon and regurgitated barely-researched content, creating the echo chamber of confusion. Chaos ensued. Some technology vendors pivoted from bitcoin (not well paid) to industry workflow tools (better paid) and tried to retrofit iterations of bitcoin's blockchain technology into perceived financial service problems, often without understanding the problems and the context of the financial service problems in the first place. Incumbents in turn needed a 'blockchain strategy' and started doing proof of concepts by taking well known, well understood problems with well known, well understood solutions, and attempting to apply blockchain solutions to them. This makes sense if the purpose is to get hands dirty and explore the technology, though it doesn't make sense from a pure IT architecture perspective. This is where we stand in Q2 2016. What has the effect been? Bitcoin: a way for people to pay each other across the world without interference from financial institutions. Industry workflow tools: mechanisms to share and update data between entities without a central point of control, creating efficiencies for incumbent industry participants. By referring to these both as blockchains, the connotations have got jumbled up. Here's how it should look: Some benefits, reality checks, and points to consider Now that we understand the difference between cryptocurrencies such as bitcoin, and industry workflow tools, let's explore where the benefits lie. Industry workflow tools will benefit incumbents A shared or distributed ledger/fabric/communication tool/chat-app is only useful to business if it makes businesses better, more efficient, more competitive. The promise of the proposed industry workflow technologies is to join up participants and create a strategic advantage for them. The benefits of participation are potentially both cost reduction (cheaper IT) and oligopoly-consolidation (let's maintain our advantage, together). It's a no-brainer for incumbent financial institutions to pursue what this technology can bring. Industry workflow tools may benefit regulators Regulators may want to insist on being able to 'plug in' to the workflow tools to get a better understanding of what is going on under their watch - something they have wanted to do for a long time. The transparency of asset ownership promised by industry workflow tools may have positive implications for systemic risk understanding and reduction. However regulators should watch out for creating a new systemic risk - is something being created which is globally too big to fail? Are regulators enabling a monopoly or oligopoly? Industry workflow tools may speed up asset settlement, but traders don't want real-time gross settlement. There is a difference between same-day settlement (T+0) and Real Time Gross Settlement. Same day settlement Same day settlement is beneficial to participants as it means that you get what you bought more quickly. Instead of waiting 3 days (T+3) to own your shares, the shares would be yours by the end of the same trading day (T+0). This frees up your pre-pledged collateral for use in another trade. The technology to reduce the time taken to settle equities from T+3 to T+0 (ie 3 days to same-day) has been available ever since we have known how to edit a row in a database and tell someone else about it quickly. The Kuwait and Saudi exchanges operate on T+0 according to a member of NASDAQ staff. It's not the technology that has prevented the change, It's the market structure, practices, regulation, and habits . The DTCC made this point clear in their blockchain whitepaper . So if it's T+0 settlement is not a technology problem, why will using newer technology help? Real Time Gross Settlement This is where trades are settled individually in as close to real time as possible, and there is no 'netting'. Gross settlement means when I buy some shares one minute and sell them the next, we settle both trades in full (we don't ""settle the difference""). However, netting is efficient. In real life you net wherever possible - for example if you buy your friend dinner, then later she buys you dinner, you wouldn't insist that you pay each other back the full amounts on the restaurant bills - no, instead you settle the difference, ie 'net' them off against each other. Market players don't want real time gross settlement . Being able to trade in and out of positions during the day and only settle up at the end of the day provides a lot of benefit: having to settle every trade would reduce the fun that can be had and money to be made. It would also increase the amount of collateral that would need to be posted. There is confusion between what bitcoin does (fairly real-time settlement of a BTC-denominated payment) and industry workflow tools being pitched as a solution to frictionless, real-time gross asset settlements (that traders don't want). Industry workflow tools may make innovation harder and more expensive Unless built extremely carefully, a system affecting multiple participants could be harder to upgrade and could ossify more quickly than a system run by a single entity. If you think it's hard to innovate a single banking app, just think how hard it will be to innovate one that touches multiple banks. Who would coordinate it? Who bears the costs? For analogy, the internet uses a couple of protocols called TCP/IP. There has been amazing stuff built on top. However, changing TCP/IP itself is incredibly hard, partly because there is so much software and firmware built on top of it. What about the 3rd parties who were going to be disintermediated? Today, 3rd parties set the rules and also enforce them, with some enjoying a quasi monopolistic status, and reaping the pricing benefits that that allows. With the industry utilities being built, the role of the 3rd parties may change a bit - they could become technology service centres and standards agencies. I don't think they will be disintermediated. Perhaps with these workflow tools, control and execution of the rules will lie with the participants (the banks running the nodes), and the 3rd parties will end up setting the rules and coordinating the upgrades: someone has to. Incumbent 3rd parties are shaping the narrative and working hard to make sure they remain relevant. The promise of disintermediation is driven by bitcoin & cryptocurrency side, not from industry workflow tools. Cryptocurrencies and Financial inclusion The promise of bitcoin and its bag of technological tricks gave us for the first time in the history of the world a way for two online people anywhere on the planet to send value electronically without needing to onboard or rely on specific third parties . For or better or for worse, this is truly a step towards financial inclusion . But here's the rub: ask a policymaker what they want, they'll say Financial Inclusion. Ask them what they don't want, and many will say bitcoin: the most financially inclusive tool we have ever seen . Bitcoin is the most financially inclusive technology that exists today. Industry workflow tools have no direct impact on financial inclusivity. So what are next steps? I am an incumbent, what should I do? Make best use of technology You should be using the best technology to increase revenues, reduce costs, increase efficiency, keep customers happy, and deliver profits to shareholders It's important to use technology to remain competitive Get involved with industry workflow tools, or you might miss out! Defend against the disruptive innovation of cryptocurrencies Aside from political lobbying, the only defence against disruption is to get down and play on the disruptor's terms. To compete with bitcoin you need to create an open, permissionless, censorship-resistant payment network that is better than bitcoin. That may be difficult to do while maintaining a banking licence Perhaps one long-term play might be investing in bitcoin firms (actual cryptocurrency firms, not 'blockchain solution providers') I am interested in disrupting the financial industry, what should I do? Keep working on the public blockchains, the unprofitable stuff, the stuff that people are uncomfortable talking about Keep making the open networks better, solving the hard problems, proving the skeptics wrong Don't lose focus: you are not trying to solve the incumbent industry's problems. Industries aren't disrupted by having better widgets built for them. Do you really need to dance with the devil? Don't spend your VC money too quickly pre-empting traction. It will probably take longer than you think! You are trying to create a new way of doing things - for better or for worse, and however it may end up Be careful what you enable. I am sitting on the sidelines, what should I do? Keep reading, talking, learning, and understanding. Challenge the hand-wavers if something doesn't sound or feel right Keep watching the evolution of this space, the future is exciting! What does the future hold? The disruptive innovators will keep working on bitcoin and variants, improving them, solving problems. Disruptive companies will come and go, the majority will fail. The incumbents will keep working to remain competitive, keep their customers happy, and deliver profits to shareholders. They will ignore bitcoin because it's not what they think their customers want, and the market is too small ! Then, when bitcoin or its progeny gets good enough, something will happen. Disruption is uncomfortable, it's dirty, it's subversive, it's ""Oh shit, we can't do anything about this"". There will be intense lobbying and a concerted effort to ban or make the technology illegal. The technology will win. This is the story of disruption , which has been told over and over again. The financial service providers in a cryptocurrency world will look very different to today.",en,70
185,2240,1472727922,CONTENT SHARED,7187598944032799890,-2979537012405607453,-4362351633011051065,,,,HTML,http://fossbytes.com/top-15-facebook-open-source-projects-you-must-know/,top 15 facebook open source projects you must know,"I am starting with Facebook as I am always impressed with all the project they have open sourced till date and how other companies including fossBytes uses some of these technologies. Facebook uses, maintains, and contributes to a significant number of major projects- in areas as diverse as native mobile tools, big data systems, client-side web libraries, backend runtimes and infrastructure, and through the Open Compute Project, server and storage hardware. Facebook's GitHub account alone, now has more than 90 repos comprising over 40,000 commits and that have collectively been forked 15,000 times. Facebook contribution to open source can be largely categorized into Mobile, Web, Back-end and Infrastructure. Top open source projects made in these categories are: Mobile: Buck is a high performance build system for Android that encourages creation of small, reusable modules consisting of code and resources. Because Android applications are predominantly written in Java, Buck also functions as a Java build system. Rebound is a Java library that models spring dynamics. Rebound spring models can be used to create animations that feel natural by introducing real world physics to your application. Rebound uses the same spring constants as making it easy to convert Origami interaction mockups directly into your Android application. Origami is a tool for designing modern user interfaces. Quickly put together a prototype, run it on your iPhone or iPad, iterate on it, and export code snippets your engineers can use. Stetho is an all new debugging platform for Android. It enables the powerful Chrome Developer Tools which is implemented using a client/server protocol which the Stetho software provides for your application. Once your application is integrated, simply navigate to chrome://inspect and click ""Inspect"" to get started! Infer Facebook is a static analysis tool to detect bugs in Android and iOS apps before they ship. If you give Infer some Objective-C, Java, or C code, it produces a list of potential bugs. Anyone can use Infer to intercept critical bugs before they have shipped to people's phones, and help prevent crashes or poor performance. Infer targets critical bugs such as null pointer exceptions, resource leaks and memory leaks. Web: React Js is a declarative, efficient, and flexible JavaScript library for building user interfaces. Lots of people use React as the V in MVC. Since React makes no assumptions about the rest of your technology stack, it's easy to try it out on a small feature in an existing project. HHVM (Hip Hop VM) is an open-source virtual machine designed for executing programs written in Hack and PHP. HHVM uses a just-in-time (JIT) compilation approach to achieve superior performance while maintaining the development flexibility that PHP provides. It has realized more than a 5x increase in throughput for Facebook compared with Zend PHP 5.2. HipHop is most commonly run as a standalone server, replacing both Apache and modphp, but it can also run standalone scripts from the command line. Flux is the application architecture that Facebook uses for building client-side web applications. It complements React's composable view components by utilizing a unidirectional data flow. It's more of a pattern rather than a formal framework, and you can start using Flux immediately without a lot of new code. Flow adds static typing to JavaScript to improve developer productivity and code quality. The goal of Flow is to find errors in JavaScript code with little programmer effort. Flow relies heavily on type inference to find type errors even when the program has not been annotated - it precisely tracks the types of variables as they flow through the program. fb-flo is a Chrome extension that lets you modify running apps without reloading. It's easy to integrate with your build system, dev environment, and can be used with your favorite editor. Jest is unit testing framework for JavaScript. It is built on top of the Jasmine test framework, using familiar expect(value).toBe(other) assertions. It automatically mocks CommonJS modules returned by require(), making most existing code testable. Nuclide is a suite of packages for to provide IDE-like functionality for a variety of programming languages and technologies. It is designed to provide a unified developer experience for engineers throughout the company - whether they work on native iOS apps, on React and React Native code, or on Hack to run on our HHVM web servers. Back-end: Presto is an open source distributed SQL query engine for running interactive analytic queries against data sources of all sizes ranging from gigabytes to petabytes. Facebook uses Presto for interactive queries against several internal data stores, including their 300PB data warehouse. Over 1,000 Facebook employees use Presto daily to run more than 30,000 queries that in total scan over a petabyte each per day. Osquery gives you a SQL interface to try out new queries and explore your operating system. With the power of a complete SQL language and dozens of useful tables built-in, osquery is an invaluable tool when performing incident response, diagnosing system operations problem, or troubleshooting a performance issue. Deploy a security tool that also enables developers and administrators. RocksDB builds on LevelDB to be scalable to run on servers with many CPU cores, to efficiently use fast storage, to support IO-bound, in-memory and write-once workloads, and to be flexible to allow for innovation. Not only softwares, but Facebook has also built various custom hardware competent to meet its ever increasing scale under Open Compute Project . Unlike Google and Microsoft, Facebook not only contributes its research work but also the end implementation to open source community. Let us know in comments your views towards Facebook's open source contribution. Check out our other articles on open source projects here .",en,70
186,1334,1465558374,CONTENT SHARED,3818189513627822856,-8020832670974472349,-6121624872835463728,,,,HTML,http://blog.cloudfour.com/autofill-what-web-devs-should-know-but-dont/,"autofill: what web devs should know, but don't","Autofill: What web devs should know, but don't Many people know that you can scan your credit credit in Mobile Safari. But how many web developers know how to create a form that supports that feature? Not many I'd bet. Photo source: Auto-Fill Credit Card Forms Using Your iPhone's Camera in iOS 8 . Used with permission. It doesn't help that Apple has provided zero documentation on how the scan credit card feature works. But there is another factor at play here. The scan credit card feature is a subset of browser functionality that web developers have long ignored: autofill. It's understandable why web developers haven't paid much attention to autofill. When you're filling out forms with test data on a regular basis, autofill tends to get in the way. But autofill is an important feature for our users. Google has found that "" users complete forms up to 30% faster "" when using autofill. So let's learn how autofill works, how to build forms that support cross browser autofill, and take advantage of new features like scanning credit cards. How does autofill work? Until recently, there were no standards when it came to autofill. Each browser implemented their autofill features differently and there was little documentation on how a browser determines what content a field expects. Despite this chaotic situation, browsers seem to have settled on two main approaches: 1. Pre-determined autofill fields Chrome, Opera and Safari have all taken the approach of identifying high-value form fields and providing a way to manage what the browser will autofill for those fields. For example, Opera provides autofill for addresses and credit cards. These can be managed in the preferences as shown above. Chrome, Opera and Safari all differ on which fields they provide autofill for, but the basic fields needed to complete a checkout process are well supported. Most users never have to see or edit these preferences in order to utilize autofill. The browser watches the person filling out forms and when it recognizes an address or a credit card, it will ask if the user wants them to save that information to reuse later. 2. Autofill any field If the previous approach is like a scalpel applied to preselected fields only, this second approach is a chainsaw cutting down every field in view. When a form is submitted, Microsoft Edge and Firefox will store the value submitted along with the value of the name attribute. If the browser sees a field in the future with a matching name attribute, it will provide autofill options. Firefox also appears to look at the id in addition to the name attribute Because there are security and privacy concerns with this approach, the autocomplete off value has long been supported to prevent the browser from storing and autofilling sensitive information. Which approach is better? While the second approach works for more fields, as a developer, I much prefer the pre-determined autofill fields approach. It makes it much easier to figure out what information the browser has to autofill. It is also easier to set up test profiles. Plus, with the second approach, you actually need to submit a form in order for the browser to store values to use with autofill. The browser won't remember your answers without the form submission. It also makes me nervous to think that the browser might store credit card information in a non-encrypted way if it can't clearly identify the type of field. Given how concerned Microsoft and Mozilla are about security and privacy, I'm certain they've put protections in place. But I personally feel more secure looking at an autofill preferences pane and seeing credit card information clearly separated and understood by the browser. All that said, I don't know what end users prefer. The second system works in more places, but I've seen quite a few support questions where people have been trying to remove their autofill options from the browser history. It will be interesting to see how Edge and Firefox change when they began to support the new autofill standard. One behavior to watch for Sometimes browsers require more than one field of a certain type before they will present you with autofill options. For example, in the following animated GIF, Safari won't offer to autofill the single cardholder field, but it will offer to autofill it once there is a second card number field. However, if the only field being collected is the card number, then Safari will offer the autofill option. My experience has been that creating isolated test cases for single fields can be challenging because of this behavior. At one point in my testing, I encountered Opera requiring three fields before it would autofill, but I can no longer recreate that behavior. This should never be an issue for a user so long as your form is written to support autofill (more on this soon), but I note it here in case you're attempting to troubleshoot autofill and encounter this behavior. The standards-based approach to autofill Thankfully, there is a way forward for autofill. HTML5 recently expanded the autocomplete attribute to provide hints to the browser about what content a field expects. The autocomplete attribute has been around for several years. It started with two values: on and off . By default, the autocomplete attribute is set to on which means that the browser is free to store values submitted and to autofill them in the form. However, some fields are poor candidates for autofill behavior. In that case, the autcomplete attribute can be set to off to tell the browser that this field should be not autofilled. Recently, additional values have been added as options for the autocomplete attribute. These new options are called autofill detail tokens . These tokens tell the browser what information the field is looking for. One type of token is called the autofill field names. The autofill field names tell the browser what type of information a field expects. For example, one of the autofill field names is organization . The HTML5 specification says that organization refers to: Company name corresponding to the person, address, or contact information in the other fields associated with this field If you wanted to tell the browser to autofill the organization field, your code would look something like this: The HTML5 specification has a great table listing all 53 possible autofill field names , what their intended use is, and what type of input must be used with it. That's autocomplete at its simplest, but it gets more powerful and a bit more complex. Shipping and billing The value of the autocomplete attribute is actually a space-separated list of tokens. So for example, if you wanted to collect shipping information, you would prepend the autocomplete value with the ""shipping"" token like so: The billing token works exactly the same way as shipping . Telephones, email and instant messaging Another token option applies to telephone, emails and instant messaging. For those services, there is an optional token to indicate if the autofill field name is referring to home , work , mobile , fax or pager . For example: Broad versus narrow autofill field names The specification provides for both broad and narrow autofill field names for many of the types of information. For example, in addition to the tel field which would be a single input containing a full telephone number, there are also: The specification authors encourage you to use the broad autofill field names as often as possible: Generally, authors are encouraged to use the broader fields rather than the narrower fields, as the narrower fields tend to expose Western biases. For example, while it is common in some Western cultures to have a given name and a family name, in that order (and thus often referred to as a first name and a surname), many cultures put the family name first and the given name second, and many others simply have one name (a mononym). Having a single field is therefore more flexible. I agree with this recommendation. And as a practical matter, it means that it is important to pay attention to the table of values and make sure you're using the right one for the field you're working with. Sections The final feature of the new autocomplete attribute tokens is the ability to declare an arbitrary section to group fields. A section is defined using a token that starts with section- . What comes after the dash is up to you. The specification provides this example of sections: All the tokens Put all together, we've now got a much more complex set of tokens for the autocomplete attribute. And the order of the tokens matters. First, you're either using on and off values OR you're using autofill field names. You can't use them at the same time. If you're using the autofill tokens, the order is: And keep in mind that [home|work|mobile|fax|pager] only applies for the telephone, email and chat autofill field names. The longest possible set autofill token might look something like this: Yay for standards! We're done, right? I'm afraid not. My hope is that eventually all of the browsers will support the expanded autocomplete standard, but that's not the current situation. I tested browsers on mobile and desktop to see what attributes the autofill appeared to honor. This is what I found: Thus far, only Chrome and Opera clearly support the new autocomplete features. Safari appears to have partial support, but because they don't have documentation, I can't tell if this is intentional or simply a regular expression match that happens to be looking at the autocomplete attribute as well as name and other attributes. Safari's strange behavior Since the release of iOS 8's credit card scanning feature, web developers have been reading tea leaves trying to divine what combination of markup Safari is looking for. Some suggest that you have to have specific values in the name field . Others found values in ids work . Even labels seem to matter : The Cardholder's name is quite a bit more tricky. We messed around with various IDs for a long time and almost gave up. We couldn't figure out an ID that would cause Card Scan to populate it. After much frustration we finally discovered that the TEXT of the label associated with the field matters. Setting the label text to ""Name on card"" was the magic trick. I've done quite a bit of testing, and I'm still not confident that I understand fully what Safari is doing. However, I've seen enough to be able to draw some basic conclusions: Contact and address fields support autocomplete Safari recognizes the form I created that only includes autocomplete attributes . The moment I start typing in the first field, it offers to fill in the form with my contact information. This all works as expected with a couple of caveats. First, it is unclear what Safari is using to make decisions about what information to autofill from my contact in the Mac's address book. My job title is filled in, but the company I work for isn't. Second, it doesn't give users the option to select which information they want to use. My contact contains both my home address and my work address. Safari only wants to autofill my home address. I'm out of luck if I want to ship something to the office. Payment fields are completely wonky When it comes to the payment fields, Safari's behavior changes completely. The autocomplete attribute is ignored. Instead, there is some sort of magical heuristic that Safari is using. And because I'm not an Apple magician, it has been difficult to discern what Safari is actually doing: In the video above, I edit the labels of two fields. Both have autocomplete set as well as name and id which should help Safari identify the field. And yet, Safari doesn't recognize the fields until the labels are changed to Name on Card and Credit Card Number . As mentioned earlier, Safari needs to see more than one field to trigger autofill. Then I try changing the label to CCNumber which continues to work. However, changing it to CC Number breaks the autofill. Basically, Safari has an unpublished list of terms that it is searching for. Fortunately, Jacques Caron was able to extract a list of strings from the iOS Simulator that Safari appears to be looking for: In my experiments, both: and: will trigger Safari's autofill and the scan credit card feature on iOS. However, putting the same values into the autocomplete attribute will not work. Building a cross browser autofill form Given what we've learned, is it truly possible to build a form that supports autofill across browsers? I think it is. Or at least, we can get very close by taking these four steps. 1. Add autocomplete attributes This is the future of autofill. Browsers that don't recognize the values will ignore them. This is progressive enhancement at its best. 2. Use common values for input name attributes To take advantage of autofill for Firefox and Edge users, you have to hope that the values you pick for the name attribute match what developers on other sites have used. One way to do this would be to survey popular sites and see what they use and then use the same values. Or perhaps use the same values as the autocomplete field in hopes that as more web developers become familiar with the standard that they will start using those names for their fields. Unfortunately, there is no way to guarantee that your Firefox and Edge users will have previously visited a form that uses the same name values as your form does. 3. Add name and/or label values that match the list Safari is looking for Using the list that Jacques Caron was able to extract, you can modify the values for the name attribute or the label to match what Safari expects. 4. Make autofill part of your test plans Lately, I've been asking audiences to raise their hands if autofill is part of their test plans. No one does. I've been working on the web since 1996, and I have yet to see autofill as part of the test plan. I suspect that autofill is a blindspot for web developers and designers. Therefore, it is critical we test our products to ensure that they work well with autofill. The final form Here is a sample form that supports autofill on Chrome, Safari, Opera, Firefox and Edge: See the Pen Cross Browser Autofill Form - Selected Fields by Jason Grigsby ( @grigs ) on CodePen . To see it in action, you'll need to view it on CodePen under https or the browser won't fill in the credit card information. I've also built a form with all 53 fields in the autocomplete specification . No browser currently supports all 53 fields. The future of autofill and forms There is a lot of movement from browser makers towards tackling the problem of web payments. The Payment Request API is being co-authored by Mozilla, Microsoft, Google and Facebook. Apple is participating in the Web Payments Working Group which is where the Payment Request API is being hashed out. So it appears that Apple is at least nominally onboard with the Payment Request API. Plus, rumor has it that Apple Pay will be available on mobile web for the holiday shopping season which makes it feel like this new momentum around web payments might be real this time. This renewed focus on streamling the web checkout process has me optimistic that support for autofill detail tokens will grow in the near term. And these tokens make creating forms that work with autofill so much simpler. And most importantly, supporting autofill will make filling out forms less tedious for our users and lead to increased sales for e-commerce sites.",en,70
187,1266,1465138844,CONTENT SHARED,-401664538366009049,-5868110530814399805,6837395310047141668,,,,HTML,http://www.adweek.com/news/technology/pinterests-real-world-pins-let-store-shoppers-save-real-items-virtual-boards-171815,pinterest's real-world pins let in-store shoppers save real items to virtual boards,"It's a real pin that looks like a virtual pin that looks like a real pin. In a bit of reverse engineered skeuomorphism, Pinterest has taken its virtual ""Pin"" button, which was always visually modeled after a physical pin, and actually made it a physical pin-for an intriguing campaign in Brazil that lets people pin items in stores and save them immediately to virtual inspiration boards on Pinterest itself. Agency DM9DDB created the technology and tested it in a campaign for Tok&Stok, Brazil's biggest design furniture store. The biggest challenge, of course, was how to get the pins to know who is pressing them, and thus to whose Pinterest boards the furniture items should be posted. To solve this, the pins use Bluetooth low energy (BLE) to connect with a ""PinList"" app that the shopper must download to his or her phone. This technology helps the pin locate the nearest person and connect with their app. It's explained in more detail in this video: ""The innovation is in the app's easy and intuitive use and all the technology backing it up, so that you don't have to leave the app open or pair your cellphone's Bluetooth with the physical button,"" says Igor Puga, vp for integration and innovation at DM9DDB. E-commerce has tried so hard to mimic the in-store experience; it's fun to see the reverse happening (even if these real-world Pinterest buttons are awfully big, and look kind of goofy when sitting on every item in the store). ""Pinterest has become a source of inspiration in the decor segment with 10 million ideas in Brazil every month. Tok&Stok has been so innovative in combining the process of discovery with saving what you find,"" says Mariana Sensini, managing partner of Pinterest in Brazil. CREDITS Client: Tok&Stok Title: PinList Agency: DM9DDB Chief Creative Officer: Aricio Fortes Executive Creative Director: Paulo Coelho Digital Interactive VP: Igor Puga Creative Director: Adriano Alarcon, Carlos Schleder e João Mostério Content Coordinator: Pedro Baptista Community Manager: Thiago Martinez Art Director: Daniel Lobo Designer: Daniel Matsumoto Illustrator: Big Studios - Rafael Nakahayashi and Rodrigo Alves. Project: Eduardo Martin, Fernando Tolusso, Rafael Gomes Account: Marcelo Passos, Claudia Almeida, Tania Pena, Beatriz Rodrigues, Thais Moura RTVC: Fabiano Beraldo, Juliana Henriques, Ana Lucia Marques Production: Clariana Regiani da Costa, Nereu Marinho Digital Production: Bizsys Approved by: Flavia Lucena",en,70
188,169,1459767022,CONTENT SHARED,-7660505434580831027,5746645399823844475,2679947461581233081,,,,HTML,http://www1.folha.uol.com.br/mercado/2016/04/1756923-empresa-britanica-adota-licenca-remunerada-no-periodo-menstrual.shtml,empresa britânica adota licença remunerada no período menstrual,"Divulgação/Coexist A diretora da empresa britânica Coexist, Bex Baxter (centro), e parte da sua equipe no escritório JULIANO MACHADO DE BERLIM Uma pequena empresa de Bristol (Reino Unido) chamada Coexist passa a adotar, a partir deste mês, uma licença-menstruação, pela qual funcionárias terão flexibilidade de ir para casa, se for necessário, e compensar depois as horas não trabalhadas -ou mesmo trabalhar de casa. ""Não fixaremos um número de dias remunerados ao mês porque não queremos associar isso a uma doença. As mulheres precisam de apoio para valorizar seu ciclo, e não se culpar por ele"", afirmou à Folha Bex Baxter, diretora da Coexist, que administra um espaço cultural na cidade. Baxter disse esperar que a iniciativa, considerada inédita no mercado britânico, chame a atenção de empresas maiores -a Coexist tem apenas 24 empregados, dos quais 15 são mulheres. ""Cada companhia tem de analisar o que é melhor, mas o importante é levantarmos a questão."" Ela mesma diz que sofria de dores terríveis, mas esse período diminuiu bastante desde que passou a flexibilizar sua jornada. ""Agora passo um dia trabalhando de casa em vez de três debilitada, sem produzir."" Um dos principais defensores da licenças, o ginecologista britânico Gedis Grudzinskas afirma que grandes corporações teriam até mais facilidade para aplicar a medida. ""Não existe sincronia menstrual, ou seja, ninguém ficaria sem todas as suas funcionárias por um período determinado."" A única grande empresa ocidental que se sabe que adota licença-menstruação remunerada é a Nike. A prática foi implementada em 2007 e se criou um memorando de entendimento para que seus parceiros comerciais também fizessem o mesmo em todos os países onde a empresa atua. preconceito O Brasil não possui legislação sobre a questão nem se sabe de companhias que adotem formalmente a prática por conta própria. Em janeiro, um boato circulou pela internet segundo o qual havia sido aprovada uma lei que permitia às mulheres três dias de descanso em casa por mês. O direito à licença-menstruação existe há décadas em vários países asiáticos, mas esbarra no medo que as mulheres têm de serem discriminadas caso o utilizem. O Japão foi o primeiro a introduzir a prática, em 1947, em meio à necessidade de atrair mão de obra feminina no combalido mercado pós-Segunda Guerra. A lei diz que cabe às empresas decidir o período da licença e se será remunerada. Na prática, porém, quase nenhuma mulher deixa de trabalhar por cólica menstrual. Na China, três províncias já adotam a licença -a última, Anhui, desde 1º de março. Nas redes sociais locais, a reação das mulheres foi de ceticismo sobre como isso será encarado por patrões, quase sempre homens. ""Parece que protege nossos direitos, mas no fim vai fazer as coisas piorarem"", escreveu @Woshiyamiedie.",pt,70
189,1394,1465953522,CONTENT SHARED,-3581194288660477595,-709287718034731589,8812735263251701985,,,,HTML,https://www.buzzfeed.com/katienotopoulos/the-end-of-apple-man,the end of apple man,"In the same way that Pinterest, with its made-from-scratch recipes and bespoke home decorating ideas, fills some people with a creeping sense of despair that they lead an inadequate life, Apple advertisements and keynote demos have always made me feel terrible about myself. The prototypical Apple demo person is someone I'll call Apple Man. Apple Man is a 40-something dad who just wants to FaceTime his adorable children while he's on a business trip, and also find a local pourover coffee shop while he's in town. Apple Man has an Apple Watch (obvious). He needs a way to manage his photos of his adorable children and hiking trips with friends. He loves jogging and mountain biking and wants to use his Apple Watch to monitor his workouts, because he LOVES working out. Apple Man is very fit for his age - you can just barely tell he's totally ripped through his light blue, off-the-rack, wrinkle-free button-down shirt. Apple Man has a great head of hair. Apple Man owns his home and wants to be able to open his garage door from his phone to park his family-sensible-yet-sporty-crossover. (He's on the Tesla Model 3 pre-order list.) He wants to make brunch plans, and it would be great if he could add a brunch plan to his calendar app directly from text messages. Apple Man wants to track his health, but of course he has no need for a period tracker. His calendar is full; his inbox is zero. If you're like me, somewhat disorganized and more likely to have a photo roll full of your drunk idiot friends than a white water rafting trip with your children, this feels not really relevant to your life. Or if you're basically anything OTHER than a rich white businessman who loves dim sum and jogging, then this might not feel relevant to you. When the Apple Watch came out it seemed like the ultimate hardware realization of Apple Man. A bulky fashion device with a huge price tag that seemed mostly useful for outdoor running or biking and getting alerted about business meetings. Indeed, some (frankly sketchy) sales analysis suggested that initial buyers of the watch were 80% male (the gender gap is closing, but still very real). But at yesterday's WWDC keynote, Apple announced new features for the Apple Watch that feel like they're designed with someone other than Apple Man in mind. The first one is the wheelchair activity monitor. While this of course, still appeals to people who love exercising (just like Apple Man), it's a feature launch devoted to people with disabilities, presented at a major event. The second feature is the emergency alert system. To me, this seemed so clearly designed for women - a safety alert system for walks home at night or through a deserted parking lot. Safety was one of the features women liked about the Apple Watch to begin with - like being able to call an Uber without taking their phone out of their purse. For women, safety while walking down the street is something we think about pretty much daily, most times we leave the house. Women have long adopted their own safety measures for walking in public: holding their keys a certain way to use as a weapon, carrying pepper spray, checking the backseat of a car before unlocking it, taking a longer route because the streets are brighter and more crowded. This isn't an afterthought or a minor convenience; it's a core user experience of being a woman or person vulnerable to violence. While certainly emergency calls are made by men and women, adding in an emergency alert feature to the wrist feels very obviously designed with women's safety in mind. The bonus feature that the Watch automatically knows what the country-specific version of 911 is if you're traveling addresses something women have been wary of for a long time: safety while traveling alone . Of course, plenty of the other demos for other products at this year's WWDC seemed to have Apple Man in mind - work productivity tools, messaging enhancements for wholesome activities like wishing your niece a happy graduation or planning to buy LCD Soundsystem tickets (Apple Man LOVES LCD Soundsystem). But the effervescent demo of Apple Music by the charismatic Bozoma Saint John - a black woman who looked and acted nothing like the typical Apple Men on stage before her and who in her opening remarks mentioned being a mother - felt like a breath of fresh air signaling that perhaps the winds are changing. There were other signals too. In the video segment cheering on developers using Apple's Swift programming language, the video ended with a black woman joyfully expounding how awesome coding was - certainly not the stereotype of a coder, and not totally reflective of the crowd there watching the video. At another Apple event in March, another female African American Apple executive, Lisa Jackson, took the stage to talk about Apple's environmental efforts. Breaking the Apple Man stereotype in the people who appear on stage as the Apple's evangelists is symbolic. Having a black woman present on stage might just mean the company is more aware of the optics of its events. But there is evidence that it's not just a hollow gesture - the actual features and hardware being announced on stage at Apple events are changing along with those presenters. When Health kit was announced last spring, the main complaint was that a period tracker wasn't included - for many women that is the only kind of health tracking they care about. By the next update that fall, they actually added the feature . The smaller iPhone SE seemed like an admission that not all men and women have giant Trump hands. People who use Apple products are sometimes young, they are women, they have disabilities, they don't work outside the home, they are single, they don't have kids, they are disorganized or just plain lazy; they don't exercise. It's nice to see features made for us . Apple Man isn't dead, not by a long shot. But maybe, hopefully, he's in retreat.",en,70
190,3092,1487154848,CONTENT SHARED,8729086959762650511,7645894863578715801,7395860765143290921,Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:51.0) Gecko/20100101 Firefox/51.0,SP,BR,HTML,https://martinfowler.com/articles/201701-event-driven.html,"what do you mean by ""event-driven""?","Towards the end of last year I attended a workshop with my colleagues in ThoughtWorks to discuss the nature of ""event-driven"" applications. Over the last few years we've been building lots of systems that make a lot of use of events, and they've been often praised, and often damned. Our North American office organized a summit, and ThoughtWorks senior developers from all over the world showed up to share ideas. The biggest outcome of the summit was recognizing that when people talk about ""events"", they actually mean some quite different things. So we spent a lot of time trying to tease out what some useful patterns might be. This note is a brief summary of the main ones we identified. Event Notification This happens when a system sends event messages to notify other systems of a change in its domain. A key element of event notification is that the source system doesn't really care much about the response. Often it doesn't expect any answer at all, or if there is a response that the source does care about, it's indirect. There would be a marked separation between the logic flow that sends the event and any logic flow that responds to some reaction to that event. Event notification is nice because it implies a low level of coupling, and is pretty simple to set up. It can become problematic, however, if there really is a logical flow that runs over various event notifications. The problem is that it can be hard to see such a flow as it's not explicit in any program text. Often the only way to figure out this flow is from monitoring a live system. This can make it hard to debug and modify such a flow. The danger is that it's very easy to make nicely decoupled systems with event notification, without realizing that you're losing sight of that larger-scale flow, and thus set yourself up for trouble in future years. The pattern is still very useful, but you have to be careful of the trap. A simple example of this trap is when an event is used as a passive-aggressive command. This happens when the source system expects the recipient to carry out an action, and ought to use a command message to show that intention, but styles the message as an event instead. An event need not carry much data on it, often just some id information and a link back to the sender that can be queried for more information. The receiver knows something has changed, may get some minimal information on the nature of the change, but then issues a request back to the sender to decide what to do next. Event-Carried State Transfer This pattern shows up when you want to update clients of a system in such a way that they don't need to contact the source system in order to do further work. A customer management system might fire off events whenever a customer changes their details (such as an address) with events that contain details of the data that changed. A recipient can then update it's own copy of customer data with the changes, so that it never needs to talk to the main customer system in order to do its work in the future. An obvious down-side of this pattern is that there's lots of data schlepped around and lots of copies. But that's less of a problem in an age of abundant storage. What we gain is greater resilience, since the recipient systems can function if the customer system is becomes unavailable. We reduce latency, as there's no remote call required to access customer information. We don't have to worry about load on the customer system to satisfy queries from all the consumer systems. But it does involve more complexity on the receiver, since it has to sort out maintaining all the state, when it's usually easier just to call the sender for more information when needed. Event-Sourcing The core idea of event sourcing is that whenever we make a change to the state of a system, we record that state change as an event, and we can confidently rebuild the system state by reprocessing the events at any time in the future. The event store becomes the principal source of truth, and the system state is purely derived from it. For programmers, the best example of this is a version-control system. The log of all the commits is the event store and the working copy of the source tree is the system state. Event-sourcing introduces a lot of issues, which I won't go into here, but I do want to highlight some common misconceptions. There's no need for event processing to be asynchronous, consider the case of updating a local git repository - that's entirely a synchronous operation, as is updating a centralized version-control system like subversion. Certainly having all these commits allows you to do all sorts of interesting behaviors, git is the great example, but the core commit is fundamentally a simple action. Another common mistake is to assume that everyone using an event-sourced system should understand and access the event log to determine useful data. But knowledge of the event log can be limited. I'm writing this in an editor that is ignorant of all the commits in my source tree, it just assumes there is a file on the disk. Much of the processing in an event-sourced system can be based on a useful working copy. Only elements that really need the information in the event log should have to manipulate it. We can have multiple working copies with different schema, if that helps; but usually there should be a clear separation between domain processing and deriving a working copy from the event log. When working with an event log, it is often useful to build snapshots of the working copy so that you don't have to process all the events from scratch every time you need a working copy. Indeed there is a duality here, we can look at the event log as either a list of changes, or as a list of states. We can derive one from the other. Version-control systems often mix snapshots and deltas in their event log in order to get the best performance. [1] Event-sourcing has many interesting benefits, which easily come to mind when thinking of the value of version-control systems. The event log provides a strong audit capability (accounting transactions are an event source for account balances). We can recreate historic states by replaying the event log up to a point. We can explore alternative histories by injecting hypothetical events when replaying. Event sourcing make it plausible to have non-durable working copies, such as a Memory Image . Event sourcing does have its problems. Replaying events becomes problematic when results depend on interactions with outside systems. We have to figure out how to deal with changes in the schema of events over time. Many people find the event processing adds a lot of complexity to an application (although I do wonder if that's more due to poor separation between components that derive a working copy and components that do the domain processing). CQRS Command Query Responsibility Segregation (CQRS ) is the notion of having separate data structures for reading and writing information. Strictly CQRS isn't really about events, since you can use CQRS without any events present in your design. But commonly people do combine CQRS with the earlier patterns here, hence their presence at the summit. The justification for CQRS is that in complex domains, a single model to handle both reads and writes gets too complicated, and we can simplify by separating the models. This is particularly appealing when you have a difference in access patterns, such as lots of reads and very few writes. But the gain for using CQRS has to be balanced against the additional complexity of having separate models. I find many of my colleagues are deeply wary of using CQRS, finding it often misused. Making sense of these patterns As a sort of software botanist, keen to collect samples, I find this a tricky terrain. The core problem is confusing the different patterns. On one project the capable and experienced project manager told me that event sourcing had been a disaster - any change took twice the work to update both the read and write models. Just in that phrase I can detect a potential confusion between event-sourcing and CQRS - so how can I figure out which was culprit? The tech lead on the project claimed the main problem was lots of asynchronous communications, certainly a known complexity-booster, but not one that's a necessary part of either event-sourcing or CQRS. Furthermore we have to beware that all these patterns are good in the right place and bad when put on the wrong terrain. But it's hard to figure out what the right terrain is when we conflate the patterns. I'd love to write some definitive treatise that sorts all this confusion out, and gives solid guidelines on how to do each pattern well, and when it should be used. Sadly I don't have the time to do it. I write this note in the hope it will be useful, but am quite aware that it falls well short of what is really needed.",en,70
191,2253,1472785321,CONTENT SHARED,-6523871595334455509,-534549863526737439,-6893819804532920307,,,,HTML,https://research.google.com/pubs/pub45542.html,"csp is dead, long live csp! on the insecurity of whitelists and the future of content security policy","Content Security Policy is a web platform mechanism designed to mitigate cross-site scripting (XSS), the top security vulnerability in modern web applications. In this paper, we take a closer look at the practical benefits of adopting CSP and identify significant flaws in real-world deployments that result in bypasses in 94.72% of all distinct policies.",en,69
192,2026,1470833640,CONTENT SHARED,1734266821019430183,2416280733544962613,-5892805645558108633,,,,HTML,http://www.economist.com/news/business/21703428-chinas-wechat-shows-way-social-medias-future-wechats-world,wechat's world,"YU HUI, a boisterous four-year-old living in Shanghai, is what marketing people call a digital native. Over a year ago, she started communicating with her parents using WeChat, a Chinese mobile-messaging service. She is too young to carry around a mobile phone. Instead she uses a Mon Mon, an internet-connected device that links through the cloud to the WeChat app. The cuddly critter's rotund belly disguises a microphone, which Yu Hui uses to send rambling updates and songs to her parents; it lights up when she gets an incoming message back. Like most professionals on the mainland, her mother uses WeChat rather than e-mail to conduct much of her business. The app offers everything from free video calls and instant group chats to news updates and easy sharing of large multimedia files. It has a business-oriented chat service akin to America's Slack. Yu Hui's mother also uses her smartphone camera to scan the WeChat QR (quick response) codes of people she meets far more often these days than she exchanges business cards. Yu Hui's father uses the app to shop online, to pay for goods at physical stores, settle utility bills and split dinner tabs with friends, just with a few taps. He can easily book and pay for taxis, dumpling deliveries, theatre tickets, hospital appointments and foreign holidays, all without ever leaving the WeChat universe. As one American venture capitalist puts it, WeChat is there ""at every point of your daily contact with the world, from morning until night"". It is this status as a hub for all internet activity, and as a platform through which users find their way to other services, that inspires Silicon Valley firms, including Facebook, to monitor WeChat closely. They are right to cast an envious eye. People who divide their time between China and the West complain that leaving WeChat behind is akin to stepping back in time. Among all its services, it is perhaps its promise of a cashless economy, a recurring dream of the internet age, that impresses onlookers the most. Thanks to WeChat, Chinese consumers can navigate their day without once spending banknotes or pulling out plastic. It is the best example yet of how China is shaping the future of the mobile internet for consumers everywhere. That is only fitting, for China makes and puts to good use more smartphones than any other country. More Chinese reach the internet via their mobiles than do so in America, Brazil and Indonesia combined. Many leapt from the pre-web era straight to the mobile internet, skipping the personal computer altogether. About half of all sales over the internet in China take place via mobile phones, against roughly a third of total sales in America. In other words, the conditions were all there for WeChat to take wing: new technologies, business models built around mobile phones, and above all, customers eager to experiment. The service, which is known on the mainland as Weixin, began five years ago as an innovation from Tencent, a Chinese online-gaming and social-media firm. By now over 700m people use it, and it is one of the world's most popular messaging apps (see chart). More than a third of all the time spent by mainlanders on the mobile internet is spent on WeChat. A typical user returns to it ten times a day or more. WeChat has worked hard to make sure that its product is enjoyable to use. Shaking the phone has proven a popular way to make new friends who are also users. Waving it at a television allows the app to recognise the current programme and viewers to interact. A successful stunt during last year's celebration of Chinese New Year's Eve saw CCTV, the official state broadcaster, offer millions of dollars in cash rewards to WeChat users who shook their phones on cue. Punters did so 11 billion times during the show, with 810m shakes a minute recorded at one point. Most importantly, over half of WeChat users have been persuaded to link their bank cards to the app. That is a notable achievement given that China's is a distrustful society and the internet is a free-for-all of cybercrime, malware and scams. Yet using its trusted brand, and putting to work robust identity and password authentication, Tencent was able to win over the public. In contrast, Western products such as Snapchat and WhatsApp have yet to persuade consumers to entrust them with their financial details. Japan's Line (which recently floated shares on the New York and Tokyo stock exchanges) and South Korea's KakaoTalk (in which Tencent is a big investor) have done better, but they cannot match the Chinese platform. One app to rule them all How did Tencent take WeChat so far ahead of its rivals? The answer lies partly in the peculiarities of the local market. Unlike most Westerners, many Chinese possessed multiple mobile devices, and they quickly took to an app that offered them an easy way to integrate them all into a single digital identity. In America messaging apps had a potent competitor in the form of basic mobile-phone plans, which bundled in SMS messaging. But text messages were costly in China, so consumers eagerly adopted the free messaging app. And e-mail never took off on the mainland the way it has around the world, mainly because the internet came late; that left an opening for messaging apps. But the bigger explanation for WeChat's rise is Tencent's ability to innovate. Many Chinese grew up using QQ, a PC-based messaging platform offered by Tencent that still has over 800m registered users. QQ was a copy of ICQ, a pioneering Israeli messaging service. But then the Chinese imitator learned to think for itself. Spotting the coming rise of the mobile internet, Tencent challenged several internal teams to design and develop a smartphone-only messaging app. The QQ insiders came up with something along the lines of their existing product for the PC, but another team of outsiders (from a just-acquired firm) came up with Weixin. When Tencent launched the new app, it made it easy for QQ's users to transfer their contacts over to the new app. Another stroke of brilliance came two years ago when the service launched a ""red packet"" campaign in which WeChat users were able to send digital money to friends and family to celebrate Chinese New Year rather than sending cash in a red envelope, as is customary. It was clever of the firm to turn dutiful gift-giving into an exciting game, notes Connie Chan of Andreessen Horowitz, a VC firm. It also encouraged users to bind together into groups to send money, often in randomised amounts (if you send 3,000 yuan to 30 friends, they may not get 100 yuan each; WeChat decides how much). That in turn led to explosive growth in group chats. This year, over 400m users (both as individuals and in groups) sent 32 billion packets of digital cash during the celebration. The enthusiasm with which WeChat users have adopted the platform makes them valuable to Tencent in ways that rivals can only dream of. After years of patient investment, its parent now earns a large and rising profit from WeChat. While other free messaging apps struggle to bring in much money, Duncan Clark of BDA, a technology consultancy in Beijing, estimates that WeChat earned about $1.8 billion in revenues last year. By the reckoning of HSBC, a bank, according to current valuations for tech firms, WeChat could be worth over $80 billion already. Over half of its revenues come from online games, where Tencent, the biggest gaming firm, is extremely strong. E-commerce is another driver of the business model. The firm earns fees when consumers shop at one of the more than 10m merchants (including some celebrities) that have official accounts on the app. Once users attach their bank cards to WeChat's wallet, they typically go on shopping sprees involving far more transactions per month than, for instance, Americans make on plastic. Three years ago, very few people bought things using WeChat but now roughly a third of its users are making regular e-commerce purchases directly though the app. A virtuous circle is operating: as more merchants and brands set up official accounts, it becomes a buzzier and more appealing bazaar. Users' dependence on the portal means a treasure-trove of insights into their preferences and peccadilloes. That, in turn, makes WeChat much more valuable to advertisers keen to target consumers as precisely as possible. There are few firms better placed to take advantage of the rise of social mobile advertising than WeChat, reckons Goldman Sachs, an investment bank. When BMW, a German carmaker, launched the first-ever ad to appear on the WeChat Moments page (which is akin to a Facebook feed) of selected users, there followed nothing like pique at the commercial intrusion, but rather an uproar from people demanding to know why they had not received the ad. Even though Tencent has deliberately trodden carefully in introducing targeted ads on users' Moments pages, its official corporate accounts enjoy billions of impressions each day. For Western firms, the most telling lesson from WeChat's success is that consumers and advertisers will handsomely reward companies that solve the myriad problems that bedevil the mobile internet. The smartphone is a marvellous invention, but it can be frustrating. In much of the world, there are too many annoying notifications and updates and the proliferation of apps is baffling. WeChat provides an answer to these problems. Better-known rivals in the West regard WeChat's rise with more than a tinge of jealousy. One executive, David Marcus, who runs Facebook Messenger, a popular messaging app run by the social network, is willing to talk about it openly. He calls WeChat, simply, ""inspiring"". His plan, to transform Messenger into a platform where people can communicate with businesses and buy things, sounds familiar. Even enthusiasts acknowledge that the mobile ecosystem is different in the West and that WeChat's reach and primacy in the eyes of consumers will not be easily replicated. It took off in China well before the app ecosystem had taken hold, as it has now in America and Europe. Western consumers are accustomed to using many different apps to access the internet, not just one. It would require a lot of nudging to encourage use of a single, central hub. Nor is there much chance that Facebook could make a significant dent in WeChat's dominance in China. The Silicon Valley darling enjoys incumbency and the network effect in many of its markets. That has sabotaged WeChat's own efforts to expand abroad (despite splashy ad campaigns featuring Lionel Messi, a footballer). But the same rule applies if Facebook enters China, which could happen this year or next. ""We have the huge advantage of incumbency and local knowledge,"" says an executive at Tencent. ""Weixin is quite simply more of a super-app than Facebook."" Indeed, WeChat has already proved itself in the teeth of competition. Many Chinese champions have succeeded only because the government has hobbled domestic rivals and blocked foreign entrants. Here, too, Tencent breaks the mould. It has withstood numerous attempts by Alibaba, a formidable local rival, to knock it and its creations off their perch. And it is Facebook's WhatsApp that is WeChat's most obvious rival. Unlike Facebook itself, and Twitter, both of which are blocked on the mainland, WhatsApp is free to operate. WeChat has flourished for simple, commercial reasons: it solves problems for its users, and it delights them with new and unexpected offerings. That will change the mobile internet for everyone-those outside China included, as Western firms do their all to emulate its success.",en,69
193,439,1460727723,CONTENT SHARED,6583734846225935852,-2623844842607044962,7622703213275261749,,,,HTML,https://macuserbrasil.wordpress.com/2016/04/01/no-ano-em-que-a-apple-completa-40-anos-confira-a-evolucao-do-iphone/,"no ano em que a apple completa 40 anos, confira a evolução do iphone","Nesse ano em que a firma de Cupertino completou 40 anos de existência, amada por muitos, odiada por alguns. Ninguém pode negar que ela mudou a história no ramo da tecnologia, uma empresa que começou em uma garagem de casa, por 2 gênios: Steve Jobs e Steve Wosniak. Ela foi fundada no dia 1 de abril de 1976, revolucionou o segmento de computador pessoal, tocadores de música, notebooks e eu acho que nem preciso citar as marcas que já estão eternizada na mente dos seus usuários. Porém, uma criação dela, merece um super infográfico, o blockbuster iPhone. Nunca um aparelho de telefone, ou melhor smartphone, mudou e influenciou tanto o ramo da telefonia móvel como o aparelho da Apple. Quando ele foi lançado em 2007 , não existia um aparelho com uma tela tão grande como o do iPhone 2G/Classic/Original com os seus 3,5 polegadas de tela , um dos primeiros a implementar a tela sensível ao toque, efetuar ligações telefônicas e o principal, navegar na internet , que convenhamos foi o grande diferencial, porque a partir daí, todos os outros fabricantes de smartphones praticamente seguiram a mesma trilha iniciada pelo iPhone, com telas maiores, acesso a internet, etc. O resto da história você já sabe, mas como recordar é viver, confira um infográfico bem interessante a seguir: Bacana né?!&#x1f609; via tecnologiauol",pt,69
194,2119,1471623409,CONTENT SHARED,-2440247087447106971,-7611460419696903236,8699291783730431926,,,,HTML,http://startupi.com.br/2016/08/itau-segue-tendencia-das-fintechs-e-se-torna-primeiro-banco-tradicional-permitir-abertura-de-contas-por-app/,itaú segue tendência das fintechs e se torna primeiro banco tradicional a permitir abertura de contas por app - startupi,"O banco Itaú lançou esta semana o Abreconta , aplicativo que permite abertura de contas de forma 100% digital. Assim como acontece com outras fintechs que não têm agências físicas, no app é possível enviar todos os dados para se tornar novo correntista, inclusive enviar os documentos necessários por foto, pelo próprio celular. A novidade foi anunciada por Marco Bonomi, diretor de varejo do Itaú Unibanco, em evento da Fenabrave. Por enquanto, o aplicativo está disponível apenas para smarphones iOS, mas em breve usuários de Android também poderão baixar. ""O Itaú já iniciou os testes com um grupo de clientes para oferecer a possibilidade de abertura de conta corrente de forma 100% online. O banco já disponibilizou o programa em uma loja de aplicativos e em breve o serviço estará disponível"", disse o diretor para o Startupi. Segundo informações do Estadão Conteúdo, com este lançamento o Itaú se torna o primeiro grande banco a seguir a resolução do Banco Central nº 4.480, que permite que contas-corrente passem a ser abertas por meio eletrônico, sem a necessidade de comparecer a uma agência bancária. Segundo Marco Bonomi, operações do banco junto a clientes digitais já somam cerca de 45% do resultado do varejo do Itaú, o que torna este segmento o principal gerador de lucro do banco.",pt,69
195,2245,1472750564,CONTENT SHARED,90383487344892230,7645894863578715801,535059460675595168,,,,HTML,http://www.kennybastani.com/2016/08/strangling-legacy-microservices-spring-cloud.html,building spring cloud microservices that strangle legacy systems,"The method that I explained above came to me about a year after completing a greenfield microservices project on a similar architecture. The project would be a pilot for building microservices that would extend legacy components of a retail banking platform-a system that was already serving millions of users in production. The result of the project was a success, as we realized the direct benefits of being agile with the microservices approach . While we were able to deliver business differentiating features quickly, our speed to market came at the cost of tightly coupling microservices to the existing components of the legacy system. There were a few factors that required us to create this tight coupling. We shackled ourselves into vertically scaled infrastructure provisioned in a private data center We didn't have a platform that supported cloud-native application development We didn't have a self-service tool in place to automate provisioning of databases for new microservices Because of these factors, we had to use the legacy system's large shared database for persistence by our new microservices. We would use database access control features to isolate our microservice's tables from being directly accessed by other applications. Even though these access features are for multitenancy , it would allow us to migrate the schema easily to a separate database at a later time. The fundamental issue with this approach was that it took us seven months to get the first microservice release into production. The early dependency on the shared database posed too much of a risk of impacting millions of production users. We realized that risk when we discovered a framework defect that caused our new microservices to be unable to release database cursors when undergoing stress testing in the performance environment. The lesson learned in this experience was an important one. A new microservice should encapsulate both the unit of service and the unit of failure -in production-on the very first day of development. When I say unit of service and unit of failure I am referring to a quote by storied computer scientist, Jim Gray. Gray wrote a technical report in 1985 titled Why Do Computers Stop and What Can Be Done About It? In the report, Gray talks about how to achieve fault-tolerance in software. As with hardware, the key to software fault-tolerance is to hierarchically decompose large systems into modules, each module being a unit of service and a unit of failure. A failure of a module does not propagate beyond the module. When I hear thought leaders talk about microservices and say that the ideas are not new, I always think back to this quote by Jim.",en,69
196,584,1461619724,CONTENT SHARED,7933360486658437274,-2626634673110551643,-5804099311266469071,,,,HTML,http://www.coindesk.com/brazils-bank-itau-blockchain-consortium-r3/,brazil's bank itaú joins r3 blockchain consortium - coindesk,"São Paulo-based Itaú Unibanco has become the first Latin America-based bank to join blockchain and distributed ledger consortium R3CEV. Launching last September with nine global banking partners, Itaú is the 45th global bank to join the R3 consortium, following South Korea's Hana Financial and Japan's SBI Holdings . Itaú saw R$21.9 billion ($6.1bn) in profits in 2014 as well as R$360 billion ($101bn) in assets under management, according to its latest annual report . In statements, Itaú general director for technology and operations Márcio Schettini said that Itaú joined to contribute to what he called the ""international drive"" toward building distributed ledger solutions for enterprise finance. Schettini said: ""We are convinced that these innovations will bring benefits to our customers and real gains in efficiency for the sector as a whole."" Notably, Itaú is not the only bank in R3 to offer services to Latin America. Group Santander and BBVA, for instance, serve Argentina, Brazil and Mexico, through franchise or subsidiary efforts, while HSBC had more than 60 branches in Latin America as of 2012. However, the addition comes amid a broader push by R3 to expand its membership beyond traditional banks, and as more regional banks join the growing global effort.",en,69
197,2436,1474946849,CONTENT SHARED,-2069509552243850466,3609194402293569455,-1454522461531275738,,,,HTML,https://msdn.microsoft.com/pt-br/virtualization/windowscontainers/quick_start/quick_start_windows_10,contêiner do windows no windows 10,"O exercício vai guiá-lo pela implantação e uso básicos do recurso de contêiner do Windows no Windows 10 Professional ou Enterprise (Anniversary Edition). Após a conclusão, você terá instalado a função de contêiner e implantado um contêiner do Hyper-V simples. Antes de começar esse início rápido, familiarize-se com a terminologia e os conceitos básicos do contêiner. Essas informações podem ser encontradas na Introdução rápida . Este início rápido é específico para contêineres do Hyper-V no Windows 10. É possível encontrar documentação adicional de início rápido no sumário à esquerda desta página. Pré-requisitos: Um sistema de computador físico executando o Windows 10 Anniversary Edition (Professional ou Enterprise). Esse início rápido pode ser executado em uma máquina virtual do Windows 10, porém a virtualização aninhada precisará ser habilitada. É possível encontrar mais informações no Guia de virtualização aninhada . 1. Instalar o recurso de contêiner O recurso de contêiner deve ser habilitado antes de trabalhar com contêineres do Windows. Para fazer isso, execute o seguinte comando em uma sessão do PowerShell com privilégios elevados. Como o Windows 10 dá suporte apenas a contêineres de Hyper-V, o recurso Hyper-V também deve ser habilitado. Para habilitar o recurso Hyper-V usando o PowerShell, execute o comando a seguir em uma sessão do PowerShell com privilégios elevados. Quando a instalação for concluída, reinicialize o computador. Após o backup ter sido realizado, execute o seguinte comando para corrigir um problema conhecido com os Contêineres do Windows no Windows 10. Nas versões atuais, você precisa desabilitar OpLocks para usar contêineres confiáveis do Hyper-V. Para reabilitar OpLocks, use o seguinte comando: Set-ItemProperty -Path 'HKLM:SOFTWARE\Microsoft\Windows NT\CurrentVersion\Virtualization\Containers' -Name VSmbDisableOplocks -Type DWord -Value 0 -Force 2. Instalar o Docker O Docker é necessário para trabalhar com contêineres do Windows. O Docker é composto pelo mecanismo do Docker e o cliente do Docker. Para este exercício, ambos serão instalados. Execute os comandos a seguir para fazer isso. Baixe o mecanismo e o cliente Docker como um arquivo zip. Expanda o arquivo zip em Arquivos de Programas, o conteúdo do arquivo já está no diretório do docker. Adicione o diretório do Docker ao caminho do sistema. Reinicie a sessão do PowerShell para que o caminho modificado seja reconhecido. Para instalar o Docker como um serviço Windows, execute o seguinte. Após ser instalado, o serviço pode ser iniciado. 3. Instalar imagens de contêiner base Os contêineres do Windows são implantados por meio de modelos ou imagens. Antes de ser possível implantar um contêiner, uma imagem do sistema operacional base do contêiner precisa ser baixada. Os comandos a seguir baixarão a imagem base do Nano Server. Faça o pull da imagem base do Nano Server. Assim que é feito o pull da imagem, executar docker images retornará uma lista de imagens instaladas, neste caso, a imagem do Nano Server. Para obter informações detalhadas sobre imagens de contêiner do Windows, consulte Gerenciar imagens de contêiner . 4. Implantar o primeiro contêiner Para este exemplo simples, uma imagem de contêiner ""Hello World"" será criada e implantada. Para obter a melhor experiência, execute estes comandos em um shell CMD do Windows elevado. Primeiro, inicie um contêiner com uma sessão interativa da imagem nanoserver . Depois que o contêiner for iniciado, você receberá um shell de comando de dentro do contêiner. Dentro do contêiner, criaremos um script 'Hello, World' simples. Quando tiver concluído, saia do contêiner. Agora, você criará uma nova imagem de contêiner do contêiner modificado. Para ver uma lista de contêineres, execute o seguinte e anote a ID do contêiner. Execute o seguinte comando para criar uma nova imagem 'HelloWorld'. Substitua com a ID do seu contêiner. Quando tiver concluído, você terá uma imagem personalizada que contém o script hello world. Isso pode ser visto com o seguinte comando. Finalmente, para executar o contêiner, use o comando docker run . O resultado desse comando docker run é que um contêiner do Hyper-V foi criado por meio da imagem 'HelloWorld', um script 'Hello World' de exemplo foi executado (saída ecoada para o shell) e, em seguida, o contêiner foi parado e removido. Os inícios rápidos de contêiner e Windows 10 subsequentes se aprofundarão sobre a criação e implantação de aplicativos em contêineres no Windows 10. Próximas etapas Contêineres do Windows no Windows Server",pt,69
198,2912,1482888269,CONTENT SHARED,-1572252285162838958,9210530975708218054,-7673376373070890422,"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36",SP,BR,HTML,https://www.lpi.org/devops,lpic-ot devops engineer | linux professional institute,"As more and more companies introduce DevOps methodology to their workflows, skills in using tools supporting the collaboration model of DevOps become increasingly important. Focussing on the most relevant DevOps tools, LPIC-OT DevOps Engineers will be able to implement a DevOps workflow and to optimize their daily administration and development tasks.",en,68
199,2791,1480444518,CONTENT SHARED,1873952032843878490,1374824663945909617,6069689568663799842,"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.100 Safari/537.36",SP,BR,HTML,https://blog.daftcode.pl/hype-driven-development-3469fc2e9b22?gi=e33d58da65a1,hype driven development,"Software development teams often make decisions about software architecture or technological stack based on inaccurate opinions, social media, and in general on what is considered to be ""hot"", rather than solid research and any serious consideration of expected impact on their projects. I call this trend Hype Driven Development, perceive it harmful and advocate for a more professional approach I call ""Solid Software Engineering"". Learn more about how it works and find out what you can do instead. New technology - new hope Have you seen it? A team picking newest, hottest technology to apply in the project. Someone reads a blog post, it's trending on Twitter and we just came back from a conference where there was a great talk about it. Soon after, the team starts using this new shiny technology (or software architecture design paradigm), but instead of going faster (as promised) and building a better product they get into trouble. They slow down, get demotivated, have problems delivering next working version to production. Some teams even keep fixing bugs instead of delivering new features. They need 'just a few more days' to sort it all out. Hype Driven Development Hype Driven Development (HDD) has many flavors and touches your project in many different ways: Reddit driven development  - when a team or individual decide on technology/architecture/design based on what popular blogger wrote or what is hot on reddit, hackernews, blogs twitter, facebook, GitHub or other social media. Conference driven development  - watch carefully what happens after people are back from conference. People get inspired. And that's a two-edged sword. Starting to use newest hottest lib/framework/architecture paradigm without enough research might be a highway to hell. Loudest guy driven decisions  - is when one guy is talking all the time about this new framework/lib/tech's that he has no experience with, but talks about it all the time and finally the team decides to use it. Gem/lib/plugin driven development  - a specially strong in Ruby On Rails community, where occasionally I can see a Gemfile so long that the only thing longer is the time it takes to load the app. It comes from the idea that every problem in rails should be solved with a gem. Sometimes it would take a couple of lines to build solution ourselves. But we're just solving problems by adding libs, plugins, gems or frameworks. I would also mention here behavior popular among hype driven developers -  Stack Overflow driven development  - when developers copy-paste solutions from Stackoverflow (or in general from the internet) without really understanding them. HDD is how teams bring doom on themselves The problem with hype is that it easily leads to bad decisions. Both bad architectural decisions and technological stack decisions often hunt a team months or even years later . In worst case they may lead to another very problematic situation in software engineering: The Big Rewrite . Which almost never works out. The root of all evil seems to be social media  - where new ideas spread much faster than they get tested.Muchfaster than people are able to understand their pros and cons. The Anatomy of Hype Most hypes have similar structure. Here it goes: Step 1: Real problem and solution They start in some company with a problem. A team within some company decides that solution to the problem is beyond the current technological stack, process or architecture. The company creates a new framework, library or paradigm and soon the problem is solved. Step 2: Announcement, buzz and keywords The team is excited to show their work to the rest of the world and soon they write blog posts and do talks on conferences. The problem oftentimes is non-trivial, so they are proud to present the impressive results of a non-trivial solution. People get excited about the new technology. The only problem is not everybody who gets excited is able to fully understand what the exact problem was and all the details of the solution. It was a non-trivial problem with a non-trivial solution after all. Takes more than a tweet, chit-chat or even blog post to explain. With communication tools like social media, blog posts and conference lightning talks the message gets blurred along the way. Step 3: Mania starts All shades of hype driven developers read blog posts and attend conferences. Soon the teams all over the world start using the new technology. Due to the blurred message - some of them make hasty decision to use framework even though it does not solve any of their actual problems. Yet the team does have expectation that this new technology will help. Step 4: Disappointment As the sprints go by, the technology does not improve the team's life as much as people hoped, but brings a lot of extra work. There's a lot of rewriting the code and extra learning for the team. Teams slow down, management gets pissed off. People feel cheated. Step 5: Realisation! Finally the team does retrospection and realizes what are the tradeoffs of the new technology and for what purpose it would be more relevant. They get wiser... till the next hype shows up. Examples of Hype: Let's examine some examples of hypes and see how those went through. Example 1: React.js Step 1: Facebook has a problem - advanced one page apps like Facebook itselves have, so many state changing events that it is hard to keep track what's going on and keep the application state consistent. Step 2: Facebook promotes new paradigm with buzzwords: functional, virtual DOM, components. Step 3: Mania: Facebook has created the front-end framework of the future! Let's write everything in react from now on! Step 4: Wait there is a lot of work, but no quick return on investment! Step 5: React is great for advanced one page app with lots of real-time notifications, but does not necessarily pay off for simpler applications. Example 2: TDD is dead by DHH Step 1: David Heinemeier Hansson (DHH, creator of Ruby on Rails framework) realises that it is hard to do TDD with Rails as this framework doesn't have architecture supporting good OOP. Makes a pragmatic choice - not to write tests upfront. Step 2: Hype starts with DHH blog post and conference talk . Hype keywords: TDD is DEAD. Step 3: Let's skip tests! Our Guru says so. We didn't write them anyway. Now we're at least not pretending. We're finally honest. Step 4: Wait! Even fewer things work now than before. We've built a buggy code. Step 5: ""TDD is not dead or alive. TDD is subject to tradeoffs, including risk of API changes, skill of practitioner and existing design"" - Kent Beck. Example 3: Microservices Step 1: Big monolith application scales hard. There is a point when we can break them down into services. It will be easier to scale in terms of req/sec and easier to scale across multiple teams. Step 2: Hype keywords: scalability, loose coupling, monolith. Step 3: Let's rewrite all to services! We have a 'spaghetti code' because we have a monolith architecture! We need to rewrite everything to microservices! Step 4: Shit! It is now way slower to develop the app, difficult to deploy and we spend a lot of time tracking bugs across multiple systems. Step 5: Microservices require a lot of devops skills in the team and with right investment might pay off as a way to scale the system and team. Before you reach serious scale issues it's an overinvestment. Microservices are extracted not written. You must be this tall to use microservices. Example 4: NoSQL Step 1: SQL databases have problems with high loads and unstructured data. Teams around the world start developing new generation of databases. Step 2: Hype keywords: Scalability, BigData, High Performance. Step 3: Our database is too slow and not big enough! We need NoSql! Step 4: We need to join tables? That is a no go. Simple SQL operations are becoming increasingly challenging. Development is slow and our core problems are not solved. Step 5: NoSql are tools to solve very specific problems (either extremely high volumes of data, unstructured data or very high load). SQL is actually a great tool and handles high load and huge data volumes well if used skillfully. The case for NoSql is still pretty rare in 2016. Example 5: Elixir and Phoenix (or put your favorite lang/framework pair here) Step 1: Web frameworks like Ruby On Rails don't deal well with with high performance applications, distributed applications and websockets. Step 2: Hype keywords: Scalability, High Performance, Distributed, Fault-tolerant. Step 3: Oh my good, our application is slow and our chat is not scalable! Step 4: Wow, learning functional programming and distributed approach is not that easy. We are now really slow. Step 5: Elixir and Phoenix is great framework, but takes significant effort to learn. It will pay back in a long run if you need specifically high performance app. The list goes on and on: In this crowded space of computer engineering we have a lot of areas where hypes are common. In JavaScript world new frameworks are born everyday. Node.js (keywords: event programming), reactive programming, Meteor.js (keywords: shared state), front-end MVC, React.js. You name it. In software engineering new architectures are born: Domain Driven Development, Hexagon, DCI. What is your favorite hype?",en,68
200,1542,1467035159,CONTENT SHARED,6426224004219108539,6735372008307093370,-6487028072647062097,,,,HTML,http://g1.globo.com/sp/campinas-regiao/noticia/2016/06/mesmo-sem-apoio-parada-lgbt-de-campinas-arrasta-multidao-pelas-ruas.html,"mesmo sem apoio, parada lgbt de campinas arrasta multidão pelas ruas","Multidão lota as ruas do centro de Campinas durante a realização da 16ª Parada do Orgulho LGBT (Foto: Renata Victal/G1) Apesar da falta de apoio financeiro da Prefeitura e de uma recomendação do Ministério Público para que a Polícia Militar não apoiasse o evento, a 16ª Parada do Orgulho LGBT (Lésbicas, Gays, Bissexuais, Travestis, Transexuais e Transgêneros) arrastou uma multidão pelas ruas do centro da cidade na tarde deste domingo (26). Confira as fotos da 16ª Parada do Orgulho LGBT de Campinas. ""Mesmo sem apoio, estamos aqui, lutando pelas nossas causas, contra qualquer tipo de preconceito. Muitos homosexuais são espancados e mortos a cada ano. Não podemos fazer de conta que o preconceito não existe"", destacoua drag queen Heloá Meireles. Look branco para celebrar a paz na 16ª edição da Parada LGBT (Foto: Renata Victal/G1) No começo do evento, os organizadores fizeram um minuto de silêncio em memória às vítimas do ataque à boate Pulse , em Orlando, no estado da Flórida (EUA) e também um protesto contra a falta de apoio da Prefeitura, da Polícia Militar e do Ministério Público. ""Entregamos toda a documentação em fevereiro, fizemos várias reuniões e faltando uma semana para o evento disseram que não dariam mais o apoio. Mas o povo veio mostrar que nada nos intimida"", ressaltou um dos organizadores da parada, Douglas Holanda. A Presidente da Comissão da Diversidade da Ordem dos Advogados do Brasil de Campinas , Ana Carolina Camargo de Oliveira, classificou a falta de apoio como um retrocesso. ""O Ministério Público fez um ofício pedindo que não apoiassem alegando falta de segurança, mas, como não houve uma ordem judicial, conversei com os organizadores e garanti a eles que a parada poderia acontecer. Esta é uma posição contrária ao que vem acontecendo em todo o mundo. Somos todos seres humanos e lutamos por uma causa. Essa falta de apoio foi um retrocesso. A homofobia ainda é tipificada como um crime simples"", lamentou Ana. Douglas Holanda, organizador Transfobia Nesta edição, a parada teve como tema ""Diga sim à educação e não à transfobia. Intolerância: o vírus mais assassino. Contra qualquer forma de opressão"", um alerta a qualquer tipo de intolerância, explica um dos organizadores da parada, Douglas Holanda. ""É preciso falar sobre o preconceito contra as transexual. Muita gente ainda tem certo receio de se aproximar de uma trans. Nossa luta é diária. As coisas mudaram muito, mas ainda temos alguns preconceituosos que nos atacam. Fizemos nosso percurso em paz, lembrando que nós representaremos as 49 vidas de Orlando, assim como todas as que se vão diariamente no Brasil e no mundo"", registrou Douglas Pastor Thiago Carlos dos Santos discursou pela igualdade. (Foto: Renata Victal/G1) Percurso e fé Este ano a concentração da 16ª edição da Parada foi ao lado do Fórum, na Avenida Dr. Campos Sales. De lá, a multidão subiu a Avenida Francisco Glicério até Dr. Moraes Sales, seguiu até o cruzamento com a Rua Irmã Serafina, continuando pela Avenida Anchieta até a Avenida Benjamin Constant. Ao retornarem à Avenida Francisco Glicério, o grupo seguiu até o Largo do Rosário. Como nos anos anteriores,o pastor Thiago Carlos dos Santos, da igreja Cidade do Refúgio, uma comunidade cristã inclusiva, usou o microfone para pregar contra a homofobia. Segundo ele, é importante que todos saibam que podem frequentar a igreja sem qualquer receio de ser vítima de preconceito. ""Na nossa igreja todos são bem-vindos. Pode ser homosexual, trans, homem, mulher, família. Não temos nenhum preconceito e estamos aqui para apoiar a causa da igualdade"", afirmou o pastor. Looks coloridos marcaram a 16ª Parada LGBT de Campinas (Foto: Renata Victal/G1)",pt,68
201,889,1462954505,CONTENT SHARED,-8085935119790093311,-1032019229384696495,7449982717083592275,,,,HTML,https://www.elastic.co/webinars/sneak-peek-of-graph-capabilities-with-elasticsearch,graph capabilities with the elastic stack,"Elasticsearch can power graph exploration at scale by collecting signals like clicks or purchases to identify meaningful connections between subjects on the fly. Watch this video for a preview and demo of Graph capabilities in the Elastic Stack, including: Detecting Sources of Risk - What are the shared behaviors of people trying to hack my website? Recommending Content - If users bought this type of gardening gloves, what other products might they be interested in? Identifying Relationships - Which people on Stack Overflow have expertise in both Hadoop-related technologies and python-related tech? Graph lets users leverage the relevance and distributed query execution capabilities of Elasticsearch - and we look forward to telling you how. Elasticsearch can power graph exploration at scale by collecting signals like clicks or purchases to identify meaningful connections between subjects on the fly. Watch this video for a preview and demo of Graph capabilities in the Elastic Stack, including: Detecting Sources of Risk - What are the shared behaviors of people trying to hack my website? Recommending Content - If users bought this type of gardening gloves, what other products might they be interested in? Identifying Relationships - Which people on Stack Overflow have expertise in both Hadoop-related technologies and python-related tech? Graph lets users leverage the relevance and distributed query execution capabilities of Elasticsearch - and we look forward to telling you how.",en,68
202,2472,1475174899,CONTENT SHARED,8779890754987103603,3609194402293569455,-1256266615792357100,"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.116 Safari/537.36",SP,BR,HTML,http://startupi.com.br/2016/09/mastercard-lanca-plataforma-de-apis-abertas-para-solucoes-de-pagamentos/,mastercard lança plataforma de apis abertas para soluções de pagamentos - startupi,"A Mastercard acaba de facilitar ainda mais a vida dos desenvolvedores que trabalham na criação da próxima geração de soluções para o comércio, com o lançamento do Mastercard Developers . Este portal único permite aos parceiros da Mastercard acessar uma diversidade de APIs (interfaces de programação de aplicativos) de pagamentos, dados e segurança. A plataforma também inclui a inédita categoria de APIs ""novas e experimentais"", permitindo que os parceiros testem as novas tecnologias e aplicativos. As APIs experimentais, desenvolvidas pelos oito laboratórios de P&D da empresa ao redor do mundo, ajudarão os parceiros a construir, dar escala e realizar pagamentos usando novas plataformas. Ao mesmo tempo, esses parceiros contarão com uma base para explorar novas modalidades de integração de pagamentos, como realidade aumentada e virtual, e a Internet das Coisas. ""Imaginamos o Mastercard Developers como algo que capacita os nossos clientes, parceiros e seus desenvolvedores para que eles possam inovar e fazer os seus negócios crescerem"", disse Oran Cummins, vice-presidente sênior de APIs da Mastercard. ""A nova plataforma será crucial para integrar as tecnologias e os serviços da Mastercard às soluções digitais desses parceiros de modo fácil e econômico. Com isso, os consumidores terão uma experiência mais simples, rápida e segura."" Os parceiros agora têm acesso a mais de 25 APIs da empresa - incluindo serviços essenciais como o MasterPass e os Mastercard Digital Enablement Services - em um formato que facilita a conexão ou integração com seus serviços. Dentre elas, há uma nova API desenvolvida para maximizar as ações de inclusão financeira. A Mastercard Aid Network (Rede de Ajuda Mastercard) ajuda a simplificar a distribuição de ajuda humanitária, mesmo em locais sem infraestrutura de telecomunicações, ao mesmo tempo em que torna a gestão de ONGs e organizações humanitárias mais transparente. Essa plataforma vai proporcionar uma experiência simplificada, documentação clara e intuitiva para os desenvolvedores com APIs suportadas por kits de desenvolvimento de software (SDKs), ferramentas de desenvolvimento e exemplos de código em seis linguagens de programação. A plataforma está crescendo e no ano passado seu uso já aumentou em 400%. Principais APIs no Mastercard Developers: Pagamento: MasterPass :Serviço de pagamento digital que simplifica a experiência de compras. Os consumidores podem fazer pagamentos com qualquer cartão, em qualquer lugar e com qualquer dispositivo. Serviços de Dados: Media Measurement : Mede o impacto que as campanhas de mídia digital têm nas vendas on-line e offline. Segurança: Experimental: Qkr! com MasterPass : Uma plataforma de pagamentos móveisque permite aos consumidores fazer pedidos e pagar produtos e serviços direto de seus dispositivos móveis. Mastercard Vending : Ajuda a conectar o seu aplicativo celular à plataforma de vendas - com isso os consumidores podem comprar diretamente pelo aplicativo. Inclusão Financeira: Outros serviços serão adicionados ao Mastercard Developers nos próximos meses. Para mais informações sobre a plataforma, acesse o site .",pt,68
203,1535,1466982935,CONTENT SHARED,-4027027091658759481,-1032019229384696495,-2318276786697086259,,,,HTML,https://medium.com/@marcushellberg/how-i-sped-up-the-initial-render-of-my-polymer-app-by-86-eeff648a3dc0,how i sped up the initial render of my polymer app by 86%,"How I sped up the initial render of my Polymer app by 86% I was trying to optimize the startup time of my Polymer application ( Expense Manager ) and realized that the main reason it took so long to render was that even though the user lands on the login page, the browser was busy setting up the main page that wasn't even visible yet. In addition, I had all my Polymer elements in one bundle file that needed to get loaded before anything could get shown. Here's what I did to speed up the first meaningful render from about 2,200ms to just over 300ms on localhost. Step 1: Split the bundle The first step was to ensure that I'm not loading more than needed for the initial login screen render. The way I decided to do that was to adapt my application to the new project structure that Polymer CLI uses. With this structure, instead of one bundle containing all the elements in the entire app, I got two bundles, one that just contained the components needed to show the login page, and one that contained the rest. Splitting up the bundle and deferring the load of the second bundle until navigation sped up the load time considerably. But there was one new issue that it brought with it. Now, after logging in, I had a brief flash of empty screen before the new bundle was actually downloaded and rendered. Step 2: Load the second page in the background While searching for ways to fix the flicker, I stumbled upon an undocumented feature in Polymer, a callback that gets called after a given component has finished rendering. With this callback, I was able to begin downloading and rendering the overview page as soon as the login page was rendered. Because users are slow compared to computers, this meant that by the time the user actually logs in, everything is already set up for them and the transition is instant. Step 3: Fetch data only once render is complete One final optimization I did using the same trick was to delay the Pouch DB setup until the second page was fully rendered. This way, the user gets quick visual results and the overall experience feels snappier . Conclusion Polymer.RenderStatus.afterNextRender gives you a simple way to defer things that need to get done soon, but that don't need to get done in the next render. You can find the demo online at: and source code at",en,68
204,2660,1477497746,CONTENT SHARED,5847042211895226591,-6786856227257648356,-9157637981906058578,"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.101 Safari/537.36",MG,BR,HTML,https://www.elastic.co/blog/elastic-stack-5-0-0-released,elastic stack 5.0.0 released,"In February of 2016, following Elastic{ON} 16, I wrote a post titled Heya, Elastic Stack and X-Pack . Today, after almost a year of substantial effort, including 5 Alphas, 1 Beta, and 1 Release Candidate we are pleased to announce the GA release of the Elastic Stack. And, importantly, it is available - today - on Elastic Cloud . If you want hosted Elasticsearch and Kibana there is no other place to start with the most recent code. We are committed to making Elastic Cloud the best place to run hosted Elasticsearch. In fact, we even made the Release Candidate available on cloud for testing purposes. Our team is celebrating today. I hope you join us. The GA release is available today. Join the Elastic Team for a live virtual event on November 3 to learn more about the release and ask the creators questions (AMA style). Register now! Before exploring the release in detail, I want to take the opportunity to reflect on what has brought us to this point. Our Community During the recent Elastic{ON} Tour, I have begun each session discussing a brief history of the last several years. This session culminates in the announcement that we have reached a combined 75 Million downloads. When I first began the project, I hoped for widespread adoption. But the passion and fervor of our community continues to delight and amaze me. Pioneer Program With that in mind, I want to share the results of the Pioneer Program . The program began with a simple premise. Your usage of the Elastic Stack is of the utmost import in informing our development as well as ensuring we release the highest quality product available. I am pleased to say that the community has filed 146 issues since the first Alpha release in April. Our community is one of our most valued assets at Elastic. In fact, one of the most discussed changes in this release was the name ""Elastic Stack"". The Elastic Stack But Elastic Stack is more than just a name. When we began this release cycle we committed to developing, building, testing, and releasing the entirety of the Stack together. This is important, internally, to ensure compatibility. And, for you, it helps speed deployment, decrease version confusion, and make it easier for developers to add capabilities across the entirety of the Elastic Stack. A Feature Tour When I began this post, I intended to provide an overview of key features in each product. But, it was hard to know where to begin and where to stop. Each of our team and tech leads have created a post that discusses the features specific to their product. And there is no one better suited to tell the story than them. I am, particularly, excited about a few items but rather than enumerate in detail, I will provide a brief overview and encourage you to read the detail posts for each product. Ingest Node - Ingest Node is an Elasticsearch node type enabling some data enrichment capabilities like grok, geoip, date, and other basic event manipulation options at index (or re-index) time. Pipelines are constructed with processors, and accessed through the REST API by suffixing a query parameter ""?pipeline=x"" . The ability to add pre-processing to documents, natively in Elasticsearch, prior to indexing allows for a variety of creative ingest deployments. This doesn't replace Logstash. This doesn't remove the need for Beats, this just allows greater flexibility in designing your ingest architecture. Elasticsearch Performance - Benchmarks tend to have an agenda...especially competitive benchmarks. With that in mind, we have spent substantial effort comparing 5.0.0 to prior releases. This data is available to you. This data is what we inspect when we want to ensure that we are doing the right things with performance and we are doing so in public to work towards preventing the secrecy, and doubt, that are associated with benchmark numbers. In fact, not only are the results available but we also document our hardware configuration, we have open sourced the tooling (called Rally ) and the benchmarks themselves ( Rally-Tracks ). Metricbeat - Metricbeat replaces Topbeat as the primary tool for collecting metrics in the Elastic stack. Like Topbeat, Metricbeat collects ""top"" like statistics about host and per process resources (CPU, memory, disk, network). Unlike Topbeat, Metricbeat also collects metrics from systems such as Apache, HAProxy, MongoDB, MySQL, Nginx, PostgreSQL, Redis, or Zookeeper, with more to come in the near future. Logstash Monitoring APIs - A new monitoring feature provides runtime visibility into the Logstash pipeline and its plugins. This component collects various kinds of operational metrics while Logstash processes your data, and all of this information can be queried using simple APIs. Timelion - After being introduced as a {Re}search project, Timelion is now natively available in Kibana core. Timelion provides a query DSL and visualizations that let you explore your data over time. This is but a sample, I've left out BKD trees, scaled_float and half_float , the immense effort put into Elasticsearch Resiliency , the eye-meltingly beautiful redesign of Kibana (we never knew how much we hated borders until we removed them), Kafka output in Beats, and so much more. This is a massive release. Reading the individual posts is a must to begin to understand the scope of improvement. X-Pack At Elastic we loved extensions. So much so that we built them and gave them interesting names. Shield, Marvel, and Watcher all described individual closed source features that didn't take away for open source capability but were additive for our customers. Unfortunately, as the range of these features grew to include Graph and Reporting, the install process became difficult and, at times, quite confusing. Say Heya to X-Pack! One pack that adds security, alerting, monitoring & management, reporting, and graph capabilities to the Elastic Stack. Our engineering process for 5.0 wasn't limited to the Elastic Stack, but we've also extended X-Pack by adding: Management & Monitoring UIs to Kibana Security UIs to Kibana for creating both users and roles Greatly simplified the installation process X-Pack is available to trial and has both commercial and free (Basic) license options. We are particularly excited to make some X-Pack features available for free and details are available on our Subscriptions page. In Closing I am in awe of the effort that went into this release, the involvement from our community and customers, and the groundwork that this sets for future releases. As always, the best way to understand a release is to experience it.",en,67
205,1237,1464923401,CONTENT SHARED,-3605608255586388018,5127372011815639401,-2629452978222634122,,,,HTML,https://medium.freecodecamp.com/github-broke-my-1-000-day-streak-6ec0c4c3a7d9?gi=ec294df41928,"github broke my 1,000 day streak - free code camp","GitHub broke my 1,000 day streak ... and this is what I learned about myself, and open source, along the way. I was always planning to write a post when I broke my contribution streak. My first child (a beautiful baby girl we named Grace) was born five weeks ago, and I honestly didn't think my ""commit some code every day"" habit would last through that experience, or even leading up to it. Well, my habit did last. And this in no way reflects how present I was as a new parent, nor my ability to sort out my own priorities like a proper adult. Unlike some streak criticism, mine was personal and not for show or fame and wasn't a sign of a distressed work/life balance. Then again, when I started I didn't think it would last about a thousand days  - yet that's roughly how long it has been (how long for sure? GitHub won't tell you anymore, but based on my math it's about 980 days) Ultimately, all streaks will be broken, and like many long-lived ones, I guess I've been wondering why and how to end it. Out-lasting the feature on GitHub then - the one that inspired me to start it in the first place - seems like a fitting way to go out. This is my story, more because I want to tell it, than because I think you (or anyone) should read it. It's been a personal journey played out in a public space, and so this seems like a fitting way to wrap it up. Warning: this is a bit rambly. It's a snapshot in time of someone who's just worked at something for nearly three years and seen it disappear. The number isn't important, and the publicity less so. But it's been a constructive motivation in my life that I deliberately opted into, and I'm feeling pretty introspective and processing a lot now that the whole thing is done. My streak started in September 15th, 2013. I had returned from a five week honeymoon around Europe with my wife in July, and was ready to start a new life; by coincidence, that was also the day my company Thinkmill was officially registered. So, two birthdays in one. But it was probably two weeks later when I consciously decided to keep it up; like a personal challenge. I had also recently open sourced KeystoneJS which was one of the scariest moments in my life (what if people hate it! what if I get judged? what if I'm not good enough?) ... which lasted all of a few days until I realized nobody had noticed. Nobody cared, and nobody knew who I was. Heh. It's not actually that scary. Turns out if you want somebody to pay any attention to your open source stuff, you have to work for it . So the streak started as a public way to say ""Hey! I'm committed to this! You can trust my scrappy little open source project because I'm obviously paying attention to it and dedicated to building it!"" I went about two weeks before I knew I'd have to tell my wife; if I didn't have her buy-in, there was no way it was going to hold up. So I did, expecting her to dismiss it; instead, she got that I was doing something different and important to me, and encouraged me instead. I thought I'd go for a few months maybe, if I was strong. A few months in, my initial motivation of ""something to prove"" had disappeared and two patterns emerged instead: It got hard. There were days when I didn't want to keep it up. I wanted to tap out and let it go. Hardest of all was when I was under the gun at work and already pulling 12 hour days on things that didn't count. But I'd come so far; just do something. So I did, and invariably a day or two later I would be past it, and things were easy again. Lesson #1 I realised that when I didn't have time to fix something big, I'd pick off an easy issue from the pile (turned out people did like Keystone, and the natural result of this is GitHub Issues) and fix it. I was addressing things users cared about, and the quality of my project was better for it. This ""good maintainer"" thing led to more interest, more engaged users, and eventually some amazing contributors / collaborators. Lesson #2. Buoyed by this, I was feeling really good about the streak. I was also refining my art. I love coding, even though I often get caught up with business / management / architectural concerns, and no matter what else I do, coding is my art. Dedicating time to it every day (#1) and collaborating with an ever-expanding community (#2) are the best things I've ever done to develop my skill and experience. April the next year, my wife and I went on holiday to China for a couple of weeks. I was sure this was the End Of The Streak. Because I'm not going to let it be my life! It's not healthy . Plus, there's no way my wife would have patience for me coding on holiday :) ... but I was wrong. Initially, because I checked my notifications at the airport and merged a really good looking PR (being responsive to contributors is really important, more on this later!) Then I fixed a simple bug. It had momentum, and was so easy to keep up. About halfway through my wife asked ""have you done your streak yet?"" and I told her ""no, I'm not worrying about that"". She insisted. She'd seen the positive impact it had on me, and encouraged me to continue. For all the months I'd benefit from it after we'd come back, it was worth 15 minutes a day for the next week. I'd worked so hard for it, she said, don't throw it away now. ""Have you done your streak yet"" became a common refrain in our house over the next couple of years, always with encouragement and never resignation. If things are good for you, the people who love you will notice and encourage them. Lesson #3 Having an unbroken streak of nearly a year seemed a bit crazy. One of my friends went on his own self-driven coding adventure (Hi Tom ��) and made up a status dashboard on a big TV in our office, Panic-style, and one of the modules on it was my streak, in an industrial ""days since last accident"" design. It was fun, and egged me on a bit. Turns out being disciplined can be social and fun. Lesson #4 Worth noting, though, at this point my life was set up around what I now recognized as a discipline. It wasn't always easy, but no discipline is, and I worked at it. Having the support of my friends and family was crucial, and running a business where we were able to construct a healthy relationship between commercial and open source work was also extremely important. If you're going to do something hard, be smart about it, and set it up to be sustainable. Lesson #5 The next year was a bit of a blur. Keystone grew, but I was always humbled when people had heard of it, or liked it. Through this, what developed strongly was a sense of ""building something for good"". I'm not giving my time to work for free, I've just got these ideas to express (in the form of software) that I think will create benefit beyond what I could achieve on my own or for myself. I feel like I'm pretty blessed in life. Some would call it privilege, luck, fortune, whatever. I'm OK with that, because while I can't repay everyone who's ever helped me I can pay it forward and be as generous as possible with time, code or advice. But you know what? it comes back. At the start of 2015 I had the opportunity to go to the first React Conf where I made some excellent friends and started being part of the React Community. By this point, maintaining my streak was pretty much second nature. Almost forgotten, but still a discipline held, and foundational to the way I behave. One of the absolute highlights of 2015 was traveling to Paris for React Europe to talk about React and Mobile Apps. But not just for the conference or the talk. That year I'd become friends with Camille Reynders, a KeystoneJS team member, over many hours spent talking on Slack and working together. But we'd never met, because he lived in Belgium while I'm in Sydney. He and his wife drove down to hang out that weekend, just to spend some time in real life. We sat in cafes on the streets of Paris, watching the Pride Parade go past, eating cheese, drinking beer and hanging out. Starting an Open Source project and dedicating yourself to it can get you some great friends and unique experiences. Lesson #6 Coming into 2016, my wife and I were expecting our first child. I had a frantic start to the year, running a growing business and traveling overseas nearly monthly getting things done before I went seriously into family mode for a while. There was Nodevember - my first major conference talk on KeystoneJS, in Nashville. Then PhoneGap Day US in Utah where I pitched React to what felt like a room full of Angular developers. Then React Conf 2016 in San Francisco in February. At this point, each trip felt like catching up with friends, learning new things and developing ideas with developers I admire and am just flat-out stoked to spend time with in person. To contrast with my first experience open sourcing KeystoneJS to crickets, I've now got packages on npm that get over a million downloads a month, thousands of followers, and something like 15,000 stars across my various personal projects. But more importantly I've been told by several people that I've inspired them to get into open source as well. That's not a small thing, and something I'm really proud of. Because I believe contributing to open source is personal, powerful, and good. These days, it's not uncommon for me to hear feedback that the community around KeystoneJS is supportive, inclusive and inspiring. There are now other people inspiring yet more people around something that I created and imbued with my personal energy and philosophy, and that is just awesome . No other word for it. That is why I do this . Not to mention all the thanks, credit, and opportunities that have come flooding back in return. I do this for me, because I believe in this way of creating value, but having that realized with feedback is powerfully helpful. It's frankly been a lot of work, and sacrifice. But also? Totally worth it. Now, while I've taken personal and private pride in my streak and various other metrics, it's worth making some points. This is personal. It's been good for me. It's not for everyone. There is no judgement if you benefit from a more clear-cut work/life balance. I have no argument to make against those who say the GitHub streak guilts them into working on weekends and they want it gone. There are also downsides to open source that I didn't understand when I started. Guilt is massive and real. When someone submits a PR and I sit on it, I feel terrible for ignoring (or even just apparently ignoring) their work. Their time is valuable and they gave me some of it; in balance, they're implicitly asking for more of mine to review and maintain their suggestion. Sometimes I'm a great maintainer, and respond quickly with helpful feedback or just a ""Merge PR"" click. Other times, I'm not. I've been lucky to have people help me out as well, with vetting, triaging and taking on maintenance roles in various projects. I can't thank those people enough. This morning when I woke up and found out my streak had been removed, it just felt... weird . Not like I was robbed of something, not like I was liberated either. Just a bit empty, a bit numb. All those time I'd thought about it and been encouraged not to, and the choice was taken away from me. How anticlimactic. I don't know if the campaign to remove streaks from GitHub's UI was something I really agree with. I get the arguments, especially from a mental health perspective. Just because I can look back and feel good about my experience, maybe on balance I was an exception. Maybe for every developer who was constructively motivated by it, there was another who was unhealthily punished by it. Addressing health and balance in our (and every) industry is a positive thing, along with equality and inclusion. Clearly it's been good for some people. John Dalton's been on a streak longer than mine and is one of the most inspiring developers I know. John Resig, I think, motivated a great number of developers with his "" Write Code Every Day "" post. As he said: I consider this change in habit to be a massive success and hope to continue it for as long as I can. In the meantime I'll do all that I can to recommend this tactic to others who wish to get substantial side project work done. Having kept it up for nearly a thousand days, I'm OK to stop streak-counting. Maybe I'll keep that graph all green, or maybe some grey will sneak in. It doesn't really matter; after three years, the discipline has done its work, and I'll be the beneficiary of it for the rest of my life. And hey, I was planning to break it sometime anyway. I do wonder, when mechanisms for self discipline and motivation can cut both ways, and then be unilaterally removed (I assume in response to a vocal movement) what the next thing will be for a generation of developers who are just getting started? We humans respond well to pressure, competition and games. It's the very instinct that made the streak such a powerful metric in the first place. I hope something new emerges that doesn't have such a negative downside (the rock → hard place text was always a bit caustic) yet inspires developers to challenge themselves the way the GitHub Streak did.",en,67
206,1883,1469548372,CONTENT SHARED,-5848514031542611523,1908339160857512799,3464895365884046539,,,,HTML,https://googleblog.blogspot.com.br/2016/07/promoting-gender-equality-through-emoji.html,promoting gender equality through emoji �� ��,"More than 90 percent of the world's online population use emoji. But while there's a huge range of emoji, there aren't a lot that highlight the diversity of women's careers, or empower young girls. There are emoji like these for men: but with options like these for women: ... the emoji representing women aren't exactly, well, representative. So we've been working to make things better. In May, we proposed a set of new emoji to the Unicode Technical Committee that represent a wider range of professions for women (as well as men), and reflect the pivotal roles that women play in the world. Since then, we've worked closely with members of the Unicode Emoji Subcommittee to bring the proposal to life. Today, the Unicode Emoji Subcommittee has agreed to add 11 new professional emoji, in both male and female options and with all the skin tones. That's more than 100 new emoji to choose from! Unicode is also adding male and female versions to 33 existing emoji. For example, you'll be able to pick both a female runner emoji and a male runner emoji, or a man or woman getting a haircut: These additions can be included in future versions of Android and other platforms-because Unicode helps make sure that people with different phones can send and receive the same emoji. These new emoji are one of several efforts we're making to better represent women in technology, and to connect girls with the education and resources they need to pursue careers in STEM. One such effort is Made with Code, which helps girls pursue and express their passions using computer science. Ahead of World Emoji Day this weekend, Made with Code is releasing a new project that teaches coding skills through the creation of emoji-inspired stickers. We hope these updates help make emoji just a little more representative of the millions of people around the �� who use them.",en,66
207,2563,1476444319,CONTENT SHARED,-4487024160266973763,7308881151087125462,2824438335258380441,Android - Native Mobile App,SP,BR,HTML,https://www.linkedin.com/pulse/visa-inaugura-co-creation-center-em-s%C3%A3o-paulo-erico-fileno,visa inaugura co-creation center em são paulo,"Espaço será referência e protagonista na condução da revolução das novas tecnologias de pagamento no Brasil O Visa Brasil Co-Creation Center abre suas portas em São Paulo, como parte do plano global de inovação da Visa. A iniciativa tem como proposta trabalhar em conjunto com importantes players do mercado brasileiro para cocriar o futuro das soluções de pagamento, aproximar-se dos clientes e expor os ativos da Visa. O espaço atenderá aos emissores de cartão, credenciadoras, comércios, fintechs e startups e será um local de trabalho colaborativo, prático e dinâmico. O Centro expande o sucesso do Miami Innovation Center, instalação inaugurada em junho de 2016 e que funciona como um hub para a região da América Latina. O Visa Brasil Co-Creation Center será o primeiro ponto de contato da Visa com desenvolvedores e parceiros brasileiros. "" Estamos criando uma rede. Esse espaço representa uma continuação do esforço e jornada da Visa, e permitirá que nossos clientes locais participem de um grupo global de inovação para cocriar a próxima geração de aplicativos de pagamento e do comércio "", diz Eduardo Coello, Presidente da Visa Inc. para a América Latina e Caribe . A Visa está revolucionando a indústria global de meios eletrônicos de pagamento e esse espaço no Brasil vem para solidificar e fortalecer a liderança da empresa na região. "" Estamos no meio de uma grande mudança de paradigmas. Com a migração do cartão de plástico para a internet das coisas, o desenvolvimento dos pagamentos móveis, e a entrada de novas empresas no tradicional grupo de players do setor de pagamentos, a Visa assume o compromisso de protagonizar e potencializar essa revolução e de garantir que todas essas novidades sejam eficazes e seguras "", afirma Percival Jatobá, Vice-presidente de produtos da Visa do Brasil . No Brasil, esse espaço inovador está totalmente atrelado à abordagem de design thinking - conjunto de métodos e processos que busca solucionar desafios de forma colaborativa e mais humana, na qual todos os envolvidos são colocados no centro de desenvolvimento do produto. Os parceiros terão acesso a APIs (interfaces de programação de aplicativos) e SDKs (kits de desenvolvimento de software) da Visa por meio do Visa Developer Program - que transforma a maior rede de pagamento do mundo em uma plataforma aberta para desenvolvedores de softwares e aplicativos. O país foi escolhido para sediar um Co-Creation Center por seu potencial de crescimento e ambiente diverso e pujante de startups, e também porque os brasileiros são conhecidos como early adopters de tecnologias. Nos últimos meses, a Visa lançou algumas tecnologias pioneiras, como a tokenização e os vestíveis ( wearables) - o relógio Swatch Bellamy, a pulseira Bradesco Visa e o anel de pagamento Visa, além do cartão Riocard Duo, que traz dupla funcionalidade, para transporte e pré-pago. Para Percival, o Visa Brasil Co-Creation Center é um marco dentro da empresa por simbolizar o início de uma nova era. "" Temos convicção de que, ao aliar a criatividade dos empreendedores e desenvolvedores brasileiros com a expertise e a inovação gerados pela Visa, grandes negócios e novas parcerias surgirão no País. Vamos elevar o nível do setor de pagamentos no Brasil "", diz o executivo. Foco no Consumidor Fernando Teles , anunciado recentemente como o novo diretor geral da Visa do Brasil , acredita que o Co-Creation Center acelerará a criação de novas experiências comerciais, além de apoiar os consumidores que, cada vez mais, fazem uso de dispositivos conectados para comprar, pagar e receber pagamentos. "" Nosso Centro tem a missão clara de inspirar nossos clientes e parceiros com uma combinação de novas tecnologias que vão ao encontro das necessidades presentes e futuras do consumidor. Como líder em tecnologia de pagamentos, é imperativo que habilitemos nossos clientes a atender a demanda dos consumidores, e os ajudemos a acompanhar a velocidade da inovação no comércio digital "", diz o executivo.",pt,66
208,313,1460201474,CONTENT SHARED,-6840859460575237450,-6030696784871381528,3156557011070914976,,,,HTML,http://premierleaguebrasil.com.br/2016/04/06/esse-texto-do-ranieri-e-a-melhor-coisa-que-voce-lera-hoje/,este texto do ranieri é a melhor coisa que você lerá hoje,"*texto e imagens originalmente publicados no site . NÓS NÃO SONHAMOS por Claudio Ranieri Eu me lembro da primeira reunião que tive com o presidente quando cheguei ao Leicester City no verão. Ele sentou comigo e me disse: ""Claudio, esse é um ano muito importante para o clube. É muito importante que fiquemos na Premier League. Temos que nos livrar do rebaixamento"". Minha resposta foi: ""Ok, claro. Vamos trabalhar duro nos treinos e tentar alcançar esse objetivo"". Quarenta pontos. Esse era o plano. Esse era o total que precisávamos para ficar na primeira divisão, para dar aos nossos torcedores mais um ano de Premier League. Naquela época, eu nem imaginava que iria olhar aquele papel no dia 4 de abril e ver o Leicester no topo da tabela com 69 pontos. No ano passado, nesse mesmo dia, o clube era o último colocado na classificação. Inacreditável. Eu tenho 64 anos, e, por isso, não saio muito de casa. Minha esposa está comigo há 40 anos. Nos meus dias de folga, tento ficar perto dela. Nós vamos ao lago perto da nossa casa ou, se estivermos mais animados, assistimos a um filme. Porém, ultimamente, eu tenho ouvido o barulho que vem de todas as partes do mundo. É impossível ignorar. Tenho ouvido até que temos novos torcedores na América nos seguindo. Para vocês, eu digo: bem-vindos ao clube. Estamos felizes por termos vocês. Quero que vocês amem a maneira com a qual jogamos futebol, e quero que amem meus jogadores, porque a jornada deles é inacreditável . Vocês talvez já tenham ouvido os nomes deles. Jogadores que eram considerados muito pequenos ou muito lentos para outros grandes times. N'Golo Kanté. Jamie Vardy. Wes Morgan. Danny Drinkwater. Riyad Mahrez. Quando eu cheguei no meu primeiro dia de treinos e vi a qualidade desses jogadores, eu sabia quão bons eles poderiam ser. Bem, eu sabia que teríamos uma chance de sobreviver na Premier League. Esse tal de Kanté corria tanto que eu pensei que ele tinha uma caixa cheia de baterias sob os shorts. Ele nunca parava de correr no treino. Tive que dizer a ele: ""N'Golo, pega leve. Vá devagar. Não corra atrás da bola toda hora, ok?"" Ele me respondeu: ""Ok, chefe. Pode deixar"". Dez segundos depois, eu olho e ele está correndo de novo. Eu disse: ""Um dia, eu ainda vou ver você cruzar a bola e correr para cabeçear"". Ele é inacreditável, mas não é a única peça-chave. Existem muitas outras para citar nessa temporada incrível . Jamie Vardy, por exemplo. Ele não é um jogador de futebol. É um cavalo fantástico. Ele precisa estar livre no campo . Eu digo a ele: ""Você é livre para se movimentar da forma que quiser, mas você tem que nos ajudar quando perdermos a bola. Isso é tudo o que eu te peço. Se você começar a pressionar o adversário, todos os seus companheiros vão segui-lo"". Antes de jogarmos a primeira partida da temporada, eu disse aos jogadores: ""Eu quero que vocês joguem pelos seus companheiros. Somos uma equipe pequena, então temos que lutar com todo nosso coração, com toda nossa alma. Eu não quero nem saber o nome do oponente . Tudo o que eu quero é que vocês lutem. Se eles forem melhores que nós, ok. Parabéns. Mas eles têm que mostrar pra gente que são melhores "". Existia uma eletricidade fantástica em Leicester desde meu primeiro dia. Começa com o presidente e passa pelos jogadores, pela equipe de profissionais e pelos torcedores. Foi inacreditável o que eu senti. No King Power Stadium, existe uma energia incrível . Os torcedores só cantam quando temos a bola? Oh, não, não, não. Quando estamos sob pressão, eles entendem nosso sofrimento e cantam com o coração. Eles entendem a complexidade do jogo e sabem quando os jogadores estão sofrendo. Eles são muito, muito próximos a nós . Nós começamos muito bem a temporada. Mas nosso objetivo, eu repito, era salvar o time do rebaixamento. Nos primeiros nove jogos, nós estávamos ganhando, mas sofríamos muitos gols. Tínhamos que marcar dois ou três gols todo jogo para conseguirmos vencer. Isso me preocupava muito . Antes de cada partida, eu dizia: ""Vamos, pessoal! Quero uma clean sheet hoje"". Nenhuma clean sheet. Tentei todo tipo de motivação. Então, finalmente, antes da partida contra o Crystal Palace, eu disse: ""Vamos, galera! Eu ofereço uma pizza se vocês conseguirem uma clean sheet "". Claro que meus jogadores conseguiram a clean sheet contra o Crystal Palace . 1 a 0 pra gente. Então, eu cumpri a promessa e levei meus jogadores à Peter Pizzeria, em Leicester City Square. Mas eu tinha uma surpresa preparada lá. Eu disse: ""Vocês têm trabalhado por tudo. Vocês vão trabalhar pela sua pizza também . Nós mesmos as faremos"". Entramos na cozinha com a massa, o queijo e o molho. Fizemos tudo. Estava muito bom, também. Comi vários pedaços. O que eu posso dizer? Sou italiano. Adoro minha pizza e minha pasta. Agora, conseguimos várias clean sheets . Uma dúzia de jogos sem tomar gols depois da pizza, na verdade. Não acho que foi coincidência . Faltam seis jogos, e nós devemos continuar lutando com nosso coração e nossa alma. Esse é um clube pequeno que está mostrando ao mundo o que pode ser alcançado através de espírito e determinação. Vinte e seis jogadores. Vinte e seis cérebros diferentes. Mas um só coração . Há poucos anos, muitos dos meus jogadores estavam em ligas menores. Vardy trabalhava em uma fábrica . Kanté estava na terceira divisão na França. Mahrez, na quarta. Agora, estamos brigando por um título. Os torcedores do Leicester que encontro na rua me dizem que estão sonhando. Mas eu digo a eles: ""Ok, vocês sonham pela gente. Nós não sonhamos. Nós simplesmente trabalhamos duro "". Não importa o que aconteça ao fim dessa temporada. Eu acho que nossa história é importante para todos os torcedores ao redor do mundo. Isso dá esperança aos jovens jogadores que já ouviram um ""você não é bom o suficiente"". Eles podem dizer a eles mesmos: ""Como posso chegar ao topo? Se Vardy pode, se Kanté pode, eu também posso"". O que você precisa pra chegar lá? Um grande nome? Não . Um grande contrato? Não . Você só precisa manter a mente aberta, o coração aberto, e correr livre . Quem sabe? Talvez, ao fim da temporada, nós teremos duas festas da pizza. Comments comments About the Author Jornalista formado pela Unesp - Bauru. Assessor de Comunicação do CEPID-CeMEAI e apaixonado pelo futebol europeu.",pt,66
209,604,1461708262,CONTENT SHARED,4118743389464105405,-1032019229384696495,-940735523977516255,,,,HTML,https://cloudplatform.googleblog.com/2016/04/why-Google-App-Engine-rocks-a-Google-engineers-take.html,why google app engine rocks: a google engineer's take,"In December 2011, I had been working for Google for nine years and was leading a team of 10 software developers, supporting the AdSense business. Our portfolio consisted of over 30 software systems, mostly web apps for business intelligence that had been built over the past decade, each on a stack that seemed like a good idea at the time. Some were state-of-the-art custom servers built on the (then) latest Google web server libraries and running directly on Borg. Some were a LAMP stack on a managed hosting service. Some were running as a cron job on someone's workstation. Some were weird monsters, like a LAMP stack running on Borg with Apache customized to work with production load balancers and encryption. Things were breaking in new and wonderful ways every day. It was all we could do to keep the systems running - just barely. The team was stressed out. The Product Managers and engineers were frustrated. A typical conversation went like this: PM : ""You thought it would be easy to add the foobar feature, but it's been four days!"" Eng : ""I know, I know, but I had to upgrade the package manager version first, and then migrate off some deprecated APIs. I'm almost done with that stuff. I'm eager to start on the foobar, too."" PM : ""Well, now, that's disappointing."" I surveyed the team to find the root cause of our inefficiency: we were spending 60% of our time on maintenance. I asked how much time would be appropriate, and the answer was a grudging 25%. We made a goal to reduce our maintenance to that point, which would free up the time equivalent of three and a half of our 10 developers. Google App Engine had just come out of preview in September 2011. A friend recommended it heartily - he'd been using it for a personal site - and he raved that it was low-maintenance, auto-scaling and had built-in features like Google Cloud Datastore and user-management. Another friend, Alex Martelli, was using it for several personal projects. I myself had used it for a charity website since 2010. We decided to use it for all of our web serving. It was the team's first step into PaaS. Around the same time, we started using Dremel, Google's internal version of BigQuery . It was incredibly fast compared to MapReduce, and it scaled almost as well. We decided to re-write all of our data processing to use it, even though there were still a few functional gaps between it and App Engine at the time, for example visualization and data pipelines. We whipped up solutions that are still in use by hundreds of projects at Google. Now Google Cloud Platform users can access similar functionality using Google Cloud Datalab . What we saw next was an amazing transformation in the way that software developers worked. Yes, we had to re-write 30 systems, but they needed to be re-written anyway. WIth that finished, developing on the cloud was so much faster -- I recall being astonished at seeing the App Engine logs, that I had done 100 code, test, and deploy cycles in a single coding session. Once things were working, they kept working for a long time. We stopped debating what stack to choose for the next project. We just grabbed the most obvious one from Google Cloud Platform and started building. If we found a bug in the cloud infrastructure, it was promptly fixed by an expert. What a change from spending hours troubleshooting library compatibility! Best of all, we quickly got the time we spent on maintenance down to 25%, and it kept going down. At the end of two years I repeated the survey; the team reported that they now only spent 5% of their time on maintenance. We started having good and different problems. The business wasn't generating ideas fast enough to keep us busy, and we had no backlog. We started to take two weeks at the end of every quarter for a ""hackathon"" to see what we could dream up. We transferred half of the developers to another, busier team outside of Cloud. We tackled larger projects and started out-pacing much larger development teams. After seeing how using PaaS changed things for my team, I want everyone to experience it. Thankfully, these technologies are available not only to Google engineers, but to developers the world over. This is the most transformational technology I've seen since I first visited Google Search in 1999 - it lets developers stop doing dumb things and get on with developing the applications that add value to our lives.",en,66
210,644,1461851503,CONTENT SHARED,-1622037268576555626,-1032019229384696495,-1941773591979139720,,,,HTML,http://www.businessinsider.com/payments-ecosystem-research-and-business-opportunities-2016-4-27,you won't recognize the new world of digital payments without this report,"BI Intelligence The modern smartphone is a remarkable device. A single device that fits in your pocket can do all the tasks that once required cameras, camcorders, GPS devices, watches, alarm clocks, calculators, and even TVs. But the next change might be the most radical of all-it could eliminate the need to carry cash and credit cards. The growing importance of the smartphone as the go-to computing device for every digital activity is having a profound effect everywhere you look, but it's only the biggest story among many exciting developments in the world of payments: Apple Pay was first out of the gate, but now mobile wallets are everywhere you look-Android Pay, Google Pay, Chase Pay and even Walmart Pay are making smartphones a real alternative to carrying credit cards. And the potential for mobile wallets to limit a merchant's fraud liability could help them really take off in acceptance for small businesses. As consumers move more purchasing online, gateway vendors that can act as a front-end processor for online businesses are seeing explosive growth. PayPal-owned Braintree grew 111% YoY in the number of cards on file in Q4 2015, while Stripe and Klarna now have multi-billion dollar valuations. Mobile Point-Of-Sale (mPOS) startups like Square and ShopKeep have pioneered a whole new payments niche-accepting payments via tablets and smartphones. Coupling their transactions capabilities with new apps can revolutionize a small business' inventory management, marketing, loyalty and even payroll. Mobile Peer-to-Peer payments in the U.S. are forecast to grow from $5.6 billion in 2014 to nearly $175 billion by 2019 as consumers increasingly skip the hassle of writing a check or going to an ATM. But smartphone vendors like Apple could cripple the dominant player of 2016 (Venmo) if they make a serious push to own the space. If your job or your company is involved in payment processing in any way, you know how complex this industry is. And you know that you simply can't understand where the next big digital opportunities are unless you know the key players and roles in each step of the payments ""supply chain:"" Fortunately, managing analyst John Heggestuen and research analyst Evan Bakker of BI Intelligence , Business Insider's premium research service, have compiled a detailed report that breaks down everything you need to know-whether you're a payments industry veteran or a newcomer who is still getting a basic knowledge of this complex world. BI Intelligence Among the big picture insights you'll get from this new report, titled The Payments Ecosystem Report : Everything You Need to Know About The Next Era of Payment Processing : The 5 key events of 2015 that have set up 2016 as a watershed year for the entire payments ecosystem. The basics of traditional card processing from the start of the process through to the very end. Why new players and innovations like prepaid cards, store cards, and PIN debit transactions are gaining market share and creating new opportunities. The effects-good and bad-of the transition to new mobile payment methods. New players and old have surprising threats and opportunities in areas as varied as carrier billing, remittances, wearables, and more. This exclusive report takes you inside these big issues to explore: The critical steps in credit card transactions and how they are changing. The six major types of organizations involved in the payments ecosystem. The significant differences for industry players who operate closed-loop networks and offer prepaid cards. The challenges and opportunities facing hardware and software providers for the payments sector. The 8 reasons why mobile wallets are growing so fast and how they will disrupt all aspects of the mobile ecosystem. The exciting possibilities ahead in fast-growing payments subsectors like remittances, connected devices and mobile P2P payments. And much more. The Payments Ecosystem Report : Everything You Need to Know About The Next Era of Payment Processing is the only place you can get the full story on the rapidly-evolving world of payments. To get your copy of this invaluable guide, choose one of these options: Subscribe to an ALL-ACCESS Membership with BI Intelligence and gain immediate access to this report AND over 100 other expertly researched deep-dive reports, subscriptions to all of our daily newsletters, and much more. >> START A MEMBERSHIP Purchase the report and download it immediately from our research store. BUY THE REPORT >> The choice is yours. But however you decide to acquire this report, you've given yourself a powerful advantage in your understanding of the fast-moving world of payments.",en,65
211,1988,1470347942,CONTENT SHARED,-5065077552540450930,2542290381109225938,-1265449797350590617,,,,HTML,http://www.sonhoseguro.com.br/2016/08/ranking-das-maiores-seguradoras-da-europa-2015/,ranking das maiores seguradoras da europa - 2015 | sonho seguro,"A AXA se manteve na liderança do ranking de maiores seguradoras da Europa, segundo estudo divulgado pela Fundacion Mapfre nesta quinta-feira, com prêmios totais de 91,9 bilhões de euros em 2015, avanço de 6,6%. O ranking traz Allianz, Generali, Prudential, Zurich, Talanx, CNP, Credit Agricole, Aviva e Mapfre. As dez maiores segurdoras europeias registraram prêmios de 483,4 bilhões de euros em 2015, crescimento de 7,8% em relação ao ano anterior. O segmento Não Vida foi responsável por 217 bilhões de euros, liderado por Allianz, AXA e Zurich. No quesito margem de solvencia, a Mapfre é a líder do ranking, seguida por Prudential, AXA, Aviva e Talanx.",pt,65
212,2185,1472084340,CONTENT SHARED,3616722904601574426,-7496361692498935601,352146653115423838,,,,HTML,https://www.infoq.com/br/presentations/explorando-o-novo-dotnet-multiplataforma,"explorando o novo .net multiplataforma: asp.net core, .net core e ef core","Resumo Nesta palestra, exploraremos o estágio atual dos frameworks Core, vendo na prática uma aplicação ASP.NET e .NET Core 1.0 (MVC) com uso do Entity Framework Core 1.0 e baseada no .NET Core 1.0 sendo criada e executada em múltiplas plataformas. São discutidas as principais limitações existentes nos três frameworks em sua versão inicial - e como as mudanças podem afetar o desenvolvimento no dia a dia. Minibiografia Palestrante frequente em diversos eventos. Reconhecido pela Microsoft como MVP por quatro anos consecutivos. Foi responsável técnico pela implantação do projeto de adoção de livros digitais em tablets em uma das maiores escolas particulares de Brasília/DF, beneficiando inicialmente cerca de mil alunos, dentre diversas outras iniciativas em Tecnologia da Informação. Entre 28 de março e 1° de abril, São Paulo recebeu a nona edição brasileira do QCon. Organizado pelo InfoQ Brasil e com palestras selecionadas por um comitê independente, esta edição contou com 3 keynotes, 100 palestras e 13 workshops, totalizando 130 horas de conteúdo, o que levou o QCon São Paulo ao patamar dos maiores QCons mundiais.",pt,65
213,1809,1468852057,CONTENT SHARED,5533752987392101383,-2726721797588771398,-3534935468229000936,,,,HTML,https://hbr.org/2016/07/7-questions-to-ask-before-your-next-digital-transformation,7 questions to ask before your next digital transformation,"Although digital investment is almost unquestionably the right course of action for most firms, organizations still struggle to create the desired results. Estimates of digital transformation failures range from 66% to 84%. Such a high failure rate isn't surprising, as leaders are trying to create entirely new competencies and wedge them into an organization with strong legacy cultures and operating models. While most executives are pros at managing change, digital transformation is a much deeper change than the usual process or system update. Of course, digital technology can be used to improve or augment existing ways of operating, but it also opens entirely new ways of doing business based on digital networks like Uber, Airbnb, Yelp, and the Apple Developer Network - which is where a great deal of the digital value resides. So as you navigate your own digital transformation, we recommend beginning with a few questions that go deeper than ""what talent do you need"" or ""how much money will you spend"" and probe broader organizational readiness. Is this a digital upgrade or a digital transformation? Most companies target digital transformation and end up with digital upgrades, using digital technology to increase efficiency or effectiveness at something your firm is already doing. For example, increasing your marketing spend for digital channels or upgrading internal communication systems. On the other hand, a digital transformation occurs when you use digital technology to change the way you operate, particularly around customer interactions and the way that value is created-for example, Apple using its developer network to create software for its devices. If you discover that you are actually embarking on an upgrade instead of a transformation, ask yourself if that will be sufficient to maintain competitiveness when business models based on digital networks create market valuations four times higher than the rest. Are you really bought in, and is your team? Digital technology and business models are on the radar of every executive, and there is an expectation that most companies must change to keep up. However, a situation that we have seen over and over again is a leadership team trying to lead a digital transformation that they aren't particularly passionate about. We all have core beliefs about what creates value in the world, and these shape the way we allocate our time, attention, and capital. Most leaders have decades of experience focusing on assets like plants, real estate, inventory, and human capital. Shifting away from these habitual priorities takes self-reflection and openness, and often a concerted effort to build new patterns in thought and action. Are you prepared to share value creation with your customers? The latest technology-enabled business model, network orchestration, is premised on the fact that companies can allow customers and other networks to share in the process of value creation. Uber relies on a network of drivers; Airbnb relies on a network of property owners; Ebay relies on a network of sellers. These networks are essential to the organizations, and, by accessing external assets, these firms are able to achieve outstanding profitability. Sharing the workload seems like an obviously winning proposition, but many leaders are hesitant to relinquish control and rely on a network that lies outside of their chain of command. Working with these external groups requires new, co-creative leadership styles, but also can allow organizations to tap into enormous pools of capabilities and under-utilized resources. Have you put walls around your digital team? A digital upgrade requires a well-defined team with a narrow scope. A digital transformation requires a team with a cross-functional mandate and strong support. This becomes an important point because organizations usually do not change their internal structure as a part of digital transformation and so the teams working on these transformations get slotted into the existing structure. Where the team actually ""sits,"" both physically and in the org chart, can affect their ability to influence the cross-functional groups integral to real digital transformation. We have seen many companies limit the progress of digital by basing their team in marketing or IT. Do you know how to measure the value you intend to create? You manage what you measure. For most organizations, the focus is on physical capital (for making and selling goods) or human capital (for delivering services). These firms track inventory, productivity, utilization, and other traditional Key Performance Indicators (KPIs). Digital transformations don't always affect the KPIs a company is already measuring. Of course the end goal of a transformation is to affect revenue, profitability, and investor value. Along the way, however, it is useful to track intermediate indicators. For many digital network companies, this includes sentiment and engagement as well as network co-creation and value sharing. For example, when judging the success of the Developer Network, Apple can measure the number of developers creating apps for their app store, the amount of money generated by those apps that Apple shares with their community, and customer satisfaction with apps. Are you ready to make the tough calls about your team? There is an old saying: ""It is easier to change the people than to change the people."" Said another way, sometimes a new vision requires new people to create it. For many, the digital people you need on your team and on your board don't reside in your organization-at all, or at least not in the right quantity. Many of your current staff will be dedicated to doing what they have always done and will create resistance and roadblocks for change. To make room for your digital transformers, make the tough calls early on regarding your team and board. In our experience, nearly half of your team and board will need to turn over during the course of a successful digital transformation. Although painful, it's really a good thing for the organization-creating balance between the old and new. Will you be ready to spin off your digital business? Sometimes the upstart inside the organization becomes bigger and more valuable than the parent that gave birth to it-or risks not attracting the right talent or suffering turf wars between digital and legacy. (A great resource on this is The Second Curve by Ian Morrison.) Often, separation is required to enable both the parent and child to continue growth. Google is an expert at both creating new ventures and enabling them to grow; witness their recent reorganization into Alphabet to enable each of its major businesses to pursue their own potential (including Google and YouTube). In other organizations, the new, digital business will actually absorb and improve its parent. Transforming an organization is difficult, and the research proves it. But it is still worth doing. Forrester's assessment is that by 2020 every business will become either predator or prey . As a leader, you likely already know the basics of managing change, but a digital transformation goes deeper, and thus makes different demands on you, your team, and your organization. In return, however, you have the opportunity to invest in the most profitable and valuable business models the market has seen.",en,64
214,1616,1467381084,CONTENT SHARED,5445999759138709405,5660542693104786364,-4372460612946273438,,,,HTML,http://www.infomoney.com.br/conteudo-patrocinado/noticia/5233259/bradesco-lanca-servico-inedito-para-estimular-habito-poupar,bradesco lança serviço inédito para estimular o hábito de poupar,"SÃO PAULO - Uma das estratégias para manter suas finanças em dia, é ter sempre uma poupança, uma reserva de dinheiro que possa ser utilizada em casos de emergência, como a perda de emprego e outros imprevistos. Com objetivo de estimular o hábito de poupar e a formação da poupança no Brasil, o Bradesco lançou recentemente o Poupa Troco Bradesco . Trata-se de um serviço inédito no País, no qual os valores dos centavos dos débitos realizados em conta corrente serão arredondados para cima com a diferença sendo direcionada para uma conta de poupança. ""É um jeito simples de poupar sem perceber. Acreditamos que num curto espaço de tempo teremos a adesão de pelo menos 300 mil clientes ao Poupa Troco Bradesco, afirma Altair Antônio de Souza, diretor executivo do Bradesco. Como funciona Na ocorrência de um débito de uma conta de consumo no valor R$100,30, por exemplo, o Banco efetua a transferência de R$0,70 para uma conta de poupança de titularidade do cliente ou de uma outra pessoa que ele escolher, em qualquer agência Bradesco. O cliente escolhe ainda um valor inteiro, entre R$1 e R$5, que será adicionado à somatória dos centavos gerados pelos débitos ocorridos em sua conta corrente. Para ter acesso ao Poupa Troco, o cliente precisará cadastrar sua conta corrente ao novo serviço. Caso o cliente não tenha saldo disponível nenhum valor será transferido. A rentabilidade é a mesma da caderneta de poupança, que hoje está em 0,68% ao mês e com liquidez diária - o que permite o saque a qualquer momento, umas das principais vantagens da poupança. E este arredondamento vale não apenas para contas de consumo. Fazem parte do Poupa Troco as transações de débito automático de contas de consumo, boletos bancários, operações com cartão de débito, fatura de cartão de crédito, entre outras.",pt,64
215,1940,1469976528,CONTENT SHARED,1436883058900979473,-1616903969205976623,-5530575741825489868,,,,HTML,http://www.mckinseyonmarketingandsales.com/the-future-of-the-shopping-mall,the future of the shopping mall,"Officially shopping malls are defined as ""one or more buildings forming a complex of shops representing merchandisers, with interconnected walkways enabling visitors to walk from unit to unit."" 1 Unofficially, they are the heart and soul of communities, the foundation of retail economies, and a social sanctuary for teenagers everywhere. In recent decades, the concept of the shopping mall, which has its origins in the U.S. and became a full-blown modern retail trend there in the post-WWII years, has proliferated across the globe. The five largest malls in the world now reside in Asia. China's New South China Mall in Dongguan stands at the top of the heap with 2.9 million square meters of space. Despite its ubiquity, the mall as it's been conceived for the last half century is at a critical inflection point. A storm of global trends are coming together at the same time to cause malls to change the role they play in people's lives. No longer are they primarily about shopping. Now, when consumers visit malls, they are looking for experiences that go well beyond traditional shopping. When today's consumers visit malls, they are looking for experiences that go well beyond traditional shopping. The trends helping to create this change include changing demographics, such as an aging population and increased urbanization, which means more people living in smaller spaces and a greater need for public spaces in which to socialize and congregate. In this environment, malls offer a welcome watering hole, especially in cities where other public spaces are not safe. Sustainability concerns are causing some consumers to prefer mixed use developments where they can live, shop and work all within walking distance - instead of having to get into a car and drive to a crowded suburban mall. The growing middle classes in Latin America and Asia maintain a strong association between consumption and pleasure, driving the need for more engaging shopping experiences. And finally, the e-commerce revolution and the rise of digital technologies are fundamentally reshaping consumer expectations and shifting the function of stores toward useful and entertaining customer experiences. As these trends advance across the global stage, they are forcing mall operators to rethink how they conceive and operate their properties. This identity crisis is most intense in the U.S., the country that pioneered malls and has the most malls per inhabitant. Thanks to a continued economic slowdown and rapid advance of the digital revolution, the U.S. mall industry is retracting and facing high vacancy levels. Websites such as deadmalls.com collect pictures of weedy parking lots and barren food courts, and try to explain how once-thriving shopping centers began to spiral downward. In the face of these considerable challenges, malls are seeking to stay relevant, drive growth and boost efficiency. We see successful players investing along three key fronts. 1. Differentiating the consumer offering, with a focus on experience and convenience. Online shopping provides consumers with ultimate levels of convenience. Malls will never be able to compete with the endless product selection, price comparisons and always-on nature of online. Nor should they try. Instead, malls need to move in a different direction, away from commoditized shopping experiences and toward a broadened value proposition for consumers. In 2011, the Australian mall company Westfield launched an online mall (and later a mobile app) with 150 stores, 3,000 brands, and over 1 million products. Innovative malls are incorporating value-added elements that attempt to recast the mall as the new downtown, including concerts, arts centers, spas, fitness clubs, and farmer's markets. These services provide a level of leisure and entertainment that can never be satisfied online. Xanadu, a mall 30 km from Madrid, for instance, has gone out of its way to provide the means for parents to spend quality time with their children. The mall features a ski slope, go karts, balloon rides, bowling and billiards. Similarly, the Mall of America in Minnesota has an underwater aquarium, a theme park, and a dinosaur walk museum. In Brazil, for instance, a new focus on leisure and entertainment is already driving growth. Revenue coming into malls from these offerings grew 41 percent in 2013 compared to 2012. An emphasis on fine dining and events is also helping to make malls the hub of the local community - a place to share quality time with friends and family, not just wolf down a meal at the food court. The King of Prussia Mall, located 30 km from Philadelphia, has a Morton's Steakhouse and Capital Grille. The Crystal Cove shopping center in Newport Beach, CA has more than a dozen upscale restaurants, including Tamarind of London and Mastro's Ocean Club. On the tenant mix front, innovative malls are strategically rethinking the types of stores that consumers will respond to. Anchor tenants that drive traffic are still key, but we also see a new emphasis on a curated mix of smaller stores that add a sense of novelty to the mall offering. Additionally, some malls are making greater use of temporary, flexible spaces that can accommodate different stores over time. Pop up stores, showroom spaces and kiosks provide customers with a sense of the unexpected and give them a reason to treasure hunt. Finally, malls are overcoming the commoditization problem by focusing on specific consumer segments and/or creating specific zones within the mall that allow consumers to find an area that caters to them. In the Dubai Mall, for instance, ""Fashion Avenue"" is an area dedicated to luxury brands and services tailored to the upscale customer, including a separate outside entrance and parking area. In the 7-story CentralWord mall in Bangkok, home décor is on the 5th level, technology on the 4th, and fashion apparel on 1-3. This approach also represents a way for malls to ensure that customers don't get lost inside the ever increasing square footage of malls. 2. Transforming the mall experience by leveraging technology and multichannel strategies. The digital transformation of retail is not all bad news for malls. On the contrary, it presents new opportunities for malls to engage consumers throughout their decision journeys. There are three primary ways in which malls are leveraging technology: The most innovative malls today look nothing like their predecessors. First, they are extending their relationships with customers to before and after the mall visit. This is about engaging customers through compelling content and creating deeper bonds with them through social media and proprietary sites and apps, as well as loyalty programs. Social media can be used, for instance, to create buzz about new tenants or solicit ideas from consumers about ideas for new stores. One mall company has utilized segmented Facebook communication to speak to different communities, such as different geographies or interest groups or specific malls. Mall loyalty programs can provide the means for malls to establish a direct relationship with customers that goes beyond each visit to the mall, while allowing malls to collect precious information about customers. Just like retailers, malls should reach out to their customers with customized offers, gift ideas and other targeted advertisements based on real time intelligence and location-based marketing. While malls face the challenge of not having direct access to shopper purchase data, this can be overcome by inducing shoppers to use their smartphone to scan purchase receipts in exchange for points that can be redeemed for concerts tickets, books, discount vouchers for participating merchants, free parking or invitations to events (e.g., a fashion show). Alternatively, technologies such as face recognition, location-based mobile ads, and beacons are already being successfully applied in order to identify and establish targeted contact with repeat customers. Such technologies are also valuable for gathering consumer behavioral data from which malls can glean useful insights. Secondly, malls are using technology to transform mall usability as a means of improving customer satisfaction. There is ample opportunity for malls to decrease customer pain points, while simultaneously creating entirely new delight points. Technology, for instance, can be used to address one of the biggest challenges shoppers face at the mall - finding parking. Sensors located in parking lots detect how many spots are available on each level and give visual indicators to drivers. Once within the mall, mobile apps can offer quick, easy guides to help shoppers find what they're looking for at today's increasingly large and multi-level malls. Thirdly, malls are utilizing digital capabilities to take the shopping experience to the next level. It critical for malls to take a more active role in shaping the shopping experience, either by acting more like retailers or by partnering with them. Mall players are experimenting with a variety of different business models to make this happen, but there are no certain winners yet. To introduce elements of e-commerce into the mall, Taubman partnered with Twentieth Century Fox to put virtual storefronts - ""Fox Movie Mall"" - in at least 18 luxury malls. There, shoppers can purchase movie tickets by scanning a QR code with their smartphone. As the barriers between online and offline blur, some mall operators are venturing into online with a complete virtual mall offering. In 2011, the Australian mall company Westfield launched an online mall (and later a mobile app) with 150 stores, 3,000 brands, and over 1 million products. The company collects a small listing fee from merchants, as well as a commission of between 20-30 percent on every sale. Driven by the knowledge that 60 percent of the 1.1 billion annual shoppers in its malls use mobile devices, Westfield also created a research lab located in San Francisco, with the mission of finding technology applications and services that can further enhance the retail experience for both shoppers and retailers. 3. Exploration of new formats and commercial real estate opportunities. The most innovative malls today look nothing like their predecessors. Although location remains the key real estate consideration for malls, a differentiated design and structure is increasingly important. Open air malls go a long ways toward lending an atmosphere of a town center, especially when they incorporate mixed use real estate. Many of the malls being built in urban areas are open and fully integrated with the landscape. The Cabot Circus Shopping Centre in Bristol, England, for instance, has a unique shell-shaped glass roof that's the size of one and a half football fields. Incorporating environmental sustainability considerations, the mall is accessible by public transportation and features a rainwater harvesting system. Even malls that are enclosed are now incorporating more natural ambiance into their design, installing plants and trees, wood walls and floors, waterfalls, and lots of glass to let in natural lighting. Such elements help malls better blend in with their surroundings. It is critical that malls be about much more than stores. We see the mix of tenant/public space moving from the current 70/30 to 60/40, or even 50/50. When this happens, these expanded public spaces will need to be planned and programed over the year much like an exhibition. They will be managed more like content and media, instead of real estate. Mixed used developments offer consumers an attractive, integrated community in which to live, work and shop. They also serve to generate additional traffic for the malls while maximizing returns on invested capital. Other commercial real estate opportunities that can add alternative revenue streams are hotels, office buildings and airports. Lastly, outlets malls are an increasingly popular alternate format in more mature markets such as the U.S., particularly after the downturn of the economy, and they have been a key driver of growth for many players. In emerging economies like Brazil, outlets are also gaining attention and we see mall operators experimenting with this format as a means of attracting price conscious consumers and deal seekers. Implications for malls Although these trends are expressing themselves to varying degrees in different markets around the world, we believe they are relevant globally and should be taken to heart no matter where mall companies operate. There are three strategic considerations that players should understand when figuring out how to best react. 1) Evolve the offering by defining a clear value proposition for both consumers and retailers, anchoring it on deep consumer insights and bullet-proof economics. Among the large universe of options for enhancing the customer experience, it is possible to identify initiatives that will be both ROI-positive and substantially boost the satisfaction customers have toward malls. To do this, mall players must first isolate and quantify the consumer touch points that are most responsible for driving satisfaction. Use these touch points to prioritize areas of investment and to design a cohesive customer experience program that will yield higher visit and/or spend rates, and ultimately greater consumer loyalty. 2) Increase productivity and efficiency of the current mall base through a strategic review of the tenant mix, taking into account consumer needs and retailer economics. This analysis should guide the management of rent pricing and overall commercial planning. On the cost front, the focus should be on strict management of direct and indirect costs, combined with operational efficiency, which is critical for successful customer experience transformations. 3) Think surgically about where and how to grow in a way that won't jeopardize returns. Focus on city clusters and regions that have distinctive opportunities for growth. This includes thinking purposefully about disciplined capex management and which formats are going to create the biggest impact, whether that's traditional, multi-use, neighborhood or outlet. Executing against these considerations will often require that mall players develop new capabilities. Westfield, for example, has established a Digital Office group that reports to the CEO with the mission of spearheading digital initiatives across the organization. Other companies have created ""customer experience"" teams that are responsible for creating and integrating a unified vision of customer initiatives. Still others have created retail teams responsible for working on partnerships with retailers, or alternatively, operating retail operations themselves. The world of retail is changing dramatically, but the mall still can have a central role in urban and suburban societies. To avoid becoming what one chief executive calls a ""historical anachronism - a sixty-year aberration that no longer meets the public's needs,"" mall operators must expand their horizons of what a mall can be. They must envision themselves no longer as real estate brokers, but instead as customer-facing providers of shoppable entertainment.",en,64
216,1172,1464779739,CONTENT SHARED,-8370744479086515302,1895326251577378793,2217251992734926463,,,,HTML,http://computerworld.com.br/mobile-banking-conquista-o-coracao-dos-brasileiros,mobile banking conquista o coração dos brasileiros,"Depois das facilidades trazidas pelos serviços oferecidos via Internet, agora chegou a vez do mobile banking conquistar o coração dos brasileiros. Aos poucos, os smartphones revolucionam a forma como as pessoas se relacionam com o setor financeiro. Segundo dados da Febraban, o número de transações realizadas através de dispositivos móveis no Brasil saltou de 4,7 bilhões para 11,2 bilhões entre 2014 e 2015. O volume representa um aumento brutal da ordem de 138%. A escalada da mobilidade no setor financeiro foi intensa, ainda mais se considerarmos que esse canal registrava menos de 1% do total de operações realizadas em 2012. Atualmente, já responde por 21% dentre um total de 54 bilhões de transações realizadas nos 17 maiores bancos em atividade no País. Grande parte (95%) das operações realizadas através do mobile banking no ano passado são classificadas como movimentação não financeira, ou seja, referem-se a serviços de consulta de saldos e extratos, por exemplo. Apesar disso, o telefone celular se consolidou como ferramenta de relacionamento entre correntistas e seus bancos. ""O cliente pede e se mostra cada vez mais interessado na utilização desse canal"", enfatiza Gustavo Fosse, diretor setorial de tecnologia e automação bancária da Febraban, projetando que o uso acentuado da mobilidade se mantenha forte pelos próximos anos. O executivo baseia suas apostas nas estimativas do IBGE, que estima que 40% dos brasileiros possuem um smartphone atualmente. Esse percentual deve subir para 65% em 2020. ""Isso mostra espaço para o serviço avançar"", resume, citando que atualmente, 33 milhões de brasileiros usam mobile banking. Vetor digital Considerando todos os canais eletrônicos (web, mobile e POS), o digital respondeu por 69% das transações registradas nos bancos brasileiros em 2015. O internet banking registrou 17,7 bilhões de transações no último ano, pouco abaixo das 18 bilhões verificadas em 2014. A representatividade do canal caiu de 37% para 33%, talvez influenciada pelo avanço da mobilidade. Porém, segundo Fosse, mais pessoas têm utilizado a rede mundial de computadores para se relacionarem com instituições financeiras no Brasil. ""Tivemos um crescimento de 6 milhões de contas correntes que passaram a utilizar o canal entre 2014 e 2015"", cita o executivo, apontando que, atualmente, 62 milhões de brasileiros usam internet banking. ""A tecnologia se mostra um grande indutor da bancarização no Brasil"", avalia Fosse, comentando que 89,6% dos brasileiros já se relacionam com o sistema financeiro tradicional. Para suportar os avanços dos canais eletrônicos, a indústria investe pesado em TI. No último ano, o segmento aplicou nada menos que R$ 19 bilhões em projetos de tecnologia .",pt,64
217,2735,1478790716,CONTENT SHARED,-1425776303341065806,3609194402293569455,1020328625857133308,"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.116 Safari/537.36",SP,BR,HTML,http://startupi.com.br/2016/11/natura-inova-e-cria-bot-para-auxiliar-consumidores-a-escolherem-presentes-de-natal/,natura inova e cria bot para auxiliar consumidores a escolherem presentes de natal - startupi,"A Natura inaugura uma nova fronteira no atendimento ao cliente no Facebook por meio do Messenger, o aplicativo de mensagens instantâneas da plataforma digital. A empresa é uma das primeiras do setor de cosméticos a criar um ' bot ', aplicação de inteligência artificial que opera em aplicativos de mensagens para automatizar respostas, que irá auxiliar os consumidores a escolher o presente de Natal ideal para cada tipo de pessoa. Com a ajuda do 'bot', os clientes podem fazer escolhas mais adequadas ao responder a perguntas por meio de texto sobre, por exemplo, o estilo de vida da pessoa que vai ganhar o presente. Para acionar o bot, basta entrar na página da Natura no Facebook e mandar uma mensagem para a marca utilizando o Messenger. Depois, clicar em ""começar"" e em ""assistente de presentes"". ""Presentear nos aproxima ainda mais das pessoas queridas. É um momento em que nos dedicamos a esse relacionamento, ao fazer escolhas que correspondam às características de cada uma delas"", diz João Paulo Ferreira, presidente da Natura. ""Mas nem sempre é fácil acertar no presente. O 'bot' no Facebook simplifica e torna essa tarefa ainda mais agradável, oferecendo soluções surpreendentes para o consumidor"". O Facebook lançou os 'bots' para o Messenger em abril de 2016, durante a F8, a conferência para desenvolvedores, e já existem mais de 30 mil bots ativos no mundo. ""O Messenger do Facebook tem mais de 1 bilhão de pessoas ativas no mundo todo, o que representa um potencial de negócio incrível para as empresas"", afirma Marcos Angelini, diretor-geral do Facebook no Brasil. ""Nossas tecnologias aproximam as pessoas no Brasil inteiro, permitindo que elas se conectem e compartilhem o que mais importa para eles. Estamos felizes com esta parceria com a Natura, que vai usar nossa plataforma para revolucionar a interação com seus clientes"", completa o executivo. Para implementar o 'bot' na página da Natura, a empresa contou com a parceria das agências Salve e Insight, que foram as responsáveis pelo desenvolvimento da ferramenta. Além da assistência na escolha dos presentes, o 'bot' também está preparado para responder a diversas perguntas sobre a empresa e seus produtos.",pt,64
218,1543,1467046806,CONTENT SHARED,7270966256391553686,-4467650312287951120,6410806869226558749,,,,HTML,https://blogs.msdn.microsoft.com/dotnet/2016/06/27/announcing-net-core-1-0/,announcing .net core 1.0,"We are excited to announce the release of .NET Core 1.0, ASP.NET Core 1.0 and Entity Framework 1.0 , available on Windows, OS X and Linux! .NET Core is a cross-platform, open source, and modular .NET platform for creating modern web apps, microservices, libraries and console applications. This release includes the .NET Core runtime, libraries and tools and the ASP.NET Core libraries. We are also releasing Visual Studio and Visual Studio Code extensions that enable you to create .NET Core projects. You can get started at . Read the release notes for detailed release information. We are also releasing .NET documentation today at docs.microsoft.com , the new documentation service for Microsoft. The documentation you see there is just a start. You can follow our progress at core-docs on GitHub. ASP.NET Core documentation is also available and open source . Today we are at the Red Hat DevNation conference showing the release and our partnership with Red Hat. Watch the live stream via Channel 9 where Scott Hanselman will demonstrate .NET Core 1.0. .NET Core is now available on Red Hat Enterprise Linux and OpenShift via certified containers. In addition, .NET Core is fully supported by Red Hat and extended via the integrated hybrid support partnership between Microsoft and Red Hat. See the Red Hat Blog for more details. This is the biggest transformation of .NET since its inception and will define .NET for the next decade. We've rebuilt the foundation of .NET to be targeted at the needs of today's world: highly distributed cloud applications, micro services and containers. Moving forward .NET Framework and .NET Core and Xamarin are all important products that will continue to evolve, for Windows, cross-platform cloud and cross-platform mobile, respectively. The .NET Framework and traditional ASP.NET will continue to be relevant for your existing workloads. You can share code and reuse your skills across the entire .NET family so you can decide what to use and when, including mobile apps with Xamarin. And because we designed .NET to share a common library (the .NET standard library) .NET Framework, .NET Core and Xamarin apps will share new common capabilities in the future. It's really easy to try out .NET Core and ASP.NET Core on Windows, OS X or Linux. You can have an app up and running in a few minutes. You only need the .NET Core SDK to get started. The best place to start is the .NET Core home page. It will offer you the correct .NET Core SDK for the Operating System (OS) that you are using and the 3-4 steps you need to following to get started. It's pretty straightforward. To give you an idea, once you have the SDK installed, you can type these three simple commands for your first ""Hello World"" app. The first generates a template for you for a console app, the second restores package dependencies and the last builds and runs the app. You'll see (no surprise!): You'll likely get bored of ""Hello World"" quite quickly. You can read more in-depth tutorials at .NET Core Tutorials and ASP.NET Core Tutorials . Check out the Announcing EF Core 1.0 to find out how to get started with Entity Framework Core 1.0. About two years ago, we started receiving requests from some ASP.NET customers for "".NET on Linux"". Around the same time, we were talking to the Windows Server Team about Windows Nano, their future, much smaller server product. As a result, we started a new .NET project, which we codenamed ""Project K"", to target these new platforms. We changed the name, shape and experience of the product a few times along the way, at every turn trying to make it better and applicable to more scenarios and a broader base of developers. It's great to see this project finally available as .NET Core and ASP.NET Core 1.0. Open source is another important theme of this project. Over time, we noticed that all of the major web platforms were open source. ASP.NET MVC has been open source for a long time, but the platform underneath it, the .NET Framework, was not. We didn't have an answer for web developers who cared deeply about open source, and MVC being open wasn't enough. With today's releases, ASP.NET Core is now an open source web platform, top to bottom. Even the documentation is open source. ASP.NET Core is a great candidate for anyone who has open source as a requirement for their web stack. We'd like to express our gratitude for everyone that has tried .NET Core and ASP.NET Core and has given us feedback. We know that tens of thousands of you have been using the pre-1.0 product. Thanks! We've received a lot of feedback about design choices, user experience, performance, communication and other topics. We've tried our best to apply all of that feedback. The release is much better for it. We couldn't have done it without you. Thanks! If you are not a .NET developer or haven't used .NET in a while, now is a great moment to try it. You can enjoy the productivity and power of .NET with no constraints, on any OS, with any tool and for any application. All of that fully open source, developed with the community and with Microsoft's support. Check out dot.net to see the breadth of .NET options. This is a huge milestone and accomplishment for the entire .NET ecosystem. Nearly 10k developers contributed to .NET Core 1.0. We never imagined that many folks contributing to the product. We've also been impressed by the quality of the contributions. There are significant components that the community is driving forward. Nice work, folks! We also found that another 8k developers are watching these same repos, which effectively doubles the count. We believe that these developers watch these repos to either find that first opportunity to contribute or want to stay up-to-date on the project as part of their approach to .NET Core adoption. At this point, nearly half of all pull requests for .NET Core related projects (e.g. corefx, coreclr) come from the community. That's up from 20% one year ago. The momentum has been incredible. Check out the set of developers who contributed pull requests that were merged to the product. Thanks! Here's the breakdown of developers that created pull requests, created issues or made comments in any one of the .NET Core related repos, per organization, as determined by using the GitHub API: Total unique users: 9723 Note: The counts don't sum to the total because some users contribute to multiple organizations (thanks!) and we've tried to avoid double-counting. Note: The counts from the Microsoft org are specific to the few .NET Core-related repos that exist there, such as visualfsharp. Note: These numbers include Microsoft employees, which are (at most) 10% of the count. Samsung joins the .NET Foundation Increased interest in .NET Core has also driven deeper engagement in the .NET Foundation , which now manages more than 60 projects. Today we are announcing Samsung as the newest member . In April, Red Hat, Jet Brains and Unity were welcomed to the .NET Foundation Technical Steering Group. "".NET is a great technology that dramatically boosts developer productivity. Samsung has been contributing to .NET Core on GitHub - especially in the area of ARM support - and we are looking forward to contributing further to the .NET open source community. Samsung is glad to join the .NET Foundation's Technical Steering Group and help more developers enjoy the benefits of .NET."" Hong-Seok Kim, Vice President, Samsung Electronics. The contributions from Samsung have been impressive. They have a great team of developers that have taken an interest in .NET Core. We're glad to have them as part of the larger team. Some customers couldn't wait until the final 1.0 release and have been using preview versions of .NET Core in production, on Windows and Linux. These customers tell us that .NET Core has had a significant impact for their businesses. We look forward to seeing many of the applications that will get built over the next year. Please keep the feedback coming so that we can decide what to add next. Illyriad Games, the team behind Age of Ascent, reported a 10-fold increase in performance using ASP.NET Core with Azure Service Fabric. We are also extremely greatful for their code contributions to this performance. Thanks @benaadams ! NetEase, a leading IT company in China, provides online services for content, gaming, social media, communications and commerce, needed to stay on the leading edge of the ever-evolving mobile games space and chose .NET Core for their back end services. When compared to their previous Java back-end architecture: "".NET Core has reduced our release cycle by 20% and cost on engineering resources by 30%."" When speaking about the throughput improvements and cost savings: ""Additionally, it has made it possible to reduce the number of VMs needed in production by half."" We used industry benchmarks for web platforms on Linux as part of the release, including the TechEmpower Benchmarks . We've been sharing our findings as demonstrated in our own labs, starting several months ago. We're hoping to see official numbers from TechEmpower soon after our release. Our lab runs show that ASP.NET Core is faster than some of our industry peers. We see throughput that is 8x better than Node.js and almost 3x better than Go, on the same hardware. We're also not done! These improvements are from the changes that we were able to get into the 1.0 product. .NET developers know that the platform is a great choice for productivity. We want them to know that it's also a great choice for performance. We've been talking about .NET Core for about two years now, although it has changed significantly over that time. It's good to recap in this post what defines and is included in .NET Core 1.0. .NET Core is a new cross-platform .NET product. The primary selling points of .NET Core are: Cross-platform: Runs on Windows, macOS and Linux. Flexible deployment: Can be included in your app or installed side-by-side user- or machine-wide. Command-line tools: All product scenarios can be exercised at the command-line. Compatible: .NET Core is compatible with .NET Framework, Xamarin and Mono, via the .NET Standard Library . Open source: The .NET Core platform is open source, using MIT and Apache 2 licenses. Documentation is licensed under CC-BY . .NET Core is a .NET Foundation project. Supported by Microsoft: .NET Core is supported by Microsoft, per .NET Core Support Composition .NET Core is composed of the following parts: A .NET runtime , which provides a type system, assembly loading, a garbage collector, native interop and other basic services. A set of framework libraries , which provide primitive data types, app composition types and fundamental utilities. A set of SDK tools and language compilers that enable the base developer experience, available in the .NET Core SDK . The 'dotnet' app host , which is used to launch .NET Core apps. It selects the runtime and hosts the runtime, provides an assembly loading policy and launches the app. The same host is also used to launch SDK tools in the same way. Workloads By itself, .NET Core includes a single application model - console apps - which is useful for tools, local services and text-based games. Additional application models have been built on top of .NET Core to extend its functionality, such as: .NET Core Tools You typically start .NET Core development by installing the .NET Core SDK. The SDK includes enough software to build an app. The SDK gives you both the .NET Core Tools and a copy of .NET Core. As new versions of .NET Core are made available, you can download and install them without needing to get a new version of the tools. Apps specify their dependence on a particular .NET Core version via the project.json project file. The tools help you acquire and use that .NET Core version. You can switch between multiple apps on your machine in Visual Studio, Visual Studio Code or at a command prompt and the .NET Core tools will always pick the right version of .NET Core to use within the context of each app. You can also have multiple versions of the .NET Core tools on your machine, too, which can be important for continuous integration and other scenarios. Most of the time, you will just have one copy of the tools, since doing so provides a simpler experience. The dotnet Tool Your .NET Core experience will start with the dotnet tool . It exposes a set of commands for common operations, including restoring packages, building your project and unit testing. It also includes a command to create an empty new project to make it easy to get started. The following is a partial list of the commands. dotnet new - Initializes a sample console C# project. dotnet restore - Restores the dependencies for a given application. dotnet build - Builds a .NET Core application. dotnet publish - Publishes a .NET portable or self-contained application. dotnet run - Runs the application from source. dotnet test - Runs tests using a test runner specified in the project.json. dotnet pack - Creates a NuGet package of your code. dotnet works great with C# projects. F# and VB support is coming. .NET Standard Library The .NET Standard Library is a formal specification of .NET APIs that are intended to be available on all .NET runtimes. The motivation behind the Standard Library is establishing greater uniformity in the .NET ecosystem. ECMA 335 continues to establish uniformity for .NET runtime behavior, but there is no similar spec for the .NET Base Class Libraries (BCL) for .NET library implementations. The .NET Standard Library enables the following key scenarios: Defines uniform set of BCL APIs for all .NET platforms to implement, independent of workload. Enables developers to produce portable libraries that are usable across .NET runtimes, using this same set of APIs. Reduces and hopefully eliminates conditional compilation of shared source due to .NET APIs, only for OS APIs. .NET Core 1.0 implements the standard library, as does the .NET Framework and Xamarin. We see the standard library as a major focus of innovation and that benefits multiple .NET products. Support .NET Core is supported by Microsoft . You can use .NET Core in a development and deploy it in production and request support from Microsoft, as needed. Each release also has a defined lifecycle, where Microsoft will provides fixes, updates, or online technical assistance. The team adopted a new servicing model for .NET Core, with two different release types: Long Term Support (LTS) releases Typically a major release, such as ""1.0"" or ""2.0"" Supported for three years after the general availability date of a LTS release And one year after the general availability of a subsequent LTS release Fast Track Support (FTS) releases Typically a minor release, such as ""1.1"" or ""1.2"" Supported within the same three-year window as the parent LTS release And three months after the general availability of a subsequent FTS release And one year after the general availability of a subsequent LTS release Some customers want to deploy apps on very stable releases and do not want new features until the app is developed again. Those customers should consider LTS releases. Other customers want to take advantage of new features as soon as possible, particularly for apps that are almost always in development. Those customers should consider FTS releases. Note: We haven't released an FTS verion yet. .NET Core 1.0 is an LTS version. .NET Core Tools Telemetry The .NET Core tools include a telemetry feature so that we can collect usage information about the .NET Core Tools. It's important that we understand how the tools are being used so that we can improve them. Part of the reason the tools are in Preview is that we don't have enough information on the way that they will be used. The telemetry is only in the tools and does not affect your app. Behavior The telemetry feature is on by default. The data collected is anonymous in nature and will be published in an aggregated form for use by both Microsoft and community engineers under a Creative Commons license. You can opt-out of the telemetry feature by setting an environment variable DOTNET_CLI_TELEMETRY_OPTOUT (e.g. export on OS X/Linux, set on Windows) to true (e.g. ""true"", 1). Doing this will stop the collection process from running. Data Points The feature collects the following pieces of data: The command being used (e.g. ""build"", ""restore"") The ExitCode of the command For test projects, the test runner being used The timestamp of invocation The framework used Whether runtime IDs are present in the ""runtimes"" node The CLI version being used The feature will not collect any personal data, such as usernames or emails. It will not scan your code and not extract any project-level data that can be considered sensitive, such as name, repo or author (if you set those in your project.json). We want to know how the tools are used, not what you are using the tools to build. If you find sensitive data being collected, that's a bug. Please file an issue and it will be fixed. We use the MICROSOFT .NET LIBRARY EULA for the .NET Core Tools, which we also use for all .NET NuGet packages. We recently added a ""DATA"" section re-printed below, to enable telemetry from the tools. We want to stay with one EULA for .NET Core and only intend to collect data from the tools, not the runtime or libraries. You can build .NET Core apps with Visual Studio, Visual Studio Code or at the command-line. Visual Studio Code is the newest experience for building .NET apps. Let's take a look at building .NET Core apps with it. Using Visual Studio Code Show the experience using Visual Studio Code. To get started with .NET Core on Visual Studio Code, make sure you have downloaded and installed: You can verify that you have the latest version of .NET Core installed by opening a command prompt and typing dotnet --version . Your output should look like this: Next, you can create a new folder, scaffold a new ""Hello World"" C# application inside of it with the command line via the dotnet new command, then open Visual Studio Code in that directory with the code . command. If you don't have code on your PATH, you'll have to set it. If you don't have C# language plugin for Visual Studio Code it installed already, you'll want to do that. Next, you'll need to create and configure the launch.json and tasks.json files. Visual Studio Code will have asked if it can create these files for you. If you didn't allow it to do that, you will have to create these files yourself. Here's how: Create a new folder at the root level called .vscode and create the launch.json and tasks.json files inside of it. Open launch.json and configure it like this: Open the tasks.json file and configure it like this: Navigate to the Debug menu, click the Play icon, and now you can run your .NET Core applications! Note that if you open Visual Studio code from a different directory, you may need to change the values of cwd and program in launch.json and tasks.json to point to your application output folders. You can also debug your application by setting a breakpoint in the code and clicking the Play icon. The major differences between .NET Core and the .NET Framework: App-models - .NET Core does not support all the .NET Framework app-models, in part because many of them are built on Windows technologies, such as WPF (built on top of DirectX). The console and ASP.NET Core app-models are supported by both .NET Core and .NET Framework. APIs - .NET Core contains many of the same, but fewer, APIs as the .NET Framework, and with a different factoring (assembly names are different; type shape differs in key cases). These differences currently typically require changes to port source to .NET Core. .NET Core implements the .NET Standard Library API, which will grow to include more of the .NET Framework BCL APIs over time. Subsystems - .NET Core implements a subset of the subsystems in the .NET Framework, with the goal of a simpler implementation and programming model. For example, Code Access Security (CAS) is not supported, while reflection is supported. Platforms - The .NET Framework supports Windows and Windows Server while .NET Core also supports macOS and Linux. Open Source - .NET Core is open source, while a read-only subset of the .NET Framework is open source. While .NET Core is unique and has significant differences to the .NET Framework and other .NET platforms, it is straightforward to share code, using either source or binary sharing techniques. Thanks for all the feedback and usage. It's been a pleasure to build .NET Core and see so many people try it out. We really appreciate it. Please continue exploring the product and learning what it's capable of. We'll update the .NET Core Roadmap as we have clear plans for upcoming versions. Thanks for being part of the .NET community!",en,64
219,1256,1464982050,CONTENT SHARED,-5410531116380081703,2416280733544962613,3686082102675618374,,,,HTML,http://www.mckinsey.com/industries/consumer-packaged-goods/our-insights/meet-the-new-brazilian-consumer,meet the new brazilian consumer,"Amid one of the country's most severe recessions, how can consumer-goods companies and retailers succeed in Brazil? So much can change in just a few years. For most of the past decade, up until a couple of years ago, Brazil enjoyed a dramatic boom in consumption. Thirty-five million Brazilians-18 percent of the population-ascended to the middle class. As of 2012, Brazil's middle class encompassed 115 million people, or more than half of all Brazilians. But economic conditions in Brazil have taken a negative turn. Today, the country is facing one of the most severe recessions in its history. Will this recession be a short-term dip or a long slog? How are consumers changing their buying behavior? What can consumer-packaged-goods (CPG) and retail companies expect, and how should they adjust? What can Brazil learn from other countries' postrecession experiences-and are those lessons even applicable in Brazil? Our survey methodology Our recent survey of 1,000 Brazilian consumers confirms that most of them are indeed worried about their financial prospects and curtailing their spending. The Brazil survey was part of our broader global survey involving more than 22,000 respondents in 26 countries. Our aim in conducting the survey was to understand how consumers feel about their financial prospects and how these sentiments are affecting their buying behavior (see sidebar, ""Our survey methodology""). We found that even amid sharp declines in consumer confidence and private consumption, consumer companies can still find pockets of opportunity in Brazil. Cautious, concerned, and conservative Consumer confidence in Brazil was the lowest among 26 countries surveyed: only 8 percent of Brazilians were optimistic about the national economy (Exhibit 1). This bleak outlook is a departure from the positivity and optimism that Brazilians have historically displayed even in troubled times. It appears that the country's most recent economic woes have significantly shaken Brazilians' confidence. Fully 72 percent of Brazilians said they were worried that someone in their household would lose a job in the next year, and 49 percent said they were living paycheck to paycheck. A bump up in income wouldn't spur Brazilian consumers to spend much more. Even if their earnings were to rise by 10 percent, they said they'd spend only one-fourth of that extra money, much of it on everyday necessities such as food and drink. Almost half of the extra income would go toward savings and one-third toward paying off debt. Five attributes The survey responses brought to light a set of behavioral shifts among Brazilian consumers. Most of these behaviors are also evident among consumers elsewhere in the world, but they are amplified and intensified in Brazil. Consumer companies operating in the country would do well to view these shifts as a call to action. 1. They proactively search for savings. Consumers in Brazil are changing their buying behaviors in a variety of ways. Almost three out of every four respondents in Brazil agreed that they're ""increasingly looking for ways to save money"" (Exhibit 2). More than half said they are paying more attention to prices, actively looking for sales and promotions or delaying purchases. Many are also shopping at multiple stores to find the best deals or waiting for products to go on sale. In addition, Brazilian consumers are making thriftier food choices, with 42 percent saying they chose to eat at home more in the past year. 2. Some remain brand loyal-but only if the price is right. More than one-third of Brazilians claimed they haven't abandoned their preferred brands but are shopping around to find retailers that sell these brands at lower prices; 19 percent are purchasing in smaller quantities; and 14 percent are waiting until the brands are on sale or buying only with discount coupons (Exhibit 3). 3. Once they 'trade down,' they might not go back. Twenty-one percent of Brazilian consumers (versus 12 percent globally) said they traded down to less expensive brands. This number varied considerably by category; laundry supplies, household-cleaning products, and bottled water had the highest trade-down rates. Only 18 percent of consumers who traded down opted for private-label products. By contrast, in Latin America as a whole, 26 percent of down-traders chose private-label products; in more mature markets such as the United Kingdom and the United States, the figures were above 60 percent. Indeed, private-label or store brands in Brazil, although growing, still account for only a small fraction of total retail sales (5.1 percent, equivalent to approximately $970 million, according to data from Nielsen). Among Brazilian down-traders, 60 percent said they don't intend to go back to the more expensive brand (Exhibit 4). It's worth noting that US and European consumers who traded down during the global financial crisis have only recently begun to trade up again. If Brazil follows the same pattern, down-traders in Brazil will be trading down for at least the next few years. 4. There are splurgers in select categories. Even as most Brazilians sought to save money, some opted to trade up in certain categories. Although Brazil's trade-up rate of 5 percent is much lower than the global average of 11 percent, it's still meaningful in the handful of categories that clearly matter to consumers-in particular, alcoholic beverages and personal-care products. For example, 15 percent of consumers indicated they traded up in beer, 11 percent in wine, 10 percent in spirits, and 9 percent in cosmetics. Interestingly, in beer, a higher percentage of consumers stated having traded up versus traded down (12 percent). 5. They shop across channels. Brazilian consumers are shifting some of their spending to discount chains and the cash-and-carry format known locally as atacarejo , which combines both retail and wholesale. This trend isn't unique to Brazil: in more mature markets, such as the United Kingdom, consumers have already shifted much of their spending to discounters and are beginning to make distinctions between discount and ""premium"" discount. However, one channel that has made huge strides in some developed markets-the online channel-isn't yet a factor in Brazil's grocery categories. Imperatives for consumer companies In light of these new consumer behaviors, how can consumer-goods companies and retailers position themselves for success in a tough economic environment? In our view, they would be wise to act on the following imperatives. At every price point, think 'value for money.' With many consumers seeking savings opportunities, brands must give consumers solid reasons to choose their product over lower-priced alternatives. That means emphasizing not just the emotional attributes of a product but the functional benefits as well. They need to communicate a clear value proposition that resonates with consumers. Consider the success of salon-quality hair-care brands in several markets, including Brazil and the United States: these brands have been able to persuade consumers that they're getting good value for money, even though the salon-quality brand might be twice as expensive as their old shampoo brand. Urban world: The global consumers to watch Read the article Invest in advanced revenue-growth-management capabilities. By investing in cutting-edge revenue-growth-management solutions and analytical talent, leading companies arrive at data-driven answers to critical questions like these: Which promotions are most effective and why? How should prices and promotions be communicated? Based on these insights, companies determine optimal tactics and then devise granular strategies for their brands, portfolio, and promotions-and refine these strategies for each channel, customer segment, and region. Find granular opportunities for growth. To survive and thrive in Brazil's challenging economic environment, CPG companies and retailers need a granular, data-driven approach to identify pockets of growth. Consumption growth can vary considerably, both by region (for example, certain small and medium-size cities in Brazil's countryside are projected to grow at higher rates than state capitals) and by category (juices, for instance, are growing at two to three times the rate of carbonated drinks). Furthermore, a category's growth rates can vary significantly across cities and states. Review price architecture to capture both up-traders and down-traders. During downturns, Brazilians tend to gravitate to either high-end or low-end brands. CPG companies should have a clear and complete price architecture, with a premium offering to attract up-traders and a compelling low-priced offering aimed at down-traders and mass consumers. Higher-end products can justify their price points through a rich narrative (such as an artisanal process or a unique provenance) or new features. Value offerings, on the other hand, could emphasize bulk sizes, ""basic"" value cues in packaging or design, and no-frills but reliable quality and performance. Retailers, too, should consider offering a wider range of both high-end and low-end brands. Be thoughtful about channel changes. With Brazilian consumers doing more of their grocery shopping in discount and cash-and-carry formats, CPG manufacturers must launch initiatives to serve these fast-growing channels. Such initiatives might include the creation of exclusive SKUs, second-tier brands, or new pack sizes (especially as Brazilians have shown a willingness to buy in bulk and to engage in ""community shopping,"" or sharing their purchases with family and friends). Relentlessly optimize investments and operations. Efficiency is an imperative. Companies must untiringly pursue opportunities to optimize returns on their marketing investments, excel in store operations, and improve sales-force effectiveness. No drivers of value should remain unexplored; no excuses for delaying improvement initiatives should be made. The time is now to improve performance and efficiency and aim for excellence. In the early part of the past decade, for many consumer companies, building a strong presence in Brazil was synonymous with capturing rapid growth. We believe that, in the long run, the country will resume its growth trajectory-but in the current environment, the six imperatives discussed above must be at the top of the executive agenda. Efficiency, in particular, is crucial for any company that hopes to succeed in Brazil. About the author(s) Mariana Donatelli is an associate principal in McKinsey's São Paulo office, where Fernanda Hoefel is a partner, Suzana Resstom is an analyst, and Fábio Stul is a senior partner. Report - McKinsey Global Institute",en,63
220,1905,1469714751,CONTENT SHARED,-3780822597455574960,3609194402293569455,-3607853734254017965,,,,HTML,http://spectrum.ieee.org/computing/software/top-programming-languages-trends-the-rise-of-big-data,top programming languages trends: the rise of big data,"Illustration: Grzegorz Knec/Alamy Now that IEEE Spectrum is into the third year of annually ranking languages , we can start looking at some trends over time. What languages are ascendant? Which are losing traction? And which of the data sources that we use to create our rankings are contributing the most to these shifts? In this article I'm going to focus on so-called big-data languages, such as Julia , Python , R , and Scala . Most of these are purpose-built for handling large amounts of numeric data, with stables of packages that can be tapped for quick big-data analytic prototyping. These languages are increasingly important, as they facilitate the mining of the huge data sets that are now routinely collected across practically all sectors of government, science, and commerce. The biggest mover in this category was Go , an open source language created by Google to help solve the company's issues with scaling systems and concurrent programming back in 2007. In the default Spectrum ranking , it's moved up 10 positions since 2014 to settle into 10th place this year. Other big-data languages that saw moves since 2014 in the Spectrum ranking were R and Scala, with R ascending 4 spots and Scala moving up 2 (although down from 2015, when it was up 4 places from its 2014 position). Julia was added to the list of languages we track in 2015, and in the past year it's moved from rank 40 to 33, still a marginal player but clearly possessing some momentum in its growth. The chief reason for Go's quick rise in our ranking is the large increase in related activity on the GitHub source code archive. Since 2014, the total number of repositories on GitHub that list Go as the primary language went up by a factor of more than four. If we look at just active GitHub repositories, then there are almost five times as many. There's also a fair bit more chatter about the language on Reddit , with our data showing a threefold increase in the number of posts on that site mentioning the language. Explore the Interactive Rankings Another language that has continued to move up the rankings since 2014 is R, now in fifth place. R has been lifted in our rankings by racking up more questions on Stack Overflow -about 46 percent more since 2014. But even more important to R's rise is that it is increasingly mentioned in scholarly research papers. The Spectrum default ranking is heavily weighted toward data from IEEE Xplore , which indexes millions of scholarly articles, standards, and books in the IEEE database. In our 2015 ranking there were a mere 39 papers talking about the language, whereas this year we logged 244 papers. Contrary to the substantial gains in the rankings seen by open source languages such as Go, Julia, R, and Scala, proprietary data-analysis languages such as Matlab and SAS have seen a drop-off: Matlab fell four places in the rankings since 2014 and SAS has fallen seven. However, it's important to note that both of those languages are still growing; it's just that they're not growing as fast as some of the languages that are displacing them. When we weight the rankings toward jobs, we continue to see heavily used languages like Java and Python dominate. But recruiters are much more interested in R and Scala in 2016 then they were in 2014. When we collected data in 2014, there were only 136 jobs listed for Scala on CareerBuilder and Dice. But by 2016 there was more than a fourfold increase, to 631 jobs. This growth invites the question whether R can ever unseat Python or Java as the top languages for big data. But while R has seen huge gains over the last few years, Python and Java really are 800-pound gorillas. For instance, we found roughly 15 times as many job listings for pythonistas as for R developers. And while we measured about 63,000 new GitHub repositories in the last year for R, there were close to 458,000 for Python. Although R may be great for visualization and exploratory analysis and is clearly popular with academics writing research papers, Python has significant advantages for users in production environments: It's more easily integrated into production data pipelines, and as a general purpose language it simply has a broader array of uses. These data illustrate that despite the desire of some coders to evaluate languages on purely internal merits-the elegance of their syntax, or the degree and nature of the abstractions used-a big driver for a language's popularity will always be the domains that it targets, either by design or through the availability of supporting libraries.",en,63
221,2321,1473695293,CONTENT SHARED,4039031731864440026,881856221521045800,-2980396868411506251,,,,HTML,https://www.ibm.com/blogs/internet-of-things/olli-ai/,olli: artificial intelligence for the real world of connected commuting,"Automotive Moving fast with Olli. From concept, to prototype, through design, out into the market, and taking orders for sales of vehicles - all in less than three months. That's what disruptive technology is about. Olli is built by Local Motors , the start-up famous for creating the first 3D printed car. However, the brains for Olli were provided by IBM, which contributed its Watson artificial intelligence technology to the vehicle. Together, IBM and Local Motors have produced a vehicle that does far more than simply drive a predetermined route. Instead, Olli combines the capabilities of a chauffeur, a tour guide and a technology expert to communicate with passengers using spoken conversational language. Powered by IBM's Watson Internet of Things (IoT) for Automotive , Olli can take passengers to requested destinations, provide recommendations on where to go and answer questions about the vehicle, the journey and the surrounding environment. Learn more. Read the full story here . The adage 'garbage in, garbage out' may once have held true, particularly in the land of software programming where it originated, but in today's connected world, the Internet of Things is flipping it into 'one man's trash is another man's treasure' - literally. A team from the National Taipei University of Technology's Department of Electronic [...] For those wanting to dive in to IoT, our partnership with Cisco and SilverHook is the perfect example of why you should take the plunge, and get your feet wet... SilverHook powerboats These stunning vehicles reach speeds surpassing 100 MPH in the open ocean. SilverHook Powerboats pride themselves on their advanced monohull designs. These specialized [...] What's the future of mobility? How soon will we see self-driving cars? With ever increasing numbers of us living in cities, will we be able to get around or will we be trapped in perpetual gridlock? These are some of the thorny topics that AutoBlog wrestles with and on 6 October in Detroit, Michigan, they're [...]",en,63
222,2992,1485172712,CONTENT SHARED,5484061377044071389,102305705598210278,5527770709392883642,"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.116 Safari/537.36",SP,BR,HTML,https://www.narwhl.com/2015/03/the-ultimate-solution-to-versioning-rest-apis-content-negotiation/,the ultimate solution to versioning rest apis: content negotiation,"Versioning your API is terrifying. If you push out a ""breaking change"" - basically any change that runs counter to what client developers have planned for, such as renaming or deleting a parameter or changing the format of the response - you run the risk of bringing down many, if not all, of your customers' systems, leading to angry support calls or - worse - massive churn. For this reason, versioning is the number one concern among the development teams I work with when helping them design their APIs. The traditional way of handling versioning in REST is to apply some version number to the URL - either in the domain (i.e. apiv1.example.com/resource) or in the resource path (api.example.com/v1/resource). Non-breaking changes are frequently pushed out without much fanfare aside from a changelog posted to a blog or an update to the documentation. Breaking changes, on the other hand, require excessive testing, customer hand-holding, voluminous communication and a high level of planning, code maintenance and creative routing as you bend over backwards to ensure your customers can continue to work smoothly while you delicately push out a new release. With all that, everything can still go wrong. Most API producers handle these issues by placing tight restrictions on their development teams regarding when and how the API may be updated, leading to convoluted policies that often confuse client developers and confound innovation and iteration. For example, Facebook recently described their new versioning strategy for their Marketing API . In the old days, Facebook was rather cavalier about pushing out breaking changes. ""Move fast and break things"" worked fine for them, but annoyed the developers who relied on their API. Though they have apparently learned their lesson, their solution to versioning - which, to be fair, is common among RESTful API providers - prevents their API from taking advantage of continous releases and forces their client developers to assiduously watch for announcements about new releases. There's a better way. REST is closely tied to the HTTP specification, which has long had a way to communicate the format of the data exchanged between client and server - content negotiation . In fact, the ""Representational"" part of ""Representational State Transfer"" (for which REST is named) refers to this directly. Roy Fielding calls this out specifically in his 2000 thesis that defined REST (and which anyone talking about REST is obligated to reference). A resource may express one or more representations of its data based on the state of the resource. You typically see this manifest in APIs that support both JSON and XML responses - the client uses either a file extension in the URI or the ""Accept"" header to request their desired format. But that's just the tip of the iceberg. Resources should support more than simply the top-level format of their data - they should specify exactly what data that response contains. According to the Media Type specifications , you can define your own media types using either the ""vendor tree"" (i.e. ""application/vnd.example.resource+json"") or, to avoid registration requirements, the ""Unregistered x. Tree"" (i.e. ""application/x.example.resource+json""). In either case, you're clearly communicating to the client developer that the response they're receiving has a specified format beyond simply XML or JSON. You will need to provide documentation to your developers to describe the data each media type contains , but you should be doing that already. It seems odd to many to define new media types that effectively take advantage of existing media types. In this case, you can define the specific format of your response using the parameters afforded by the Media Type specification. For example, if I have a resource named ""product"", the media type for its basic JSON repesentation could be ""application/json; profile=vnd.example.product"". Defining the profile as a parameter indicates to the client that they should treat the JSON formatted data in a predetermined way. I have also seen - and advocated for - the looser format ""application/json;vnd.example.product"", but RFC wonks may shout you down on that. Regardless of how you present your media type, the version of your resource should also be reflected in it. For example, if you've made one or more breaking changes to the Product resource, you should add the version parameter to it (i.e. ""application/x.example.product+json; version=2"", ""application/json; profile=vnd.example.product version=2"" or ""application/json;vnd.example.product+v2""). Non-breaking changes should also be reflected using sub-versions (i.e. ""version=2.1""). Removing versioning from the URI in this way has a number of benefits: Rather than versioning the entire API when a single resource has a breaking change, you can version directly at the resource level. This allows teams responsible for only a handful of resources to operate independently and enforces the constraint that a resource is solely responsible for its state. Clients will only need to update their code when the resources they commonly use change rather than every time you upversion your entire API. Your team will no longer need to maintain various routing rules for URIs to redirect them to different back-end code bases, allowing for a cleaner and easier to use and cache set of URIs. Best of all, your API will no longer be tied to arcane release cycles separate from the rest of your application stack. So long as you maintain older versions of your representations for a reasonable period of time, you may release at will without breaking client code. You should also use content negotiation to define the different ""shapes"" of your resource - i.e. ""brief"" vs. ""full"", ""mobile"" vs. ""web"" and whatever else makes the most sense to provide for your customers. You can do this either through an additional custom media type or by adding a parameter (i.e. ""application/x.example.product+json; version=2 shape='full'""). It also introduces a few challenges: While URI routing is no longer an issue, routing of the correct response type gets moved up further in your API application stack. If you've stuck to an MVC pattern, your controller will be responsible for mapping the data from your model into the appropriate data structure while your views will be responsible for presenting it according to the requested media type. Clients will need to know the media type for each resource and request the same one throughout their use of your API to ensure their code continues to function normally as you push out new changes. You will still need to clearly communicate any changes to your API to allow your developers to determine how and when they will need to update their client code accordingly. Most of these challenges can be easily overcome. If you're not already taking advantage of the MVC pattern in your application stack, for example, you'd be wise to consider rearchitecting. Content negotiation doesn't necessarily eliminate the need for multiple code bases, but it does move it into a more manageble part of your code - specifically, the controllers and the views. As far as clients knowing the media type, I'd recommend they either store the media type they're using in a variable that gets passed in the Accept header or they do an ""Accept: / "" on the initial call and locally cache the media type that comes back for use with all subsequent calls. Content negotiation is not a common REST client consideration at this point, but that's no excuse for poor API design. Communicating API changes to developers has always been a challenge. You can tweet about them, update your blog, send an email and even call each developer directly and still have more than a few of them contacting your support team complaining about code broken during an update. At last year's API Craft conference in Detroit, some attendees recommended adding a link to resource responses that have been upversioned as an additional means of communication. I'm particularly fond of this as it gives the client developer the power to decide how to handle such updates directly from within their code. I recommend implementing this now, whether you're using content negotiation to handle your versioning or not. Essentially, if a developer is using an outdated version of the API, they should see an additional link in the response like this: The link relation follows the custom URN format (since this is not an IANA approved relation) with a relation of ""deprecated"". While this is not currently a standard, it should be. The link itself should go to some kind of message resource that contains the details of the deprecation - where you direct that is ultimately up to you. However, this resource should respond with at least two representations - one formatted appropriately for code (i.e. JSON or XML) and the other for human readability (i.e. an HTML page, blog post, changelog, etc.). The data found at this resource should show what the current version is and clearly explain the changes made to the resource that warranted a version change. The client developer should include a routine in their code to look for this link in all responses. If found, the client code should do whatever the developer deems necessary to alert the team responsible for maintaining this code about the change. This may include an email to an internal group, creating a Jira ticket using the details obtained by the deprecation link or any other preferred method guaranteed to get the attention of the team. Content negotiation is a clean, well-documented, standards-compliant way of handling a lot of the complexity found in managing and maintaining RESTful APIs. With everyone clamoring about hypermedia, the media type often gets overlooked. You've nothing to lose by beginning to implement it in your existing RESTful API, but so much to gain in added usability for your developer customers and flexibility and agility for you internal teams.",en,63
223,687,1461959979,CONTENT SHARED,-5756697018315640725,-1443636648652872475,-7777675406803857155,,,,HTML,https://medium.freecodecamp.com/being-a-developer-after-40-3c5dd112210c?gi=62f851a5d81a,being a developer after 40 - free code camp,"Being A Developer After 40 (This is the talk I have given at App Builders Switzerland on April 25th, 2016. The slides are available on SpeakerDeck and at the bottom of this article.) (A version in Russian is available in Habrahabr.ru .) Hi everyone, I am a forty-two years old self-taught developer, and this is my story. A couple of weeks ago I came by the tweet below, and it made me think about my career, and those thoughts brought me back to where it all began for me: I started my career as a software developer at precisely 10am, on Monday October 6th, 1997, somewhere in the city of Olivos , just north of Buenos Aires , Argentina . The moment was Unix Epoch 876142800 . I had recently celebrated my 24th birthday. The World In 1997 The world was a slightly different place back then. Websites did not have cookie warnings. The future of the web were portals like Excite.com . AltaVista was my preferred search engine. My e-mail was kosmacze@sc2a.unige.ch, which meant that my first personal website was located in We were still mourning Princess Lady Diana . Steve Jobs had taken the role of CEO and convinced Microsoft to inject 150 million dollars into Apple Computer. Digital Equipment Corporation was suing Dell. The remains of Che Guevara had just been brought back to Cuba. The fourth season of ""Friends"" had just started. Gianni Versace had just been murdered in front of his house. Mother Teresa , Roy Lichtenstein and Jeanne Calment (the world's oldest person ever) had just passed away. People were playing Final Fantasy 7 on their PlayStation like crazy. BBC 2 started broadcasting the Teletubbies . James Cameron was about to release Titanic . The Verve had just released their hit ""Bitter Sweet Symphony"" and then had to pay most royalties to the Rolling Stones. Smartphones looked like the Nokia 9000 Communicator ; they had 8 MB of memory, a 24 MHz i386 CPU and run the GEOS operating system. Smartwatches looked like the CASIO G-SHOCK DW-9100BJ . Not as many apps but the battery life was much longer. IBM Deep Blue had defeated for the first time Garry Kasparov in a game of chess. A hacker known as ""_eci"" published the C code for a Windows 3.1, 95 and NT exploit called ""WinNuke,"" a denial-of-service attack that on TCP port 139 (NetBIOS) causing a Blue Screen of Death. Incidentally, 1997 is also the year Malala Yousafzai , Chloë Grace Moretz and Kylie Jenner were born. Many film storylines take place in 1997, to name a few: Escape from New York , Predator 2 , The Curious Case of Benjamin Button , Harry Potter and the Half-Blood Prince , The Godfather III and according to Terminator 2: Judgement Day , Skynet became self-aware at 2:14 am on August 29, 1997. That did not happen; however, in an interesting turn of events, the domain google.com had been registered on September 15th that year. We were two years away from Y2K and the media were starting to get people nervous about it. My First Developer Job My first job consisted of writing ASP pages in various editors, ranging from Microsoft FrontPage , to HotMeTaL Pro to EditPlus , managing cross-browser compatibility between Netscape Navigator and Internet Explorer 4, and writing stored procedures in SQL Server 6.5 powering a commercial website published in Japanese, Russian, English and Spanish - without any consistent UTF-8 support across the software stack. The product of these efforts ran in a Pentium II server hosted somewhere in the USA, with a stunning 2 GB hard disk drive and a whooping 256 MB of RAM. It was a single server running Windows NT 4 , SQL Server 6.5 and IIS 2.0 , serving around ten thousand visitors per day. My first professional programming language was this mutant called VBScript , and of course a little bit of JavaScript on the client side, sprinkled with lots of ""if this is Netscape do this, else do that"" because back then I had no idea how to use JavaScript properly. Interestingly, it's 2016 and we are barely starting to understand how to do anything in JavaScript. Unit tests were unheard of. The Agile Manifesto had not been written yet. Continuous integration was a dream. XML was not even a buzzword. Our QA strategy consisted of restarting the server once a week, because otherwise it would crash randomly. We developed our own COM+ component in Visual J++ to parse JPEG files uploaded to the server. As soon as JPEG 2000 -encoded files started popping up, our component broke miserably. We did not use source control, not even CVS , RCS or, God forbid, SourceSafe . Subversion did not exist yet. Our Joel Test score was minus 25. 6776 Days For the past 6776 days I have had a cup of coffee in the morning and wrote code with things named VBScript, JavaScript, Linux, SQL, HTML, Makefiles, Node.js, CSS, XML, .NET, YAML, Podfiles, JSON, Markdown, PHP, Windows, Doxygen, C#, Visual Basic, Visual Basic.NET, Java, Socket.io, Ruby, unit tests, Python, shell scripts, C++, Objective-C, batch files, and lately Swift. In those 6776 days lots of things happened; most importantly, my wife and I got married. I quit 6 jobs and I was fired twice. I started and closed my own business. I finished my Master Degree. I published a few open source projects, and one of them landed me an article on Ars Technica by Erica Sadun herself . I was featured in Swiss and Bolivian TV shows. I watched live keynotes by Bill Gates and by Steve Jobs in Seattle and San Francisco. I spoke at and co-organised conferences in four continents. I wrote and published two books . I burned out twice (not the books, myself,) and lots of other things happened, both wonderful and horrible. I have often pondered about leaving the profession altogether. But somehow, code always calls me back after a while. I like to write apps, systems, software. To avoid burning out, I have had to develop strategies. In this talk I will give you my secrets, so that you too can reach the glorious age of 40 as an experienced developer, willing to continue in this profession. Advice For The Young At Heart Some simple tips to reach the glorious age of 40 as a happy software developer. 1. Forget The Hype The first advice I can give you all is, do not pay attention to hype. Every year there is a new programming language, framework, library, pattern, component architecture or paradigm that takes the blogosphere by storm. People get crazy about it. Conferences are given. Books are written. Gartner hype cycles rise and fall. Consultants charge insane amounts of money to teach, deploy or otherwise fuckup the lives of people in this industry. The press will support these horrors and will make you feel guilty if you do not pay attention to them. In 2000 it was SOAP & XML. In 2003 it was Model Driven Architecture and Software Factories . In 2006 it was Semantic Web and OLPC . In 2009 it was Augmented Reality . In 2012 it was Big Data . In 2015... Virtual Reality? Bots? Do not worry about hype. Keep doing your thing, keep learning what you were learning, and move on. Pay attention to it only if you have a genuine interest, or if you feel that it could bring you some benefit in the medium or long run. The reason for this lies in the fact that, as the Romans said in the past, Nil nove sul sole. Most of what you see and learn in computer science has been around for decades, and this fact is purposedly hidden beneath piles of marketing, books, blog posts and questions on Stack Overflow. Every new architecture is just a reimagination and a readaptation of an idea that was floating around for decades. 2. Choose Your Galaxy Wisely In our industry, every technology generates what I call a ""galaxy."" These galaxies feature stars but also black holes; meteoric changes that fade in the night, many planets, only a tiny fraction of which harbour some kind of life, and lots of cosmic dust and dark matter. Examples of galaxies are, for example, .NET, Cocoa, Node.js, PHP, Emacs, SAP, etc. Each of these features evangelists, developers, bloggers, podcasts, conferences, books, training courses, consulting services, and inclusion problems. Galaxies are built on the assumption that their underlying technology is the answer to all problems. Each galaxy, thus, is based in a wrong assumption. The developers from those different galaxies embody the prototypical attitudes that have brought that technology to life. They adhere to the ideas, and will enthusiatically wear the t-shirts and evangelize others about the merits of their choice. Actually, I use the term ""galaxy"" to avoid the slightly more appropriate if not less controversial term ""religion,"" which might describe this phenomenon better. In my personal case, I spent the first ten years of my career in the Microsoft galaxy, and the following nine in the Apple galaxy. I dare say, one of the biggest reasons why I changed galaxies was Steve Ballmer. I got tired of the general attitude of the Microsoft galaxy people against open source software. On the other hand, I also have to say that the Apple galaxy is a delightful place, full of artists and musicians and writers who, by chance or ill luck, happen to write software as well. I attended conferences in the Microsoft galaxy, like the Barcelona TechEd 2003, or various Tech Talks in Buenos Aires, Geneva or London. I even spoke at the Microsoft DevDays 2006 in Geneva. The general attitude of developers in the Microsoft galaxy is unfriendly, ""corporate"" and bound in secrecy, NDAs and cumbersome IT processes. The Apple galaxy was to me, back in 2006, exactly the opposite; it was full of people who were musicians, artists, painters; they would write software to support their passion, and they would write software with passion. It made all the difference, and to this day, I still enjoy tremendously this galaxy, the one we are in, right now, and that has brought us all together. And then the iPhone came out, and the rest is history. So my recommendation to you is: choose your galaxy wisely, enjoy it as much or as little as you want, but keep your telescope pointed towards the other galaxies, and prepare to make a hyperjump to other places if needed. 3. Learn About Software History This takes me to the next point: learn how your favorite technology came to be. Do you like C#? Do you know who created it? How did the .NET project came to be? Who was the lead architect? Which were the constraints of the project and why did the language turned out to be what it is now? Apply the same recipe to any language or CPU architecture that you enjoy or love: Python, Ruby, Java, whatever the programming language; learn their origins, how they came up to be. The same for operating systems, networking technologies, hardware, anything. Go and learn how people came up with those ideas, and how long they took to grow and mature. Because good software takes ten years , you know. The stories surrounding the genesis of our industry are fascinating, and will show you two things: first, that everything is a remix . Second, that you could be the one remixing the next big thing. No, scratch that: you are going to be the creators of the next big thing. And to help you get there, here is my (highly biased) selection of history books that I like and recommend: You will also learn to value those things that stood the test of time: Lisp , TeX , Unix , bash , C , Cocoa , Emacs , Vim , Python , ARM , GNU make , man pages . These are some examples of long-lasting useful things that are something to celebrate, cherish and learn from. 4. Keep on Learning Learn. Anything will do. Wanna learn Fortran? Go for it. Find Erlang interesting? Excellent. Think COBOL might be the next big thing in your career? Fantastic. Need to know more about Functional Reactive Programming ? Be my guest. Design? Of course. UX? You must. Poetry? You should . Many common concepts in Computer Science have been around for decades, which makes it worthwhile to learn old programming languages and frameworks; even ""arcane"" ones. First, it will make you appreciate the current state of the industry (or hate it, it depends,) and second, you will learn how to use the current tools more effectively - if anything, because you will understand its legacy and origins. Tip 1: learn at least one new programming language every year. I did not come up with this idea; The Pragmatic Programmer book did. And it works. One new programming language every year. Simple, huh? Go beyond the typical ""Hello, World"" stage, and build something useful with it. I usually build a simple calculator with whatever new technology I learn. It helps me figure out the syntax, it makes me familiar with the APIs or the IDE, etc. Tip 2: read at least 6 books per year. I have shown above a list of six must-read books; that should keep you busy for a year. Here goes the list for the second year: (OK, those are seven books.) Six books per year looks like a lot, but it only means one every 2 months. And most of the books I have mentioned in this presentation are not that long, and even better, they are outstandingly well written, they are fun and are full of insight. Look at it this way: if you are now 20 years old, by the age of 30 you will have read over 60 books, and over 120 when you reach my age. And you will have played with at least 20 different programming languages. Think about it for a second. Some of the twelve books I've selected for you have been written in the seventies, others in the eighties, some in the nineties and finally most of them are from the past decade. They represent the best writing I have come across in our industry. But do not just read them; take notes. Bookmark. Write on the pages of the books. Then re-read them every so often. Borges used to say that a bigger pleasure than reading a book is re-reading it. And also, please, buy those books you really like in paper format. Believe me. eBooks are overrated. Nothing beats the real thing. Of course, please know that as you will grow old, the number of things that qualify as new and/or important will drop dramatically. Prepare for this. It is OK to weep silently when you realise this. 5. Teach Once you have learnt, teach . This is very important. This does not mean that you should setup a classroom and invite people to hear your ramblings (although it would be awesome if you did!) It might mean that you give meaningful answers to questions in Stack Overflow; that you write a book; that you publish a podcast about your favorite technology; that you keep a blog; that you write on Medium; that you go to another continent and set up programming schools using Raspberry Pis; or that you help a younger developer by becoming their mentor (do not do this before the age of 30, though.) Teaching will make you more humble, because it will painfully show you how limited your knowledge is. Teaching is the best way to learn . Only by testing your knowledge against others are you going to learn properly. This will also make you more respectful regarding other developers and other technologies; every language, no matter how humble or arcane, has its place within the Tao of Programming , and only through teaching will you be able to feel it. And through teaching you can really, really make a difference in this world. Back in 2012 I received a mail from a person who had attended one of my trainings. She used to work as an Adobe Flash developer. Remember ActionScript and all that? Well, unsurprisingly after 12 years of working as a freelance Flash developer she suddenly found herself unemployed. Alone. With a baby to feed. She told me in her message that she had attended my training, that she had enjoyed it and also learnt something useful, and that after that she had found a job as a mobile web developer. She wrote to me to say thank you . I cannot claim that I changed the world, but I might have nudged it a little bit, into something (hopefully) better. This thought has made every lesson I gave since then much more worthwhile and meaningful. 6. Workplaces Suck Do not expect software corporations to offer any kind of career path. They might do this in the US, but I have never seen any of that in Europe. This means that you are solely responsible for the success of your career. Nobody will tell you ""oh, well, next year you can grow to be team leader, then manager, then CTO..."" Not. At. All. Quite the opposite, actually: you were, are and will be a software developer, that is, a relatively expensive factory worker, whose tasks your managers would be happy to offshore no matter what they tell you. Do not take a job just for the money. Software companies have become sweatshops where you are supposed to justify your absurdly high salary with insane hours and unreasonable expectations. And, at least in the case of Switzerland, there is no worker union to help you if things go bad. Actually there are worker unions in Switzerland, but they do not really care about situations that will not land them some kind of media exposure. Even worse; in most workplaces you will be harassed, particularly if you are a woman, a member of the LGBT community or from a non-caucasian ethnic group. I have seen developers threatened to have their work visas not renewed if they did not work faster. I have witnessed harassment of women and gay colleagues. Some parts of our industry are downright disgusting, and you do not need to be in Silicon Valley to live it. You do not need Medium to read it. You could experience that right here in Switzerland. Many banks have atrocious workplaces. Financial institutions want you to vomit code 15 hours a day, even if the Swiss working laws explicitly forbid such treatments. Pharmaceutical companies want you to write code to cheat test results and to help them bypass regulations. Startups want your skin, working for 18 hours for no compensation, telling you bullshit like ""because we give you stock options"" or ""because we are all team players."" It does not matter that you are Zach Holman and that you can claim in your CV that you literally wrote Github from scratch: you will be fired for the pettiest of reasons. It does not matter that the app brings more than half of your employer traffic and revenues; the API team will treat you and your ideas with contempt and sloppiness. I have been asked to work for free by very well known people in the industry, some of them even featured in Wikipedia, and it is simply appalling. I will not give out their names, but I will prevent any junior from getting close to them, because people working without ethics do not deserve anyone's brain. Whenever an HR manager tells you ""you must do this (whatever wrong thing in your frame of reference) because we pay you a salary,"" remember to answer the following: ""you pay me a salary, but I give you my brain in exchange, and I refuse to comply with this order."" And to top it all, they will put you in an open space, and for some reason they will be proud about it. Open spaces are a cancer. They are without a doubt the worst possible office layout ever invented, and the least appropriate for software development - or any type of brain work for that matter. Remember this: the fact that you understand something does not imply that you have to agree to it. Disobey authority. Say ""fuck you, I won't do what you tell me"" and change jobs. There are fantastic workplaces out there; not a lot, but they exist. I have been lucky enough to work in some of them. Do not let a bad job kill your enthusiasm. It is not worth it. Disobey and move on. Or, better yet, become independent. 7. Know Your Worth You have probably heard about the ""10x Software Engineer"" myth, right? Well here is the thing: it is not a myth, but it does not work they way you think it works. It works, however, from the point of view of the employer: a ""10x Software Engineer"" generates worth 10 times whatever the employer pays. That means that you she or he gets 100 KCHF per year, but she or he are actually creating a value worth over a million francs. And of course, they get the bonuses at the end of the fiscal year, because, you know, capitalism. Know your worth. Read Karl Marx and Thomas Piketty . Enough said. Keep moving; be like the shark that keeps on swimming, because your skills are extremely valuable. Speak out your salary, say it loud, blog about it, so that your peers know how much their work is worth. Companies want you to shut up about that, so that women are paid 70% of what men are paid. So speak up! Blog about it! Tweet it! I am making 135 KCHF per year. That was my current salary. How about you? And you? The more we speak out, the less inequality there will be. Any person doing my job with my experience should get the same money, regardless of race, sex, age or preferred football team. End of the story. But it is not like that. It is not. 8. Send The Elevator Down If you are a white male remember all the privilege you have enjoyed since birth just because you were born that way. It is your responsibility to change the industry and its bias towards more inclusion. It is your duty to send the elevator down. Take conscious decisions in your life. Be aware of your actions and their effect. Do not blush or be embarrased for changing your opinions. Say ""I'm sorry"" when required. Listen. Do not be a hotshot. Have integrity and self-respect. Do not critisize or make fun of the technology choices of your peers; for other people will have their own reasons to choose them, and they must be respected. Be prepared to change your mind at any time through learning. One day you might like Windows. One day you might like Android. I am actually liking some parts of Android lately. And that is OK. 9. LLVM Everybody is raving about Swift, but in reality what I pay more attention to these days is LLVM itself. I think LLVM is the most important software project today, as measured in its long-term impact. Objective-C blocks, Rust & Swift (the two most loved strongly typed and compiled programming languages in the 2016 StackOverflow developer survey ,) Dropbox Pyston , the Clang Static Analyser, ARC, Google Souper , Emscripten , LLVMSharp , Microsoft LLILC , Rubymotion , cheerp , watchOS apps, the Android NDK , Metal , all of these things were born out or powered by LLVM. There are compilers using LLVM as a backend for pretty much all the most important languages of today. The .NET CLR will eventually interoperate with it, and Mono already uses it. Facebook has tried to integrate LLVM with HHVM , and WebKit recently switched from LLVM to the new B3 JIT JavaScript compiler . LLVM is cross-platform , cross-CPU-architecture, cross-language, cross-compiler, cross-eyed-tested, free as in gratis and free as a bird. Learn all you can about LLVM. This is the galaxy where true innovation is happening now. This is the foundation for the next 20 years. 10. Follow Your Gut I had the gut feeling .NET was going to be big when I watched its introduction in June 2000 . I had the gut feeling the iPhone was going to be big when I watched its introduction in 2007 . In both cases people laughed at my face, literally. In both cases I followed my gut feeling and I guess things worked out well. Follow your gut. You might be lucky, too. 11. APIs Are King Great APIs enable great apps. If the API sucks, the app will suck, too, no matter how beautiful the design. Remember that chunky is better than chatty , and that clients should be dumb; push as much logic as you can down to the API. Do not invent your own security protocols. Learn a couple of server-side technologies, and make sure Node is one of those. Leave REST aside and embrace Socket.io, ZeroMQ, RabbitMQ, Erlang, XMPP; explore realtime as the next step in app development. Realtime is not only for chat apps. Remove polling from the equation forever. Oh, and start building bots around those APIs. Just saying. 12. Fight Complexity Simpler is better. Always. Remember the KISS principle. And I do not mean only at the UI level, but all the way until the deepest layers of your code. Refactoring, unit tests, code reviews, pull requests, all of these tools are at your disposal to make sure that the code you ship is the simplest possible architecture that works. This is how you build resilient systems for the long term. Conclusion The most important thing to remember is that your age does not matter. One of my sons said to me, ""Impossible, Dad. Mathematicians do all their best work by the time they're 40. And you're over 80. It's impossible for you to have a good idea now."" If you're still awake and alert mentally when you're over 80, you've got the advantage that you've lived a long time and you've seen many things, and you get perspective. I'm 86 now, and it's in the last few years that I've had these ideas. New ideas come along and you pick up bits here and there, and the time is ripe now, whereas it might not have been ripe five or 10 years ago. Michael Atiyah , Fields Medal and Abel Prize winner Mathematician, quoted in a Wired article. As long as your heart tells you to keep on coding and building new things, you will be young, forever. In 2035, exactly 19 years from now, somebody will give a talk at a software conference similar to this one, starting like this: ""Hi, I am 42 years old, and this is my story."" Hopefully one of you will be giving that presentation; otherwise, it will be an AI bot. You will provide some anecdotical facts about 2016, for example that it was the year when David Bowie , Umberto Eco , Gato Barbieri and Johan Cruyff passed away, or when SQL Server was made available in Linux , or when Google AlphaGo beat a Go champion, or when the Panama Papers and the Turkish Citizenship Database were leaked the same day, or when Google considered using Swift for Android for the first time , or as the last year in which people enjoyed this useless thing called privacy. We will be three years away from the Year 2038 Problem and people will be really nervous about it. Of course I do not know what will happen 19 years from now, but I can tell you three things that will happen for sure: Somebody will ask a question in Stack Overflow about how to filter email addresses using regular expressions. Somebody will release a new JavaScript framework. Somebody will build something cool on top of LLVM. And maybe you will remember this talk with a smile. Thank you so much for your attention.",en,63
224,1950,1470071010,CONTENT SHARED,5293701842202310496,-6067316262393890508,2245546651160341154,,,,HTML,https://contently.com/strategist/2016/07/29/can-accenture-take-over-advertising/,can accenture take over advertising?,"Last month, in New York City's Soho neighborhood, global strategic consulting firm Accenture did something that might surprise you: It opened a 10,000-square-foot content studio. The goal? To develop advertising and marketing creative for its clients. Accenture's content studio includes a 3,500-square-foot post-production facility where the firm has the ability to cut and edit anything from a 30-second Facebook video to a Super Bowl spot. And there are six more studios in the works. The move is the latest in a string of developments that are pitting consultancies against agencies. Both are battling for content marketing dollars, relationships with C-suite executives, and above all, longterm partnerships with brands than transcend individual campaigns-and the conflict threatens to change the landscape of the advertising world. An opening In the past, consultancies focused on providing industry-specific business and management advice to brand clients. Now, thanks to an opening created by the digital disruption of advertising, they've created studios and digital agency arms meant to attract business that traditionally went to advertising agencies. How big are digital agencies born of strategic consultancy firms? Research firm and online resource Econsultancy ranked digital agencies based on their earnings for its "" Top 100 Digital Agencies "" report. Of the top five agencies, three-IBM iX, Accenture Interactive, and Deloitte Digital UK-are subsidiaries of consultancies. Plenty of the services offered by consultancy agencies are also offered by traditional ad agencies: marketing management, branding strategy, ad and content development, media planning, and so on. What consultancy agencies like Accenture Interactive and Deloitte Digital claim is that they have advantages ad agencies simply can't match, like vertical experts, global consumer insight, the manpower to produce thousands of pieces of content, and-because of their existing consulting relationships-a better understanding of how digital marketing can fit into an overall business strategy. In other words, consultancy agencies are leveraging their relationship with their larger consultancy parent companies for an edge. Meanwhile, ad agencies find themselves under assault by publisher agencies and major tech platforms , and the agency-of-record (AOR) model on which agency-marketer relationships have long been built-whereby companies assign their creative work to a single agency-appears to be waning. In recent years, Mondelez , Best Buy , and Frito-Lay have all moved away from AORs to instead work with multiple specialized shops. At last year's Association of National Advertisers' Masters of Marketing event, the president of PepsiCo's global beverage group warned that ""the agency model is not going to bend-it's going to break."" Are consultancy agencies, with their cross-business acumen and resources, poised to disrupt an industry that dates back centuries ? ""There is no doubt that traditional agencies are feeling the heat,"" said Susan Borst, senior director of industry initiatives with the Interactive Advertising Bureau (IAB). ""This is definitely an interesting time in ad land."" How media's transformation transformed consultancies Accenture's shift toward digital content marketing began several years ago with the launch of Accenture Digital in 2013. The company acquired London-based design firm Fjord in 2013, followed by Austin-based creative studio Chaotic Moon last year. What was starting to happen is that Accenture was being asked to ""step into the creative space,"" said Donna Tuths, Accenture Interactive's managing director and global head of digital content. The thinking was that if Accenture already handled companies' business strategy and intelligence, technology, infrastructure, and cloud computing needs, why not take on digital marketing, mobility, and analytics, too? At the same time, clients were increasingly ""decoupling"" their advertising production, meaning that they transferred video duties away from their agencies to work with production and post-production companies directly. That created an opportunity for Accenture to swoop in with a full-service digital offering. Accenture Interactive (part of Accenture Digital) categorizes its services as Experience, Marketing, Commerce, and Content, with Content being the largest. ""I see many of these things as no longer being the domain of agencies,"" said Tuths, who worked with such agencies as Ogilvy and Organic prior to joining Accenture. ""In a virtual world, you are your content. ... Consumers have begun to understand that, while many companies are just beginning to."" What's more, she said, ""They're starting to see the folly in trusting their brands to short-term, campaign-driven partners."" The move from a campaign mindset to an always-on mentality factored into the growth and success of strategic consultancy agency Deloitte Digital, which launched in 2012, as well. Alan Schulman, national director of brand creative and content marketing for Deloitte Digital, spent much of the last two decades making TV campaigns and accompanying digital content for major agencies. But he believes that approach is on its way out. ""You can't just go from campaign to campaign anymore,"" Schulman said, referring to the way many brands work with agencies on a project by project basis. ""We're now living in a time when consumers can turn your brand on or off 24/7, 365 days, so you better have stuff out there."" But Denise Blasevick, CEO of advertising and public relations agency The S3 Agency in New Jersey, says there's still a place for campaigns in the modern marketing world. ""For brand-driven organizations, I see the right mix as combining carefully pulsed content with appropriate punches of campaign advertising,"" she explained. ""That mix of strategic planning and creative execution remains the stronghold of traditional agencies."" Some in the agency world also maintain consultancies simply can't match the creative power and experience of ad agencies. ""Our business is-at the core- about ideas,"" said Laurent Ezekiel, managing director of the New York region and global client services director with DigitasLBi . ""And whilst our business has evolved and we live in an era of fragmentation, the alchemy of creativity and technology is where the magic happens for brands."" Ezekiel adds that DigitasLBi, which is owned by multinational advertising company Publicis Groupe and ranked seventh on Econsultancy's top agency list , has the benefit of experience. ""Since we, the agencies, have been in the creative business for several generations, clients will continue to engage with us to help evolve their brands."" The C-suite access advantage Schulman says he's been ""mightily impressed"" with what he's seen at Deloitte Digital since joining just over a year ago. That includes having direct access to decision makers on the brand client side, an advantage ad agencies have a difficult time matching. ""Consultancies are already pretty embedded in the C-suite, so it makes sense to start from strategy first and go from there to content,"" Schulman said, noting that all strategic directives come from Deloitte Consulting's Strategy & Operations group, while Deloitte Digital provides the copywriting, art direction, user experience, experiential design, and customer experience know-how. Bridging the gap between content and business operations, he notes, helps Deloitte Digital ensure that it's not just making buzz-worthy creative, but meeting its clients' overall and long-term goals. But for agencies of all kinds, a challenge remains: convincing CFOs of the critical importance of content. Digital content can be slow to deliver a return on investment. According to the Content Marketing Institute, it can take upwards of 15 months to monetize a content marketing program-which can make it a tough sell. The Content Marketing Institute also reports that only 30 percent of B2B marketers believe their organizations are effective at content marketing. Just 37 percent of B2C marketers say their content marketing is ""sophisticated or mature."" ""There's the tech, plumbing layer of marketing, and there's the poetry, creative layer,"" Schulman explained. ""Getting big organizations to think of these together and switch from a campaign structure to content marketing is not easy."" Deloitte Digital offers workshops and education tools around ideation, workflow process, and implementing creative to its C-Suite customers with the aim of getting them on board for the long haul. Schulman adds that CFOs ""glaze over pretty quickly"" when agencies talk about likes and shares. ""They want to know you were able to move the brand, advocacy, commercial results, or engagement from X to Y,"" he said. When a firm already has a CFO's ear-as is the case with Deloitte's clients, who already use the company for strategy and business operations advice-it stands to reason that it has an edge over traditional agencies, which tend to work primarily with heads of marketing. Cross-industry capability, global strength Most of today's digital agencies are either boutique shops or offshoots of multinational advertising networks. Deloitte and Accenture believe that what they can offer in terms of consumer insight and industry expertise outstrips both of these models. ""In my agency career I never encountered such a depth of vertical industry expertise and research as what we have at Deloitte,"" said Schulman. ""As a former agency person, it's refreshing to be getting the kind of access that I have now."" Deloitte Digital, which has six offices in the U.S. and 33 abroad, boasts experts in consumer and industrial products, energy and resources, federal, financial services, life sciences and healthcare, public sector, technology, media, and telecom. ""That enables me as head of the content group to reach out and staff someone from a vertical industry who's really steeped in what's going on,"" Schulman said. That expert then works in tandem with content strategy and content creation to form a ""very purpose-built team."" Accenture, meanwhile, has about 5,000 employees working in the content space, spanning about 20 locations around the globe. ""We operate some of the largest content operations in the world,"" Tuths said. ""Even big agencies just are not able to house the same range of technical skills or attract the same level of talent."" As an example, Tuths points to one of her retail clients, whom she opted not to name in order to protect its marketing strategy. The company offers monthly subscriptions to a box of curated products. For every box, the brand has been creating four how-to videos, but it will soon be adding both more boxes and more products, so its needs for instructional videos are becoming increasingly complex. ""Honestly, these types of challenges are not really for your creative agency,"" she said, pointing out that many agencies outsource their video work. ""We take a much more strategic and long-term approach to the content space. It's about today, but more about tomorrow and helping our clients navigate the space and grow."" DigitasLBi's Ezekiel counters that agencies have evolved more than some think. ""Honestly, six or seven years ago we were not set up in a way [that] our markets could benefit from each other's knowledge, and this is because there was no pressure on the demand side,"" he said. ""Essentially, we didn't have enough global clients 'stitching' the network together."" That has since changed, largely through the growth of the agency's global client base. ""It's through these global clients, who represent over 35 percent of our revenues today, that we have developed a clear process and toolkit for multi-market engagements."" And the winner is? Last year, media research firm PQ Media reported that by the year 2019, U.S. brands will be spending upwards of $300 billion per year on content marketing. Will they opt to stick with the agencies they know, or will they jump ship to new competitors like consultancies? Or will they opt for a third route and increasingly bring content marketing in-house with the help of freelancers and content marketing technology solutions, as companies like Coca-Cola and Cisco have done? We know where consultancies are placing their bets-so how do agencies view these new competitors? ""I think both types of companies have their unique skills, experience, and heritage,"" Ezekiel said. ""The digital creative agencies will always put ideas first and therefore are set apart from the consultancies. The consultancies have a strong respected heritage in business consulting. The convergence of these two is where the future lies, but we believe that the brands that focus on ideas and their reasons to exist will prevail."" ""I would argue that brand-driven companies will win with traditional agencies,"" said Blasevick. ""The content [we] create is driven by the brand and not the channel. I have never heard a single person complain that there is a shortage of content out there. The opposite is true: people are feeling overwhelmed by content-and because of that, they are tuning out if it isn't on brand."" According to the IAB's Susan Borst, the spoils will belong to the firms that best grasp advertising's technological transformation and the ""radically new way of thinking about creative and messaging"" that it requires. ""Those who crack the digital and mobile nut,"" she said, ""with clients that understand and embrace the need for fundamental change, will win in the long-term.""",en,63
225,2384,1474400010,CONTENT SHARED,-7623502978685822577,-709287718034731589,-7053709891702726147,,,,HTML,https://blog.nugget.one/2016/09/04/dont-start-big-start-a-little-snowball/,"don't start big, start a little snowball","Little Snowballs When myself and my co-host interviewed Travis Kalanick on our podcast, he had recently co-founded a little snowball called UberCab. It was so early in Uber's existence he didn't even mention it. I notice Uber falls into a category of companies I call little snowballs. There are some fundamental features these companies have in common. I thoguht it might be helpful to list a few little snowballs and then talk about how you can go about starting your own. Uber Travis Kalanick and Garrett Camp commissioned the creation of a simple app that enabled users to hail a black car on demand. To validate they asked friends and colleagues to install the app. They hired a driver & watched what happened. After a few months of hustle the app hit 10 rides in one day. Airbnb Brian Chesky and Joe Gebbia bought a few airbeds and put up a static site called ""Air Bed and Breakfast"". They expanded the concept so that other people could offer airbeds in their homes and make money too. Then they expanded the idea to rooms, then to entire apartments. Their journey was a hustle from hell, but they made it happen. Google Sergey Brin and Larry Page built a web search engine. It was one page with a search box and submit button. The backend was a simple database search returning ranked results based on the number of backlinks. It did a really good job at providing search results, and took off. Slack Stewart Butterfield worked with his Tiny Speck group to build a better team messaging app. Slack makes it really easy for people to signup and start chatting as a team. At it's core Slack is a simple IM client. Other examples AT&T, Dollar Shave Club, Buffer, Shazam, DropBox What is a Little Snowball? ""A startup that is founded to do-one-simple-thing-well, and that can be massively grown outward from that starting point."" Typical markers: Very simple idea with a single facet Extremely low product surface area (frontend and backend) Can generally be prototyped and brought to market in months Is instantly useful and quickly gains early-adopter traction Each of these markers makes it considerably easier for you as a founder to start and grow the first version of your business. A Simple Single Facet Just about every part of growing your business becomes easier if the foundational idea is do-one-thing-well. I've been part of a number of startups where it was almost impossible to explain what we do in a single sentence. This wreaks havok during sales meetings and investor pitches. However, when your startup is a single cell organism, customers and investors are not confused about what you do. Word of mouth and viral loop growth becomes significantly easier to achieve. You can focus on growing the business, rather than trying to understand what the business is. A Low Product Surface Area This is a huge, almost unfair, advantage. Dollar Shave Club is the ultimate example. An idea you can build and launch in a weekend. Then, from Monday forward, spend all your time, effort and money on marketing and learning about your customers. I learnt this lesson the hard way. I lunched my tool Pluggio, a tool that buffers social content, at the same time as Buffer. Buffer focused on doing one-thing-well while I built Pluggio out to become a social media dashboard. Ironically, no matter how many features I added, Pluggio's main selling point continued to be social buffering. So, while I was adding 100k lines of code to make a really slick single page web app, buffer was marketing and integrating their very simple app. Due to my high product surface area my time was sucked up by support requests. Sales were harder because the product was harder to explain. Integrations were a non starter because the product was to complex. After 3 years Pluggio's yearly revenue was $50k and Buffer's was $1m. Fast to Prototype and Launch The sooner you can get to market the sooner you can validate and start learning about how your customers make purchase or adoption decisions. Often times you can quickly validate a business by taking very manual steps that don't scale. For example, if you wanted to validate the business hypothesis - People will order coffee from a delivery app - you could get 250-500 business cards printed out saying - We deliver coffee. Call us on 123-123-1234. - then hand them out at the mall at breakfast time. The very next day you will have some great information about your basic hypothesis. Build Something People Want The final little snowball component is to make sure your core idea is something people actually want. Without that je ne sais quoi your rolling stone will gather no moss. How Can I Find My Own Little Snowball? Full disclosure, I am founder of Nugget (startup incubator & community). Our primary purpose is to help you find, and grow, your own little snowball. That said, here are some tips to help you come up with off the cuff ideas: Think Small This sounds easier than it is. Many founders think at large scale because they want to build a Google or Uber. They feel it's too much of a gamble to think about smaller things. Another common trap is to get caught in the mindset of thinking more features equals more sales. To think small, you should notice details of everyday problems going on around you and try to find do-one-thing-well opportunities. Notice Problems Problems are everywhere. The trick is to open your consciousness to the stream of problems that life presents us with. You would be surprised at how capable we are of drowning out regularly annoying tasks that become part of our daily life. Now it's time to start to notice those things we've trained ourself to ignore. One note about this, it's very important you solve a problem that other people have. That sounds like obvious advice, but if no one else has the problem, you're dead in the water. Look For Optimizations Optimization essentially means - to make something better - and is really just another way to solve a problem. However, thinking through the mental model of optimization, can lead you to come up with some interesting product ideas. For example the core essence of Buffer is an optimization of Twitter. As a side note. Question: Why didn't Twitter simply add that feature when they saw it take off in Buffer, Hootsuite and other apps? Answer: They were too busy staying on task and building their own little snowball. Explore Old Ideas You probably already have a number of ideas you've been thinking about and possibly even products you've launched. Have another look at them through the lens of what we've discussed here. With any luck you might find a single facet that can be extracted from that old work and turned into a do-one-thing-well little snowball. . . . I'm building Nugget (startup incubator & community). We send a new SaaS startup idea to your inbox every single day. Signup for free .",en,63
226,1227,1464890552,CONTENT SHARED,-8771338872124599367,3609194402293569455,6147896666617046192,,,,HTML,https://medium.com/coolhow-creative-lab/funcion%C3%A1rios-do-m%C3%AAs-no-coolhow-os-slackbots-32bbcc322575,funcionários do mês no coolhow: os slackbots - coolhow creative lab,"Funcionários do mês no CoolHow: os Slackbots Vamos ser sinceros, nem sempre é fácil mudar os costumes e adotar novas técnicas ou ferramentas. O Slack também não foi amor à primeira vista, mas hoje em dia é o queridinho do CoolHow - ainda mais depois que a Íris descobriu os bots maravilhosos da plataforma. Para os que veem o Slack ""apenas"" como uma forma de comunicação e organização, hora do mindblow: dá pra pedir Uber , falar com psicólogos de verdade, jogar Pokémon e fazer basicamente qualquer coisa nele. E hoje vamos apresentar os bots que andam nos ajudando demais. Se você precisa analisar métricas do Google Analytics , New Relic ou Mixpanel , conheça o seu novo melhor amigo: Statsbot. Com ele, você pode conectar os dados das plataformas diretamente ao Slack e pedir que o bot produza relatórios diários, semanais ou mensais. Caso você tenha várias contas, você ainda pode dividi-las em canais e ajustar para relatar apenas os dados necessários. Obrigado por existir, Statsbot! ❤ Quase uma mãe para os que vivem esquecendo as coisas, o Luno responde dúvidas de forma automática. Por exemplo: se você ou sua equipe querem ter o e-mail do cliente sempre à mão, basta inserir a pergunta ""qual é o e-mail do cliente?"" e a resposta está em sua base de dados. Da próxima vez que bater a dúvida, basta perguntar. O fofoqueiro, mas do bem. Sempre que sua empresa ou outra palavra-chave for mencionada, ele te conta na hora. Ele pesquisa, gratuitamente, menções no Twitter, Tumblr, Hacker News, Produc Hunt e Reddit, mas a versão beta está disponibilizando Google +, Instagram e YouTube, que futuramente serão pagos. Fontes como Facebook, Blogs, portais de notícia, Medium e outras plataformas (igualmente pagas) ainda serão adicionadas. Agora que você é o mestre dos bots da empresa, seja também o mais legal. Diariamente, o Humblebot dá dicas para que você se torne uma pessoa melhor. O nosso de hoje foi reconhecer alguém por algo que ele tenha feito recentemente. Bônus: Facebook Bot O And Chill não é do Slack, mas merece estar aqui. Disponível para SMS ou Facebook Messenger, o bot te ajuda a escolher o próximo filme que você assistirá. Como? Primeiro, ele pede que você cite um filme que gostou e porquê. A partir de um algorítimo secreto que seus criadores não revelaram como funciona, ele sugere alguns outros filmes. Achou bom demais para ser verdade? A galera da Tech Crunch também, e eles contam como foi colocar o bot à prova.",pt,63
227,441,1460735810,CONTENT SHARED,3689128258624052102,-5803328010978180572,166134078931949885,,,,HTML,http://www.jornaldacidadeonline.com.br/noticias/2501/deputada-mais-jovem-da-camara-silencia-o-plenario-com-lucidez-do-discurso-veja-o-video,deputada mais jovem da câmara silencia o plenário com lucidez do discurso (veja o vídeo),"Deputada mais jovem da Câmara silencia o plenário com lucidez do discurso (veja o vídeo) Mariana Carvalho, deputada de Rondônia, 29 anos, médica e advogada, surpreendeu o plenário da Câmara Federal com o discurso mais incisivo e lúcido até o momento, a favor do impeachment da presidente Dilma Rousseff. A paulistana, radicada em Rondônia, onde iniciou suas atividades políticas com apenas 16 anos de idade, conseguiu em apenas 15 minutos, sintetizar todos os motivos e motivações que frustraram a população brasileira, após a reeleição da presidente Dilma Rousseff. O plenário da Câmara silenciou para ouvir a jovem parlamentar. Mariana Carvalho, não obstante a pouca idade, tem uma imensa bagagem política. Foi presidente do PSDB jovem em Rondônia, vereadora em Porto Velho, candidata a prefeita em 2012 e deputada federal em 2014, a terceira mais votada do estado, alcançando o percentual de 7,55% dos votos. Vale a pena ver o discurso da Redação Veja o vídeo: Se você é a favor de uma imprensa totalmente livre e imparcial, colabore curtindo a nossa página no Facebook e visitando com frequência o site do Jornal da Cidade Online",pt,63
228,51,1459338001,CONTENT SHARED,4774970687540378081,1895326251577378793,-167447336240321466,,,,HTML,http://www.mckinsey.com/business-functions/strategy-and-corporate-finance/our-insights/the-economic-essentials-of-digital-strategy,the economic essentials of digital strategy,"A supply-and-demand guide to digital disruption. In July 2015, during the championship round of the World Surf League's J-Bay Open, in South Africa, a great white shark attacked Australian surfing star Mick Fanning. Right before the attack, Fanning said later, he had the eerie feeling that ""something was behind me."" Then he turned and saw the fin. A digital-strategy framework Thankfully, Fanning was unharmed. But the incident reverberated in the surfing world, whose denizens face not only the danger of loss of limb or life from sharks-surfers account for nearly half of all shark victims-but also the uncomfortable, even terrifying feeling that can accompany unseen perils. Just two years earlier, off the coast of Nazaré, Portugal, Brazilian surfer Carlos Burle rode what, unofficially, at least, ranks as the largest wave in history. He is a member of a small group of people who, backed by board shapers and other support personnel, tackle the planet's biggest, most fearsome, and most impressive waves. Working in small teams, they are totally committed to riding them, testing the limits of human performance that extreme conditions offer. Instead of a threat of peril, they turn stormy seas into an opportunity for amazing human accomplishment. Digital Disruption These days, something of a mix of the fear of sharks and the thrill of big-wave surfing pervades the executive suites we visit, when the conversation turns to the threats and opportunities arising from digitization. The digitization of processes and interfaces is itself a source of worry. But the feeling of not knowing when, or from which direction, an effective attack on a business might come creates a whole different level of concern. News-making digital attackers now successfully disrupt existing business models-often far beyond the attackers' national boundaries: Simple (later bought by BBVA) took on big-cap banks without opening a single branch. A DIY investment tool from Acorns shook up the financial-advisory business. Snapchat got a jump on mainstream media by distributing content on a platform-as-a-service infrastructure. Web and mobile-based map applications broke GPS companies' hold on the personal navigation market. No wonder many business leaders live in a heightened state of alert. Thanks to outsourced cloud infrastructure, mix-and-match technology components, and a steady flood of venture money, start-ups and established attackers can bite before their victims even see the fin. At the same time, the opportunities presented by digital disruption excite and allure. Forward-leaning companies are immersing themselves deeply in the world of the attackers, seeking to harness new technologies, and rethinking their business models-the better to catch and ride a disruptive wave of their own. But they are increasingly concerned that dealing with the shark they can see is not enough-others may lurk below the surface. Deeper forces Consider an insurance company in which the CEO and her top team have reconvened following a recent trip to Silicon Valley, where they went to observe the forces reshaping, and potentially upending, their business. The team has seen how technology companies are exploiting data, virtualizing infrastructure, reimagining customer experiences, and seemingly injecting social features into everything. Now it is buzzing with new insights, new possibilities, and new threats. The team's members take stock of what they've seen and who might disrupt their business. They make a list including not only many insurance start-ups but also, ominously, tech giants such as Google and Uber-companies whose driverless cars, command of data, and reimagined transportation alternatives could change the fundamentals of insurance. Soon the team has charted who needs to be monitored, what partnerships need to be pursued, and which digital initiatives need to be launched. Just as the team's members begin to feel satisfied with their efforts, the CEO brings the proceedings to a halt. ""Hang on,"" she says. ""Are we sure we really understand the nature of the disruption we face? What about the next 50 start-ups and the next wave of innovations? How can we monitor them all? Don't we need to focus more on the nature of the disruption we expect to occur in our industry rather than on who the disruptors are today? I'm pretty sure most of those on our list won't be around in a decade, yet by then we will have been fundamentally disrupted. And how do we get ahead of these trends so we can be the disruptors, too?"" This discussion resembles many we hear from management teams thoughtful about digital disruption, which is pushing them to develop a view of the deeper forces behind it. An understanding of those forces, combined with solid analysis, can help explain not so much which companies will disrupt a business as why -the nature of the transformation and disruption they face rather than just the specific parties that might initiate them. In helping executives to answer this question, we have-paradoxically, perhaps, since digital ""makes everything new""-returned to the fundamentals of supply, demand, and market dynamics to clarify the sources of digital disruption and the conditions in which it occurs. We explore supply and demand across a continuum: the extent to which their underlying elements change. This approach helps reveal the two primary sources of digital transformation and disruption. The first is the making of new markets, where supply and demand change less. But in the second, the dynamics of hyperscaling platforms, the shifts are more profound (exhibit). Of course, these opportunities and threats aren't mutually exclusive; new entrants, disruptive attackers, and aggressive incumbents typically exploit digital dislocations in combination. We have been working with executives to sort through their companies' situations in the digital space, separating realities from fads and identifying the threats and opportunities and the biggest digital priorities. Think of our approach as a barometer to provide an early measure of your exposure to a threat or to a window of opportunity-a way of revealing the mechanisms of digital disruption at their most fundamental. It's designed to enable leaders to structure and focus their discussions by peeling back hard-to-understand effects into a series of discrete drivers or indicators they can track and to help indicate the level of urgency they should feel about the opportunities and threats. We've written this article from the perspective of large, established companies worried about being attacked. But those same companies can use this framework to spot opportunities to disrupt competitors-or themselves. Strategy in the digital age is often asymmetrical, but it isn't just newcomers that can tilt the playing field to their advantage. Realigning markets We usually start the discussion at the top of the framework. In the zone to the upper right, digital technology makes accessible, or ""exposes,"" sources of supply that were previously impossible (or at least uneconomic) to provide. In the zone to the upper left, digitization removes distortions in demand, giving customers more complete information and unbundling (or, in some cases, rebundling) aspects of products and services formerly combined (or kept separate) by necessity or convenience or to increase profits. The newly exposed supply, combined with newly undistorted demand, gives new market makers an opportunity to connect consumers and customers by lowering transaction costs while reducing information asymmetry. Airbnb has not constructed new buildings; it has brought people's spare bedrooms into the market. In the process, it uncovered consumer demand-which, as it turns out, always existed-for more variety in accommodation choices, prices, and lengths of stay. Uber, similarly, hasn't placed orders for new cars; it has brought onto the roads (and repurposed) cars that were underutilized previously, while increasing the ease of getting a ride. In both cases, though little has changed in the underlying supply-and-demand forces, equity-market value has shifted massively: At the time of their 2015 financing rounds, Airbnb was reported to be worth about $25 billion and Uber more than $60 billion. Airbnb and Uber may be headline-making examples, but established organizations are also unlocking markets by reducing transaction costs and connecting supply with demand. Major League Baseball has deployed the dynamic pricing of tickets to better reflect (and connect) supply and demand in the primary market for tickets to individual games. StubHub and SeatGeek do the same thing in the secondary market for tickets to baseball games and other events. Let's take a closer look at how this occurs. Unmet demand and escalating expectations Today's consumers are widely celebrated for their newly empowered behaviors. By embracing technology and connectivity, they use apps and information to find exactly what they want, as well as where and when they want it-often for the lowest price available. As they do, they start to fulfill their own previously unmet needs and wants. Music lovers might always have preferred to buy individual songs, but until the digital age they had to buy whole albums because that was the most valuable and cost-effective way for providers to distribute music. Now, of course, listeners pay Spotify a single subscription fee to listen to individual tracks to their hearts' content. Similarly, with photos and images, consumers no longer have to get them developed and can instead process, print, and share their images instantly. They can book trips instantaneously online, thereby avoiding travel agents, and binge-watch television shows on Netflix or Amazon rather than wait a week for the next installment. In category after category, consumers are using digital technology to have their own way. In each of these examples, that technology alters not only the products and services themselves but also the way customers prefer to use them. A ""purification"" of demand occurs as customers address their previously unmet needs and desires-and companies uncover underserved consumers. Customers don't have to buy the whole thing for the one bit they want or to cross-subsidize other customers who are less profitable to companies. Skyrocketing customer expectations amplify the effect. Consumers have grown to expect best-in-class user experiences from all their online and mobile interactions, as well as many offline ones. Consumer experiences with any product or service-anywhere-now shape demand in the digital world. Customers no longer compare your offerings only with those of your direct rivals; their experiences with Apple or Amazon or ESPN are the new standard. These escalating expectations, which spill over from one product or service category to another, get paired with a related mind-set: amid a growing abundance of free offerings, customers are increasingly unwilling to pay, particularly for information-intensive propositions. (This dynamic is as visible in business-to-business markets as it is in consumer ones.) In short, people are growing accustomed to having their needs fulfilled at places of their own choosing, on their own schedules, and often gratis. Can't match that? There's a good chance another company will figure out how. What, then, are the indicators of potential disruption in this upper-left zone, as demand becomes less distorted? Your business model may be vulnerable if any of these things are true: Your customers have to cross-subsidize other customers. Your customers have to buy the whole thing for the one bit they want. Your customers can't get what they want where and when they want it. Your customers get a user experience that doesn't match global best practice. When these indicators are present, so are opportunities for digital transformation and disruption. The mechanisms include improved search and filter tools, streamlined and user-friendly order processes, smart recommendation engines, the custom bundling of products, digitally enhanced product offerings, and new business models that transfer economic value to consumers in exchange for a bigger piece of the remaining pie. (An example of the latter is TransferWise, a London-based unicorn using peer-to-peer technology to undercut the fees banks charge to exchange money from one currency into another.) Exposing new supply On the supply side, digitization allows new sources to enter product and labor markets in ways that were previously harder to make available. As ""software eats the world""-even in industrial markets-companies can liberate supply anywhere underutilized assets exist. Airbnb unlocked the supply of lodging. P&G uses crowdsourcing to connect with formerly unreachable sources of innovation. Amazon Web Services provides on-the-fly scalable infrastructure that reduces the need for peak capacity resources. Number26, a digital bank, replaces human labor with digital processes. In these examples and others like them, new supply becomes accessible and gets utilized closer to its maximum rate. What are the indicators of potential disruption in this upper-right zone as companies expose previously inaccessible sources of supply? You may be vulnerable if any of the following things are true: Customers use the product only partially. Production is inelastic to price. Supply is utilized in a variable or unpredictable way. Fixed or step costs are high. These indicators let attackers disrupt by pooling redundant capacity virtually, by digitizing physical resources or labor, and by tapping into the sharing economy. Making a market between them Any time previously unused supply can be connected with latent demand, market makers have an opportunity to come in and make a match, cutting into the market share of incumbents-or taking them entirely out of the equation. In fact, without the market makers, unused supply and latent demand will stay outside of the market. Wikipedia famously unleashed latent supply that was willing and elastic, even if unorganized, and unbundled the product so that you no longer had to buy 24 volumes of an encyclopedia when all you were interested in was, say, the entry on poodles. Google's AdWords lowers search costs for customers and companies by providing free search for information seekers and keyword targeting for paying advertisers. And iFixit makes providers' costs more transparent by showing teardowns of popular electronics items. To assess the vulnerability of a given market to new kinds of market makers, you must (among other things) analyze how difficult transactions are for customers. You may be vulnerable if you have any of these: high information asymmetries between customers and suppliers high search costs fees and layers from intermediaries long lead times to complete transactions Attackers can address these indicators through the real-time and transparent exchange of information, disintermediation, and automated transaction processing, as well as new transparency through search and comparison tools, among other approaches. Extreme shifts The top half of our matrix portrays the market realignment that occurs as matchmakers connect sources of new supply with newly purified demand. The lower half of the matrix explains more extreme shifts-sometimes through new or significantly enhanced value propositions for customers, sometimes through reimagined business systems, and sometimes through hyperscale platforms at the center of entirely new value chains and ecosystems. Attacks may emerge from adjacent markets or from companies with business objectives completely different from your own, so that you become ""collateral damage."" The result can be not only the destruction of sizable profit pools but also the emergence of new control points for value. Established companies relying on existing barriers to entry-such as high physical-infrastructure costs or regulatory protection-will find themselves vulnerable. User demand will change regulations, companies will find collaborative uses for expensive infrastructure, or other mechanisms of disruption will come into play. Companies must understand a number of radical underlying shifts in the forces of supply and demand specific to each industry or ecosystem. The power of branding, for example, is being eroded by the social validation of a new entrant or by consumer scorn for an incumbent. Physical assets can be virtualized, driving the marginal cost of production toward zero. And information is being embedded in products and services, so that they themselves can be redefined. Taken as a whole, these forces blur the boundaries and definitions of industries and make more extreme outcomes a part of the strategic calculus. New and enhanced value propositions As we saw in the top half of our framework, purifying supply and demand means giving customers what they always wanted but in new, more efficient ways. This isn't where the disruptive sequence ends, however. First, as markets evolve, the customers' expectations escalate. Second, companies meet those heightened expectations with new value propositions that give people what they didn't realize they wanted, and do so in ways that defy conventional wisdom about how industries make money. Few people, for example, could have explicitly wished to have the Internet in their pockets-until advanced smartphones presented that possibility. In similar ways, many digital companies have gone beyond improving existing offerings, to provide unprecedented functionality and experiences that customers soon wanted to have. Giving consumers the ability to choose their own songs and bundle their own music had the effect of undistorting demand; enabling people to share that music with everyone via social media was an enhanced proposition consumers never asked for but quickly grew to love once they had it. Many of these new propositions, linking the digital and physical worlds, exploit ubiquitous connectivity and the abundance of data. In fact, many advances in B2B business models rely on things like remote monitoring and machine-to-machine communication to create new ways of delivering value. Philips gives consumers apps as a digital enrichment of its physical-world lighting solutions. Google's Nest improves home thermostats. FedEx gives real-time insights on the progress of deliveries. In this lower-left zone, customers get entirely new value propositions that augment the ones they already had. What are the indicators of potential disruption in this position on the matrix, as companies offer enhanced value propositions to deepen and advance their customers' expectations? You may be vulnerable if any of the following is true: Information or social media could greatly enrich your product or service. You offer a physical product, such as thermostats, that's not yet ""connected."" There's significant lag time between the point when customers purchase your product or service and when they receive it. The customer has to go and get the product-for instance, rental cars and groceries. These factors indicate opportunities for improving the connectivity of physical devices, layering social media on top of products and services, and extending those products and services through digital features, digital or automated distribution approaches, and new delivery and distribution models. Reimagined business systems Delivering these new value propositions in turn requires rethinking, or reimagining, the business systems underlying them. Incumbents that have long focused on perfecting their industry value chains are often stunned to find new entrants introducing completely different ways to make money. Over the decades, for example, hard-drive makers have labored to develop ever more efficient ways to build and sell storage. Then Amazon (among others) came along and transformed storage from a product into a service, Dropbox upped the ante by offering free online storage, and suddenly an entire industry is on shaky ground, with its value structure in upheaval. The forces present in this zone of the framework change how value chains work, enable step-change reductions in both fixed and variable costs, and help turn products into services. These approaches often transform the scalability of cost structures-driving marginal costs toward zero and, in economic terms, flattening the supply curve and shifting it downward. Some incumbents have kept pace effectively. Liberty Mutual developed a self-service mobile app that speeds transactions for customers while lowering its own service and support costs. The New York Times virtualized newspapers to monetize the demand curve for consumers, provide a compelling new user experience, and reduce distribution and production costs. And Walmart and Zara have digitally integrated supply chains that create cheaper but more effective operations. Indicators of disruption in this zone include these: redundant value-chain activities, such as a high number of handovers or repetitive manual work well-entrenched physical distribution or retail networks overall industry margins that are higher than those of other industries High margins invite entry by new participants, while value-chain redundancies set the stage for removing intermediaries and going direct to customers. Digital channels and virtualized services can substitute for or reshape physical and retail networks. Hyperscaling platforms Companies like Apple, Tencent, and Google are blurring traditional industry definitions by spanning product categories and customer segments. Owners of such hyperscale platforms enjoy massive operating leverage from process automation, algorithms, and network effects created by the interactions of hundreds of millions, billions, or more users, customers, and devices. In specific product or service markets, platform owners often have goals that are distinct from those of traditional industry players. Moreover, their operating leverage provides an opportunity to upsell and cross-sell products and services without human intervention, and that in turn provides considerable financial advantages. Amazon's objective in introducing the Kindle was primarily to sell books and Amazon Prime subscriptions, making it much more flexible in pricing than a rival like Sony, whose focus was e-reader revenues. When incumbents fail to plan for potential moves by players outside their own ecosystems, they open themselves up to the fate of camera makers, which became collateral damage in the smartphone revolution. Hyperscale platforms also create new barriers to entry, such as the information barrier created by GE Healthcare's platform, Centricity 360, which allows patients and third parties to collaborate in the cloud. Like Zipcar's auto-sharing service, these platforms harness first-mover and network effects. And by redefining standards, as John Deere has done with agricultural data, a platform forces the rest of an industry to integrate into a new ecosystem built around the platform itself. What are the indicators that hyperscale platforms, and the dynamics they create, could bring disruption to your door? Look for these situations: Existing business models charge customers for information. No single, unified, and integrated set of tools governs interactions between users and suppliers in an industry. The potential for network effects is high. These factors invite platform providers to lock in users and suppliers, in part by offering free access to information. Finding vulnerabilities and opportunities in your business All of these forces and factors come together to provide a comprehensive road map for potential digital disruptions. Executives can use it to take into account everything at once-their own business, supply chain, subindustry, and broader industry, as well as the entire ecosystem and how it interacts with other ecosystems. They can then identify the full spectrum of opportunities and threats, both easily visible and more hidden. Digital's impact on strategy By starting with the supply-and-demand fundamentals, the insurance executives mentioned earlier ended up with a more profound understanding of the nature and magnitude of the digital opportunities and threats that faced them. Since they had recognized some time ago that the cross-subsidies their business depended on would erode as aggregators made prices more and more transparent, they had invested in direct, lower-cost distribution. Beyond those initial moves, the lower half of the framework had them thinking more fundamentally about how car ownership, driving, and customer expectations for insurance would evolve, as well as the types of competitors that would be relevant. It seems natural that customers will expect to buy insurance only for the precise use and location of a car and no longer be content with just a discount for having it garaged. They'll expect a different rate depending on whether they're parking the car in a garage, in a secured parking station, or on a dimly lit street in an unsavory neighborhood. Rather than relying on crude demographics and a driver's history of accidents or offenses, companies will get instant feedback, through telematics, on the quality of driving. In this world, which company has the best access to information about where a car is and how well it is driven, which could help underwrite insurance? An insurance company? A car company? Or is it consumer device makers that might know the driver's heart rate, how much sleep the driver had the previous night, and whether the driver is continually distracted by talking or texting while driving? If value accrues to superior information, car insurers will need to understand who, within and beyond the traditional insurance ecosystem, can gather and profit from the most relevant information. It's a point that can be generalized, of course. All companies, no matter in what industry, will need to look for threats-and opportunities-well beyond boundaries that once seemed secure. Digital disruption can be a frightening game, especially when some of the players are as yet out of view. By subjecting the sources of disruption to systematic analysis solidly based on the fundamentals of supply and demand, executives can better understand the threats they confront in the digital space-and search more proactively for their own opportunities. About the Authors Angus Dawson is a director in McKinsey's Sydney office, Martin Hirt is a director in the Taipei office, and Jay Scanlan is a principal in the London office. The authors would like to thank Chris Bradley, Jacques Bughin, Dilip Wagle, and Chris Wigley for their valuable contributions to this article.",en,62
229,2591,1476888660,CONTENT SHARED,2725842723638001955,-3595444231792050977,798149484404780068,"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.59 Safari/537.36",SP,BR,HTML,http://blog.caelum.com.br/micro-profile-javaee-com-wildfly-swarm/,micro profile javaee com wildfly swarm,"Foi-se o tempo em que desenvolver uma aplicação JavaEE era algo extremamente burocrático e que tínhamos um servidor de aplicação inchado e com um consumo excessivo de recursos. Hoje temos servidores que foram pensado na reutilização dos recursos e ainda temos a possibilidade de escolher quais módulos vamos inicializar junto com o servidor de aplicação, os chamados profiles . Mas e quando pensamos em micro-serviço? Como podemos trabalhar com micro-serviços no JavaEE? A ideia do micro-serviço é de ter serviços isolados e independentes. Até aí tudo bem, podemos criar aplicações JavaEE separadas(isto é cada uma com seu .war ). Mas como podemos fazer o deploy dessas aplicações? Podemos ter um único contêiner (servidor de aplicação) para onde efetuamos o deploy de todas as aplicações. Mas com isso estamos indo de frente com a ideia de micro-serviço. Pois estamos gerando um ponto único de falha ( SPOF ). Em outras palavras se precisarmos por alguma razão interromper o contêiner, todas as aplicações penduradas nele cairam também. Outra alternativa seria cada serviço ter seu próprio contêiner, mas dessa forma estaríamos subutilizando o próprio contêiner. Pois nele temos diversos serviços rodando e que nossas aplicações não iriam precisar. Se não tivessemos usando servidor de aplicação poderiamos usar um contêiner embutido na aplicação (Tomcat, Jetty, Undertown e etc..). Porém dessa forma perderiamos as facilidades que o servidor de aplicação nos proporcionam. E aí que entra uma ferramenta nova para nos ajudar, Wildfly-Swarm . Com ele podemos declarar quais ""módulos"" da plataforma JavaEE queremos utilizar. A partir daí é desenvolver sua aplicação JavaEE normalmente e ao empacotar nossa aplicação, o wildfly-swarm pegará nosso pacote .war e irá embrulhar em um pacote dele com um contêiner micro para rodar nossa aplicação. Daí o nome de Micro-Profile . Esse artefato final (o pacote que foi gerado a partir do nosso .war ) é um arquivo .jar e podemos executar com um simples comando java -jar no nosso terminal. Wildfly-Swarm usa o conceito de UberJar para gerar o artefato final. Que nada mais é do que um arquivo .jar que contém além do seu .war todas as dependências necessárias para que o wildfly-swarm consiga rodar. Logo um UberJar é um arquivo .jar um pouco mais inchado mas somente com o necessário para rodar a aplicação. O mínimo necessário para rodar o wildfly-swarm é JDK8 e Maven ou Gradle . No wildfly-swarm temos o conceito de fraction , que nada mais é do que uma funcionalidade/configuração do servidor de aplicação. Na maioria das vezes um fraction pode ser comparado (mapeado) com um subsystem do servidor de aplicação (ex.: Datasource, Driver, Pool, socket-binding e etc...), temos outros casos em que um fraction é uma funcionalidade que antes não tínhamos (nativamente) no servidor de aplicação (Ex.: Jolokia, Spring, NetflixOSS ). Para usar o wildfly precisamos importar um pom.xml do wildfly-swarm no nosso projeto, e adicionar um plugin que fará a geração do UberJar . Nesse post vamos construir uma simples aplicação rest usando o wildfly-swarm. Vamos lá! Com o projeto (web) maven criado precisamos importar o pom.xml do wildfly-swarm ao nosso projeto. Esse pom.xml é importado declarando uma denpendencia gerenciada e declarando que o escopo da mesma será import . Para não ficar espalhando a versão do wildfly-swarm estamos utilizando por todo nosso pom.xml vamos declarar uma propriedade com ela. Além disso já vamos definir a versão do Java que iremos utilizar, e como se trata de um projeto web teoricamente precisariamos de um arquivo web.xml porém não é necessário para nosso caso. Então vamos definir que não deve ser gerado um erro caso não exista o arquivo web.xml . Agora vamos importar o arquivo pom.xml do wildfly-swarm que tem o nome de BOM (Bill of Materials). Além disso precisamos adicionar o plugin que irá gerar o UberJar baseado no nosso war . Precisamos também adicionar a dependência referente à api do JavaEE e esta será provida pelo wildfly-swarm. E esse é o setup para utilizar o wildfly-swarm, ao final teriamos o nosso arquivo pom.xml mais ou menos da seguinte maneira: Agora vamos adicionar as dependências que queremos do servidor de aplicação, para o nosso caso vamos adicionar, jax-rs , cdi , ejb , jpa , datasources . Vamos começar a ""codar"" nosso projeto de livraria. Vamos iniciar criando nossas classes de domínio Livro e Autor e seus respectivos DAOs Vamos criar uma classe para configurar o JAX-RS para receber requisições a partir da raiz: Agora que já temos como receber requisições vamos criar nossos recursos que serão expostos pela nossa api LivroResource e AutorResource além disso já vamos marcar-los como EJB Stateless para que já tenhamos transação entre outras coisas disponíveis. Nesse recurso temos as seguintes URIs /livros - GET (Retorna uma lista com todos os livros) /livros/id - GET (Retorna o livro com ID especificado) /livros - POST (Cria um novo livro) Nesse recurso temos as seguintes URIs /livros/idDoLivro/autores - GET (retorna todos os autores de um livro especifico) /livros/idDoLivro/autores/idDoAutor - GET (retorna um autor especifico de um livro especifico) /livros/idDoLivro/autores - POST (cria um novo autor associado ao livro) Agora que temos nosso projeto pronto temos que registrar um datasource e associa-lo no nosso arquivo persistence.xml . Para o nosso caso, esta configuração será feita em uma classe com um método main . Porém para configurar o datasource precisamos dizer qual o driver, e no nosso caso será um driver para mysql. para não precisarmos registrar um driver, podemos adicionar a dependência para esse driver diretamente no pom.xml Para que seja adicionado corretamente o driver para mysql precisamos excluir o driver default que vem quando adicionamos a dependência para JPA que é o H2 . Para isso vamos alterar a dependência de JPA e remover o H2 . (Se não fizermos essa configuração ao subirmos o wildfly-swarm iremos receber um erro, informando que esta rolando um conflito entre essas dependências H2 , MYSQL ). Pronto agora vamos registrar nosso datasource dentro de um método main: Como estamos sobrescrevendo o comportamento default do wildfly-swarm precisamos ensinar ele como ele deve fazer o deploy da nossa aplicação (ou seja dentro do nosso .war quais classes devem estar disponíveis, quais arquivos e etc...). Para fazer isso iremos usar uma biblioteca chamada ShrinkWrap da própria JBoss que serve para criar um pacote programaticamente, ela é muito utilizada quando estamos usando o Arquillian para testes de aceitação/integração. Com ele podemos criar JARArchive ( .jar ) e WARArchive ( .war ). Além desses temos outros tipos mais especificos por exemplo RibbonArchive um archive especifico que pode ser registrar em aplicações baseadas em Ribbon, Secured que já injeta o arquivo keycloak.json e já configura a parte de seguraça. Temos também um outro tipo especifico que é JAXRSArchive que é um archive que já configura o jax-rs e faz o binding para classe de configuração Application/@ApplicationPath . Vamos utilizar esse archive. Além disso precisamos dizer que ao fazer o deploy da aplicação seja levado os arquivos persistence.xml e beans.xml necessários para o funcionamento da JPA e CDI . O arquivo beans.xml deve estar dentro do diretório WEB-INF a partir do classpath e o arquivo persistence.xml , deve estar em META-INF a partir do classpath também. E precisaremos configurar isso também. Como ultima configuração precisamos indicar ao plugin do wildfly-swarm que ele não deve usar sua classe padrão para iniciar. Ele deve usar nossa classe Boot e com isso alterar o arquivo de manifesto para usar essa classe como ponto inical da nossa aplicação. Vamos alterar a declaração do plugin no arquivo pom.xml e adicionar a configuração "". Para que seja gerado o UberJar devemos executar o goal package do maven (Ex.: mvn package ). Ao termino dessa execução no diretório target teremos um arquivo livraria.war e um livraria-swarm.jar (entre outros arquivos). O arquivo livraria-swarm.jar é o nosso UberJar . Para executarmos podemos faze-lo pelo plugin do maven através de mvn wildfly-swarm:run ou executando o comando java -jar livraria-swarm.jar . Dessa forma temos uma aplicação JavaEE somente com o mínimo necessário e um contêiner bem mais leve para rodar a mesma. Aprendendo mais Quer aprender mais sobre como utilizar a especificação JavaEE ? Não deixe de conferir nosso curso Plataforma JavaEE , nele abordamos jax-rs , jpa , cdi , ejb entre outros recursos da especificação JavaEE . E aí o que você achou do wildfly-swarm ?",pt,62
230,2746,1479178420,CONTENT SHARED,6750029764020453263,-1578287561410088674,2000151301182442550,"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) SmartCanvas/0.1.0 Chrome/49.0.2623.75 Electron/0.37.2 Safari/537.36",SP,BR,HTML,https://macmagazine.com.br/2016/11/14/microsoft-anuncia-visual-studio-para-mac-disponivel-ainda-nesta-semana/,"microsoft anuncia visual studio para mac, disponível ainda nesta semana | macmagazine.com.br","Acredito que, a este ponto, dizer que a Microsoft deu uma reviravolta de 180º (para melhor!) nos últimos anos sob o comando de Satya Nadella é chover no molhado. A empresa está mais amigável, mais aberta, mais criativa e acessível que nunca - e, se você quer mais uma prova disso, basta saber que, pela primeira vez nos seus 20 anos de história, o Visual Studio está ganhando uma versão para Mac. A novidade foi anunciada por meio de um post no blog oficial da Microsoft - talvez um pouco mais cedo do que o planejado, já que ele foi apagado um pouco depois ( aqui pode-se acessar uma versão em cachê), mas enfim: o fato é que uma das ferramentas de desenvolvimento mais populares do mundo está encontrando o seu caminho para o mundo Apple e agora desenvolvedores que queiram utilizar o Visual Studio num Mac não mais precisarão recorrer a alternativas como o Boot Camp, o Paralells ou o VMware Fusion. Obviamente, a intenção aqui não é que os usuários escrevam aplicações inteiras para Windows num Mac - afinal de contas, para rodar tais criações ainda seria necessária uma máquina virtual -, mas para edições rápidas ou criação de softwares para outras plataformas, como iOS, Android ou o próprio Mac, o Visual Studio cumprirá muito bem a sua função. Como dizia um trecho do post agora deletado: Se você gosta da experiência de desenvolvimento do Visual Studio, mas precisa ou quer utilizar o macOS, você certamente se sentirá em casa. A interface de usuário [da versão para Mac] é inspirada pela do Visual Studio, mas desenhada para que o programa transpareça como um cidadão nativo do macOS. Ainda que traga boa parte das capacidades da sua versão original do Windows, o Visual Studio para Mac terá algumas limitações, como falta de suporte a alguns tipos específicos de projeto. A grande vantagem do novo programa, entretanto, é a possibilidade de compartilhar projetos entre Macs e PCs instantaneamente, sem necessidade de conversão. Segundo o anúncio, o Visual Studio para Mac será apresentado em maiores detalhes amanhã no evento para desenvolvedores Connect(), da Microsoft, com uma versão de testes disponibilizada já nesta semana. [via The Verge ] Se houver algum erro no post acima, selecione-o e pressione Shift + Enter ou clique aqui para nos notificar. Obrigado! aplicativo Boot Camp desenvolvedor desenvolvimento developer Mac macOS Microsoft programa Software Visual Studio Windows Sobre o Autor Aviso: nossos editores/colunistas estão expressando suas opiniões sobre o tema proposto e esperamos que as conversas nos comentários sejam respeitosas e construtivas. O espaço acima é destinado a discussões, debates sobre o tema e críticas de ideias, não às pessoas por trás delas. Ataques pessoais não serão tolerados de maneira nenhuma e nos damos ao direito de ocultar/excluir qualquer comentário ofensivo, difamatório, preconceituoso, calunioso ou de alguma forma prejudicial a terceiros, assim como textos de caráter promocional e comentários anônimos (sem nome completo e/ou email válido). Em caso de insistência, o usuário poderá ser banido.",pt,61
231,2357,1474028337,CONTENT SHARED,-6545872007932025533,-1402490765047599382,-9043465215175509636,,,,HTML,https://medium.com/chris-messina/silicon-valley-is-all-wrong-about-the-airpods-8204ede08f0f,why silicon valley is all wrong about apple's airpods - chris messina,"So you think Apple is a tech company? No, you're wrong. In July of 1997, right before his return to Apple, Steve Jobs told BusinessWeek: ""The products suck! There's no sex in them anymore! Start over"". Ten years later, building on the dripping sex and rock and roll of the iPod (touched with a Bono �� no less!), Jobs revealed the iPhone and changed computing forever. Last week, Apple did it again, but for some reason, nearly everyone in Silicon Valley is confused about what just happened. I mean, I understand the confusion, but do people really think that the most significant announcement was the removal of the 3.5mm analog headphone jack ? I mean, it was, but not for the reasons everyone's panties seems to be bunched up about. Apple doesn't give a shit about neckbeard hipsters who spent thousands of dollars on expensive audiophile gear that rely on 100-year-old technology to transmit audio signals . They'll readily drop them faster than Trump drops facts to make an argument in a televised debate . Apple is securing its future , and to do that, it must continue to shrink the physical distance between its products and its customers' conceptions of self. The ᴡᴀᴛᴄʜ came first, busting our sidekick supercomputer out of our pockets and onto our skin. Apple's next move will put its products literally within earshot of our minds. This is no accident. How quickly we forget the past In 2007, not only did Apple launch the iPhone, but they also changed their name from Apple Computer, Inc., to Apple Inc. This change was perhaps as big a deal as the iPhone itself, but it's taken another decade for its implications to become clear. Oops, did you blink and miss it? No problem. Apple made you a movie: Maybe it's still not clear to you. That's ok, I'll spell it out. Repeat after me: Apple is not a technology company The problem with Silicon Valley is Benedict Evans . I mean, not Benedict specifically, because he's actually incredibly smart and holds sophisticated perspectives on the tech industry and adoption cycles, and also, he gives good tweets , but he's not a product designer. And yet, we look to him and other folks of his ilk to understand Apple's moves. But there are no users like Benedict Evans in the world, except in Silicon Valley, and as much as we like to think of Silicon Valley as the center of the universe, it's not ( aside to my SV friends: I know, I know, deep breaths ). While we live and breathe tech products, and love to play armchair product quarterbacks (side note: Product Hunt is the NFL of product design ), we don't represent the masses of normals. He and I like to indulge the fantasy that Apple makes things exclusively for us, but they obviously care way less about two Twitter-loving technophiles in Silicon Valley than they do about the rest of the world. Thus when I consider who influences my thoughts on Apple's moves, I need to be mindful of the Kool-aid I'm drinking, who's making it, and what their lived context is. Do they represent the broader whole of humanity, or a narrow sliver of land on the West Coast of the United States of America? So, then, why should you listen to me? Who died and crowned me an expert? No one. I just kind of became an expert by virtue of the sheer number of hours I've spent on this stuff. Kind of like Benedict, but it's also his job. But, I'm also kind of a fraud, like the rest of you. I grew up never fitting in with any crowd and never being popular, but I learned to observe people, and then chameleon myself into their cliques so I could feign belonging . It makes me a good faker and it makes me pretty good at listening to the words people use, but better at paying attention to what their behavior actually says . I've learned to differentiate how experts think about things from the way laypersons do, and how to discount each respective perspective accordingly (including my own, definitely including my own ). Most Silicon Valley pundits that we enjoy listening to or reading only reinforce our own over-developed, over-informed (and thus, unrepresentative ) viewpoints. They say things that validate our shallow egos and make us feel less alone, like when they decry the death of the 3.5 mm analog jack as anathema. We tweet our adolescent angst in solidarity because it feels good to belong and to rage in unison, and because we recoil from physical affection from each other, we seek likes and retweets to soothe our wounded inner children because that kind of validation is the closest human connection to getting a hug that we're willing to tolerate. And fuck yeah, Techmeme, thank you for showing me that I'm not alone ! But, I digress. What was I talking about? Oh, right. Apple is a fashion brand that makes jewelry that connects to the internet The thing that makes me crazy about Apple (and not in the fanboi sense) is that they both give a shit and don't give a shit about what anybody else thinks, and what everyone else is doing. Like, under Tim Cook, they're a lot more ""out there"" and verbally responsive to customer complaints , but in a totally controlled and measured way. Not like Jobs didn't write emails to customers , but Cook is a little faster and looser. A little. And from an industry perspective, Apple doesn't seem to want to keep up with the Jones's (Google, Facebook, Amazon, Tencent, et al), except when they do. For example, Samsung showcased a waterproof S7 back in February and then Apple followed suit in the iPhone 7. In other areas, however, Apple is out on their own. That's where it's worth paying attention, and that's what brings us back to nixing the headphone jack with the one-two punch of a Lightning port coupled with Bluetooth audio. Yes, others, from Slate to Chris Saad ( 1 ), have pointed out that this change is not about music, but about how Apple's new AirPods will usher in the wonderful (and yet unproven) world of voice computing. And, I agree, but that perspective is insufficient to understand why Apple doing this is significant. It's not like they're the first. This image, though, helps: ""You've got to start with the customer experience and work backwards to the technology ."" - Steve Jobs The thing is, we've had wireless headsets for a while, but they've always made people (mostly men) look like dickwads. They're confusing to pair, and frustrating to use. And so if you're willing to put up with them, you're letting technology ruin your life. And so, you're a dickwad. Don't take my word for it, look at this unlicensed "" confident businessman with wireless headset "" stock photo: Pure dickwad. Poor guy. Heck, even if you kind of look like Chris Hemsworth, you can't really make a once state of the art wireless headset look like something you'd choose to adorn your pretty little head with: The reality is, the "" Bionic Man "" look isn't really in, no matter how much utility these devices provide (I say this even as Bluetooth headphones sales eclipse the wired sort ) or people attempt to get the design right . Yves Behar couldn't make it happen when he partnered with Jawbone. I mean, would you choose to wear something like this on a hot date? Probably not if you wanted a second date, amirite? But the EarPods, and now AirPods, for some reason* (* no, it's a very specific reason ), defeat this crisis of user acceptance. What Apple has done is produce something that isn't a technology product, but is, rather, a fashion object -a piece of jewelry, an entertainment product, a status symbol, a genie in a bottle-that drips with sex appeal. I mean, that iPhone 7 launch video probably was directed by The Weeknd , because I want to watch it, Often, followed by a cold shower. �� So, don't confuse AirPods with just another Bluetooth headset; that's not what they're replacing. AirPods offer a new relationship because they're alluring, sensuous, and sultry: AirPods are sex sticks that fuck your ears . (Hmm. Or maybe your ears spoon them? I can't decide.) Regardless, the fucking or the cuddling goes both ways, and if I'm saying anything, it's that AirPods aren't a technology device, but instead a way to get Her's Scarlett Johansson character into your bed... errr... I mean, head because whatever is going on in this image, it's the equivalent of what we all know actually takes place on Snapchat (or used to), except it's happening between you and a bot named Siri: And this is what Apple can do that no one else can: make the behavior of talking to a disembodied entity on your face so socially acceptable that the voice computer revolution can finally get underway. Nor are they starting from square one. They've already taught us to behave this way, even if we don't realize it. How many times have you walked down the street talking to a colleague or family member on your EarPods? It's normal. It's not weird. Who cares if you're talking instead to your robot overlord? What sets AirPods apart is that they build on existing habits, require only slightly modified expectations on behalf of the user, and benefit from the wisdom of the phalanx of fashion luminaries that Apple has brought in-house over the past decade. In contrast, here's a weird product with no sex appeal which had no prior user adoption to build on and that was doomed to fail from the outset, no matter how many models showed up for the fashion walks : You can buy fashionable friends, and you can pay them to wear your stuff to make some photos, but you can't get them to choose to wear what you're offering in their real lives unless there's a bridge to the familiar, essential, and down-to-earth. AirPods build on the success of the iPod, which is related to the story of Napster and taking on the record industry, saying "" Fuck you! "" to Metallica ( especially to Lars ), putting 1,000 songs in your pocket, the clickwheel, trading 128kbps MP3s in internet forums, suffering through dial-up download speeds, Firewire, USB, and basically punk rock. AirPods are legit like Richard Branson because they've been around forever and yet they're still new and cool as fuck. Patience is a virtue lost on Silicon Valley Here in Silicon Valley, we're a bunch of inchoate Peter Pans , which affects how we approach relationships, how we design, build, and grow apps, and it affects our ability to relate to the people that use the things we make (because everything we make is soooo important, magical, revolutionary, changing the world, solving world hunger, making life less demanding by making everything available on-demand). Somehow ( maybe it was the acid trip Jobs went on ), Apple learned to take their time with products, and to pace their product evolution. They seem slow at times, but maybe it's just because they resist the short-sighted approach that most tech companies feel forced to take to try to get ahead. That means most tech companies struggle to fully understand the problems they're solving, and don't stop to saddle up alongside their users to develop empathy-to really understand what their users are willing to put up with and what they never will. Apple began the journey of promoting user acceptance of technology apparatuses as fashion accessories with the introduction of the iPod in 2001, fifteen years ago . You can hear it when Jobs explains why he decided to pursue music in the first place: he knew it was universal and represented a huge addressable market in which there was no market leader . He also knew that everyone loved music, and that their personal, emotional relationships with music would give him the opening he needed to send in the  ᴛʀᴏᴊᴀɴ ʜᴏʀsᴇ to permeate their lives for a generation. And now, by exploiting that same relationship, Apple is doing it again: offering a sexy fashion statement, an expensive luxury item, an entertainment accessory, which will usher in the era of voice-controlled intimate computing. Apple won't sell the AirPods by enumerating their tech specs but by evoking an emotional, aspirational response-which is an approach vividly different from nearly anything else that comes out Silicon Valley's burgeoning nerdtopia. When we decry the lack of diversity in Silicon Valley (and yes, Apple absolutely should examine its own house ), we should remember that true diversity is complex with many dimensions. The broad, eventual appeal of AirPods come from the diversity of talent working behind the scenes to bring this product to life-beyond the engineering and industrial design-which includes disciplines from marketing to retailing to storytelling to fashion, as well as the disciplinary will to resist shipping shit products. A diversity of perspectives had to be brought together to make this product happen in this moment, with this narrative, with the relatively reserved emphasis on Siri. No, people aren't quite ready for the conversational software world of the future  - but that's okay, because, guys, Apple's on it, and they've got plenty of time to get it right. And I hope you understand what Apple's up to a little bit better now.",en,61
232,2047,1471002666,CONTENT SHARED,6540624159201421051,-1602833675167376798,-4293036500619421365,,,,HTML,https://pagamento.me/bradesco-vai-lancar-novo-banco-digital-o-next/,bradesco vai lançar novo banco digital: o next.,"Bradesco vai lançar novo banco digital. Com o nome de Next , o banco pretende lançar o novo produto em breve. Já não bastasse as iniciativas do grupo Elopar, o banco Bradesco vai tentar inovar e atingir o público jovem. O banco já usava o termo ""Next"" em alguns espaços das agências e agora assume o nome para o novo negócio. O banco já está em desenvolvimento dentro da Cidade de Deus, em São Paulo. O nome ""Next"" foi escolhido para falar com o potencial público alvo: millenials . A diretoria do banco nome nomeou Maurício Minas, VP do grupo, para tocar a operação. Tamanho do investimento: R$120 milhões (fontes do Estadão). A notícia vem logo após anúncio da chegada do Digio (da Elopar) , que também é controlado pelo Bradesco. O domínio Banco Next ( www.banconext.com.br ) foi registrado em 15/07/2015, o que demonstra que o banco está planejando o lançamento desde o ano passado. Parece que a ""nova onda"" do setor financeiro são os bancos digitais. Resta saber se o Next vai conseguir manter a ""pegada"" de inovação de Nubank , Banco Neon e Original . Vale frisar que esse novo ""business"" é um projeto essencialmente de inovação. Apesar da base do Bradesco e da rede de atendimento valerem o investimento, vão ter que aprender ""na marra"" como um negócio digital pode escalar. Vamos acompanhar, bem de perto.",pt,61
233,1443,1466287803,CONTENT SHARED,6829640091575814990,-1130272294246983140,-782450246769532337,,,,HTML,http://www.updateordie.com/2016/06/09/branding-e-problema-seu-e-meu/,branding é problema seu. e meu.,"B randing é um assunto injustiçado, coitado. Tachado de complexo além do que é, de fato. Que ironia: o Branding tem um problema de imagem. E, como consequência, duas coisas. Primeiro, um abismo enorme entre os que se acham ""branders"" e os que se acham leigos. Depois, a inevitável consequência da distância: o efeito não-é-problema-meu. Voluntária ou involuntariamente, na mente dos menos envolvidos com o tema, o trabalho dos ""branders"" se torna algo muito específico, restrito a marqueteiros, publicitários, comunicólogos e seres de espécies similares. Restringir a função de gestão da marca a um setor específico da empresa, que geralmente é a comunicação, é um erro. Temos aí a origem de um dos principais - senão o próprio - problemas das empresas: a fragmentação. Diferente de outras disciplinas presentes no dia-a-dia das empresas, não é possível fazer Branding eficiente se apenas uma área olhar para isso. Não dá! Sabe por que? Porque a marca é como o ar. Ela vive em todo lugar. Dentro da empresa, está em todas as áreas. Fora dela, perambula no seu ecossistema. Onde houver experiências de marca, haverá marca. E o que são experiências de marca? Toda e qualquer interação que se tem com ela. Vamos considerar o atendimento, que é uma interação típica entre marca e cliente. Se eu fui bem atendido, inevitavelmente foi criada uma pastinha da marca na minha mente. Dentro dessa pasta, o atributo ""atendimento de qualidade"" foi gravado. E lá, dependendo da próxima experiência, ele pode aumentar seu peso, diminuir ou ser excluído e substituído pelo atributo ""atendimento zoado"". Mas não estamos falando apenas de interações ou experiências diretas com a marca, mas também as indiretas. O ecossistema de uma marca é composto por todos os públicos que tem algum contato com ela. Esses públicos interagem com a marca, claro, mas também interagem entre si. Em todas essas interações, experiências de marca podem ser criadas, mesmo que ela não participe ""oficialmente"" dessa interação. É por isso que dizemos que a marca vive dentro, sim, mas principalmente fora da empresa. É como um filho, sabe? A gente cria pro mundo. :P É por isso também que fazer a gestão de uma marca é muito mais difícil que tirar doce da mão de criança - mas muito menos cruel. Marca é um nome ao qual relacionamos atributos, sentimentos e uma estética. Ou, para simplificar, um nome com associações. Essas associações são feitas a partir de toda e qualquer experiência que se tem com ela, como o exemplo típico do parágrafo acima. Ou o exemplo do parágrafo abaixo, não tão típico assim. Se um caminhão da Coca-Cola tombar em cima de mim - o exagero ajuda nesse caso - posso nunca mais querer sentir o cheiro nem o gosto daquela bebida escrota que quase me afogou depois do acidente. A culpa pode ter sido toda do motorista terceiro bêbado, mas a Coca-Cola estará relacionada a isso eternamente em minha mente. RIP Coke. Pronto. Isso é marca. Um nome ao qual fazemos associações de todo tipo, partindo das experiências mais diversas que se pode imaginar. Agora, pense você mesmo: existe uma área da empresa responsável por criar e gerir todas as experiências de marca possíveis? Eu respondo: não existe nem nunca existirá - frase de efeito irresponsável. Nem mesmo uma empresa inteira focada em fazer a gestão das experiências de marca daria conta, porque elas são só parcialmente controláveis. Existe todo um ecossistema criando experiências o tempo todo, das mais diversas formas, que não podemos gerir completamente. A essa altura, você, que não é ""brander"", deve ter percebido que você cria experiências de marca o tempo todo. Seja no papel de representante oficial da marca, quando você atende mal um telefonema de um fornecedor, ou no papel de cliente reclamão (público externo à empresa, mas essencial no ecossistema de qualquer marca), quando você xinga muito no Twitter aquela marca de tênis que usa mão de obra infantil e escrava. Ou no papel de membro fofoqueiro, quando você fala mal do padre. Como o padre é representante da Igreja, você está, por tabela, falando mal dela - Deus tá vendo! Caso mais grave do que apenas uns bilhões de dólares perdidos por uma imagem de marca manchada. Nesse cenário, o que as empresas precisam fazer? Gerir com a maior excelência possível todas as experiências que ela cria com seus públicos - atendimento, comunicação, redes sociais, embalagens, força de vendas etc. Assim, as chances de as demais experiências serem positivas será grande. Fora isso, vale rezar - pense bem antes de falar mal do padre - pra que nenhum caminhão tombe, nenhum rato invada suas garrafas, nenhuma barragem se rompa liberando 40 bilhões de litros de lama. E pra você, amigo, resta assumir esses vários chapeus na sua cabeça. Você tem a marca pela qual trabalha, a marca da qual é cliente, a marca da qual é fornecedor. Tem até sua marca pessoal, cujas experiências geram uma boa ou uma má reputação. Ou seja, sua reputação depende de um bom trabalho de Branding de você mesmo. Se a primeira impressão é a que fica, sua missão começa cedo. Seja no trabalho ou na vida social, um ""bom dia"" dá um belo pontapé. É por isso que Branding é problema seu. E meu.",pt,61
234,1297,1465334139,CONTENT SHARED,-5997769775112032630,-1032019229384696495,-7696592431575292648,,,,HTML,http://venturebeat.com/2016/06/05/how-bot-to-bot-could-soon-replace-apis/,how bot-to-bot could soon replace apis,"By now it's clear that bots will cause a major paradigm shift in customer service, e-commerce, and, quite frankly, all aspects of software-to-human interaction. For the moment, the state of the art of bots is bot-to-consumer, meaning bots communicating with humans. But at some point soon, bots will start talking to other bots. Enter the bot-to-bot era. Imagine that a bot - let's call her Annie - needs to answer a question from a customer but lacks information from her own backend systems. Annie is powered with artificial intelligence and spontaneously decides to reach out to another bot to get the information she needs. Annie aggregates the information and delivers it back to the customer. Death of the API ? Today when two software systems have to talk to each other, software developers need to implement an integration using APIs (application programming interfaces). This integration process is time consuming. That's why, over the last couple of years, services such as Zapier , Scribe , and IFTT have become popular. They provide out-of-the-box interfaces to hundreds of software applications, allowing you to connect, for example, your CRM system with a mailing tool or analytics platform. In the bot-to-bot era, however, each software application can talk to each other system, regardless of whether they have an actual API integration in place. Granted, bot-to-bot communication will not be used to exchange large amounts of data, but it will allow for ad-hoc communication between, for example, my banking software and a web shop. My banking software could talk to the webshop bot and ask for that missing invoice: ""Niko needs an invoice for order 45678, can you provide that?"" The big finale: bot-to-bot-to-consumer The beauty of bot-to-bot communication will be that it is in plain English, it will be conversations that every human can understand. Assuming that all conversations between my bot Annie and other bots are archived, I will be able to go back and see how my two little bots came to a certain conclusion. In my banking example, when an invoice is missing after all, I could click on a ""details"" button, which would show me the conversation Annie had with the webshop. The archived bot-to-bot conversation would show me the webshop bot response, that the invoice will not be available for another couple of weeks. But it gets better. If my bot is stuck in a conversation with another bot, she can call me in for help: ""Niko, it's Annie here, your finance bot. I'm talking to a supplier, but I'm having some trouble understanding what they are saying."" I could chime in - when I have time of course, a couple of hours later, since bots have unlimited patience - and I would rephrase the question of Annie and get the answer from the other bot. Next, Annie could continue the conversation and handle my business. Semantic web Didn't we talk about connecting every online service with every other online service 10 years ago? What was it called again? The Semantic web? Every website was going to be annotated using standard data formats, allowing other services to crawl that data and use it in their own business logic. I believe that bots will deliver on that promise in the next 3 to 5 years, and this will not mean that all data will have to be uniformly formatted. Instead, bots will expose online services and data in plain English, allowing both humans and other bots to interact, even if they have never communicated before. Calling all software developers So, software developers, when you develop your platform for e-commerce, online marketing, finance, ERP or any other software solution, please think about the implementation of a smart bot, besides your traditional APIs, so next time when I buy a new BBQ online, my bot will alert me that it's going to rain for the next two weeks. Niko Nelissen is VP of Mobile, Data and Engagement at Etouches , an event management solution. Follow Niko on Twitter. AI. Messaging. Bots. Arm yourself for the next paradigm shift at MobileBeat 2016. July 12-13 at The Village in San Francisco. Reserve your place here.",en,61
235,1670,1467813602,CONTENT SHARED,3180828616327439381,1895326251577378793,6337372998984359835,,,,HTML,http://computerworld.com.br/governo-define-cronograma-para-plano-nacional-para-internet-das-coisas,governo define cronograma para plano nacional de internet das coisas,"Ricardo Rivera, gerente setorial das indústrias de tecnologia da informação do BNDES, anunciou durante a Rio Info 2016 que 29 propostas foram entregues para a contratação da consultoria que vai desenhar o plano de ação nacional para Internet das Coisas, a ser conduzido pelo banco estatal e pelo MCTIC. Segundo ele, na próxima semana serão conhecidas as cinco finalistas e até o final de julho será revelada a empresa vencedora. A expectativa é que a pesquisa sobre o tema aconteça de outubro até o final de dezembro. A meta, explicou o executivo, é que no segundo semestre de 2017, o plano nacional de IoT comece a ser implantado.""O levantamento vai fazer um estudo completo do mercado, mas terá recursos para uma implantação imediata. A intenção é remover as principais barreiras para Internet das Coisas"", sustentou. O plano terá com validade de 2017 a 2022. ""O modelo terá recurso para garantir a implantação. Ele não será um estudo sem medidas práticas"", detalhou Rivera, ao afirmar que um ponto central do estudo BNDES/MCTIC é obrigar o monitoramento das ações. ""Queremos que a política seja implantada. Isso é crucial para o Brasil ter um lugar nesse mercado"", afirmou. O levantamento terá pontos voltados às questões regulatórias, as melhores práticas de financiamento e o compartilhamento de informações. ""Há ações de IoT acontecendo no mundo. Precisamos dividir conhecimento. Há muitas oportunidades de negócios, mas temos que estruturar quais são as competências do Brasil"", completou. A Internet das Coisas desperta muito interesse das empresas. Em contrapartida, muitas dúvidas rondam os executivos. ""Precisamos entender se o que estamos fazendo é realmente Internet das Coisas. Se não corremos o risco de fazer investimentos que ser perderão"", observou Gabriel Antônio Mourão, consultor de novas tecnologias da Perception. Ele afirmou que este é o momento ideal para investir em IoT. ""Temos que aproveitar ao máximo esse momento"", comentou. ""A IoT já agrega diferentes tecnologias, como cloud computing e big data, e precisa se preparar para as novas que vão surgir. É preciso ainda encontrar novas soluções sobre questões importantes como segurança, privacidade ou volume de dados nas comunicações. A aplicabilidade e compatibilidade entre diferentes plataformas também precisa ser pensada"", ponderou o executivo. Para Fabio Porto, professor e pesquisados do Laboratório Nacional de Computação Cientifica (LNCC), que tem desenvolvido vários projetos em parceria com instituições e empresas para aplicação de IoT com uso de dados captados por sensores, para desenvolver sistemas é preciso entender bem a estrutura e característica dos dados a serem avaliados e da aplicação que se pretende construir. ""Cada tipo de dado, sejam eles de séries temporais, comprimidos, criptografados, preciso ou impreciso, tem uma interpretação diferente"", indicou. Um dos projetos desenvolvidos pelo laboratório de pesquisa é o SAHA, de acompanhamento do desempenho de atletas de alto rendimento a partir do monitoramento de dados em diferentes áreas, como nutrição e bioquímica. ""A IoT é a próxima grande onda. E três pontos chave são mais relevantes para se pensar neste momento: variedade, velocidade e veracidade"", definiu.",pt,60
236,605,1461715957,CONTENT SHARED,-8618420761918493321,-1032019229384696495,-2767767474126851917,,,,HTML,http://www.greenbot.com/article/3060757/android/what-the-google-i-o-schedule-tells-us-about-the-future-of-android.html,what the google i/o schedule tells us about the future of android,"Google I/O may be a conference for developers, but what happens there will have a major impact on how you interact with Android and Google's other services in the near future. The conference covers all of Google's projects, but Android is definitely the star of this year's show, which will be held in Mountain View from May 18 to 20. Some of the focus will be on better app performance and design, which is always in demand. Google will also have plenty to say about how Android will usher in a future of virtual reality and connectedness across your phone, car, television, and other devices that may not even be here yet. After studying the schedule , there are four key themes that emerge, each illustrating how Android will move forward in the next year and what it will mean for putting your digital life in Google's hands. Virtual Reality is big Google has big ambitions in virtual reality. Cardboard is just the start, as there have been rumors of the company building its own VR headset and indications from Android N about how the operating system will give more native support to VR . So set your eyes on the VR at Google session on May 19, which is hosted by Clay Bavor, Google's vice president of virtual reality (who also has a fascinating photography blog ). Right now Facebook-owned Oculus is leading the VR game and Google's frenemy Samsung makes the most popular consumer device in the Gear VR. So expect Google to invest heavily to ensure the company's services are where the Internet is going. YouTube, as an example, recently added support for VR and 360-degree video. And the outdoor venue for Google I/O may not be a coincidence; it could be a showcase for all Google plans to do with digitizing the outside world. YouTube YouTube now supports videos shot specifically for VR. We have more tangible evidence of the company's plans for augmented reality with Project Tango . Two sessions are devoted to the technology, which empowers phones and tablets to see and sense the environment around them: one focused on gaming with Project Tango and another titled, "" Introducing Project Tango Area Learning ."" Lenovo announced at CES that it would have a first phone with Project Tango technology by the end of this summer, so perhaps we'll get to see a near-final version of this. Either way, Project Tango is inching out of the lab and into actual consumer products soon. And let's not forget about Android Auto. There's a session titled "" Android Auto for everyone ,"" which is all about helping developers extend their apps to the car dashboard. Android Auto has been gaining steam this year with expansions into new vehicles, which makes it a key piece of Google's strategy to have Android and Google's contextual information follow you wherever you are. Going global Another session that caught my eye was, "" Building for billions on Android ."" Led by a team of Android developer advocates, it's likely to include strategies for making apps work across the wide swath of Android hardware. Google Android One is part of Google's effort to make Android the OS of choice in the developing world. Google's OS is growing strongest in what's called emerging markets, places like India, China, and South American nations where many people are just now getting their first smartphone. Usually such phones are lower cost than a flagship sold in the U.S., and they're powered by Android. That's why Google sought to get directly involved with phone sales with Android One , though the program has had a mixed record. In order for Google to keep people hooked into Android, the ecosystem needs good apps that will work well on devices that have lower computing power and Internet speed access. It's the reality for developers who want to build on the platform, and it'll be interesting to see what solutions Google offers. Material Improvements Material Design is nearing its second birthday, as Google first announced the design language and guidelines at the 2014 I/O conference. However, even now some apps still haven't fully embraced the look, which is probably due in part to how many mobile app developers put their initial emphasis on iOS. It also doesn't help that, as a recent App Annie report revealed, the App Store earns almost double the revenue as does Google Play despite half the number of app downloads. The Plaid app is all material, all the time. Day one has two different sessions focused on design issues: "" Material improvements "" and "" Discover the expanded material design motion guidelines ."" The former is to highlight all the tweaks to Material Design during the past couple of years, while the latter will give developers details on how the language has evolved. Nick Butcher, a design and developer advocate at Google, will lead the first session. He's one of the major evangelists for Material Design, which is highlighted notably in his Android app Plaid . These sessions should also serve as a reminder that Material Design is an evolving design language, not a static textbook. Speeding up Android apps No one likes a slow app. Google doesn't either, and there are multiple sessions geared towards strategies for faster performance and less memory usage. For example, a workshop entitled "" Android battery and memory optimizations "" is led by two Googlers who should offer good insight into this area: Ashish Sharma heads Android's Volta Team that focuses on lengthening battery life. Meghan Desai works on Android framework battery and memory management features for Google. The less battery Android apps use, the longer your phone will last. Then there's "" Lean and fast: putting your app on a diet ."" The goal is to reduce the size of an app's APK, which will certainly be welcome in an era where many phones don't offer expandable storage. Hopefully there will be some game developers in this session, as they tend to be some of the heftiest apps around. There's going to be a lot to devour during the conference's three days, and the entire Greenbot team will be there for all the action. Be sure to check our live coverage of the keynote on the morning of May 18 and follow along as we provide analysis of all the major reveals.",en,60
237,1142,1464620108,CONTENT SHARED,1929674614667189969,-1032019229384696495,-2446252650393115078,,,,HTML,http://techcrunch.com/2016/05/30/diane-greene-wants-to-put-the-enterprise-front-and-center-of-google-cloud-strategy/,diane greene wants to put the enterprise front and center of google cloud strategy,"When Google bought bebop Technologies last fall for $348 million , it got more than a stealthy startup. It also landed Diane Greene as executive vice president of Google Cloud Enterprise and that perhaps was the bigger prize. Greene brought with her years of industry experience having co-founded and been CEO at VMware for a decade, building it into a virtualization powerhouse. In fact, under Greene's watch, EMC bought VMware in 2003 for $635 million . She stuck around for 5 years seeing the company spun off in an IPO in 2007 , before departing in 2008 a wealthy woman. She spent the next several years helping other companies as a board member. One of those companies was Alphabet, and it was through this relationship that she was lured back to the big corporation where she was charged with taking Google's struggling cloud business and turning it into an enterprise powerhouse, many suspected it always could be. Way back in the pack In a world with three or four big players, by just about any measure AWS is light years ahead of everyone with a market share lead that, as of last year, was 10 x bigger than its closest 14 competitors combined. Google is fourth in that mix behind Microsoft in second and IBM in third, according to numbers from Synergy Research . The bad news for Google is that it has under five percent marketshare as of Q4 2015. The good news is that it grew at a brisk 108 percent for the quarter, second only behind Microsoft's 124 percent growth rate. That means Greene has her work cut out for her, but she doesn't seem all that worried. She says AWS has a big lead simply because it got a head start on everyone else including Google. ""They were there in the public cloud long before Google. We didn't decide to do public cloud for about four years after AWS,"" Greene told TechCrunch. She says that AWS has a big chunk of what essentially is a very small piece of the potential market, and she believes her company has plenty of time to catch up and grab a substantial share of the remainder. While that's all true, it's worth pointing out that Google has had 6 years to work at this and in spite of all its resources, has managed to garner less than five percent of market share. The right woman for the job After having so much success, why did Greene want to go back to high-level executive job at a big company and take on this challenge to improve Google's cloud position? She says she just sort of fell into it, but given her background and experience in the enterprise, she certainly appears to be the right person for the job. One thing led to another and we were having trouble finding someone, and I eventually said I would do it. As she tells it, she had a lot of conversations with the folks at Google as part of her job as an Alphabet board member, and she began to see a role for herself there. It all started when she became friendly with Urs Hölzle, senior VP of technical infrastructure at Google Cloud while walking their dogs together. She knew they were ramping Google Cloud pretty aggressively and as their friendship grew, they were discussing possible candidates for the role to run the overall cloud business. ""He is a totally brilliant and wonderful person. We started taking our dog for walks and became pretty good friends,"" she explained. Eventually they focused on her. ""One thing led to another and we were having trouble finding someone, and I eventually said OK I would do it - and here I am,"" she said. With Greene, Google scored someone with a tremendous enterprise pedigree. At the time of her hiring in November, Steve Herrod, who was CTO at VMware under Greene, and who is currently managing partner at venture capital firm General Catalyst spoke of her in glowing terms . ""She is awesome and immediately changes the game for Google's cloud efforts. The engineering team at bebop was outstanding as well and they'll bring a ton of enterprise DNA to Google,"" he told TechCrunch at the time. Taking care of business When Greene came on board, the cloud business was fragmented and one of the first things she did was unify all of the pieces under a single umbrella with her at the top of the unit. Sundar Pichai outlined the new organization in a company blog post when he announced Greene's hiring. As he stated at the time , Greene would be in charge of a newly integrated enterprise cloud businesses, that brought together Google for Work, Google Cloud Platform, and Google Apps under a consolidated product, engineering, marketing and sales team. She said one of the reasons for this single Google Cloud view was her experience at VMware where they valued integrated execution across the company. She felt that, in order for this to work well, all of these cloud pieces had to be working in sync with a consistent message up and down the entire cloud stack. The other thing she emphasized at VMware that she brought to Google was the importance of building a broad partnership network. ""We were super friendly to partners at VMware and I brought that in here. There are huge opportunities to partner with companies and we've been accelerating that very quickly. Google is committed to open source and open APIs and part of that is creating a partner-friendly place,"" she explained. While she wasn't ready to name names yet because the ink was still drying on some of the agreements, Google did let me know that they have 13,000 partners in the network, so it's not something they just started after Greene came to the company, but she is attempting to build on that existing effort. The enterprise heats up As Greene applies her enterprise chops to the Google Cloud platform, she rightly sees a market that's heating up and one that Google should by all rights be well-positioned to grab a big piece of. ""The enterprise has become a super interesting and exciting place,"" she said. ""Everybody is realizing they have to digitally transform themselves ."" She sees that cloud having a big role in that as it allows companies to communicate better, flatten hierarchies and move much more quickly than they could running equipment inside a data center. The enterprise has become a super interesting and exciting place. - Diane Greene, Google One of the big difference makers Greene is seeing is the power of data and the role the cloud plays in that. ""It's all about data and getting insights out of data. This is all fairly recent, that everybody is like 'whoa, if I don't do this and my competitor does, I'm going to be left behind,'"" she said. As you might expect, she sees Google as a strong contender for these transforming businesses because it has had to do this itself as a company, building data centers, processing huge amounts of data and applying analytics to it to understand what it has. She sees that experience as a big differentiator for her company. ""It is Google's time for enterprise. Google has a lot to offer the world and it all came together in a nice way. There was just a lot of effort going on around the enterprise that has set things up really well,"" she explained. That could be true, but Google needs to win the hearts and minds of enterprise IT staff, who might mistakenly see Google as merely Google Apps and not the entire cloud stack it has become. Having a spokesperson like Greene should help in that regard and she says a big part of her job is talking to customers and finding out what they need and how Google can help. There's plenty of time AWS is clearly the big dawg at the moment, but Greene doesn't seem all that worried. In spite of AWS existing for a decade, she sees Google as a worthy competitor in a market that's just getting started . ""I don't feel like we both started off building cloud at the same time. We came from different places. The place where we are behind and that's changing quickly is in workloads. Every customer wants to run on all the clouds. Pilots have been deployed and are up and running and growing incredibly fast right now,"" she said. She gets the current landscape, but with her in charge, she sees a market that's wide open for competition and a company that's ready to seize that opportunity. ""They have more workloads and features and more partners, but those are very straight forward things to change."" she said. Featured Image: Courtesy of Google",en,60
238,188,1459792892,CONTENT SHARED,4814419120794996930,268671367195911338,-3964029441019551093,,,,HTML,http://fortune.com/2016/03/27/netflix-predicts-taste/,"netflix says geography, age, and gender are ""garbage"" for predicting taste","Netflix rolled out to 130 new countries earlier this year, and you might expect that it began carefully tailoring its offerings for each of them, or at least for various regions. But as a new Wired feature reveals, that couldn't be further from the truth-Netflix uses one predictive algorithm worldwide, and it treats demographic data as almost irrelevant. Get Data Sheet , Fortune 's technology newsletter. ""Geography, age, and gender? We put that in the garbage heap,"" VP of product Todd Yellin said. Instead, viewers are grouped into ""clusters"" almost exclusively by common taste, and their Netflix homepages highlight the relatively small slice of content that matches their taste profile. Those profiles could be the same for someone in New Orleans as someone in New Delhi (though they would likely have access to very different libraries ). Netflix seems to have discovered (or built on) a powerful insight from sociology and psychology: That in general, the variation within any population group is much wider than the collective difference between any two groups. So if you want to, say, get someone to stream more of your content, you're better off leveraging what you know about similar individuals in completely different demographic groups, than trying to cater to broad generalizations. For more on Netflix, watch our video: As an example, Wired shares that 90% of Netflix's total anime streaming volume comes from outside Japan. That's because how much you like anime is determined less by your nationality than by (pardon me) how big of a nerd you are. There's a huge, crucial lesson here for other businesses-and perhaps a slightly scary reality for consumers. In the era of big data, consumer profiling can't rely on broad categories like race or location. To target the customers who want what you're offering, you have to get past the surface and see what really makes them tick.",en,60
239,599,1461695700,CONTENT SHARED,-5571606607344218289,6003902177042843076,119592526124196527,,,,HTML,http://www.infoq.com/br/news/2015/06/falta-compentencias-testes,agile: falta competência nos testes,"Fran O'Hara, diretor e principal consultor da Inspire Quality Services, recentemente compartilhou suas lições aprendidas integrando teste s no ciclo de vida do desenvolvimento ágil . A principal mensagem de O'Hara é que competências de teste são necessárias mesmo quando os papéis tradicionais de testes não existem. Quando equipes ágeis focam somente em automatização de testes funcionais, falhas aparecem em termos de testes exploratórios e outras áreas de risco tais como: testes de integração de sistemas e testes não funcionais (desempenho e usabilidade, por exemplo). Embora o objetivo seja ter equipes multi-funcionais, estas não devem ser compostas somente de especialistas (como um testador, um desenvolvedor, um designer, entre outros) ou por profissionais com formação generalista. Neste último caso a equipe pode não ter as habilidades maduras o suficiente para entregar efetivamente um software. Competências de testes em particular (como levantar casos de teste, esclarecimento de requisitos de negócio ou testes automáticos limpos) tendem a se perder quando organizações interpretam o Scrum ao pé da letra, assim, montando equipes compostas apenas por desenvolvedores (além do Scrum Master e do Product Owner). De acordo com O'Hara, equipes com profissionais com habilidades de testes regulares tem desempenho melhor do que equipes que não tem, provando a necessidade de misturar as habilidades e até mesmo os traços de personalidades (testadores tendem a ter uma pedante atenção aos detalhes para encontrar equívocos e brechas nos requisitos antes que eles sejam inseridos no produto) da equipe. Outras competências tipicamente atribuídas para gerente de testes ou qualidade no desenvolvimento tradicional como definição do processo e estratégia de testes e planejamento de testes podem ser necessárias no desenvolvimento ágil, adiciona O'Hara. Por um lado, muitas organizações ainda agrupam novas funcionalidades em grandes versões, que exigem algum nível de integração de sistemas para entregar. Por outro lado, quando equipes de desenvolvimento ágil que fazem testes durante o desenvolvimento (usando TDD e BDD por exemplo) ainda focam tipicamente em aspectos funcionais das estórias de usuário, deixando de fora o desempenho, usabilidade e outros requisitos não funcionais. O último ainda precisa ser executado em algum momento antes da entrega, necessitando assim de coordenação e planejamento. O'Hara recomenda o apontamento de campeões ou consultores de testes que podem trabalhar aconselhando múltiplas equipes sobre como preencher competências de testes faltantes. O objetivo final é alcançar uma integração completa dos testes nas equipes multi-funcionais, como no cenário C da imagem a seguir: O'Hara alerta que múltiplas práticas de trabalho são necessárias para alcançar este nível de integração de testes, a partir de uma abordagem de testes de aceitação, incluindo tarefas de testes (como testes de ambiente / configuração inicial ou testes exploratórios) no planejamento da sprint, limitando o trabalho em progresso em algumas histórias de cada vez, evitando que os testes e validações sejam o último passo (por exemplo, ""validação"" ser a última coluna do quadro de tarefas é um mau sinal), efetivo e frequente refinamento do backlog, foco na qualidade do código (padrões de código, revisões, TDD, BDD) e aderente a uma rigorosa definição de pronto (mesmo em tempos de estresse). Discutindo os requisitos de uma perspectiva de negócio durante o refinamento do backlog (diminuindo a extensão do planejamento da sprint) é também uma atividade crucial, mas de acordo com O'Hara, para que esta atividade seja efetiva o Product Owner deve trazer para a mesa não apenas funcionalidades de alto nível, mas também um conjunto inicial de critérios de aceitação que podem em seguida ser discutido e refinado pela equipe em pequenas estórias dentro do escopo. Finalmente, retrospectivas devem focar em melhorar a definição de pronto para ajudar a gerenciar débito técnico e garantir a qualidade interna (código) e externa (funcional e não funcional), diz O'Hara.",pt,60
240,2115,1471607487,CONTENT SHARED,2417383163258637695,7645894863578715801,5690372062060589522,,,,HTML,https://m.signalvnoise.com/eat-sleep-code-repeat-is-such-bullshit-c2a4d9beaaf5?gi=699b51bde52c,"""eat, sleep, code, repeat"" is such bullshit - signal v. noise","I'm on my way back home from Google I/O 2016. It was a fantastic conference - I met some great people and learned a lot. But while I was there, I saw something horrifying, something I couldn't shake from the moment I saw it... ""Eat. Sleep. Code. Repeat."" was printed on everything. I'd seen the phrase before, but this time it burned into my brain, probably because it was being so actively marketed at a large conference. I literally let out an ""ugh"" when I saw it. What's the big deal? It's just a shirt. Look, I get it - Google I/O is a developer conference, and the ""eat, sleep, code, repeat"" phrase is intended to be a clever way (albeit a completely unoriginal one) of saying ""coding is awesome and we want to do it all the time!"" I appreciate the enthusiasm, I do. But there's a damaging subtext, and that's what bothers me. The phrase promotes an unhealthy perspective that programming is an all or nothing endeavor - that to excel at it, you have to go all in. It must be all consuming and the focus of your life. Such bullshit. In fact it's the exact opposite. At Basecamp I work with some of the best programmers in the world. It's no coincidence that they all have numerous interests and talents far outside of their programming capabilities. Whether it's racing cars, loving art, reading, hiking, spending time in nature, playing with their dog, running, gardening, or just hanging out with their family, these top-notch programmers love life outside of code. That's because they know that a truly balanced lifestyle - one that gives your brain and your soul some space to breathe non-programming air - actually makes you a better programmer. Life outside of code helps nurture important qualities: inspiration, creative thinking, patience, flexibility, empathy, and many more. All of these skills make you a better programmer, and you can't fully realize them by just coding. Don't believe the hype It's no secret that the tech industry loves hyperbole. How will you ever reach the coveted title of ninja, rock star, or wizard if you don't spend all your waking, non-eating hours programming?! I'll give my standard advice: ignore the hype. It's wonderful to be so dedicated to your craft that programming is all you ever want to do. I love that enthusiasm. It can carry you to great heights. But if you want to become the very best programmer you can be, make space for some non-programming activities. Let your brain stretch its legs and you might find a whole new level of flow. ��",en,60
241,1969,1470224517,CONTENT SHARED,7087600436874507849,3609194402293569455,-2345143130174579669,,,,HTML,http://www.inova.unicamp.br/inovajovem/workshop-de-design-thinking/,workshop de design thinking - unicamp,"O Workshop de Design Thinking é a primeira capacitação do Programa Inova Jovem 2016. Sob o comando de Luiz Borges, engenheiro de produto e desenvolvimento da 3M, a atividade pretende introduzir a metodologia Design Thinking e apresentar as etapas de montagem de um projeto, a fim de qualificar os alunos para a entrega das propostas durante a fase de inscrição de projetos no Inova Jovem. O evento contará com uma dinâmica. É indicado que os alunos inscritos levem materiais para auxiliar na atividade, como: papéis coloridos, tesoura, lápis, giz de cera, canetinha, fitas adesivas, tampinhas de garra, canudo, palito de sorvete, entre outros. As vagas são limitadas e a capacitação não é obrigatória para a inscrição de projetos na competição. Inscrições abertas: de 1º a 23 de agosto. Data: 27 de agosto, das 9h às 13h Local: Auditório 5 da FCM (Faculdade de Ciências Médicas) da Unicamp",pt,60
242,2266,1473162790,CONTENT SHARED,-6772940823843058601,-1402490765047599382,6366377273742956136,,,,HTML,http://exame.abril.com.br/tecnologia/noticias/visa-inaugura-centro-para-inovacao-em-pagamento-digital,visa inaugura centro para inovação em pagamento digital | exame.com,"São Paulo - A Visa deu mais um passo para acabar com o cartão de crédito físico. A empresa inaugura nesta quinta-feira seu primeiro centro de inovação no Brasil, localizado em sua sede na capital paulista. O foco é o desenvolvimento de novos métodos de pagamento digital a partir de parcerias com startups , empresas e desenvolvedores. ""O centro de inovação muda a forma como dialogamos com nossos desenvolvedores e clientes"", falou a EXAME.com Érico Fileno, diretor executivo de produtos da Visa no Brasil. ""Ele amplia a atuação da empresa e desenvolve inovações de uma forma mais rápida."" A ideia é que a parceria com startups traga inovação a pagamentos digitais. ""Queremos mudar a imagem de que um produto foi 'feito pela Visa' para 'habilitado pela Visa'"", falou a EXAME.com Percival Jatobá, vice-presidente de produtos da Visa no Brasil. Neste momento, o foco da Visa é em duas tecnologias que podem mudar o mercado de pagamentos digitais: biometria e internet das coisas. ""A internet das coisas vai transformar a maneira como a sociedade faz e recebe pagamentos"", diz Jatobá. ""Não teremos conexão com uma máquina, mas com várias máquinas ao mesmo tempo."" Segundo uma pesquisa da empresa, 50 bilhões de máquinas estarão interconectadas em 2020. Para o executivo, essas mudanças não assustam. Ele explica que a Visa passou por um momento de educar o mercado na transição do cartão de tarja para o chip (com o qual se usa uma senha eletrônica). ""Naquela época, fizemos um projeto de aculturamento do comércio. Com pagamentos móveis, teremos que fazer esse processo novamente."" Para que tudo isso fosse possível, no entanto, uma mudança iniciada no começo deste ano foi crucial: a permissão para que desenvolvedores externos acessem os sistemas da Visa . ""Apenas com a abertura dos nossos sistemas conseguiremos criar novas tecnologias"", falou a EXAME.com José María Ayuso, vice-presidente de produtos da Visa na América Latina. Uber dos pagamentos Pelo programa Visa Developer , a empresa já concede acessos a 11 APIs (sigla para interfaces de programa de aplicação), que permitem a comunicação entre sistemas da Visa e externos. Kits de desenvolvimento de software também foram disponibilizados. Essa estratégia, diz Ayuso, faz parte da ""uberização"" do mercado. O termo é uma alusão ao Uber , que não criou sistemas de mapeamento, pagamento ou comunicação. Em vez disso, usou APIs abertas para viabilizar o serviço. O espaço em São Paulo não é o primeiro do tipo. Outros estão em cidades como Dubai, São Francisco e Londres. Aqui, fica localizado dentro da própria sede da empresa. O espaço tem clima mais descontraído do que o restante da empresa. Produtos como relógios , pulseiras e anéis de pagamentos ficam disponíveis para testes. Ayuso diz que o Brasil foi uma escolha fácil. ""O país comporta mais da metade do volume de negócios da Visa na América Latina. De todas as transações de e-commerce feitas na América Latina, 60% são feitas aqui."" Outro fator que pesou foi o fato de que os bancos brasileiros inovam rápido. Neste início, a Visa fechou uma parceria com a Farm, aceleradora de startups. ""Afinal, a Visa também já foi uma startup e se tornou uma grande empresa devido ao pensamento disruptivo de um indivíduo, o nosso fundador"", diz Jatobá. Centros como esse podem ajudar a acabar com os cartões físicos. Mas é um plano para longo prazo. ""Talvez, no futuro, as pessoas não utilizem mais o cartão de crédito físico. Mas não acredito que a minha geração ou ainda a próxima irá descarta-lo"", diz Ayuso.",pt,60
243,1449,1466398173,CONTENT SHARED,-4338308747999225618,-1032019229384696495,2118450654323055722,,,,HTML,http://www.businessinsider.com/how-diane-greene-transformed-googles-cloud-2016-6,how the queen of silicon valley is helping google go after amazon's most profitable business,"Google The first thing to understand about Diane Greene, the woman Google acqui-hired in November to transform its fragmented cloud business, is that she has the mind of an engineer. Cool technology, elegantly designed and built, lights her up.Even her jokes tend to be geek oriented. (A lifelong competitive sailor, she was a mechanical engineer who built boats and windsurfers before she became an iconic Silicon Valley computer scientist.) The second thing to understand about her is that she hates the limelight. While she's fine with standing on stage talking about all the cool things Google is building for their new target customer, big companies, she prefers not to talk about herself. In fact, she's so ego-free, her office at Google's Mountain View, California, headquarters is just a tiny windowless room, big enough to hold an ordinary desk and two chairs. Business Insider Before she took the job, Google had been building products and pursuing business customers in a sort of hodgepodge way. Its Google for Work unit had Google Apps, Chromebooks, and an assortment of other products like videoconferencing. It had poached Amit Singh from Oracle a few years back to help turn Google Apps into a more professional business unit, capable of taking on Microsoft Office. He had hired salespeople and created a support organization. (He's since moved on to work for Google's young virtual reality unit .) But Google for Work wasn't working very closely with Google's nascent cloud computing business, running under Urs Hölzle. That unit included a huge cadre of people running Google's data centers (600 computer security experts alone, for instance), but only a small separate sales force. In the seven months since Greene came in that's changed. She: hired experienced enterprise sales and support personnel. created the office of the CTO which handles the technical questions, design or customization a large customer needs. created units that focus on specific industries, because an agriculture firm has different needs than a retailer. created programs for getting more ""reseller"" partners on board, the small consultants who will sell and support Google's cloud to smaller customers, offering niche services. created a Global Alliance program for working with big global partners. ""So these are all new,"" Greene tells us. Now, all the teams are working together. ""We all get together once a week, we share and discuss and debate,"" she says. ""It wasn't possible before I came because sales and marketing were in a different division than cloud. And cloud was in a different division than Apps. I feel like the structure is in place now and we're hiring very aggressively."" Hölzle wooed her to the job Greene made her name as cofounder of VMware, with her famous Stanford professor husband. Vmware has gone on to become a giant tech company. She left the VMware CEO role about eight years ago, after EMC bought it. Google+ Until taking this Google job, she was quietly doing her own thing, raising her kids, advising and angel investing in startups (many of which did spectacularly well ), and being on a few boards, including Google's board since 2012. She was under the radar but still highly and widely respected, the queen of enterprise computing. She was also working on a new startup, Bebop Technologies, until Google bought it for $380 million when it hired her. Greene's take was $149 million, and she and her husband dedicated that money to charity. Hölzle, the engineer who famously built Google's data centers and runs the technical side of the cloud business is Greene's partner. He believes that within a few years, Google's cloud business can be bigger than its ad business. That's a big goal: Google currently makes the vast majority of its $75 billion in annual revenue from ads. Hölzle is the one who talked Greene into taking this job as they hung out walking their dogs together. ""Through being on the board, I got to know Urs and started working with him informally,"" Greene says. ""We knew we needed an overall business leader. He's a brilliant person and fun to work with. He really wanted to me to do it. I just realized, wow, partnering with Urs, we can really do this, with the backdrop of Google which is just this amazing company,"" she says. A new phenom Google has placed itself at the center of one of the biggest, newest trends happening in the enterprise market. Some people call this trend digital transformation. But it's more than just automating manual processes or turning paper forms into iPad apps. Flickr/Amanda Parsons More and more, the IT departments at large companies have started treating their tech vendors as partners that help them co-create the tech they need. ""This is new for me. I've never been in the enterprise where your customers are your partners. It was always, you had customers and you had partners. But almost every customer of a certain size is a partner. It's going both ways now,"" Greene says. She points to one customer, Land O'Lakes, as an example. Land O'Lakes is probably best known for its butter and dairy products. It took crop and weather data from Google and worked with Google to build an app hosted on Google's cloud. The app helps its farm and dairy co-op members improve their crop yields. ""It's fun for us to help them do that,"" she says. Unlike the old days, where an IT company would be the one to build the app and sell it to agriculture companies, ""we don't have to do it ourselves."" 'More and more' This idea of partnering with customers is the key to her strategy. Tim Stenovec/Business Insider ""For me, this is such a revolution,"" she says. ""Everything is changing now that we are in the cloud in terms of sharing our data, understanding our data using new techniques like machine learning."" Google's competitive strength, Greene believes, is the breadth of the tech it can offer an enterprise. Enterprise app developers can tap into things like Maps, Google's computer vision engine (the tech that powers Google Photos), weather data, language/translation/speech recognition. They can build apps on top of Google's Calendar, documents, spreadsheet and presentation apps. And, under Greene's new integrated organization, they can even tap into the tech that powers Google's ads or YouTube, search, or its many other services. ""And we're going to have more and more,"" she says. When a company can take its own data and combine it with all of Google's technology and Google's data, ""there's just huge possibilities,"" she says. Google Greene will tell you, ""We're the only public cloud company with all of that."" When pointing out that Microsoft also offers a computer vision API, translation services and APIs for Office 365, IBM also offers weather data and language services, and so on, Greene's got a come-back ready. ""We have Chromebooks."" Well, Microsoft has Surface. ""But Chromebooks can run all the Android apps, are totally secure, they have administration ... and they have a nice keyboard,"" she laughs. In fact, Greene says, ""I only use a Chromebook now. I never thought I could do that but I love it."" She's watching Amazon In truth, she's not laser focused on overtaking Microsoft, widely considered the No. 2 cloud player, with Google trailing behind. Google She, like all the cloud vendors, are looking at market leader Amazon Web Services, which is raking in the enterprise cloud customers. AWS is even convincing a growing number of them to shut down all of their data centers and just rent everything from AWS. This includes Intuit , the other company where Greene is a board member. AWS is so successful it's currently on track to do $10 billion in revenue this fiscal year and it's also Amazon's most profitable business unit . And it blows all the competition out of the water in the sheer number of features on its cloud , as well as its partner ecosystem. So how is she going to beat Amazon? By offering better tech, she says. ""I'm a little biased but I really do think, on the hard stuff, we're the world's best cloud,"" she says. Google ""I agree we have more features to do, although we have the basics for enterprise that you need. We have more partners to bring on, but we're doing that very quickly. But the hard stuff, I do think we're the world's best."" While Greene would not share the the cloud unit's growth numbers, she says that ""growth is really good and we're doing great stuff with some really big customers."" She adds: ""We've been moving customers to our cloud both from Amazon and on-prem."" 'On-prem' means getting companies to move the apps they have running in their own computers on their own premises into Google's cloud. Google has even been engaging Amazon with its price cuts war, she says. ""They've been following our price cuts. We've been initiating them,"" she says. She jokes, ""We should make a T-shirt 'The highest quality, lowest-cost cloud.'"" Disclosure: Jeff Bezos is an investor in Business Insider through his personal investment company Bezos Expeditions. SEE ALSO: Netflix, Juniper, and Intuit explain how Amazon is eating the $3.5 trillion IT industry SEE ALSO: The inside story of why Cisco's 4 most powerful engineers really quit - and why many employees are relieved NOW WATCH: NASA released the sharpest photos of Pluto in history - and they're spectacular",en,60
244,2222,1472499986,CONTENT SHARED,2103268612948910635,1908339160857512799,-8312304323942241499,,,,HTML,http://revistamarieclaire.globo.com/Noticias/noticia/2016/08/nao-ao-preconceito-onu-celebra-dia-da-visibilidade-de-mulheres-lesbicas-e-bissexuais.html,não ao preconceito! onu celebra dia da visibilidade de mulheres lésbicas e bissexuais,"ONU celebra o Dia da Visibilidade de Mulheres Lésbicas e Bissexuais (Foto: Reprodução / Faceboook / Carolina Rosseti) Com o propósito de dar visbilidade às contribuições de lésbicas e bissexuais para a sociedade, a ONU lança nesta segunda-feira (29) a campanha ""Livres & Iguais"". A data foi escolhida por ser o Dia da Visibilidade de Mulheres Lésbicas e Bissexuais. As Nações Unidas veicularão, nas redes sociais, três vídeos de conscientização e ilustrações da designer Carolina Rosseti com histórias de algumas de mulheres. A morte de Luana Barbosa dos Reis em abril deste ano vítima de espancamentos por oficiais da Polícia Militar foi um caso destacado pelo organismo internacional. Ela era negra, lésbica e moradora da periferia de Ribeirão Preto, em São Paulo. A campanha ressalta também a importância do apoio familiar. ""Quando os direitos LGBTI são afetados, todos nós somos afetados"", afirmou o coordenador residente do Sistema ONU no Brasil, Niky Fabiancic. Segundo ele, a luta por uma sociedade mais justa para gays, lésbicas, bissexuais, pessoas trans e intersex é um compromisso da ONU. Jaime Nadal, representante nacional do Fundo de População das Nações Unidas (UNFPA), lembrou que, em outubro do ano passado, a ONU expressou preocupação com a tramitação do Estatuto da Família (PL 6583/2013) no Congresso. O organismo internacional fez uma pelo para que o governo reconhecesse todos os arranjos familiares existentes no Brasil, não apenas aqueles formados por casais heterossexuais. ""É muito importante que uma nova legislação garanta os direitos dos casais LGBT que a jurisprudência hoje já permite"", reiterou Nadal.",pt,59
245,524,1461211879,CONTENT SHARED,7229629480273331039,-1032019229384696495,-6158072573466935774,,,,HTML,http://www.businessinsider.com/organization-ranks-google-as-best-cloud-2016-4,"an independent organization just ranked google as the best cloud, beating amazon","Fortune Global Forum Amazon may be the 800-pound gorilla of the cloud computing market, but some new research indicates that Google offers the best cloud in the for the money. So says a cloud benchmark of top US clouds developed by French IT business news site JDN, based on a similar benchmark of clouds in France . JDN banded together with two companies that test cloud performance, CloudScreener and Cedexis, to determine the best clouds. They looked at Amazon Web Services, Google Compute Engine, IBM SoftLayer, Microsoft Azure, and Rackspace. Cloudscreener tested the clouds for performance, prices and level of service. Cedexis tested for network performance. All told, they came up with four rankings: a price ranking, a performance ranking, a service level ranking (a way to measure reliability), and an overall ranking. Google won the overall ranking with a score of 85 out of a 100. Amazon came in second with 75 and IBM was third at 72. (They're not listed on the chart, but Microsoft Azure landed in No. 4, with 63 and Rackspace in last with 62.) JDN/CloudScreener/Cedexis U.S. Cloud Benchmark Google's victory is largely because it won the price war, coming in as less expensive than the others for the same type of cloud configuration. The index tested three common types of cloud services companies buy: a large Windows machine, a large Linux machine, and a combination of machines (a ""cluster""). JDN/CloudScreener/Cedexis U.S. Cloud Benchmark Rackspace won as the best performing cloud. And Amazon won the ""Service Level"" rankings which looked at things like having data centers in different regions and certifications that prove the cloud is secure. While this is just one attempt to compare and rank clouds, it's a good sign for Google, because most enterprises will do their own kinds of testing before picking them. Right now, Amazon is the big cloud leader. But the market is young and Google intends to be a player. ""Every business in the world is going to run on cloud eventually,"" Google CEO Sundar Pichai said a few months ago on a post earnings call . ""There's great buzz at Google around this area, and we continue to heavily ramp up investment here."" Disclosure: Jeff Bezos is an investor in Business Insider through his personal investment company Bezos Expeditions.",en,59
246,2160,1471966811,CONTENT SHARED,2468005329717107277,-709287718034731589,-8543976727396364315,,,,HTML,https://uxdesign.cc/how-netflix-does-a-b-testing-87df9f9bf57c?gi=27177b9af62,how netflix does a/b testing - uxdesign.cc - user experience design,"How Netflix does A/B Testing Have you ever wondered why Netflix has such a great streaming experience? Do you want to learn how they completed their homepage plus other UI layout redesigns through A/B testing? If so, then this article is for you! I'll start with sharing my takeaways from a Designers+Geeks event I attended last week at Yelp . The two great speakers Anna Blaylock and Navin Iyengar, both product designers at Netflix , walked through insights gleaned from their years of A/B testing on tens of millions of Netflix members, and showed some relevant examples from the product to help attendees think about their own designs. Experimentation I really liked this first slide of the presentation and think it's smart to use an image from the TV show "" Breaking Bad "" to explain the concept of experimentation! The Scientific Method Hypothesis In science, a hypothesis is an idea or explanation that you then test through study and experimentation. In design, a theory or guess can also be called a hypothesis . The basic idea of a hypothesis is that there is no pre-determined outcome. It is something that can be tested and that those tests can be replicated. ""The general concept behind A/B testing is to create an experiment with a control group and one or more experimental groups (called ""cells"" within Netflix) which receive alternative treatments. Each member belongs exclusively to one cell within a given experiment, with one of the cells always designated the ""default cell"". This cell represents the control group, which receives the same experience as all Netflix members not in the test."" - Netflix blog Here's how A/B testing is done at Netflix: as soon as the test is live, they track specific metrics of importance. For example, it could be elements like streaming hours and retention. Once the participants have provided enough meaningful conclusions, they move onto the efficacy of each test and define a winner out of the different variations. Experiment Experimentation is the act of experimenting .Many companies like Netflix run experiments to generate user data. It is also important to take time and effort to organize the experiment properly to ensure that both the type and amount of data is sufficient and available to clarify the questions of interest as efficiently as possible. You probably have noticed that the featured show on the Netflix homepage seems to change whenever you log in. They're all part of Netflix's complex experiments to get you to watch their shows. The idea of A/B testing is to present different content to different user groups, gather their reactions and use the results to build strategies in the future. According to this blog post written by Netflix engineer Gopal Krishnan : If you don't capture a member's attention within 90 seconds, that member will likely lose interest and move onto another activity. Such failed sessions could at times be because we did not show the right content or because we did show the right content but did not provide sufficient evidence as to why our member should watch it. Netflix did an experiment back in 2013 to see if they can create a few artwork variants that increase the audience for a title. Here is the result: It was an early signal that members are sensitive to artwork changes. It was also a signal that there were better ways they could help Netflix members find the types of stories they were looking for within the Netflix experience. Netflix later created a system that automatically grouped artwork that had different aspect ratios, crops, touch ups, localized title treatments but had the same background image. They replicated experiment on their other TV shows to track relative artwork performance. Here are some examples: How Netflix selects the best artwork for videos through A/B testing The Netflix experimentation platform , a service which makes it possible for every Netflix engineering team to implement their A/B tests with the support of a specialized engineering team. Check out these two blog posts to learn more about Netflix A/B testing: What I learned When and why A/B testing Once you have a design in production, use A/B testing to tweak the design and target two key metrics: retention and revenue. By A/B testing changes throughout the product and tracking users over time, you can see whether your change improves retention or increases revenue. If it does, make it the default. In this way A/B testing can be used to continuously improve business metrics. Are your users finding or doing one thing you want them to find or to do? My experience is that often times users cannot always complete a task as fast as you expect, and sometimes they can't even find a certain button you put on a page. The reasons can vary: it might because the design is not intuitive enough; the color is not vibrant enough; the user is not tech savvy; they don't know how to make a decision because there are too many options on one page, and so on. Are your intuitions correct? Sadly, when it comes to user behavior, our intuitions could be wrong, and the only way to prove it is through A/B testing. It is the best way to validate whether one UX design is more effective than another. At work, our consumer product team have proved that through A/B testing on our real estate website. For example, they wanted to figure out whether they can make a design change to improve the registration rate for users who clicked on a Google Ad. They created a few different experimental designs and tested them. They thought the design that only hides the property image would win, but found that the design that hides both the property image and the price got the highest conversation rate. Explore the boundaries The best ideas come from many idea explorations. At work, our product team works collaboratively across many different projects. With so many parties involved (from designers to product managers to developers), we get to explore the boundaries together. Some of the best ideas are sometime from the developers or the product managers after testing out our prototypes. Observe what people do, not what they say When talking to users, it's important to keep this in mind: they always say one thing but do it differently. I conducted a few user testing sessions this week and have one perfect example to show you why. I had this one user testing out a Contacts list view prototype and asked him if he usually sorts/filters his Contacts. He said no because he wouldn't need do so. However, when he discovered the new filters dropdown menu, he was amazed by how convenient it is to sort and filter multiple options at a time and immediately asked when that can roll out in production. Use data to estimate size of opportunity * It's always about the whys * Data can help shape ideas * Check if any A/B testing are in conflict A/B testing is the most reliable way to learn user behaviors. As designers, we should think about our work through the lens of experimentation. Isn't it so fun to be a UI and UX designer? :) Knowing your user is the most exciting part of design process! There is no finished design, but many chances for iteration to improve the design and give our users the best experience possible! I enjoy the opportunity to make subtle tweaks for our users, measure their reactions and work with the product team to figure out the next steps. If you like this article, please kindly tap the ❤ button below! :)",en,59
247,1244,1464962240,CONTENT SHARED,-541666025693385823,-1032019229384696495,7836530673378398837,,,,HTML,http://thenextweb.com/apple/2016/05/29/google-material-desing-ios-complaints/,calm down: google's use of material design on ios is fine,"A recent bit of commentary from Macworld posits that Google's use of Material Design on iOS is a mistake. Another site piled on , going so far as to call it ""dumb"" and ""rude."" The main takeaway is that design can be divisive, and is always subjective. Material Design is no different, but there needs to be a lot of chill on this one. I don't need to like it So, here's some personal insight: I fucking hate Material Design. I think it's too reliant on dumbed-down, wannabe 'clever' animations and feigned shadowing. It tries way too hard to look like it's not digital (Google's design lead Matias Duarte notes Material Design's creation had a lot of paper cut-outs at inception to help designers understand its 'real world' feel), but it is. Material Design wants to make you feel as though you're touching something tangible, but you're not. You. Are. Tapping. A. Screen. I don't care to have a button spin and slide away when I press it, and I don't need a clever spring-to-life page pop-up. So, no, I don't like Material Design. I hope that's very clear. Don't drink the 'haterade' I will, however, defend Google's right to use it as they please. Macworld points out that Google's widespread use of Material Design across platforms is akin to Microsoft in the 90's, and that's fair, but it didn't end there. Microsoft is still committed to its square design (which I'm also not fond of, and have now realized I'm some sort of design-snob wolf in developer's sheepish wool), but nobody seems concerned about that. Instead, we laud it for making its apps and services available across platforms. The convenient trope is 'if you don't like it, don't use it,' but Google is hard to avoid. I do for the most part, though; I use DuckDuckGo for search , and have eschewed just about all Google-y apps for iOS. There are many reasons for that, and design is definitely part of it. But many can't - or won't - move away from Google. That should tell us that while design is fundamentally important, it takes a backseat to experience, and Google does a great job there. I do think we should be celebrating diversity in design, even if we don't like it. The rare times I have to dig into my Google Drive folder (it's literally my lone holdout; damn you, cloud storage!), I'm reminded just how much i dislike Material Design. As for the narrative that Google is trying to give iOS users a 'sneak peak' of Android - that's just laughable. Design isn't likely to make anyone switch platforms. Gotta love the sensationalism, though. Be fair The opposite point can be made for Apple. Material Design was built with the Web in mind, so should the iCloud Web client use Material Design? Hell no. It's a slippery slope. If we're asking Google to adhere to Apple's design philosophy for native apps, are we also going to ask Fantastical to look and act more like Apple's (terrible) stock calendar? Should PCalc be more like Apple's (boring) calculator, or your favorite note-taking app more like - well, Notes? ( which is awesome , so yes?) The orginal argument makes Material Design about platforms, but it's not. Google designed it to be used anywhere and everywhere because it's 2016 and platforms aren't as important. The Android vs. iOS debate is tired, and we should stop entertaining it. I doubt Apple cares how many Apple Music users are on Android versus iOS; Google is also a company of services, not a single platform. the same is now true for Microsoft. There's something to be said for using platform-specific tools, but even that's a grey area. Unless Google did something like completely disable the share sheet on iOS, I don't see much harm. Material Design is a shitshow, but it's Google's shitshow. I don't have to stop and look (unless it's Drive; stupid easy-to-use cloud storage). I think a lot of what Apple does with design sucks too, and developers like to bitch and moan about how Apple makes it hard to paint outside the lines in Xcode. And shouldn't we be celebrating diversity on iOS, which has suffered through that type assimilation for a long time? We say 'damn Google for Material Design,' then complain about Apple making it difficult to stray too far from center when it comes to making apps. Maybe we should stop complaining so much on the internet (now that's laughable). Be fair, be objective, and chill out. Also, try Pages or iCloud Drive on iOS. Complaints about Material Design seems downright trite when you attempt to use Apple's services on mobile. At least Google nailed UX, even if its UI is terrible. Read next: Bring your surroundings to life with the futuristic Touch Board DIY Starter Kit",en,59
248,1017,1463617819,CONTENT SHARED,-1995591062742965408,-9016528795238256703,5698124852693708605,,,,HTML,https://firebase.google.com/docs/test-lab/,firebase test lab for android,"Test your app on devices hosted in a Google datacenter. Firebase Test Lab for Android provides cloud-based infrastructure for testing Android apps. With one operation, you can initiate testing of your app across a wide variety of devices and device configurations. Test results-including logs, videos, and screenshots-are made available in your project in the Firebase console. Even if you haven't written any test code for your app, Test Lab can exercise your app automatically, looking for crashes. Key functions Test on real devices Use Test Lab to exercise your app on physical devices installed and running in a Google data center. Test Lab helps you to find issues that only occur on specific device configurations (for example, a Nexus 5 running a specific Android API level and specific locale settings). Run app tests, even if you haven't written any You can use Robo test to find issues with your app so you can test your app even if you haven't written app tests. Robo test analyzes the structure of your app's user interface and then explores it, automatically simulating user activities. If you have written instrumentation tests for your app, Test Lab can also run those tests. Workflow integration Test Lab is integrated with Android Studio, the Firebase console, and the gcloud command line. You can also use Test Lab with Continuous Integration (CI) systems. How does it work? Test Lab uses devices running in a Google data center to test your app. The devices used for testing are real production devices that are flashed with updated Android API levels or locale settings that you specify so that you can road-test your app against a real-world collection of real devices and device configurations. Devices in a data center Test Lab lets you run Espresso , Robotium , or UI Automator 2.0 instrumentation tests written to exercise your app from the Firebase console, Android Studio , or the gcloud command line interface . You can also use Robo test to automatically exercise your app from the Firebase console or the gcloud command line. Robo test captures logs, creates an ""activity map"" that shows a related set of annotated screenshots, and creates a video from a sequence of screenshots to show you the simulated user operations that it performed. Learn more about Robo test . Implementation path If you are running instrumentation tests, write your app-specific test. When developing instrumentation tests for your app, remember to add the Test Lab screenshot library to your app test project so that you can more easily interpret test results. Choose a test environment and a test matrix. Using a test environment of your choice (the Firebase console, Android Studio, or the gcloud command line interface), define a test matrix by selecting a set of devices, API levels, locales, and screen orientations. Run your tests and review test results. Depending on the size of your test matrix, it may take several minutes for Test Lab to complete your test run. After your test run is complete, you can review test results in the Firebase console. Next steps",en,59
249,2318,1473688554,CONTENT SHARED,5008913705695203000,3302556033962996625,-7667010802419955831,,,,HTML,http://jakewharton.com/just-say-no-to-hungarian-notation/,post: just say mno to hungarian notation,"Just Say mNo to Hungarian Notation Every day new Java code is written for Android apps and libraries which is plagued with an infectious disease: Hungarian notation. The proliferation of Hungarian notation on Android is an accident and its continued justification erroneous. Let's dispel its common means of advocacy: "" The Android Java style guide recommends its use "" There is no such thing as an Android Java style guide that provides any guidance on how you should write Java code. Most people referencing this non-existent style guide are referring to the style guide for contributions to the Android Open Source Project (AOSP) . You are not writing code for AOSP so you do not need to follow their style guide. If you're working on code that might someday live in AOSP you don't even need to follow this style guide. Almost all of the Java libraries imported by AOSP do not follow it, and even some of the ones developed inside of AOSP don't either. "" The Android samples use it "" These samples started life in the platform inside of AOSP so they adhere to the AOSP style. For those which did not come from AOSP, the author either incorrectly believes the other points of advocation in this post or simply forget to correct their style when writing the sample. "" The extra information helps in code review "" The 'm' or 's' prefix on name indicates a private/package instance field or private/package static field, respectively, where this would otherwise not be known in code review. This assumes the field isn't visible in the change, since then its visibility would obviously be known regardless. Before I attempt to refute this, let's define Hungarian notation. According to Wikipedia , there are two types of Hungarian notations: System notation encoded the data type of the variable in its name. A user ID that was a long represented in Java would name a variable lUserId to indicate both usage and type information. Apps notation encoded the semantic use of the variable rather than it's logical use or purpose. A variable for storing private information had a prefix (like mUserId ) whereas a variable for storing public information had another prefix, or none whatsoever. So when you see the usage of a field, which piece of information is more important for the review: the visibility of that field or the type of that field? The visibility is a useless attribute to care about in a code review. The field is already present and available for use, and presumably its visibility was code-reviewed in a previous change. The type of a field, however, has a direct impact on how that field can being used in the change. The correct methods to call, the position in arguments, and the methods which can be called all are directly related to its type. Not only is advocating for 'apps' Hungarian wrong because it's not useful, but it's doubly wrong since 'system' Hungarian would provide more relevant info. That's not to say you should use 'system', both the type and visibility of a field changes and you will forget to update the name. It's not hard to find static mContext fields , after all. "" The extra information helps in development "" Android Studio and IntelliJ IDEA visually distinguish field names based on membership (instance or static): IDEs will enforce correct membership, visibility, and types by default so a naming convention isn't going to add anything here. A popup showing all three properties (and more) of a field is also just a keypress away. "" I want to write Java code like Google does "" While Android and AOSP are part of the company, Google explicitly and actively forbids Hungarian notation in their Java style guide . This public Java style guideline is the formalization of long-standing internal conventions. Android had originated outside of Google and the team early on chose to host the Hungarian disease. Changing it at this point would be needless churn and cause many conflicts across branches and third-party partners. With your continued support and activism on this topic, this disease can be eradicated in our lifetime. mFriends don't let sFriends use Hungarian notation! - Jake Wharton",en,59
250,1936,1469819995,CONTENT SHARED,6519443272707698315,8766802480854827422,-8534301860414756085,,,,HTML,https://www.linkedin.com/pulse/two-different-sales-motions-geoffrey-moore,two different sales motions . . . .,"I have been spending a fair amount of time recently with sales organizations selling SaaS applications to enterprises of all sizes around the globe, and the most successful ones are bumping up against a familiar challenge in sales management. To put it simply, what looks like one sales cycle that can be managed by one end-to-end pipeline model is diverging into two separate ones. Here's a snapshot of how it is playing out: Sales motion 1 optimizes around deals less than $500K, often significantly so. They are normally funded out of an existing IT budget, typically where a SaaS application is directly displacing a legacy on-premise predecessor. This is frequently the ""land"" move in a land-and-expand sales strategy, and the key is to find the openings, qualify them clearly and crisply, and then win the deals. These deals are typically quite competitive, both with the legacy vendor who is about to get displaced as well as with other SaaS vendors pursuing the same opportunity. Deal size is measured in terms of ACV, the annual contract value for the first year. Lead generation focuses primarily on top-of-the-funnel pipeline development based on connecting on-line or in person at an event, the goal being to schedule an on-site meeting with a key influencer. Middle of the funnel progress is a function of strong product and solution marketing and good systems engineers who can help navigate the specifics of the prospective customer's IT environment. Bottom of the funnel is all about closing, and the recurrent winners tend to be the 